<doc id="7921" url="https://en.wikipedia.org/wiki?curid=7921" title="Derivative">
Derivative

The derivative of a function of a real variable measures the sensitivity to change of the function value (output value) with respect to a change in its argument (input value). Derivatives are a fundamental tool of calculus. For example, the derivative of the position of a moving object with respect to time is the object's velocity: this measures how quickly the position of the object changes when time advances.

The derivative of a function of a single variable at a chosen input value, when it exists, is the slope of the tangent line to the graph of the function at that point. The tangent line is the best linear approximation of the function near that input value. For this reason, the derivative is often described as the "instantaneous rate of change", the ratio of the instantaneous change in the dependent variable to that of the independent variable.

Derivatives may be generalized to functions of several real variables. In this generalization, the derivative is reinterpreted as a linear transformation whose graph is (after an appropriate translation) the best linear approximation to the graph of the original function. The Jacobian matrix is the matrix that represents this linear transformation with respect to the basis given by the choice of independent and dependent variables. It can be calculated in terms of the partial derivatives with respect to the independent variables. For a real-valued function of several variables, the Jacobian matrix reduces to the gradient vector.

The process of finding a derivative is called differentiation. The reverse process is called "antidifferentiation". The fundamental theorem of calculus states that antidifferentiation is the same as integration. Differentiation and integration constitute the two fundamental operations in single-variable calculus.

"Differentiation" is the action of computing a derivative. The derivative of a function of a variable is a measure of the rate at which the value of the function changes with respect to the change of the variable . It is called the "derivative" of with respect to . If and are real numbers, and if the graph of is plotted against , the derivative is the slope of this graph at each point.
The simplest case, apart from the trivial case of a constant function, is when is a linear function of , meaning that the graph of is a line. In this case, , for real numbers and , and the slope is given by
where the symbol (Delta) is an abbreviation for "change in". This formula is true because
Thus, since
it follows that

This gives an exact value for the slope of a line.
If the function is not linear (i.e. its graph is not a straight line), however, then the change in divided by the change in varies: differentiation is a method to find an exact value for this rate of change at any given value of .

The idea, illustrated by Figures 1 to 3, is to compute the rate of change as the limit value of the ratio of the differences as becomes infinitely small.

Two distinct notations are commonly used for the derivative, one deriving from Leibniz and the other from Joseph Louis Lagrange.

In Leibniz's notation, an infinitesimal change in is denoted by , and the derivative of with respect to is written
suggesting the ratio of two infinitesimal quantities. (The above expression is read as "the derivative of "y" with respect to "x"", ""dy" by "dx"", or ""dy" over "dx"". The oral form ""dy" "dx"" is often used conversationally, although it may lead to confusion.)

In Lagrange's notation, the derivative with respect to of a function is denoted (read as ""f" prime of "x"") or (read as ""f" prime "x" of "x""), in case of ambiguity of the variable implied by the derivation. Lagrange's notation is sometimes incorrectly attributed to Newton.

The most common approach to turn this intuitive idea into a precise definition is to define the derivative as a limit of difference quotients of real numbers. This is the approach described below.

Let be a real valued function defined in an open neighborhood of a real number . In classical geometry, the tangent line to the graph of the function at was the unique line through the point that did "not" meet the graph of transversally, meaning that the line did not pass straight through the graph. The derivative of with respect to at is, geometrically, the slope of the tangent line to the graph of at . The slope of the tangent line is very close to the slope of the line through and a nearby point on the graph, for example . These lines are called secant lines. A value of close to zero gives a good approximation to the slope of the tangent line, and smaller values (in absolute value) of will, in general, give better approximations. The slope of the secant line is the difference between the values of these points divided by the difference between the values, that is, 

This expression is Newton's difference quotient. Passing from an approximation to an exact answer is done using a limit. Geometrically, the limit of the secant lines is the tangent line. Therefore, the limit of the difference quotient as approaches zero, if it exists, should represent the slope of the tangent line to . This limit is defined to be the derivative of the function at :

When the limit exists, is said to be "differentiable" at . Here is one of several common notations for the derivative (see below).

Equivalently, the derivative satisfies the property that
which has the intuitive interpretation (see Figure 1) that the tangent line to at gives the "best linear approximation"
to near (i.e., for small ). This interpretation is the easiest to generalize to other settings (see below).

Substituting 0 for in the difference quotient causes division by zero, so the slope of the tangent line cannot be found directly using this method. Instead, define to be the difference quotient as a function of :

In practice, the existence of a continuous extension of the difference quotient to is shown by modifying the numerator to cancel in the denominator. Such manipulations can make the limit value of for small clear even though is still not defined at . This process can be long and tedious for complicated functions, and many shortcuts are commonly used to simplify the process.

Relative to a hyperreal extension of the real numbers, the derivative of a real function at a real point can be defined as the shadow of the quotient for infinitesimal , where . Here the natural extension of to the hyperreals is still denoted . Here the derivative is said to exist if the shadow is independent of the infinitesimal chosen.

The squaring function given by is differentiable at , and its derivative there is 6. This result is established by calculating the limit as approaches zero of the difference quotient of :

The last expression shows that the difference quotient equals when and is undefined when , because of the definition of the difference quotient. However, the definition of the limit says the difference quotient does not need to be defined when . The limit is the result of letting go to zero, meaning it is the value that tends to as becomes very small:

Hence the slope of the graph of the squaring function at the point is , and so its derivative at is .

More generally, a similar computation shows that the derivative of the squaring function at is :

If is differentiable at , then must also be continuous at . As an example, choose a point and let be the step function that returns the value 1 for all less than , and returns a different value 10 for all greater than or equal to . cannot have a derivative at . If is negative, then is on the low part of the step, so the secant line from to is very steep, and as tends to zero the slope tends to infinity. If is positive, then is on the high part of the step, so the secant line from to has slope zero. Consequently, the secant lines do not approach any single slope, so the limit of the difference quotient does not exist.

However, even if a function is continuous at a point, it may not be differentiable there. For example, the absolute value function given by is continuous at , but it is not differentiable there. If is positive, then the slope of the secant line from 0 to is one, whereas if is negative, then the slope of the secant line from 0 to is negative one. This can be seen graphically as a "kink" or a "cusp" in the graph at . Even a function with a smooth graph is not differentiable at a point where its tangent is vertical: For instance, the function given by is not differentiable at .

In summary: for a function to have a derivative it is "necessary" for the function to be continuous, but continuity alone is not "sufficient".

Most functions that occur in practice have derivatives at all points or at almost every point. Early in the history of calculus, many mathematicians assumed that a continuous function was differentiable at most points. Under mild conditions, for example if the function is a monotone function or a Lipschitz function, this is true. However, in 1872 Weierstrass found the first example of a function that is continuous everywhere but differentiable nowhere. This example is now known as the Weierstrass function. In 1931, Stefan Banach proved that the set of functions that have a derivative at some point is a meager set in the space of all continuous functions. Informally, this means that hardly do any random continuous functions have a derivative at even one point.

Let be a function that has a derivative at every point in its domain. We can then define a function that maps every point formula_14 to the value of the derivative of formula_15 at formula_14. This function is written and is called the "derivative function" or the "derivative of" .

Sometimes has a derivative at most, but not all, points of its domain. The function whose value at equals whenever is defined and elsewhere is undefined is also called the derivative of . It is still a function, but its domain is strictly smaller than the domain of .

Using this idea, differentiation becomes a function of functions: The derivative is an operator whose domain is the set of all functions that have derivatives at every point of their domain and whose range is a set of functions. If we denote this operator by , then is the function . Since is a function, it can be evaluated at a point . By the definition of the derivative function, .

For comparison, consider the doubling function given by ; is a real-valued function of a real number, meaning that it takes numbers as inputs and has numbers as outputs:
The operator , however, is not defined on individual numbers. It is only defined on functions:
Because the output of is a function, the output of can be evaluated at a point. For instance, when is applied to the squaring function, , outputs the doubling function , which we named . This output function can then be evaluated to get , , and so on.

Let be a differentiable function, and let be its derivative. The derivative of (if it has one) is written and is called the "second derivative of ". Similarly, the derivative of the second derivative, if it exists, is written and is called the "third derivative of ". Continuing this process, one can define, if it exists, the th derivative as the derivative of the th derivative. These repeated derivatives are called "higher-order derivatives". The th derivative is also called the derivative of order .

If represents the position of an object at time , then the higher-order derivatives of have physical interpretations. The second derivative of is the derivative of , the velocity, and by definition this is the object's acceleration. The third derivative of is defined to be the jerk, and the fourth derivative is defined to be the jounce.

A function need not have a derivative (for example, if it is not continuous). Similarly, even if does have a derivative, it may not have a second derivative. For example, let
Calculation shows that is a differentiable function whose derivative at formula_14 is given by

On the real line, every polynomial function is infinitely differentiable. By standard differentiation rules, if a polynomial of degree is differentiated times, then it becomes a constant function. All of its subsequent derivatives are identically zero. In particular, they exist, so polynomials are smooth functions.

The derivatives of a function at a point provide polynomial approximations to that function near . For example, if is twice differentiable, then
in the sense that
If is infinitely differentiable, then this is the beginning of the Taylor series for evaluated at around .

A point where the second derivative of a function changes sign is called an "inflection point". At an inflection point, the second derivative may be zero, as in the case of the inflection point of the function given by formula_25, or it may fail to exist, as in the case of the inflection point of the function given by formula_26. At an inflection point, a function switches from being a convex function to being a concave function or vice versa.

The symbols formula_27, formula_28, and formula_29 were introduced by Gottfried Wilhelm Leibniz in 1675. It is still commonly used when the equation is viewed as a functional relationship between dependent and independent variables. Then the first derivative is denoted by

and was once thought of as an infinitesimal quotient. Higher derivatives are expressed using the notation

for the "n"th derivative of formula_32. These are abbreviations for multiple applications of the derivative operator. For example,

With Leibniz's notation, we can write the derivative of formula_34 at the point formula_35 in two different ways:

Leibniz's notation allows one to specify the variable for differentiation (in the denominator), which is relevant in partial differentiation. It also makes the chain rule easier to remember:

Sometimes referred to as "prime notation", one of the most common modern notation for differentiation is due to Joseph-Louis Lagrange and uses the prime mark, so that the derivative of a function formula_15 is denoted formula_39. Similarly, the second and third derivatives are denoted
To denote the number of derivatives beyond this point, some authors use Roman numerals in superscript, whereas others place the number in parentheses:
The latter notation generalizes to yield the notation formula_44 for the "n"th derivative of formula_15 – this notation is most useful when we wish to talk about the derivative as being a function itself, as in this case the Leibniz notation can become cumbersome.

Newton's notation for differentiation, also called the dot notation, places a dot over the function name to represent a time derivative. If formula_46, then
denote, respectively, the first and second derivatives of formula_34. This notation is used exclusively for derivatives with respect to time or arc length. It is very common in physics, differential equations, and differential geometry. While the notation becomes unmanageable for high-order derivatives, in practice only few derivatives are needed.

Euler's notation uses a differential operator formula_50, which is applied to a function formula_15 to give the first derivative formula_52. The "n"th derivative is denoted formula_53.

If is a dependent variable, then often the subscript "x" is attached to the "D" to clarify the independent variable "x".
Euler's notation is then written
although this subscript is often omitted when the variable "x" is understood, for instance when this is the only variable present in the expression.

Euler's notation is useful for stating and solving linear differential equations.

The derivative of a function can, in principle, be computed from the definition by considering the difference quotient, and computing its limit. In practice, once the derivatives of a few simple functions are known, the derivatives of other functions are more easily computed using "rules" for obtaining derivatives of more complicated functions from simpler ones.

Most derivative computations eventually require taking the derivative of some common functions. The following incomplete list gives some of the most frequently used functions of a single real variable and their derivatives.


where "r" is any real number, then

wherever this is defined. For example, if formula_58, then

and the derivative function is defined only for positive "x", not for . When , this rule implies that "f"′("x") is zero for , which is almost the constant rule (stated below).


In many cases, complicated limit calculations by direct application of Newton's difference quotient can be avoided using differentiation rules. Some of the most basic rules are the following.


The derivative of the function given by

is

Here the second term was computed using the chain rule and third using the product rule. The known derivatives of the elementary functions "x", "x", sin("x"), ln("x") and , as well as the constant 7, were also used.

A vector-valued function y of a real variable sends real numbers to vectors in some vector space R. A vector-valued function can be split up into its coordinate functions "y"("t"), "y"("t"), …, "y"("t"), meaning that . This includes, for example, parametric curves in R or R. The coordinate functions are real valued functions, so the above definition of derivative applies to them. The derivative of y("t") is defined to be the vector, called the tangent vector, whose coordinates are the derivatives of the coordinate functions. That is,

Equivalently,

if the limit exists. The subtraction in the numerator is the subtraction of vectors, not scalars. If the derivative of y exists for every value of "t", then y′ is another vector-valued function.

If e, …, e is the standard basis for R, then y("t") can also be written as . If we assume that the derivative of a vector-valued function retains the linearity property, then the derivative of y("t") must be
because each of the basis vectors is a constant.

This generalization is useful, for example, if y("t") is the position vector of a particle at time "t"; then the derivative y′("t") is the velocity vector of the particle at time "t".

Suppose that "f" is a function that depends on more than one variable—for instance,
"f" can be reinterpreted as a family of functions of one variable indexed by the other variables:
In other words, every value of "x" chooses a function, denoted "f", which is a function of one real number. That is,
Once a value of "x" is chosen, say "a", then determines a function "f" that sends "y" to :
In this expression, "a" is a "constant", not a "variable", so "f" is a function of only one real variable. Consequently, the definition of the derivative for a function of one variable applies:
The above procedure can be performed for any choice of "a". Assembling the derivatives together into a function gives a function that describes the variation of "f" in the "y" direction:
This is the partial derivative of "f" with respect to "y". Here ∂ is a rounded "d" called the partial derivative symbol. To distinguish it from the letter "d", ∂ is sometimes pronounced "der", "del", or "partial" instead of "dee".

In general, the partial derivative of a function in the direction "x" at the point ("a", …, "a") is defined to be:
In the above difference quotient, all the variables except "x" are held fixed. That choice of fixed values determines a function of one variable
and, by definition,
In other words, the different choices of "a" index a family of one-variable functions just as in the example above. This expression also shows that the computation of partial derivatives reduces to the computation of one-variable derivatives.

An important example of a function of several variables is the case of a scalar-valued function on a domain in Euclidean space R (e.g., on R or R). In this case "f" has a partial derivative ∂"f"/∂"x" with respect to each variable "x". At the point ("a", …, "a"), these partial derivatives define the vector
This vector is called the gradient of "f" at "a". If "f" is differentiable at every point in some domain, then the gradient is a vector-valued function ∇"f" that takes the point ("a", …, "a") to the vector ∇"f"("a", …, "a"). Consequently, the gradient determines a vector field.

If "f" is a real-valued function on R, then the partial derivatives of "f" measure its variation in the direction of the coordinate axes. For example, if "f" is a function of "x" and "y", then its partial derivatives measure the variation in "f" in the "x" direction and the "y" direction. They do not, however, directly measure the variation of "f" in any other direction, such as along the diagonal line . These are measured using directional derivatives. Choose a vector
The directional derivative of "f" in the direction of v at the point x is the limit
In some cases it may be easier to compute or estimate the directional derivative after changing the length of the vector. Often this is done to turn the problem into the computation of a directional derivative in the direction of a unit vector. To see how this works, suppose that . Substitute into the difference quotient. The difference quotient becomes:
This is "λ" times the difference quotient for the directional derivative of "f" with respect to u. Furthermore, taking the limit as "h" tends to zero is the same as taking the limit as "k" tends to zero because "h" and "k" are multiples of each other. Therefore, . Because of this rescaling property, directional derivatives are frequently considered only for unit vectors.

If all the partial derivatives of "f" exist and are continuous at x, then they determine the directional derivative of "f" in the direction v by the formula:
This is a consequence of the definition of the total derivative. It follows that the directional derivative is linear in v, meaning that .

The same definition also works when "f" is a function with values in R. The above definition is applied to each component of the vectors. In this case, the directional derivative is a vector in R.

When "f" is a function from an open subset of R to R, then the directional derivative of "f" in a chosen direction is the best linear approximation to "f" at that point and in that direction. But when , no single directional derivative can give a complete picture of the behavior of "f". The total derivative gives a complete picture by considering all directions at once. That is, for any vector v starting at a, the linear approximation formula holds:
Just like the single-variable derivative, is chosen so that the error in this approximation is as small as possible.

If "n" and "m" are both one, then the derivative is a number and the expression is the product of two numbers. But in higher dimensions, it is impossible for to be a number. If it were a number, then would be a vector in R while the other terms would be vectors in R, and therefore the formula would not make sense. For the linear approximation formula to make sense, must be a function that sends vectors in R to vectors in R, and must denote this function evaluated at v.

To determine what kind of function it is, notice that the linear approximation formula can be rewritten as
Notice that if we choose another vector w, then this approximate equation determines another approximate equation by substituting w for v. It determines a third approximate equation by substituting both w for v and for a. By subtracting these two new equations, we get
If we assume that v is small and that the derivative varies continuously in a, then is approximately equal to , and therefore the right-hand side is approximately zero. The left-hand side can be rewritten in a different way using the linear approximation formula with substituted for v. The linear approximation formula implies:
This suggests that is a linear transformation from the vector space R to the vector space R. In fact, it is possible to make this a precise derivation by measuring the error in the approximations. Assume that the error in these linear approximation formula is bounded by a constant times ||v||, where the constant is independent of v but depends continuously on a. Then, after adding an appropriate error term, all of the above approximate equalities can be rephrased as inequalities. In particular, is a linear transformation up to a small error term. In the limit as v and w tend to zero, it must therefore be a linear transformation. Since we define the total derivative by taking a limit as v goes to zero, must be a linear transformation.

In one variable, the fact that the derivative is the best linear approximation is expressed by the fact that it is the limit of difference quotients. However, the usual difference quotient does not make sense in higher dimensions because it is not usually possible to divide vectors. In particular, the numerator and denominator of the difference quotient are not even in the same vector space: The numerator lies in the codomain R while the denominator lies in the domain R. Furthermore, the derivative is a linear transformation, a different type of object from both the numerator and denominator. To make precise the idea that is the best linear approximation, it is necessary to adapt a different formula for the one-variable derivative in which these problems disappear. If , then the usual definition of the derivative may be manipulated to show that the derivative of "f" at "a" is the unique number such that
This is equivalent to
because the limit of a function tends to zero if and only if the limit of the absolute value of the function tends to zero. This last formula can be adapted to the many-variable situation by replacing the absolute values with norms.

The definition of the total derivative of "f" at a, therefore, is that it is the unique linear transformation such that
Here h is a vector in R, so the norm in the denominator is the standard length on R. However, "f"′(a)h is a vector in R, and the norm in the numerator is the standard length on R. If "v" is a vector starting at "a", then is called the pushforward of v by "f" and is sometimes written .

If the total derivative exists at a, then all the partial derivatives and directional derivatives of "f" exist at a, and for all v, is the directional derivative of "f" in the direction v. If we write "f" using coordinate functions, so that , then the total derivative can be expressed using the partial derivatives as a matrix. This matrix is called the Jacobian matrix of "f" at a:

The existence of the total derivative "f"′(a) is strictly stronger than the existence of all the partial derivatives, but if the partial derivatives exist and are continuous, then the total derivative exists, is given by the Jacobian, and depends continuously on a.

The definition of the total derivative subsumes the definition of the derivative in one variable. That is, if "f" is a real-valued function of a real variable, then the total derivative exists if and only if the usual derivative exists. The Jacobian matrix reduces to a 1×1 matrix whose only entry is the derivative "f"′("x"). This 1×1 matrix satisfies the property that is approximately zero, in other words that

Up to changing variables, this is the statement that the function formula_110 is the best linear approximation to "f" at "a".

The total derivative of a function does not give another function in the same way as the one-variable case. This is because the total derivative of a multivariable function has to record much more information than the derivative of a single-variable function. Instead, the total derivative gives a function from the tangent bundle of the source to the tangent bundle of the target.

The natural analog of second, third, and higher-order total derivatives is not a linear transformation, is not a function on the tangent bundle, and is not built by repeatedly taking the total derivative. The analog of a higher-order derivative, called a jet, cannot be a linear transformation because higher-order derivatives reflect subtle geometric information, such as concavity, which cannot be described in terms of linear data such as vectors. It cannot be a function on the tangent bundle because the tangent bundle only has room for the base space and the directional derivatives. Because jets capture higher-order information, they take as arguments additional coordinates representing higher-order changes in direction. The space determined by these additional coordinates is called the jet bundle. The relation between the total derivative and the partial derivatives of a function is paralleled in the relation between the "k"th order jet of a function and its partial derivatives of order less than or equal to "k".

By repeatedly taking the total derivative, one obtains higher versions of the Fréchet derivative, specialized to R. The "k"th order total derivative may be interpreted as a map
which takes a point x in R and assigns to it an element of the space of "k"-linear maps from R to R – the "best" (in a certain precise sense) "k"-linear approximation to "f" at that point. By precomposing it with the diagonal map Δ, , a generalized Taylor series may be begun as
where f(a) is identified with a constant function, are the components of the vector , and and are the components of and as linear transformations.

The concept of a derivative can be extended to many other settings. The common thread is that the derivative of a function at a point serves as a linear approximation of the function at that point.

Calculus, known in its early history as "infinitesimal calculus", is a mathematical discipline focused on limits, functions, derivatives, integrals, and infinite series. Isaac Newton and Gottfried Leibniz independently discovered calculus in the mid-17th century. However, each inventor claimed the other stole his work in a bitter dispute that continued until the end of their lives.






</doc>
<doc id="7922" url="https://en.wikipedia.org/wiki?curid=7922" title="Dravidian languages">
Dravidian languages

The Dravidian languages are a language family spoken mainly in southern India and parts of eastern and central India, as well as in Sri Lanka with small pockets in southwestern Pakistan, southern Afghanistan, Nepal, Bangladesh and Bhutan, and overseas in other countries such as Malaysia, Indonesia and Singapore. The Dravidian languages with the most speakers are Telugu, Tamil, Kannada and Malayalam. There are also small groups of Dravidian-speaking scheduled tribes, who live outside Dravidian-speaking areas, such as the Kurukh in Eastern India and Gondi in Central India. The Dravidian languages are spoken by more than 215 million people in India, Pakistan, and Sri Lanka.

Though some scholars have argued that the Dravidian languages may have been brought to India by migrations in the fourth or third millennium BCE or even earlier, the Dravidian languages cannot easily be connected to any other language family, and they could well be indigenous to India.

Epigraphically the Dravidian languages have been attested since the 2nd century BCE as Tamil-Brahmi script on the cave walls discovered in the Madurai and Tirunelveli districts of Tamil Nadu. Only two Dravidian languages are spoken exclusively outside the post-1947 state of India: Brahui in the Balochistan region of Pakistan and Afghanistan; and Dhangar, a dialect of Kurukh, in parts of Nepal and Bhutan. Dravidian place names along the Arabian Sea coasts and Dravidian grammatical influence such as clusivity in the Indo-Aryan languages, namely Marathi, Konkani, Gujarati, Marwari, and Sindhi, suggest that Dravidian languages were once spoken more widely across the Indian subcontinent.

The 14th century Sanskrit text "Lilatilakam", which is a grammar of Manipravalam, states that the spoken languages of present-day Kerala and Tamil Nadu were similar, terming them as "Dramiḍa". The author doesn't consider the "Karṇṇāṭa" (Kannada) and the "Andhra" (Telugu) languages as "Dramiḍa", because they were very different from the language of the "Tamil Veda" ("Tiruvaymoli"), but states that some people would include them in the "Dramiḍa" category.

In 1816, Alexander D. Campbell suggested the existence of a Dravidian language family in his "Grammar of the Teloogoo Language", in which he and Francis W. Ellis argued that Tamil and Telugu descended from a common, non-Indo-European ancestor. In 1856 Robert Caldwell published his "Comparative Grammar of the Dravidian or South-Indian Family of Languages", which considerably expanded the Dravidian umbrella and established Dravidian as one of the major language groups of the world. Caldwell coined the term "Dravidian" for this family of languages, based on the usage of the Sanskrit word द्रविदा (Dravidā) in the work "Tantravārttika" by . In his own words, Caldwell says,

As for the origin of the Sanskrit word ' itself, researchers have proposed various theories. Basically the theories deal with the direction of derivation between ' and '. There is no definite philological and linguistic basis for asserting unilaterally that the name "Dravida" also forms the origin of the word "Tamil" (Dravida → Dramila → Tamizha or Tamil). Kamil Zvelebil cites the forms such as "dramila" (in 's Sanskrit work "Avanisundarīkathā") ' (found in the Sri Lankan (Ceylonese) chronicle "Mahavamsa") and then goes on to say, "The forms "damiḷa"/"damila" almost certainly provide a connection of ' " and "... ' < ' ...whereby the further development might have been *' > *' > '- / "damila"- and further, with the intrusive, 'hypercorrect' (or perhaps analogical) -"r"-, into "". The -"m"-/-"v"- alternation is a common enough phenomenon in Dravidian phonology"
Zvelebil in his earlier treatise states, "It is obvious that the Sanskrit ', Pali "damila", ' and Prakrit ' are all etymologically connected with '", and further remarks, "The "r" in ' → ' is a hypercorrect insertion, cf. an analogical case of DED 1033 Ta. "kamuku", Tu. "kangu" "areca nut": Skt. "kramu(ka)"."

Furthermore, another Dravidianist and linguist, Bhadriraju Krishnamurti, in his book "Dravidian Languages" states: 

Based on what Krishnamurti states (referring to a scholarly paper published in the "International Journal of Dravidian Linguistics"), the Sanskrit word ' itself is later than ' since the dates for the forms with -r- are centuries later than the dates for the forms without -r- (', '-, "damela"- etc.). The "Monier-Williams Sanskrit Dictionary" lists for the Sanskrit word "draviḍa" a meaning of "collective Name for 5 peoples, viz. the Āndhras, Karṇāṭakas, Gurjaras, Tailaṅgas, and Mahārāṣṭras".

The Dravidian languages form a close-knit family. Most scholars agree on four groups: South (or South DravidianI), South-Central (or South DravidianII), Central, and North Dravidian, but there are different proposals regarding the relationship between these groups. Earlier classifications grouped Central and South-Central Dravidian in a single branch. Krishnamurti groups South-Central and South Dravidian. Languages recognized as official languages of India appear here in boldface.

Some authors deny that North Dravidian forms a valid subgroup, splitting it into Northeast (Kurukh–Malto) and Northwest (Brahui). Their affiliation has been proposed based primarily on a small number of common phonetic developments, including:
McAlpin (2003) notes that no exact conditioning can be established for the first two changes, and proposes that distinct Proto-Dravidian *q and *kʲ should be reconstructed behind these correspondences, and that Brahui, Kurukh-Malto, and the rest of Dravidian may be three coordinate branches, possibly with Brahui being the earliest language to split off. A few morphological parallels between Brahui and Kurukh-Malto are also known, but according to McAlpin they are analyzable as shared archaisms rather than shared innovations.

In addition, "Ethnologue" lists several unclassified Dravidian languages: Allar, Bazigar, Bharia, Malankuravan (possibly a dialect of Malayalam), and Vishavan. "Ethnologue" also lists several unclassified Southern Dravidian languages: Mala Malasar, Malasar, Thachanadan, Ullatan, Kalanadi, Kumbaran, Kunduvadi, Kurichiya, Attapady Kurumba, Muduga, Pathiya, and Wayanad Chetti.

A computational phylogenetic study of the Dravidian language family was undertaken by Kolipakam, et al. (2018). Kolipakam, et al. (2018) supports the internal coherence of the four Dravidian branches South (or South Dravidian I), South-Central (or South Dravidian II), Central, and North, but is uncertain about the precise relationships of these four branches to each other. The date of Dravidian is estimated to be 4,500 years old.

Since 1981, the Census of India has reported only languages with more than 10,000 speakers, including 17 Dravidian languages. In 1981, these accounted for approximately 24% of India's population.
In the 2001 census, they included 214 million people, about 21% of India's total population of 1.02 billion. In addition, the largest Dravidian-speaking group outside India, Tamil speakers in Sri Lanka, number around 4.7 million. The total number of speakers of Dravidian languages is around 227 million people, around 13% of the population of the Indian subcontinent.

Telugu is the most spoken Dravidian language, with over 74 million native speakers. The total number of speakers of Telugu, including those whose first language is not Telugu, is around 84 million people, which is around 6% of India's total population.

The smallest branch of the Dravidian languages is the Central branch, which has only around 200,000 speakers. These languages are mostly tribal, and spoken in central India.

The second-smallest branch is the Northern branch, with around 6.3 million speakers. This is the only sub-group to have a language spoken in Pakistan — Brahui.

The next-largest is the South-Central branch, which has 78 million native speakers, the vast majority of whom speak Telugu. This branch also includes the tribal language Gondi spoken in central India.

The largest group is South Dravidian, with almost 150 million speakers. Tamil, Malayalam, and Kannada make up around 98% of the speakers, with Tamil being by far the most spoken language, with almost half of all South Dravidian speakers speaking it.

The Dravidian family has defied all of the attempts to show a connection with other languages, including Indo-European, Hurrian, Basque, Sumerian, Korean and Japanese. Comparisons have been made not just with the other language families of the Indian subcontinent (Indo-European, Austroasiatic, Sino-Tibetan, and Nihali), but with all typologically similar language families of the Old World. Nonetheless, although there are no readily detectable genealogical connections, Dravidian shares strong areal features with the Indo-Aryan languages, which have been attributed to a substratum influence from Dravidian.

Dravidian languages display typological similarities with the Uralic language group, suggesting to some a prolonged period of contact in the past. This idea is popular amongst Dravidian linguists and has been supported by a number of scholars, including Robert Caldwell, Thomas Burrow, Kamil Zvelebil, and Mikhail Andronov. This hypothesis has, however, been rejected by some specialists in Uralic languages, and has in recent times also been criticised by other Dravidian linguists such as Bhadriraju Krishnamurti.

In the early 1970s, the linguist David McAlpin produced a detailed proposal of a genetic relationship between Dravidian and the extinct Elamite language of ancient Elam (present-day southwestern Iran). The Elamo-Dravidian hypothesis was supported in the late 1980s by the archaeologist Colin Renfrew and the geneticist Luigi Luca Cavalli-Sforza, who suggested that Proto-Dravidian was brought to India by farmers from the Iranian part of the Fertile Crescent. (In his 2000 book, Cavalli-Sforza suggested western India, northern India and northern Iran as alternative starting points.) However, linguists have found McAlpin's cognates unconvincing and criticized his proposed phonological rules as "ad hoc". Elamite is generally believed by scholars to be a language isolate, and the theory has had no effect on studies of the language.

Dravidian is one of the primary language families in the Nostratic proposal, which would link most languages in North Africa, Europe and Western Asia into a family with its origins in the Fertile Crescent sometime between the last Ice Age and the emergence of Proto-Indo-European 4,000–6,000 BCE. However, the general consensus is that such deep connections are not, or not yet, demonstrable.

The origins of the Dravidian languages, as well as their subsequent development and the period of their differentiation are unclear, partially due to the lack of comparative linguistic research into the Dravidian languages. Though some scholars have argued that the Dravidian languages may have been brought to India by migrations in the fourth or third millennium BCE or even earlier, the Dravidian languages cannot easily be connected to any other language, and they could well be indigenous to India. Proto-Dravidian was spoken in the 4th or 3rd millennium BCE, and the Dravidian languages were the most widespread indigenous languages before the advance of the Indo-Aryan languages.

As a proto-language, the Proto-Dravidian language is not itself attested in the historical record. Its modern conception is based solely on reconstruction. It was suggested in the 1980s that the language was spoken in the 4th millennium BCE, and started disintegrating into various branches around 3rd millennium BCE. According to Krishnamurti, Proto-Dravidian may have been spoken in the Indus civilization, suggesting a "tentative date of Proto-Dravidian around the early part of the third millennium." Krishnamurti further states that South Dravidian I (including pre-Tamil) and South Dravidian II (including Pre-Telugu) split around the eleventh century BCE, with the other major branches splitting off at around the same time. Kolipakam et al. (2018) estimate the Dravidian language family to be approximately 4,500 years old.

The Indus Valley civilisation (3,300-1,900 BCE), located in Northwestern Indian subcontinent, is often understood to have been Dravidian. Cultural and linguistic similarities have been cited by researchers Henry Heras, Kamil Zvelebil, Asko Parpola and Iravatham Mahadevan as being strong evidence for a proto-Dravidian origin of the ancient Indus Valley civilisation. The discovery in Tamil Nadu of a late Neolithic (early 2nd millennium BCE, i.e. post-dating Harappan decline) stone celt allegedly marked with Indus signs has been considered by some to be significant for the Dravidian identification.

Yuri Knorozov surmised that the symbols represent a logosyllabic script and suggested, based on computer analysis, an underlying agglutinative Dravidian language as the most likely candidate for the underlying language. Knorozov's suggestion was preceded by the work of Henry Heras, who suggested several readings of signs based on a proto-Dravidian assumption.

Linguist Asko Parpola writes that the Indus script and Harappan language are "most likely to have belonged to the Dravidian family". Parpola led a Finnish team in investigating the inscriptions using computer analysis. Based on a proto-Dravidian assumption, they proposed readings of many signs, some agreeing with the suggested readings of Heras and Knorozov (such as equating the "fish" sign with the Dravidian word for fish, "min") but disagreeing on several other readings. A comprehensive description of Parpola's work until 1994 is given in his book "Deciphering the Indus Script".

Although in modern times speakers of the various Dravidian languages have mainly occupied the southern portion of India, in earlier times they probably were spoken in a larger area. After the Indo-Aryan migrations into north-western India, starting ca. 1500 BCE, and the establishment of the Kuru kingdom ca. 1100 BCE, a process of Sanskritisation started, which resulted in a language shift in northern India. Southern India has remained majority Dravidian, but pockets of Dravidian can be found in central India, Pakistan, Bangladesh and Nepal.

The Kurukh and Malto are pockets of Dravidian languages in central India, spoken by people who may have migrated from south India. They do have myths about external origins. The Kurukh have traditionally claimed to be from the Deccan Peninsula, more specifically Karnataka. The same tradition has existed of the Brahui, who call themselves immigrants. Holding this same view of the Brahui are many scholars such as L.H. Horace Perera and M.Ratnasabapathy.

The Brahui population of Pakistan's Balochistan province has been taken by some as the linguistic equivalent of a relict population, perhaps indicating that Dravidian languages were formerly much more widespread and were supplanted by the incoming Indo-Aryan languages. However, it has been argued that the absence of any Old Iranian (Avestan) loanwords in Brahui suggests that the Brahui migrated to Balochistan from central India less than 1,000 years ago. The main Iranian contributor to Brahui vocabulary, Balochi, is a western Iranian language like Kurdish, and arrived in the area from the west only around 1,000AD. Sound changes shared with Kurukh and Malto also suggest that Brahui was originally spoken near them in central India.

Dravidian languages show extensive lexical (vocabulary) borrowing, but only a few traits of structural (either phonological or grammatical) borrowing from Indo-Aryan, whereas Indo-Aryan shows more structural than lexical borrowings from the Dravidian languages. Many of these features are already present in the oldest known Indo-Aryan language, the language of the "Rigveda" (c.1500 BCE), which also includes over a dozen words borrowed from Dravidian.

Vedic Sanskrit has retroflex consonants (/, ) with about 88 words in the "Rigveda" having unconditioned retroflexes. Some sample words are ', ', ', ', ' and '.
Since other Indo-European languages, including other Indo-Iranian languages, lack retroflex consonants, their presence in Indo-Aryan is often cited as evidence of substrate influence from close contact of the Vedic speakers with speakers of a foreign language family rich in retroflex consonants. The Dravidian family is a serious candidate since it is rich in retroflex phonemes reconstructible back to the Proto-Dravidian stage.

In addition, a number of grammatical features of Vedic Sanskrit not found in its sister Avestan language appear to have been borrowed from Dravidian languages. These include the gerund, which has the same function as in Dravidian, and the quotative marker "iti". Some linguists explain this asymmetrical borrowing by arguing that Middle Indo-Aryan languages were built on a Dravidian substratum. These scholars argue that the most plausible explanation for the presence of Dravidian structural features in Indic is language shift, that is, native Dravidian speakers learning and adopting Indic languages. Although each of the innovative traits in Indic could be accounted for by internal explanations, early Dravidian influence is the only explanation that can account for all of the innovations at once; moreover, it accounts for several of the innovative traits in Indic better than any internal explanation that has been proposed.

The most characteristic grammatical features of Dravidian languages are:

Dravidian languages are noted for the lack of distinction between aspirated and unaspirated stops. While some Dravidian languages have accepted large numbers of loan words from Sanskrit and other Indo-Iranian languages in addition to their already vast vocabulary, in which the orthography shows distinctions in voice and aspiration, the words are pronounced in Dravidian according to different rules of phonology and phonotactics: aspiration of plosives is generally absent, regardless of the spelling of the word. This is not a universal phenomenon and is generally avoided in formal or careful speech, especially when reciting. For instance, Tamil does not distinguish between voiced and voiceless stops. In fact, the Tamil alphabet lacks symbols for voiced and aspirated stops. Dravidian languages are also characterized by a three-way distinction between dental, alveolar, and retroflex places of articulation as well as large numbers of liquids.

Proto-Dravidian had five short and long vowels: "*a", "*ā", "*i", "*ī", "*u", "*ū", "*e", "*ē", "*o", "*ō". There were no diphthongs; "ai" and "au" are treated as *"ay" and *"av" (or *"aw").
The five-vowel system is largely preserved in the descendent subgroups.

The following consonantal phonemes are reconstructed:

The numerals from 1 to 10 in various Dravidian and Indo-Aryan languages (here exemplified by Hindi, Sanskrit and Marathi).

Four Dravidian languages, Tamil, Kannada, Malayalam and Telugu, have lengthy literary traditions.
Literature in Tulu and Kodava is more recent.

The earliest known Dravidian inscriptions are 76 Old Tamil inscriptions on cave walls in Madurai and Tirunelveli districts in Tamil Nadu, dating from the 2nd century BCE.
These inscriptions are written in a variant of the Brahmi script called Tamil Brahmi.
The earliest long text in Old Tamil is the "Tolkāppiyam", an early work on Tamil grammar and poetics, whose oldest layers could date from the 1st century BCE.





</doc>
<doc id="7923" url="https://en.wikipedia.org/wiki?curid=7923" title="Dracula">
Dracula

Dracula is an 1897 Gothic horror novel by Irish author Bram Stoker. It introduced Count Dracula, and established many conventions of subsequent vampire fantasy. The novel tells the story of Dracula's attempt to move from Transylvania to England so that he may find new blood and spread the undead curse, and of the battle between Dracula and a small group of men and a woman led by Professor Abraham Van Helsing.

"Dracula" has been assigned to many literary genres including vampire literature, horror fiction, the gothic novel, and invasion literature. The novel has spawned numerous theatrical, film, and television interpretations.

The story is told in epistolary format, as a series of letters, diary entries, newspaper articles, and ships' log entries, whose narrators are the novel's protagonists, and occasionally supplemented with newspaper clippings relating events not directly witnessed. The events portrayed in the novel take place chronologically and largely in England and Transylvania during the 1890s and all transpire within the same year between 3 May and 6 November. A short note is located at the end of the final chapter written 7 years after the events outlined in the novel.

The tale begins with Jonathan Harker, a newly qualified English solicitor, visiting Count Dracula in the Carpathian Mountains on the border of Transylvania, Bukovina, and Moldavia, to provide legal support for a real estate transaction overseen by Harker's employer, Mr Peter Hawkins of Exeter. At first enticed by Dracula's gracious manners, Harker soon realizes that he is Dracula's prisoner. Wandering the Count's castle against Dracula's admonition, Harker encounters three female vampires, called "the sisters", from whom he is rescued by Dracula. Harker soon realizes that Dracula himself is also a vampire. After the preparations are made, Dracula leaves Transylvania and abandons Harker to the sisters. Harker barely escapes from the castle with his life.

Dracula boards a Russian ship, the "Demeter", taking along with him boxes of Transylvanian soil, which he required in order to regain his strength. Not long afterward, the ship having weighed anchor at Varna, runs aground on the shores of Whitby in the east coast of England. The captain's log narrates the gradual disappearance of the entire crew, until the captain alone remained, himself bound to the helm to maintain course. An animal resembling "a large dog" is seen leaping ashore. The ship's cargo is described as silver sand and 50 boxes of "mould", or earth, from Transylvania. It is later learned that Dracula successfully purchased multiple estates under the alias 'Count De Ville' throughout London and devised to distribute the 50 boxes to each of them utilizing transportation services as well as moving them himself. He does this to secure for himself "lairs" and the 50 boxes of earth would be used as his graves which would grant safety and rest during times of feeding and replenishing his strength.

Harker's fiancée, Mina Murray, is staying with her friend Lucy Westenra, who is holidaying in Whitby. Lucy receives three marriage proposals from Dr. John Seward, Quincey Morris, and Arthur Holmwood (the son of Lord Godalming who later obtains the title himself). Lucy accepts Holmwood's proposal while turning down Seward and Morris, but all remain friends. Dracula communicates with Seward's patient, Renfield, an insane man who wishes to consume insects, spiders, birds, and rats to absorb their "life force". Renfield is able to detect Dracula's presence and supplies clues accordingly.

Soon Dracula is indirectly shown to be stalking Lucy. As time passes she begins to suffer from episodes of sleepwalking and dementia, as witnessed by Mina. When Lucy begins to waste away suspiciously, Seward invites his old teacher, Abraham Van Helsing, who immediately determines the true cause of Lucy's condition. He refuses to disclose it but diagnoses her with acute blood-loss. Van Helsing prescribes numerous blood transfusions to which he, Seward, Quincey, and Arthur all contribute over time. Van Helsing also prescribes garlic flowers to be placed throughout her room and weaves a necklace of withered garlic blossoms for her to wear. However she continues to waste away – appearing to lose blood every night. While both doctors are absent, Lucy and her mother are attacked by a wolf and Mrs. Westenra, who has a heart condition, dies of fright. Van Helsing attempts to protect her with garlic but fate thwarts him each night, whether Lucy's mother removes the garlic from her room, or Lucy herself does so in her restless sleep. The doctors have found two small puncture marks about her neck, which Dr. Seward is at a loss to understand. After Lucy dies, Van Helsing places a golden crucifix over her mouth, ostensibly to delay or prevent Lucy's vampiric conversion. Fate conspires against him again when Van Helsing finds the crucifix in the possession of one of the servants who stole it off Lucy's corpse.

Following Lucy's death and burial, the newspapers report children being stalked in the night by a "bloofer lady" (i.e., "beautiful lady"). Van Helsing, knowing Lucy has become a vampire, confides in Seward, Lord Godalming, and Morris. The suitors and Van Helsing track her down and, after a confrontation with her, stake her heart, behead her, and fill her mouth with garlic. Around the same time, Jonathan Harker arrives from Budapest, where Mina marries him after his escape, and he and Mina join the campaign against Dracula.

The vampire hunters stay at Dr. Seward's residence, holding nightly meetings and providing reports based on each of their various tasks. Mina discovers that each of their journals and letters collectively contain clues to which they can track him down. She tasks herself with collecting them, researching newspaper clippings, fitting the most relevant entries into chronological order and typing out copies to distribute to each of the party which they are to study. Jonathan Harker tracks down the shipments of boxed graves and the estates which Dracula has purchased in order to store them. Van Helsing conducts research along with Dr. Seward to analyze the behaviour of their patient Renfield who they learn is directly influenced by Dracula. They also research historical events, folklore, and superstitions from various cultures to understand Dracula's powers and weaknesses. Van Helsing also establishes a criminal profile on Dracula in order to better understand his actions and predict his movements. Arthur Holmwood's fortune assists in funding the entire operation and expenses. As they discover the various properties Dracula had purchased, the male protagonists team up to raid each property and are several times confronted by Dracula. As they discover each of the boxed graves scattered throughout London, they pry them open to place and seal wafers of sacramental bread within. This act renders the boxes of earth completely useless to Dracula as he is unable to open, enter or further transport them.

After Dracula learns of the group's plot against him, he attacks Mina on three occasions, and feeds Mina his own blood to control her. This curses Mina with vampirism and changes her but does not completely turn her into a vampire. Van Helsing attempts to bless Mina through prayer and by placing a wafer of sacrament against her forehead, but it burns her upon contact leaving a wretched scar. Under this curse, Mina oscillates from consciousness to a semi-trance during which she perceives Dracula's surroundings and actions. Van Helsing is able to use hypnotism twice a day, at dawn and at sunset, to put her into this trance to further track Dracula's movements. Mina, afraid of Dracula's link with her, urges the team not to tell her their plans out of fear that Dracula will be listening. After the protagonists discover and sterilize 49 boxes found throughout his lairs in London, they learn that Dracula has fled with the missing 50th box back to his castle in Transylvania. They pursue him under the guidance of Mina. They split up into teams once they reach Europe; Van Helsing and Mina team up to locate the castle of Dracula while the others attempt to ambush the boat Dracula is using to reach his home. Van Helsing raids the castle and destroys the vampire "sisters". Upon discovering Dracula being transported by Gypsies, the three teams converge and attack the caravan carrying Dracula in the 50th box of Earth. After dispatching many Gypsies who were sworn to protect the Count, Harker shears Dracula through the throat with a kukri knife, while the mortally wounded Quincey stabs the Count in the heart with a Bowie knife. Dracula crumbles to dust, and Mina is freed from her curse of vampirism, as the scar on her forehead disappears. Soon after, Quincey dies from his wounds.

The book closes with a note left by Jonathan Harker seven years after the events of the novel, detailing his married life with Mina and the birth of their son, whom they name after all four members of the party, but address as "Quincey". Quincey is depicted sitting on the knee of Van Helsing as they recount their adventure. Seward and Arthur have each gotten married.

A small section was removed from a draft of the final chapter, in which Dracula's castle falls apart as he dies, hiding the fact that vampires were ever there.

Between 1879 and 1898, Stoker was a business manager for the Lyceum Theatre in London, where he supplemented his income by writing a large number of sensational novels, his most successful being the vampire tale "Dracula" published on 26 May 1897. Parts of it are set around the town of Whitby, where he spent summer holidays.

Throughout the 1880s and 1890s, authors such as H. Rider Haggard, Rudyard Kipling, Robert Louis Stevenson, Arthur Conan Doyle, and H. G. Wells wrote many tales in which fantastic creatures threatened the British Empire. Invasion literature was at a peak, and Stoker's formula was very familiar by 1897 to readers of fantastic adventure stories, of an invasion of England by continental European influences. Victorian readers enjoyed "Dracula" as a good adventure story like many others, but it did not reach its legendary status until later in the 20th century when film versions began to appear.

Before writing "Dracula", Stoker spent seven years researching European folklore and stories of vampires, being most influenced by Emily Gerard's 1885 essay "Transylvania Superstitions" which includes content about a vampire myth. Some historians are convinced that a historic figure, Vlad III Dracula, often called Vlad the Impaler, was the model for Stoker's Count although there is no supporting evidence. Stoker borrowed only "scraps of miscellaneous information", according to one expert, about this bloodthirsty tyrant of Wallachia and there are no comments about him in Stoker's working notes. Dracula scholar Elizabeth Miller has remarked that aside from the name and some mention of Romanian history, the background of Stoker's Count bears no resemblance to that of Vlad III Dracula.

Later he also claimed that he had a nightmare, caused by eating too much crab meat, about a "vampire king" rising from his grave.

Although a widely known vampire novel, "Dracula" was not the first. Johann Wolfgang von Goethe published "The Bride of Corinth" in 1797. (“From my grave to wander I am forc’d Still to seek The God’s long-sever’d link, Still to love the bridegroom I have lost, And the life-blood of his heart to drink;) Later Sheridan Le Fanu's 1871 "Carmilla", about a lesbian vampire could have inspired Bram Stoker's Dracula, or "Varney the Vampire", a lengthy penny dreadful serial from the mid-Victorian period by James Malcolm Rymer. John Polidori created the image of a vampire portrayed as an aristocratic man, like the character of Dracula, in his tale "The Vampyre" (1819). (He wrote "Vampyre" during a summer which he spent with "Frankenstein" creator Mary Shelley, her husband poet Percy Bysshe Shelley, and Lord Byron in 1816.)

The Lyceum Theatre where Stoker worked between 1878 and 1898 was headed by actor-manager Henry Irving, who was Stoker's real-life inspiration for Dracula's mannerisms and who Stoker hoped would play Dracula in a stage version. Irving never did agree to do a stage version, but Dracula's dramatic sweeping gestures and gentlemanly mannerisms drew their living embodiment from Irving.

"The Dead Un-Dead" was one of Stoker's original titles for "Dracula", and the manuscript was entitled simply "The Un-Dead" up until a few weeks before publication. Stoker's notes for "Dracula" show that the name of the count was originally "Count Wampyr", but Stoker became intrigued by the name "Dracula" while doing research, after reading William Wilkinson's book "An Account of the Principalities of Wallachia and Moldavia with Political Observations Relative to Them" (London 1820), which he found in the Whitby Library and consulted a number of times during visits to Whitby in the 1890s. The name Dracula was the patronym ("Drăculea") of the descendants of Vlad II of Wallachia, who took the name "Dracul" after being invested in the Order of the Dragon in 1431. In the Old Romanian language, the word "dracul" (Romanian "drac" "dragon" + "-ul" "the") meant "the dragon" and Dracula meant "son of the dragon". In the present day however, dracul means "the devil".

"Dracula" was published in London in May 1897 by Archibald Constable and Company. Costing six shillings, the novel was bound yellow cloth and titled in red letters. It was copyrighted in the United States in 1899 with the publication by Doubleday & McClure of New York. But when Universal Studios purchased the rights, it came to light that Bram Stoker had not complied with a portion of US copyright law, placing the novel into the public domain. In the United Kingdom and other countries following the Berne Convention on copyrights, the novel was under copyright until April 1962, fifty years after Stoker's death.

"Dracula" was not an immediate bestseller when it was first published, although reviewers were unstinting in their praise. The contemporary "Daily Mail" ranked Stoker's powers above those of Mary Shelley and Edgar Allan Poe, as well as Emily Brontë's "Wuthering Heights".

According to literary historians Nina Auerbach and David J. Skal in the Norton Critical Edition, the novel has become more significant for modern readers than it was for Victorian readers, most of whom enjoyed it just as a good adventure story. It reached its broad and iconic status only later in the 20th century when the movie versions appeared. A. Asbjørn Jøn has also noted that "Dracula" has had a significant impact on the image of the vampire in popular culture, folklore, and legend.

It did not make much money for Stoker. In the last year of his life, he was so poor that he had to petition for a compassionate grant from the Royal Literary Fund, and his widow was forced to sell his notes and outlines of the novel at a Sotheby's auction in 1913, where they were purchased for a little over £2. But then F. W. Murnau's unauthorized adaptation of the story was released in theatres in 1922 in the form of "Nosferatu". Stoker's widow took affront and, during the legal battle that followed, the novel's popularity started to grow.

"Nosferatu" was followed by a highly successful stage adaptation, touring the UK for three years before arriving in the US where Stoker's creation caught Hollywood's attention and, after the American 1931 movie version was released, the book has never been out of print.

However, some Victorian fans were ahead of the time, describing it as "the sensation of the season" and "the most blood-curdling novel of the paralysed century". Sherlock Holmes author Sir Arthur Conan Doyle wrote to Stoker in a letter, "I write to tell you how very much I have enjoyed reading "Dracula". I think it is the very best story of diablerie which I have read for many years." The "Daily Mail" review of 1 June 1897 proclaimed it a classic of Gothic horror, "In seeking a parallel to this weird, powerful, and horrorful story our mind reverts to such tales as "The Mysteries of Udolpho", "Frankenstein", "The Fall of the House of Usher" ... but Dracula is even more appalling in its gloomy fascination than any one of these."

Similarly good reviews appeared when the book was published in the U.S. in 1899. The first American edition was published by Doubleday & McClure in New York.

In the last several decades, literary and cultural scholars have offered diverse analyses of Stoker's novel and the character of Count Dracula. C.F. Bentley reads Dracula as an embodiment of the Freudian id. Carol A. Senf reads the novel as a response to the powerful New Woman, while Christopher Craft sees Dracula as embodying latent homosexuality and sees the text as an example of a 'characteristic, if hyperbolic instance of Victorian anxiety over the potential fluidity of gender roles'. Stephen D. Arata interprets the events of the novel as anxiety over colonialism and racial mixing, and Talia Schaffer construes the novel as an indictment of Oscar Wilde. Franco Moretti reads Dracula as a figure of monopoly capitalism, though Hollis Robbins suggests that Dracula's inability to participate in social conventions and to forge business partnerships undermines his power. Richard Noll reads "Dracula" within the context of 19th century alienism (psychiatry) and asylum medicine. D. Bruno Starrs understands the novel to be a pro-Catholic pamphlet promoting proselytization.

"Dracula" is a work of fiction, but it does contain some historical references; although it is a matter of conjecture and debate as to how much historical connection was deliberate on Stoker's part.

Attention was drawn to the supposed connections between the historical Transylvanian-born Vlad III Dracula (also known as Vlad Tepes) of Wallachia and Bram Stoker's fictional Dracula, following the publication of "In Search of Dracula" by Radu Florescu and Raymond McNally in 1972.

During his main reign (1456–1462), "Vlad the Impaler" is said to have killed from 40,000 to 100,000 European civilians (political rivals, criminals, and anyone that he considered "useless to humanity"), mainly by impaling. The sources depicting these events are records by Saxon settlers in neighbouring Transylvania who had frequent clashes with Vlad III. Vlad III is revered as a folk hero by Romanians for driving off the invading Ottoman Turks, of whom his impaled victims are said to have included as many as 100,000. There is no solid evidence that the Count in the novel was modelled on Vlad the Impaler of Wallachia. At most, Stoker borrowed only the name Dracula and "scraps of miscellaneous information" about Romanian history, according to one expert, Elizabeth Miller; as well, and there are no comments about him in the author's working notes.

Historically, the name "Dracula" is derived from a Chivalric order called the Order of the Dragon, founded by Sigismund of Luxembourg (then king of Hungary) to uphold Christianity and defend the Empire against the Ottoman Turks. Vlad II Dracul, father of Vlad III, was admitted to the order around 1431, after which Vlad II wore the emblem of the order and later, as ruler of Wallachia, his coinage bore the dragon symbol, from which the name "Dracula" is derived since "dracul" in Romanian means "the dragon". People of Wallachia only knew "voievod" (king) Vlad III as Vlad Țepeș (the Impaler). The name "Dracula" became popular in Romania after publication of Stoker's book. Contrary to popular belief, the name Dracula does not translate to "son of the devil" in Romanian, which would be ""pui de drac"".

Stoker came across the name Dracula in his reading on Romanian history, and chose this to replace the name ("Count Wampyr") originally intended for his villain. Some Dracula scholars led by Elizabeth Miller argue that Stoker knew little of the historic Vlad III except for the name "Dracula" in addition to a few bits of Romanian history. Stoker mentions that his Dracula fought against the Turks and was later betrayed by his brother, historical facts in the novel which point to Vlad III:

The Count's identity is later speculated on by Professor Van Helsing:

Many of Stoker's biographers and literary critics have found strong similarities to the earlier Irish writer Sheridan Le Fanu's classic of the vampire genre "Carmilla". In writing "Dracula", Stoker may also have drawn on stories about the sídhe, some of which feature blood-drinking women. The Irish legend of Abhartach has also been suggested as a source.

In 1983, McNally additionally suggested that Stoker was influenced by the history of Hungarian Countess Elizabeth Bathory, who allegedly tortured and killed between 36 and 700 young women. It was later a commonly believed rumor that she committed these crimes to bathe in their blood, believing that this preserved her youth.

In her book "The Essential Dracula", Clare Haword-Maden suggested that the castle of Count Dracula was inspired by Slains Castle, at which Bram Stoker was a guest of the 19th Earl of Erroll. According to Miller, he first visited Cruden Bay in 1893, three years after work had begun on "Dracula". Haining and Tremaine maintain that, during this visit, Stoker was especially impressed by Slains Castle's interior and the surrounding landscape. Miller and Leatherdale question the stringency of this connection.

Possibly, Stoker was not inspired by a real edifice at all, but by Jules Verne's novel "The Carpathian Castle" (1892) or Anne Radcliffe's "The Mysteries of Udolpho" (1794). A third possibility is that he copied information about a castle at Vécs from one of his sources on Transylvania, the book by Major E.C. Johnson. A further option is that Stoker saw an illustration of Castle Bran (Törzburg) in the book on Transylvania by Charles Boner, or read about it in the books by Mazuchelli or Crosse.

Many of the scenes in Whitby and London are based on real places that Stoker frequently visited, although he distorts the geography for the sake of the story in some cases. One scholar has suggested that Stoker chose Whitby as the site of Dracula's first appearance in England because of the Synod of Whitby, given the novel's preoccupation with timekeeping and calendar disputes.

Daniel Farson, Leonard Wolf, and Peter Haining have suggested that Stoker received much historical information from Ármin Vámbéry, a Hungarian professor whom he met at least twice. Miller argues, "there is nothing to indicate that the conversation included Vlad, vampires, or even Transylvania", and "furthermore, there is no record of any other correspondence between Stoker and Vámbéry, nor is Vámbéry mentioned in Stoker's notes for Dracula."

The short story "Dracula's Guest" was posthumously published in 1914, two years after Stoker's death. It was, according to most contemporary critics, the deleted first (or second) chapter from the original manuscript and the one which gave the volume its name, but which the original publishers deemed unnecessary to the overall story.

"Dracula's Guest" follows an unnamed Englishman traveller as he wanders around Munich before leaving for Transylvania. It is Walpurgis Night and the young Englishman foolishly leaves his hotel, in spite of the coachman's warnings, and wanders through a dense forest alone. Along the way, he feels that he is being watched by a tall and thin stranger (possibly Count Dracula).

The short story climaxes in an old graveyard where the Englishman, caught in a blizzard, takes refuge in the marble tomb of "Countess Dolingen of Gratz". Within the tomb, he sees the Countess—apparently asleep and healthy—but before he can investigate further, a mysterious force throws him clear of the tomb. A lightning bolt then strikes the tomb, destroying it and incinerating the undead screaming countess. The Englishman then loses consciousness. He awakens to find a "gigantic" wolf lying on his chest and licking at his throat; however, the wolf merely keeps him warm and protects him until help arrives.

When the Englishman is finally taken back to his hotel, a telegram awaits him from his expectant host Dracula, with a warning about "dangers from snow and wolves and night".

In 2009, an official sequel was published, written by Bram Stoker's great grand-nephew Dacre Stoker and Ian Holt.

Dacre Stoker and J. D. Barker will write a prequel to "Dracula" titled "Dracul". An interpretation of the missing 101 pages of the original novel, it was pieced together from Bram Stoker's editorial notes, artifacts, and journals.

The story of "Dracula" has been the basis for numerous films and plays. Stoker himself wrote the first theatrical adaptation, which was presented at the Lyceum Theatre on 18 May 1897 under the title "Dracula, or The Undead" shortly before the novel's publication and performed only once, in order to establish his own copyright for such adaptations. This adaption was first published only a century later in October 1997. The first motion picture to feature Dracula was "Dracula's Death", produced in Hungary in 1921. The now-lost film, however, was not an adaptation of Stoker's novel, but featured an original story.

F. W. Murnau's unauthorised film adaptation "Nosferatu" was released in 1922, and the popularity of the novel increased considerably, owing to an attempt by Stoker's widow to have the film removed from public circulation. Prana Film, the production company, had been unable to obtain permission to adapt the story from Bram's widow Florence Stoker, so screenwriter Henrik Galeen was told to alter numerous details to avoid legal trouble. Galeen transplanted the action of the story from 1890s England to 1830s Germany and reworked several characters, dropping some (such as Lucy and all three of her suitors), and renaming others (Dracula became Orlok, Jonathan Harker became Thomas Hutter, Mina became Ellen, and so on). This attempt failed to avoid a court case, however; Florence Stoker sued Prana Film, and all copies of the film were ordered to be destroyed. However, the company was bankrupt, and Stoker only recovered her legal fees in damages. Some copies survived and found their way into theatres. Eventually, Florence Stoker gave up the fight against public displays of the film. Subsequent rereleases of the film have typically undone some of the changes, such as restoring the original character names (a practice also followed by Werner Herzog in his 1979 remake of Murnau's film "Nosferatu the Vampyre").

Florence Stoker licensed the story to playwright Hamilton Deane, whose 1924 stage play adaptation toured England for several years before settling down in London. In 1927, American stage producer Horace Liveright hired John L. Balderston to revise Deane's script in advance of its American premiere. Balderston significantly compressed the story, most notably consolidating or removing several characters. The Deane play and its Balderston revisions introduced an expanded role and history for Renfield, who now replaced Jonathan Harker as Dracula's solicitor in the first part of the story; combined Mina Harker and Lucy Westenra into a single character (named Lucy); and omitted both Arthur Holmwood and Quincey Morris entirely. When the play premiered in New York, it was with Bela Lugosi in the title role, and with Edward van Sloan as Abraham Van Helsing, roles which both actors (as well as Herbert Bunston as Dr. Seward) reprised for the English-language version of the 1931 Universal Studios film production. The 1931 film was one of the most commercially successful adaptations of the story to date; it and the Deane/Balderston play that preceded it set the standard for film and television adaptations of the story, with the alterations to the novel becoming standard for later adaptations for decades to come. Universal Studios continued to feature the character of Dracula in many of their horror films from the 1930s and 1940s.
In 1958, British film company Hammer Film Productions followed the success of its "The Curse of Frankenstein" from the previous year with "Dracula", released in the US as "The Horror of Dracula", directed by Terence Fisher. Fisher's production featured Christopher Lee as Dracula and Peter Cushing as Van Helsing. It was an international hit for Hammer Film, and Lee fixed the image of the fanged vampire in popular culture. Both Lee and Cushing reprised their roles multiple times over the next decade and a half, concluding with "The Legend of the 7 Golden Vampires" (with Cushing but not Lee) in 1974. Christopher Lee also took on the role of Dracula in "Count Dracula", a 1970 Spanish-Italian-German coproduction notable for its adherence to the plot of the original novel. Playing the part of Renfield in that version was Klaus Kinski, who later played Dracula himself in 1979's "Nosferatu the Vampyre".

In 1977, the BBC made "Count Dracula", a 155-minute adaptation for television starring Louis Jourdan. Later film adaptations include John Badham's 1979 "Dracula", starring Frank Langella and inspired by the 1977 Broadway revival of the Deane/Hamilton play, and Francis Ford Coppola's 1992 "Bram Stoker's Dracula", starring Gary Oldman. The character of Count Dracula has remained popular over the years, and many films have used the character as a villain, while others have named him in their titles, including "Dracula's Daughter" and "The Brides of Dracula". As of 2009, an estimated 217 films feature Dracula in a major role, a number second only to Sherlock Holmes (223 films). A large number of these appearances are not adaptations of Stoker's novel, but merely feature the character in an unrelated story.




</doc>
<doc id="7925" url="https://en.wikipedia.org/wiki?curid=7925" title="David Hume">
David Hume

David Hume (; born David Home; 7 May 1711 NS – 25 August 1776) was a Scottish philosopher, historian, economist, and essayist, who is best known today for his highly influential system of philosophical empiricism, skepticism, and naturalism. Hume's empiricist approach to philosophy places him with John Locke, Francis Bacon and Thomas Hobbes as a British Empiricist. Beginning with his "A Treatise of Human Nature" (1739), Hume strove to create a total naturalistic science of man that examined the psychological basis of human nature. Against philosophical rationalists, Hume held that passion rather than reason governs human behaviour. Hume argued against the existence of innate ideas, positing that all human knowledge is founded solely in experience; Hume thus held that genuine knowledge must either be directly traceable to objects perceived in experience, or result from abstract reasoning about relations between ideas which are derived from experience, calling the rest "nothing but sophistry and illusion", a dichotomy later given the name "Hume's fork".

In what is sometimes referred to as Hume's problem of induction, he argued that inductive reasoning and belief in causality cannot be justified rationally; instead, our trust in causality and induction result from custom and mental habit, and are attributable only to the experience of "constant conjunction" of events. This is because we can never actually perceive that one event causes another, but only that the two are always conjoined. Accordingly, to draw any causal inferences from past experience it is necessary to presuppose that the future will resemble the past, a presupposition which cannot itself be grounded in prior experience.

Hume's opposition to the teleological argument for God's existence, the argument from design, is generally regarded as the most intellectually significant attempt to rebut the argument prior to Darwinism.

Hume was also a sentimentalist who held that ethics are based on emotion or sentiment rather than abstract moral principle, famously proclaiming that "Reason is, and ought only to be the slave of the passions". Hume's moral theory has been seen as a unique attempt to synthesise the modern sentimentalist moral tradition to which Hume belonged, with the virtue ethics tradition of ancient philosophy, with which Hume concurred in regarding traits of character, rather than acts or their consequences, as ultimately the proper objects of moral evaluation. Hume maintained an early commitment to naturalistic explanations of moral phenomena, and is usually taken to have first clearly expounded the is–ought problem, or the idea that a statement of fact alone can never give rise to a normative conclusion of what "ought" to be done. Hume also denied that humans have an actual conception of the self, positing that we experience only a bundle of sensations, and that the self is nothing more than this bundle of causally-connected perceptions. Hume's compatibilist theory of free will takes causal determinism as fully compatible with human freedom.

Hume influenced utilitarianism, logical positivism, Immanuel Kant, the philosophy of science, early analytic philosophy, cognitive science, theology, and other movements and thinkers. Kant himself credited Hume as the spur to his philosophical thought who had awakened him from his "dogmatic slumbers".

David Hume was the second of two sons born to Joseph Home of Ninewells, an advocate, and his wife The Hon. Katherine ("née" Falconer), daughter of Sir David Falconer. He was born on 26 April 1711 (Old Style) in a tenement on the north side of the Lawnmarket in Edinburgh. Hume's father died when Hume was a child, just after his second birthday, and he was raised by his mother, who never remarried. He changed the spelling of his name in 1734, because of the fact that his surname "Home," pronounced "Hume," was not known in England. Throughout his life Hume, who never married, spent time occasionally at his family home at Ninewells in Berwickshire, which had belonged to his family since the sixteenth century. His finances as a young man were very "slender". His family was not rich, and, as a younger son, he had little patrimony to live on. He was therefore forced to make a living somehow.

Hume attended the University of Edinburgh at the unusually early age of twelve (possibly as young as ten) at a time when fourteen was normal. At first, because of his family, he considered a career in law, but came to have, in his words, "an insurmountable aversion to everything but the pursuits of Philosophy and general Learning; and while [my family] fanceyed I was poring over Voet and Vinnius, Cicero and Virgil were the Authors which I was secretly devouring". He had little respect for the professors of his time, telling a friend in 1735 that "there is nothing to be learnt from a Professor, which is not to be met with in Books". Hume did not graduate.

Aged around 18, he made a philosophical discovery that opened up to him "a new Scene of Thought", which inspired him "to throw up every other Pleasure or Business to apply entirely to it". He did not recount what this scene was, and commentators have offered a variety of speculations. One popular interpretation, prominent in contemporary Hume scholarship, is that the new "scene of thought" was Hume's realization that Francis Hutcheson's "moral sense" theory of morality could be applied to the understanding as well. Due to this inspiration, Hume set out to spend a minimum of ten years reading and writing. He soon came to the verge of a mental breakdown, suffering from what a doctor diagnosed as the "Disease of the Learned". Hume wrote that it started with a coldness, which he attributed to a "Laziness of Temper", that lasted about nine months. Later, some scurvy spots broke out on his fingers. This was what persuaded Hume's physician to make his diagnosis. Hume wrote that he "went under a Course of Bitters and Anti-Hysteric Pills", taken along with a pint of claret every day. Hume also decided to have a more active life to better continue his learning. His health improved somewhat, but, in 1731, he was afflicted with a ravenous appetite and palpitations of the heart. After eating well for a time, he went from being "tall, lean and raw-bon'd" to being "sturdy, robust [and] healthful-like". Indeed, Hume would become well known in his time for his "corpulence", and his fondness for good port and cheese.

At 25 years of age, Hume, although of noble ancestry, had no source of income and no learned profession. As was common at his time, he became a merchant's assistant, but he had to leave his native Scotland. He travelled via Bristol to La Flèche in Anjou, France. There he had frequent discourse with the Jesuits of the College of La Flèche.

While Hume was derailed in his attempts to start a university career by protests over his "atheism" and bemoaned that his literary debut, "A Treatise of Human Nature", 'fell dead-born from the press', he found literary success in his lifetime as an essayist, and a career as a librarian at the University of Edinburgh. His tenure there, and the access to research materials it provided, ultimately resulted in Hume's writing the massive six-volume "The History of England", which became a bestseller and the standard history of England in its day. Hume described his "love for literary fame" as his "ruling passion" and judged his two late works, the so-called "first" and "second" enquiries, "An Enquiry Concerning Human Understanding" and "An Enquiry Concerning the Principles of Morals", respectively, as his greatest literary and philosophical achievements, asking his contemporaries to judge him on the merits of the later texts alone, rather than the more radical formulations of his early, youthful work, dismissing his philosophical debut as juvenilia: "A work which the Author had projected before he left College." Despite Hume's protestations, a general consensus exists today that Hume's most important arguments and philosophically distinctive doctrines are found in the original form they take in the "Treatise". Hume was just 23 years old when he started this work and it is now regarded as one of the most important in the history of Western philosophy.

He worked for four years on his first major work, "A Treatise of Human Nature", subtitled "Being an Attempt to Introduce the Experimental Method of Reasoning into Moral Subjects", completing it in 1738 at the age of 28. Although many scholars today consider the "Treatise" to be Hume's most important work and one of the most important books in Western philosophy, the critics in Great Britain at the time did not agree, describing it as "abstract and unintelligible". As Hume had spent most of his savings during those four years, he resolved "to make a very rigid frugality supply my deficiency of fortune, to maintain unimpaired my independency, and to regard every object as contemptible except the improvements of my talents in literature". Despite the disappointment, Hume later wrote, "Being naturally of a cheerful and sanguine temper, I soon recovered from the blow and prosecuted with great ardour my studies in the country." There, in an attempt to make his larger work better known and more intelligible, he published the "An Abstract of a Book lately Published" as a summary of the main doctrines of the "Treatise", without revealing its authorship. Although there has been some academic speculation as to who actually wrote this pamphlet it is generally regarded as Hume's creation.

After the publication of "Essays Moral and Political" in 1741, which was included in the later edition called "Essays, Moral, Political, and Literary", Hume applied for the Chair of Pneumatics and Moral Philosophy at the University of Edinburgh. However, the position was given to William Cleghorn after Edinburgh ministers petitioned the town council not to appoint Hume because he was seen as an atheist.

During the 1745 Jacobite rising, Hume tutored the Marquess of Annandale (1720–92), who was "judged to be a lunatic". This engagement ended in disarray after about a year. However, it was then that Hume started his great historical work "The History of England". This took him fifteen years and ran to over a million words. During this time he was also involved with the Canongate Theatre through his friend John Home, a preacher.

In this context, he associated with Lord Monboddo and other Scottish Enlightenment luminaries in Edinburgh. From 1746, Hume served for three years as secretary to General James St Clair, who was envoy to the courts of Turin and Vienna. At that time Hume also wrote "Philosophical Essays Concerning Human Understanding", later published as "An Enquiry Concerning Human Understanding". Often called the "First Enquiry", it proved little more successful than the "Treatise", perhaps because of the publishing of his short autobiography, "My Own Life", which "made friends difficult for the first Enquiry".
In 1749 he went to live with his brother in the countryside.

Hume's religious views were often suspect. It was necessary in the 1750s for his friends to avert a trial against him on the charge of heresy. However, he "would not have come and could not be forced to attend if he said he was not a member of the Established Church". Hume failed to gain the chair of philosophy at the University of Glasgow for his religious views, too. He had published the "Philosophical Essays" by this time which were decidedly anti-religious. Even Adam Smith, his personal friend who had vacated the Glasgow philosophy chair, was against his appointment out of concern public opinion would be against it.

Hume returned to Edinburgh in 1751. In the following year "the Faculty of Advocates chose me their Librarian, an office from which I received little or no emolument, but which gave me the command of a large library". This resource enabled him to continue historical research for "The History of England". Hume's volume of "Political Discourses", written in 1749 and published by Kincaid & Donaldson in 1752, was the only work he considered successful on first publication.

Eventually, with the publication of his six volume "The History of England" between 1754 and 1762, Hume achieved the fame that he coveted. The volumes traced events from the Invasion of Julius Caesar to the Revolution of 1688, and was a bestseller in its day.

Hume was also a longtime friend of bookseller Andrew Millar, who sold Hume's "History ("after acquiring the rights from Scottish bookseller Gavin Hamilton), although the relationship was sometimes complicated. Letters between them illuminate both men's interest in the success of the "History."

From 1763 to 1765, Hume was invited to attend Lord Hertford in Paris, where he became secretary to the British embassy. Hume was well received in Paris, and while there he met with Isaac de Pinto In 1766, Hume left Paris to accompany Jean-Jacques Rousseau to England. Once in England, Hume and Rousseau fell out. Hume was sufficiently worried about the damage to his reputation from the quarrel with Rousseau (who is generally believed to have suffered from paranoia) to have authored an account of the dispute, which he titled, appropriately enough ""A concise and genuine account of the dispute between Mr. Hume and Mr. Rousseau."" In 1765, he served as British Chargé d'affaires, writing "despatches to the British Secretary of State". He wrote of his Paris life, "I really wish often for the plain roughness of The Poker Club of Edinburgh ... to correct and qualify so much lusciousness". In 1766, upon returning to Britain, Hume encouraged Lord Hertford to invest in a number of slave plantations, acquired by George Colebrooke and others in the Windward Islands. In 1767, Hume was appointed Under Secretary of State for the Northern Department. Here he wrote that he was given "all the secrets of the Kingdom". In 1769 he returned to James' Court in Edinburgh, and then lived, from 1771 until his death in 1776, at the southwest corner of St. Andrew's Square in Edinburgh's New Town, at what is now 21 Saint David Street. A popular story, consistent with some historical evidence, suggests the street may have been named after Hume.

In the last year of his life, Hume wrote an extremely brief autobiographical essay titled "My Own Life" which summed up his entire life in "fewer than 5 pages", and notably contains many interesting judgments that have been of enduring interest to subsequent readers of Hume. The scholar of 18th-century literature Donald Seibert judged it a "remarkable autobiography, even though it may lack the usual attractions of that genre. Anyone hankering for startling revelations or amusing anecdotes had better look elsewhere." Hume here confesses his belief that the "love of literary fame" had served as his "ruling passion" in life, and claims that this desire "never soured my temper, notwithstanding my frequent disappointments." One such disappointment Hume discusses in the mini-autobiography was his disappointment that with the initial literary reception of the "Treatise", which he claims to have overcome by means of the success of the "Essays": "the work was favourably received, and soon made me entirely forget my former disappointment". Perhaps most notable is Hume's revelation of his own retrospective judgment that his philosophical debut's apparent failure "had proceeded more from the manner than the matter." Hume thus suggests that "I had been guilty of a very usual indiscretion, in going to the press too early." Hume provides an unambiguous self-assessment of the relative value of his works: "my Enquiry concerning the Principles of Morals; which, in my own opinion (who ought not to judge on that subject) is of all my writings, historical, philosophical, or literary, incomparably the best." Hume also makes a number of self-assessments in the essay, writing of his social relations that "My company was not unacceptable to the young and careless, as well as to the studious and literary", noting of his complex relation to religion, as well as the state, that "though I wantonly exposed myself to the rage of both civil and religious factions, they seemed to be disarmed in my behalf of their wonted fury", and professing of his character that "My friends never had occasion to vindicate any one circumstance of my character and conduct." Hume concludes the essay with the frank admission: " I cannot say there is no vanity in making this funeral oration of myself, but I hope it is not a misplaced one; and this is a matter of fact which is easily cleared and ascertained."

Diarist and biographer James Boswell saw Hume a few weeks before his death, which was from some form of abdominal cancer. Hume told him he sincerely believed it a "most unreasonable fancy" that there might be life after death. This meeting was dramatised in semi-fictional form for the BBC by Michael Ignatieff as "Dialogue in the Dark". Hume asked that his body be interred in a "simple Roman tomb". In his will he requests that it be inscribed only with his name and the year of his birth and death, "leaving it to Posterity to add the Rest". It stands, as he wished it, on the southwestern slope of Calton Hill, in the Old Calton Cemetery. Adam Smith later recounted Hume's amusing speculation that he might ask Charon to allow him a few more years of life in order to see "the downfall of some of the prevailing systems of superstition." The ferryman replied, "You loitering rogue, that will not happen these many hundred years ... Get into the boat this instant".

In the introduction to "A Treatise of Human Nature", Hume wrote, "'Tis evident, that all the sciences have a relation, more or less, to human nature ... Even Mathematics, Natural Philosophy, and Natural Religion, are in some measure dependent on the science of Man." He also wrote that the science of man is the "only solid foundation for the other sciences" and that the method for this science requires both experience and observation as the foundations of a logical argument. On this aspect of Hume's thought, philosophical historian Frederick Copleston wrote that it was Hume's aim to apply to the science of man the method of experimental philosophy (the term that was current at the time to imply Natural philosophy), and that "Hume's plan is to extend to philosophy in general the methodological limitations of Newtonian physics".

Until recently, Hume was seen as a forerunner of logical positivism; a form of anti-metaphysical empiricism. According to the logical positivists, unless a statement could be verified by experience, or else was true or false by definition (i.e. either tautological or contradictory), then it was meaningless (this is a summary statement of their verification principle). Hume, on this view, was a proto-positivist, who, in his philosophical writings, attempted to demonstrate how ordinary propositions about objects, causal relations, the self, and so on, are semantically equivalent to propositions about one's experiences.

Many commentators have since rejected this understanding of Humean empiricism, stressing an epistemological (rather than a semantic) reading of his project. According to this opposing view, Hume's empiricism consisted in the idea that it is our knowledge, and not our ability to conceive, that is restricted to what can be experienced. Hume thought that we can form beliefs about that which extends beyond any possible experience, through the operation of faculties such as custom and the imagination, but he was sceptical about claims to knowledge on this basis.

One of the most central doctrines of Hume's philosophy, stated in the very first lines of the "Treatise", is his notion that the mind consists of its mental perceptions, or the mental objects which are present to it, and which divide into two categories: "impressions" and "ideas". Hume's Treatise thus opens with the words: 'All the perceptions of the human mind resolve themselves into two distinct kinds, which I shall call IMPRESSIONS and IDEAS." Hume states that "I believe it will not be very necessary to employ many words in explaining this distinction" and commentators have generally taken Hume to mean the distinction between feeling and thinking. Controversially, Hume may regard the difference as in some sense a matter of degree, as he takes "impressions" to be distinguished from ideas, on the basis of their force, liveliness, and vivacity, or what Henry Allison calls the "FLV criterion" in his book on Hume. Ideas are therefore "faint" impressions. For example, experiencing the painful sensation of touching the handle of a hot pan is more forceful than simply thinking about touching a hot pan. According to Hume, impressions are meant to be the original form of all our ideas, and Don Garret has thus coined the term "the copy principle" to refer to Hume's doctrine that all ideas are ultimately all copied from some original impression, whether it be a passion or sensation, from which they derive.

After establishing the forcefulness of impressions and ideas, these two categories are further broken down into simple and complex: simple impressions and ideas, and complex impressions and ideas. Hume states that “simple perceptions or impressions and ideas are such as admit of no distinction nor separation,” while “the complex are the contrary to these, and may be distinguished into parts.” When looking at an apple, a person experiences a variety of color-sensations, which Hume sees as a complex impression. Similarly, a person experiences a variety of taste-sensations, tactile-sensations, and smell-sensations when biting into an apple, with the overall sensation again being a complex impression. Thinking about an apple allows a person to form complex ideas, which are made of similar parts as the complex impressions they were developed from, but which are also less forceful. Hume believes that complex perceptions can be broken down into smaller and smaller parts until perceptions are reached that have no parts of their own, and these perceptions are thereby referred to as being simple.
A person’s imagination, regardless of how boundless it may seem, is confined to the mind’s ability to recombine the information it has already acquired from the body’s sensory experience (the ideas that have been derived from impressions). In addition, “as our imagination takes our most basic ideas and leads us to form new ones, it is directed by three principles of association, namely, resemblance, contiguity, and cause and effect." The principle of resemblance refers to the tendency of ideas to become associated if the objects they represent resemble one another. For example, a person looking at an illustration of a flower can conceive of an idea of the physical flower because the idea of the illustrated object is associated with the idea of the physical object. The principle of contiguity describes the tendency of ideas to become associated if the objects they represent are near to each other in time or space, such as when the thought of one crayon in a box leads a person to think of the crayon contiguous to it. Finally, the principle of cause and effect refers to the tendency of ideas to become associated if the objects they represent are causally related, which explains how remembering a broken window can make someone think of the baseball that caused the window to shatter.
Hume elaborates more on this last principle of cause and effect. When a person observes that one object or event consistently produces the same object or event, it results in “an expectation that a particular event (a 'cause') will be followed by another event (an 'effect') previously and constantly associated with it." Hume calls this principle custom, or habit, saying that “custom…renders our experience useful to us, and makes us expect, for the future, a similar train of events with those which have appeared in the past." However, even though custom can serve as a guide in life, it still only represents an expectation. In other words, “experience cannot establish a necessary connection between cause and effect, because we can imagine without contradiction a case where the cause does not produce its usual effect…the reason why we mistakenly infer that there is something in the cause that necessarily produces its effect is because our past experiences have habituated us to think in this way." Continuing this idea, Hume argues that "only in the pure realm of ideas, logic, and mathematics, not contingent on the direct sense awareness of reality, [can] causation safely…be applied – all other sciences are reduced to probability." He uses this skepticism to reject metaphysics and many theological views on the basis that they are not grounded in fact and observations, and are therefore beyond the reach of human understanding.

The cornerstone of Hume's epistemology is the problem of induction. This may be the area of Hume's thought where his scepticism about human powers of reason is most pronounced. The problem revolves around the plausibility of inductive reasoning, that is, reasoning from the observed behaviour of objects to their behaviour when unobserved. As Hume wrote, induction concerns how things behave when they go "beyond the present testimony of the senses, or the records of our memory". Hume argues that we tend to believe that things behave in a regular manner, meaning that patterns in the behaviour of objects seem to persist into the future, and throughout the unobserved present. Hume's argument is that we cannot rationally justify the claim that nature will continue to be uniform, as justification comes in only two varieties—demonstrative reasoning and probable reasoning—and both of these are inadequate. With regard to demonstrative reasoning, Hume argues that the uniformity principle cannot be demonstrated, as it is "consistent and conceivable" that nature might stop being regular. Turning to probable reasoning, Hume argues that we cannot hold that nature will continue to be uniform because it has been in the past. As this is using the very sort of reasoning (induction) that is under question, it would be circular reasoning. Thus, no form of justification will rationally warrant our inductive inferences.

Hume's solution to this problem is to argue that, rather than reason, natural instinct explains the human practice of making inductive inferences. He asserts that "Nature, by an absolute and uncontroulable necessity has determin'd us to judge as well as to breathe and feel." Agreeing, philosopher John D. Kenyon writes: "Reason might manage to raise a doubt about the truth of a conclusion of natural inductive inference just for a moment ... but the sheer agreeableness of animal faith will protect us from excessive caution and sterile suspension of belief." Commentators such as Charles Sanders Peirce have demurred from Hume's solution, while, some, such as Kant and Karl Popper, saw that Hume's analysis "had posed a most fundamental challenge to all human knowledge claims."

The notion of causation is closely linked to the problem of induction. According to Hume, we reason inductively by associating constantly conjoined events. It is the mental act of association that is the basis of our concept of causation. There are at least three interpretations of Hume's theory of causation represented in the literature: (1) the logical positivist; (2) the sceptical realist; and (3) the quasi-realist.

David Hume acknowledged that there are events constantly unfolding, humanity cannot guarantee that these events are caused by events prior or if they are independent instances. Hume opposed the widely accepted theory of Causation that ‘all events have a specific course or reason.’ Therefore Hume crafted his own theory of causation, which he formed through his empiricist and skeptic beliefs. He split Causation, into two realms “All the objects of human reason or enquiry may naturally be divided into two kinds, to wit, Relations of Ideas, and Matters of Fact”. Relations of Ideas are a priori, and represent universal bonds between ideas that mark the cornerstones of human thought. Matters of Fact are dependent on the observer and experience. They are often not universally held to be true among multiple persons. Hume was an Empiricist, meaning he believed “causes and effects are discoverable not by reason, but by experience”. Hume later goes onto say that even with the perspective of the past, humanity cannot dictate future events because thoughts of the past are limited, compared to the possibilities for the future. Hume’s separation between Matters of Fact and Relations of Ideas is often referred to as “Hume’s Fork”.
Hume explains his theory of Causation and causal inference by division into three different parts. In these three branches he explains his ideas, in addition to comparing and contrasting his views to his predecessors. These branches are the Critical Phase, the Constructive Phase, and Belief. In the Critical Phase, Hume denies his predecessors' theories of causation. Next, Hume uses the Constructive Phase to resolve any doubts the reader may have while observing the Critical Phase. “Habit or Custom” mend the gaps in reasoning that occur without the human mind even realizing it. Associating ideas has become second nature to the human mind. It “makes us expect for the future, a similar train of events with those which have appeared in the past” However, Hume says that this association cannot be trusted because the span of the human mind to comprehend the past is not necessarily applicable to the wide and distant future. This leads Hume to the third branch of causal inference, Belief. Belief is what drives the human mind to hold that expectancy of the future based on past experience. Throughout his explanation of causal inference, Hume is arguing that the future is not certain to be repetition of the past and the only way to justify induction is through uniformity.

The logical positivist interpretation is that Hume analyses causal propositions, such as "A caused B", in terms of regularities in perception: "A causes B" is equivalent to "Whenever A-type events happen, B-type ones follow", where "whenever" refers to all possible perceptions. In his "Treatise of Human Nature", Hume wrote:
power and necessity ... are ... qualities of perceptions, not of objects ... felt by the soul and not perceiv'd externally in bodies.
This view is rejected by skeptical realists, who argue that Hume thought that causation amounts to more than just the regular succession of events. Hume said that when two events are causally conjoined, a necessary connection underpins the conjunction:
Shall we rest contented with these two relations of contiguity and succession, as affording a complete idea of causation? By no means ... there is a "necessary connexion" to be taken into consideration.
Philosopher Angela Coventry writes that, for Hume, "there is nothing in any particular instance of cause and effect involving external objects which suggests the idea of power or necessary connection" and that "we are ignorant of the powers that operate between objects". However, while denying the possibility of knowing the powers between objects, Hume accepted the causal principle, writing, "I never asserted so absurd a proposition as that something could arise without a cause."

It has been argued that, while Hume did not think causation is reducible to pure regularity, he was not a fully fledged realist either. Philosopher Simon Blackburn calls this a quasi-realist reading. Blackburn writes that "Someone talking of cause is voicing a distinct mental set: he is by no means in the same state as someone merely describing regular sequences. In Hume's words, "nothing is more usual than to apply to external bodies every internal sensation, which they occasion".

Empiricist philosophers, such as Hume and Berkeley, favoured the bundle theory of personal identity. In this theory, "the mind itself, far from being an independent power, is simply 'a bundle of perceptions' without unity or cohesive quality". The self is nothing but a bundle of experiences linked by the relations of causation and resemblance; or, more accurately, that the empirically warranted idea of the self is just the idea of such a bundle. This view is forwarded by, for example, positivist interpreters, who saw Hume as suggesting that terms such as "self", "person", or "mind" referred to collections of "sense-contents". A modern-day version of the bundle theory of the mind has been advanced by Derek Parfit in his "Reasons and Persons".

However, some philosophers have criticised Hume's bundle-theory interpretation of personal identity. They argue that distinct selves can have perceptions that stand in relations of similarity and causality with one another. Thus, perceptions must already come parcelled into distinct "bundles" before they can be associated according to the relations of similarity and causality. In other words, the mind must already possess a unity that cannot be generated, or constituted, by these relations alone. Since the bundle-theory interpretation portrays Hume as answering an ontological question, philosophers, like Galen Strawson, who see Hume as not very concerned with such questions have queried whether the view is really Hume's. Instead, it is suggested by Strawson that Hume might have been answering an epistemological question about the causal origin of our concept of the self. In the Appendix to the "Treatise", Hume declares himself dissatisfied with his earlier account of personal identity in Book 1. Philosopher Corliss Swain notes that "Commentators agree that if Hume did find some new problem" when he reviewed the section on personal identity, "he wasn't forthcoming about its nature in the Appendix." One interpretation of Hume's view of the self has been argued for by philosopher and psychologist James Giles. According to his view, Hume is not arguing for a bundle theory, which is a form of reductionism, but rather for an eliminative view of the self. That is, rather than reducing the self to a bundle of perceptions, Hume is rejecting the idea of the self altogether. On this interpretation, Hume is proposing a "no-self theory" and thus has much in common with Buddhist thought. On this point, psychologist Alison Gopnik has argued that Hume was in a position to learn about Buddhist thought during his time in France in the 1730s.

An essential question of practical reason for Hume was whether or not standards or principles exist (and if they do, what they are) for practical reason, that are also authoritative for all rational beings, dictating people’s intentions and actions. Hume is mainly considered an anti-rationalist, denying the possibility for practical reason as a principle to exist, although other philosophers such as Christine Korsgaard, Jean Hampton, and Elijah Millgram claim that Hume is not so much of an anti-rationalist as he is just a skeptic of practical reason.

Hume denied the existence of practical reason as a principle because he claimed reason does not have any effect on morality, since morality is capable of producing effects in people that reason alone cannot create. As Hume explains in "A Treatise of Human Nature" (1740): “Morals excite passions, and produce or prevent actions. Reason of itself is utterly impotent in this particular. The rules of morality, therefore, are not conclusions of our reason.”

Since practical reason is supposed to regulate our actions (in theory), Hume denied practical reason on the grounds that reason cannot directly oppose passions. As Hume puts it, “Reason is, and ought only to be the slave of the passions, and can never pretend to any other office than to serve and obey them.” Reason is less significant than any passion because reason has no original influence, while "A passion is an original existence, or, if you will, modification of existence".

Practical reason is also concerned with the value of actions rather than the truth of propositions, so Hume believed that reason’s shortcoming of affecting morality proved that practical reason could not be authoritative for all rational beings, since morality was essential for dictating people’s intentions and actions.

Hume's writings on ethics began in the "Treatise" and were refined in his "An Enquiry Concerning the Principles of Morals" (1751). His views on ethics are that "[m]oral decisions are grounded in moral sentiment." It is not knowing that governs ethical actions, but feelings. Arguing that reason cannot be behind morality, he wrote:
Morals excite passions, and produce or prevent actions. Reason itself is utterly impotent in this particular. The rules of morality, therefore, are not conclusions of our reason.
Hume's sentimentalism about morality was shared by his close friend Adam Smith, and Hume and Smith were mutually influenced by the moral reflections of their older contemporary Francis Hutcheson. Peter Singer claims that Hume's argument that morals cannot have a rational basis alone "would have been enough to earn him a place in the history of ethics".

Hume also put forward the is–ought problem, later called "Hume's Law", denying the possibility of logically deriving what "ought" to be from what "is". He wrote in the "Treatise" that in every system of morality he has read, the author begins with stating facts about the world, but then suddenly is always referring to what ought to be the case. Hume demands that a reason should be given for inferring what ought to be the case, from what is the case. This because it "seems altogether inconceivable, how this new relation can be a deduction from others".

Hume's theory of ethics has been influential in modern day meta-ethical theory, helping to inspire emotivism, and ethical expressivism and non-cognitivism, as well as Allan Gibbard's general theory of moral judgment and judgments of rationality.

Hume's ideas about aesthetics and the theory of art are spread throughout his works, but are particularly connected with his ethical writings, and also the essays "Of the Standard of Taste" and "Of Tragedy". His views are rooted in the work of Joseph Addison and Francis Hutcheson. In the "Treatise" he wrote of the connection between beauty and deformity and vice and virtue, and his later writings on this subject continue to draw parallels of beauty and deformity in art, with conduct and character.

In "Of the Standard of Taste", Hume argues that no rules can be drawn up about what is a tasteful object. However, a reliable critic of taste can be recognised as being objective, sensible and unprejudiced, and having extensive experience. "Of Tragedy" addresses the question of why humans enjoy tragic drama. Hume was concerned with the way spectators find pleasure in the sorrow and anxiety depicted in a tragedy. He argued that this was because the spectator is aware that he is witnessing a dramatic performance. There is pleasure in realising that the terrible events that are being shown are actually fiction. Furthermore, Hume laid down rules for educating people in taste and correct conduct, and his writings in this area have been very influential on English and Anglo-Saxon aesthetics.

Hume, along with Thomas Hobbes, is cited as a classical compatibilist about the notions of freedom and determinism. The thesis of compatibilism seeks to reconcile human freedom with the mechanist belief that human beings are part of a deterministic universe, whose happenings are governed by physical laws. Hume, to this end, was influenced greatly by the scientific revolution and by in particular Sir Isaac Newton. Hume argued that the dispute about the compatibility of freedom and determinism has been continued over two thousand years by ambiguous terminology. He wrote: "From this circumstance alone, that a controversy has been long kept on foot ... we may presume that there is some ambiguity in the expression", and that different disputants use different meanings for the same terms.

Hume defines the concept of necessity as "the uniformity, observable in the operations of nature; where similar objects are constantly conjoined together", and liberty as "a power of acting or not acting, according to the determinations of the will". He then argues that, according to these definitions, not only are the two compatible, but liberty "requires" necessity. For if our actions were not necessitated in the above sense, they would "have so little in connexion with motives, inclinations and circumstances, that one does not follow with a certain degree of uniformity from the other". But if our actions are not thus connected to the will, then our actions can never be free: they would be matters of "chance; which is universally allowed to have no existence". Australian philosopher John Passmore writes that confusion has arisen because "necessity" has been taken to mean "necessary connexion". Once this has been abandoned, Hume argues that "liberty and necessity will be found not to be in conflict one with another".

Moreover, Hume goes on to argue that in order to be held morally responsible, it is required that our behaviour be caused or necessitated, for, as he wrote:

Actions are, by their very nature, temporary and perishing; and where they proceed not from some "cause" in the character and disposition of the person who performed them, they can neither redound to his honour, if good; nor infamy, if evil.

Hume describes the link between causality and our capacity to rationally make a decision from this an inference of the mind. Human beings assess a situation based upon certain predetermined events and from that form a choice. Hume believes that this choice is made spontaneously. Hume calls this form of decision making the liberty of spontaneity.

Education writer Richard Wright considers that Hume's position rejects a famous moral puzzle attributed to French philosopher Jean Buridan. The Buridan's ass puzzle describes a donkey that is hungry. This donkey has on both sides of him separate bales of hay, which are of equal distances from him. The problem concerns which bale the donkey chooses. Buridan was said to believe that the donkey would die, because he has no autonomy. The donkey is incapable of forming a rational decision as there is no motive to choose one bale of hay over the other. However, human beings are different, because a human who is placed in a position where he is forced to choose one loaf of bread over another will make a decision to take one in lieu of the other. For Buridan, humans have the capacity of autonomy, and he recognises the choice that is ultimately made will be based on chance, as both loaves of bread are exactly the same. However, Wright says that Hume completely rejects this notion, arguing that a human will spontaneously act in such a situation because he is faced with impending death if he fails to do so. Such a decision is not made on the basis of chance, but rather on necessity and spontaneity, given the prior predetermined events leading up to the predicament.

Hume's argument is supported by modern day compatibilists such as R. E. Hobart, a pseudonym of philosopher Dickinson S. Miller. However, P. F. Strawson argued that the issue of whether we hold one another morally responsible does not ultimately depend on the truth or falsity of a metaphysical thesis such as determinism. This is because our so holding one another is a non-rational human sentiment that is not predicated on such theses.

The "Stanford Encyclopedia of Philosophy" states that Hume "wrote forcefully and incisively on almost every central question in the philosophy of religion." His "various writings concerning problems of religion are among the most important and influential contributions on this topic." His writings in this field cover the philosophy, psychology, history, and anthropology of religious thought. All of these aspects were discussed in Hume's 1757 dissertation, "The Natural History of Religion". Here he argued that the monotheistic religions of Judaism, Christianity and Islam all derive from earlier polytheistic religions. He also suggested that all religious belief "traces, in the end, to dread of the unknown." Hume had also written on religious subjects in the first "Enquiry", as well as later in the "Dialogues Concerning Natural Religion".

Although he wrote a great deal about religion, Hume's personal views are unclear, and there has been much discussion concerning his religious position. Contemporaries considered him to be an atheist, or at least un-Christian, and the Church of Scotland seriously considered bringing charges of infidelity against him. The fact that contemporaries thought that he may have been an atheist is exemplified by a story Hume liked to tell:

The best theologian he ever met, he used to say, was the old Edinburgh fishwife who, having recognized him as Hume the atheist, refused to pull him out of the bog into which he had fallen until he declared he was a Christian and repeated the Lord's prayer.

However, in works such as "Of Superstition and Enthusiasm", Hume specifically seems to support the standard religious views of his time and place. This still meant that he could be very critical of the Catholic Church, dismissing it with the standard Protestant accusations of superstition and idolatry, as well as dismissing as idolatry what his compatriots saw as uncivilised beliefs. He also considered extreme Protestant sects, the members of which he called "enthusiasts", to be corrupters of religion. By contrast, in his "The Natural History of Religion", Hume presented arguments suggesting that polytheism had much to commend it over monotheism.

Philosopher Paul Russell writes that it is likely that Hume was sceptical about religious belief, but not to the extent of complete atheism. He suggests that perhaps Hume's position is best characterised by the term "irreligion", while philosopher David O'Connor argues that Hume's final position was "weakly deistic". For O'Connor, Hume's "position is deeply ironic. This is because, while inclining towards a weak form of deism, he seriously doubts that we can ever find a sufficiently favourable balance of evidence to justify accepting any religious position." He adds that Hume "did not believe in the God of standard theism ... but he did not rule out all concepts of deity", and that "ambiguity suited his purposes, and this creates difficulty in definitively pinning down his final position on religion".

One of the traditional topics of natural theology is that of the existence of God, and one of the "a posteriori" arguments for this is the "argument from design" or the teleological argument. The argument is that the existence of God can be proved by the design that is obvious in the complexity of the world. "Encyclopædia Britannica" states that this is "the most popular, because [it is] the most accessible of the theistic arguments ... which identifies evidences of design in nature, inferring from them a divine designer ... The fact that the universe as a whole is a coherent and efficiently functioning system likewise, in this view, indicates a divine intelligence behind it."

In "An Enquiry Concerning Human Understanding", Hume wrote that the design argument seems to depend upon our experience, and its proponents "always suppose the universe, an effect quite singular and unparalleled, to be the proof of a Deity, a cause no less singular and unparalleled". Philosopher Louise E. Loeb notes that Hume is saying that only experience and observation can be our guide to making inferences about the conjunction between events. However, according to Hume, "we observe neither God nor other universes, and hence no conjunction involving them. There is no observed conjunction to ground an inference either to extended objects or to God, as unobserved causes."

Hume also criticised the argument in his "Dialogues Concerning Natural Religion" (1779). In this, he suggested that, even if the world is a more or less smoothly functioning system, this may only be a result of the "chance permutations of particles falling into a temporary or permanent self-sustaining order, which thus has the appearance of design."

A century later, the idea of order without design was rendered more plausible by Charles Darwin's discovery that the adaptations of the forms of life are a result of the natural selection of inherited characteristics. For philosopher James D. Madden, it is "Hume, rivaled only by Darwin, [who] has done the most to undermine in principle our confidence in arguments from design among all figures in the Western intellectual tradition."

Finally, Hume discussed a version of the anthropic principle, which is the idea that theories of the universe are constrained by the need to allow for man's existence in it as an observer. Hume has his sceptical mouthpiece Philo suggest that there may have been many worlds, produced by an incompetent designer, whom he called a "stupid mechanic". In his "Dialogues Concerning Natural Religion", Hume wrote:

Many worlds might have been botched and bungled throughout an eternity, ere this system was struck out: much labour lost: many fruitless trials made: and a slow, but continued improvement carried on during infinite ages in the art of world-making.
American philosopher Daniel Dennett has suggested that this mechanical explanation of teleology, although "obviously ... an amusing philosophical fantasy", anticipated the notion of natural selection, the 'continued improvement' being like "any Darwinian selection algorithm."

In his discussion of miracles, Hume argues that we should not believe that miracles have occurred and that they do not therefore provide us with any reason to think that God exists. In "An Enquiry Concerning Human Understanding" (Section 10), Hume defines a miracle as "a transgression of a law of nature by a particular volition of the Deity, or by the interposition of some invisible agent". Hume says that we believe an event that has frequently occurred is likely to occur again, but we also take into account those instances where the event did not occur. Hume wrote:

A wise man [...] considers which side is supported by the greater number of experiments [...] A hundred instances or experiments on one side, and fifty on another, afford a doubtful expectation of any event; though a hundred uniform experiments, with only one that is contradictory, reasonably beget a pretty strong degree of assurance. In all cases, we must balance the opposite experiments [...] and deduct the smaller number from the greater, in order to know the exact force of the superior evidence.

Hume discusses the testimony of those who report miracles. He wrote that testimony might be doubted even from some great authority in case the facts themselves are not credible. "[T]he evidence, resulting from the testimony, admits of a diminution, greater or less, in proportion as the fact is more or less unusual."

Although Hume leaves open the possibility for miracles to occur and be reported, he offers various arguments against this ever having happened in history: He points out that people often lie, and they have good reasons to lie about miracles occurring either because they believe they are doing so for the benefit of their religion or because of the fame that results. Furthermore, people by nature enjoy relating miracles they have heard without caring for their veracity and thus miracles are easily transmitted even where false. Also, Hume notes that miracles seem to occur mostly in "ignorant and barbarous nations" and times, and the reason they do not occur in the civilised societies is such societies are not awed by what they know to be natural events. Finally, the miracles of each religion argue against all other religions and their miracles, and so even if a proportion of all reported miracles across the world fit Hume's requirement for belief, the miracles of each religion make the other less likely.

Hume was extremely pleased with his argument against miracles in his "Enquiry". He states "I flatter myself, that I have discovered an argument of a like nature, which, if just, will, with the wise and learned, be an everlasting check to all kinds of superstitious delusion, and consequently, will be useful as long as the world endures." Thus, Hume's argument against miracles had a more abstract basis founded upon the scrutiny, not just primarily of miracles, but of all forms of belief systems. It is a common sense notion of veracity based upon epistemological evidence, and founded on a principle of rationality, proportionality and reasonability.

The criterion for assessing a belief system for Hume is based on the balance of probability whether something is more likely than not to have occurred. Since the weight of empirical experience contradicts the notion for the existence of miracles, such accounts should be treated with scepticism. Further, the myriad of accounts of miracles contradict one another, as some people who receive miracles will aim to prove the authority of Jesus, whereas others will aim to prove the authority of Muhammad or some other religious prophet or deity. These various differing accounts weaken the overall evidential power of miracles.

Despite all this, Hume observes that belief in miracles is popular, and that "The gazing populace [...] receive greedily, without examination, whatever soothes superstition, and promotes wonder."

Critics have argued that Hume's position assumes the character of miracles and natural laws prior to any specific examination of miracle claims, thus it amounts to a subtle form of begging the question. To assume that testimony is a homogeneous reference group seems unwise- to compare private miracles with public miracles, unintellectual observers with intellectual observers and those who have little to gain and much to lose with those with much to gain and little to lose is not convincing to many. Indeed, many have argued that miracles not only do not contradict the laws of nature, but require the laws of nature to be intelligible as miraculous, and thus subverting the law of nature. For example, William Adams remarks that "there must be an ordinary course of nature before anything can be extraordinary. There must be a stream before anything can be interrupted". They have also noted that it requires an appeal to inductive inference, as none have observed every part of nature nor examined every possible miracle claim, for instance those in the future. This, in Hume's philosophy, was especially problematic.

Little appreciated is the voluminous literature either foreshadowing Hume, in the likes of Thomas Sherlock or directly responding to and engaging with Hume- from William Paley, William Adams, John Douglas, John Leland and George Campbell, among others. Of Campbell, it is rumoured that, having read Campbell's Dissertation, Hume remarked that "the Scotch theologue had beaten him".

Hume's main argument concerning miracles is that miracles by definition are singular events that differ from the established laws of nature. Such natural laws are codified as a result of past experiences. Therefore, a miracle is a violation of all prior experience and thus incapable on this basis of reasonable belief. However, the probability that something has occurred in contradiction of all past experience should always be judged to be less than the probability that either ones senses have deceived one, or the person recounting the miraculous occurrence is lying or mistaken. Hume would say, all of which he had past experience of. For Hume, this refusal to grant credence does not guarantee correctness. He offers the example of an Indian Prince, who, having grown up in a hot country, refuses to believe that water has frozen. By Hume's lights, this refusal is not wrong and the Prince "reasoned justly"; it is presumably only when he has had extensive experience of the freezing of water that he has warrant to believe that the event could occur.

So for Hume, either the miraculous event will become a recurrent event or else it will never be rational to believe it occurred. The connection to religious belief is left unexplained throughout, except for the close of his discussion where Hume notes the reliance of Christianity upon testimony of miraculous occurrences. He makes an ironic remark that anyone who "is moved by faith to assent" to revealed testimony "is conscious of a continued miracle in his own person, which subverts all principles of his understanding, and gives him a determination to believe what is most contrary to custom and experience." Hume writes that "All the testimony which ever was really given for any miracle, or ever will be given, is a subject of derision."

From 1754 to 1762 Hume published "The History of England", a 6-volume work, which extends, says its subtitle, "From the Invasion of Julius Caesar to the Revolution in 1688". Inspired by Voltaire's sense of the breadth of history, Hume widened the focus of the field away from merely kings, parliaments, and armies, to literature and science as well. He argued that the quest for liberty was the highest standard for judging the past, and concluded that after considerable fluctuation, England at the time of his writing had achieved "the most entire system of liberty that was ever known amongst mankind". It "must be regarded as an event of cultural importance. In its own day, moreover, it was an innovation, soaring high above its very few predecessors."

Hume's coverage of the political upheavals of the 17th century relied in large part on the Earl of Clarendon's "History of the Rebellion and Civil Wars in England" (1646–69). Generally, Hume took a moderate royalist position and considered revolution unnecessary to achieve necessary reform. Hume was considered a Tory historian, and emphasised religious differences more than constitutional issues. Laird Okie explains that "Hume preached the virtues of political moderation, but ... it was moderation with an anti-Whig, pro-royalist coloring." For "Hume shared the ... Tory belief that the Stuarts were no more high-handed than their Tudor predecessors". "Even though Hume wrote with an anti-Whig animus, it is, paradoxically, correct to regard the "History" as an establishment work, one which implicitly endorsed the ruling oligarchy".
Historians have debated whether Hume posited a universal unchanging human nature, or allowed for evolution and development.

Robert Roth argues that Hume's histories display his biases against Presbyterians and Puritans. Roth says his anti-Whig pro-monarchy position diminished the influence of his work, and that his emphasis on politics and religion led to a neglect of social and economic history.

Hume was an early cultural historian of science. His short biographies of leading scientists explored the process of scientific change. He developed new ways of seeing scientists in the context of their times by looking at how they interacted with society and each other. He covers over forty scientists, with special attention paid to Francis Bacon, Robert Boyle, and Isaac Newton. Hume particularly praised William Harvey, writing about his treatise of the circulation of the blood: "Harvey is entitled to the glory of having made, by reasoning alone, without any mixture of accident, a capital discovery in one of the most important branches of science".

The "History" became a best-seller and made Hume a wealthy man who no longer had to take up salaried work for others. It was influential for nearly a century, despite competition from imitations by Smollett (1757), Goldsmith (1771) and others. By 1894, there were at least 50 editions as well as abridgements for students, and illustrated pocket editions, probably produced specifically for women.

It is difficult to categorise Hume's political affiliations. His writings contain elements that are, in modern terms, both conservative and liberal, although these terms are anachronistic. Thomas Jefferson banned the "History" from University of Virginia, feeling that it had "spread universal toryism over the land". By comparison, Samuel Johnson thought Hume "a Tory by chance ... for he has no principle. If he is anything, he is a Hobbist", a follower of Thomas Hobbes. A major concern of Hume's political philosophy is the importance of the rule of law. He also stresses throughout his political essays the importance of moderation in politics: public spirit and regard to the community.

This outlook needs to be seen within the historical context of 18th-century Scotland. Here, the legacy of religious civil war, combined with the relatively recent memory of the 1715 and 1745 Jacobite risings, fostered in a historian such as Hume a distaste for enthusiasm and factionalism. These appeared to threaten the fragile and nascent political and social stability of a country that was deeply politically and religiously divided. Hume thought that society is best governed by a general and impartial system of laws; he is less concerned about the form of government that administers these laws, so long as it does so fairly. However, he does write that a republic must produce laws, while "monarchy, when absolute, contains even something repugnant to law."

Hume expressed suspicion of attempts to reform society in ways that departed from long-established custom, and he counselled peoples not to resist their governments except in cases of the most egregious tyranny. However, he resisted aligning himself with either of Britain's two political parties, the Whigs and the Tories. Hume wrote:

My views of "things" are more conformable to Whig principles; my representations of "persons" to Tory prejudices.

Canadian philosopher Neil McArthur writes that Hume believed that we should try to balance our demands for liberty with the need for strong authority, without sacrificing either. McArthur characterises Hume as a "precautionary conservative", whose actions would have been "determined by prudential concerns about the consequences of change, which often demand we ignore our own principles about what is ideal or even legitimate." Hume supported the liberty of the press, and was sympathetic to democracy, when suitably constrained. American historian Douglass Adair has argued that Hume was a major inspiration for James Madison's writings, and the essay "Federalist No. 10" in particular.

Hume offered his view on the best type of society in an essay titled "Idea of a Perfect Commonwealth", which lays out what he thought was the best form of government. He hoped that, "in some future age, an opportunity might be afforded of reducing the theory to practice, either by a dissolution of some old government, or by the combination of men to form a new one, in some distant part of the world". He defended a strict separation of powers, decentralisation, extending the franchise to anyone who held property of value and limiting the power of the clergy. The system of the Swiss militia was proposed as the best form of protection. Elections were to take place on an annual basis and representatives were to be unpaid. Political philosophers Leo Strauss and Joseph Cropsey, writing of Hume's thoughts about "the wise statesman", note that he "will bear a reverence to what carries the marks of age". Also, if he wishes to improve a constitution, his innovations will take account of the "ancient fabric", in order not to disturb society.

In the political analysis of philosopher George Sabine, the scepticism of Hume extended to the doctrine of government by consent. He notes that "allegiance is a habit enforced by education and consequently as much a part of human nature as any other motive."

In the 1770s, Hume was critical of British policies toward the American colonies and advocated for American independence. He wrote in 1771 that "our union with America... in the nature of things, cannot long subsist".

Hume noted his views as economist in his "Political Discourses", which were incorporated in "Essays and Treatises" as Part II of "Essays, Moral and Political". To what extent he was influenced by Adam Smith is difficult to stress, however both of them had similar principles supported from historical events. At the same time Hume did not demonstrate concrete system of economic theory which could be observed in Smith's Wealth of Nations. However, he introduced several new ideas around which the “classical economics” of the 18th century was built. Through his discussions on politics, Hume developed many ideas that are prevalent in the field of economics. This includes ideas on private property, inflation, and foreign trade. Referring to his essay "Of the Balance of Trade", economist Paul Krugman has remarked that "David Hume created what I consider the first true economic model."

In contrast to Locke, Hume believes that private property is not a natural right. Hume argues it is justified, because resources are limited. Private property would be an unjustified, "idle ceremonial", if all goods were unlimited and available freely. Hume also believed in an unequal distribution of property, because perfect equality would destroy the ideas of thrift and industry. Perfect equality would thus lead to impoverishment.

Due to Hume's vast influence on contemporary philosophy, a large number of approaches in contemporary philosophy and cognitive science are today called "Humean."

Attention to Hume's philosophical works grew after the German philosopher Immanuel Kant, in his "Prolegomena to Any Future Metaphysics" (1783), credited Hume with awakening him from his "dogmatic slumber".

According to Schopenhauer, "there is more to be learned from each page of David Hume than from the collected philosophical works of Hegel, Herbart and Schleiermacher taken together."

A. J. Ayer, while introducing his classic exposition of logical positivism in 1936, claimed: "The views which are put forward in this treatise derive from ... doctrines ... which are themselves the logical outcome of the empiricism of Berkeley and David Hume." Albert Einstein, in 1915, wrote that he was inspired by Hume's positivism when formulating his theory of special relativity.

Hume's problem of induction was also of fundamental importance to the philosophy of Karl Popper. In his autobiography, "Unended Quest", he wrote: "Knowledge ... is "objective"; and it is hypothetical or conjectural. This way of looking at the problem made it possible for me to reformulate Hume's "problem of induction"". This insight resulted in Popper's major work "The Logic of Scientific Discovery". Also, in his "Conjectures and Refutations", he wrote:

I approached the problem of induction through Hume. Hume, I felt, was perfectly right in pointing out that induction cannot be logically justified.
The writings of Scottish philosopher and contemporary of Hume, Thomas Reid, were often criticisms of Hume's scepticism. Reid formulated his common sense philosophy in part as a reaction against Hume's views.

Hume influenced and was influenced by the Christian philosopher Joseph Butler. Hume was impressed by Butler's way of thinking about religion, and Butler may well have been influenced by Hume's writings.

Hume's rationalism in religious subjects influenced, via German-Scottish theologian Johann Joachim Spalding, the German neology school and rational theology, and contributed to the transformation of German theology in the age of enlightenment. Hume pioneered a comparative history of religion, tried to explain various rites and traditions as being based on deception and challenged various aspects of rational and natural theology, such as the argument from design.

Danish theologian and philosopher Søren Kierkegaard adopted "Hume's suggestion that the role of reason is not to make us wise but to reveal our ignorance." However, Kierkegaard took this as a reason for the necessity of religious faith, or fideism. The "fact that Christianity is contrary to reason ... is the necessary precondition for true faith." Political theorist Isaiah Berlin, for example, has pointed out the similarities between the arguments of Hume and Kierkegaard against rational theology. Berlin also writes about Hume's influence on what Berlin calls the counter-enlightenment, and German anti-rationalism.

According to philosopher Jerry Fodor, Hume's "Treatise" is "the founding document of cognitive science".

Hume engaged with contemporary intellectual luminaries such as Jean-Jacques Rousseau, James Boswell, and Adam Smith (who acknowledged Hume's influence on his economics and political philosophy).

Isaiah Berlin once said of Hume that "No man has influenced the history of philosophy to a deeper or more disturbing degree."

The "Stanford Encyclopedia of Philosophy" writes that Hume is "[g]enerally regarded as one of the most important philosophers to write in English."

His nephew and namesake, David Hume of Ninewells (1757–1838) was a co-founder of the Royal Society of Edinburgh in 1783. He was a Professor of Scots Law at Edinburgh University and rose to be Principal Clerk Of Session in the scottish High court and Baron of the Exchequer. He is buried with his uncle in Old Calton Cemetery.




</doc>
<doc id="7928" url="https://en.wikipedia.org/wiki?curid=7928" title="Dalton Trumbo">
Dalton Trumbo

James Dalton Trumbo (December 9, 1905 – September 10, 1976) was an American screenwriter and novelist who scripted many award-winning films including "Roman Holiday", "Exodus", "Spartacus", and "Thirty Seconds Over Tokyo". One of the Hollywood Ten, he refused to testify before the House Un-American Activities Committee (HUAC) in 1947 during the committee's investigation of communist influences in the motion picture industry. He, along with the other members of the Hollywood Ten and hundreds of other industry professionals, was subsequently blacklisted by that industry.

His talents as one of the top screenwriters allowed him to continue working clandestinely, producing work under other authors' names or pseudonyms. His uncredited work won two Academy Awards: for "Roman Holiday" (1953), which was given to a front writer, and for "The Brave One" (1956) which was awarded to a pseudonym of Trumbo's. When he was given public screen credit for both "Exodus" and "Spartacus" in 1960, this marked the beginning of the end of the Hollywood Blacklist for Trumbo and other screenwriters. He finally was given full credit by the Writers' Guild for all his achievements, the work of which encompassed six decades of screenwriting.

Trumbo was born in Montrose, Colorado, the son of Maud (née Tillery) and Orus Bonham Trumbo. His family moved to Grand Junction in 1908. He was proud of his paternal immigrant ancestor, a Protestant Swiss man named Jacob Trumbo, who settled in the colony of Virginia in 1736. Trumbo graduated from Grand Junction High School. While still in high school, he worked for Walter Walker as a cub reporter for the "Grand Junction Daily Sentinel," covering courts, the high school, the mortuary and civic organizations. He attended the University of Colorado at Boulder for two years, working as a reporter for the "Boulder Daily Camera" and contributing to the campus humor magazine, the yearbook, and the campus newspaper. He was a member of Delta Tau Delta International Fraternity.

For nine years after his father died, Trumbo worked the night shift wrapping bread at a Los Angeles bakery, and attended the University of Southern California. At the same time, he wrote movie reviews, 88 short stories and six novels, all of which were rejected for publication.

Trumbo began his professional writing career in the early 1930s, when several of his articles and stories were published in mainstream magazines, including the "Saturday Evening Post," "McCall's Magazine," "Vanity Fair," and the "Hollywood Spectator". In 1934 Trumbo was hired as managing editor of the "Hollywood Spectator." Later he left the magazine to become a reader in the story department at Warner Bros. studio.

His first published novel was "Eclipse" (1935), released during the Great Depression. Writing in the social realist style, Trumbo drew on his years in Grand Junction to portray a town and its people. The book was controversial in his home town, where many people took issue with his fictional portrayal. But years after his death, Trumbo was honored by installation of a statue of him in front of the Avalon Theater on Main Street, where he was depicted writing a screenplay in a bathtub.
Trumbo started working in movies in 1937 but continued writing prose. His anti-war novel "Johnny Got His Gun" won one of the early National Book Awards: the Most Original Book of 1939. It was inspired by an article Trumbo had read several years earlier, an account of a hospital visit by the Prince of Wales to a Canadian soldier who had lost all his limbs in World War I.

During the late 1930s and early 1940s, Trumbo became one of Hollywood's highest-paid screenwriters, at about $4000 per week while on assignment, and earning as much as $80,000 in one year. He worked on such films as "Thirty Seconds Over Tokyo" (1944), "Our Vines Have Tender Grapes" (1945), and "Kitty Foyle" (1940), for which he was nominated for an Academy Award for Writing Adapted Screenplay.

Trumbo aligned with the Communist Party in the United States before the 1940s, although he did not join the party until 1943. He was an isolationist. His novel "The Remarkable Andrew" featured the ghost of President Andrew Jackson appearing to caution the United States against getting involved in World War II. In a review of the book, "Time Magazine" wise-cracked, "General Jackson's opinions need surprise no one who has observed George Washington and Abraham Lincoln zealously following the Communist Party Line in recent years."

Shortly after the German invasion of the Soviet Union in 1941, Trumbo and his publisher decided to suspend reprinting "Johnny Got His Gun" until the end of the war. During the war, Trumbo received letters from individuals "denouncing Jews" and using "Johnny" to support their arguments for "an immediate negotiated peace" with Nazi Germany; Trumbo reported these correspondents to the FBI. Trumbo regretted this decision, which he called "foolish." After two FBI agents showed up at his home, he understood that "their interest lay not in the letters but in me."

In a 1946 article titled "The Russian Menace" published in Rob Wagner's "Script Magazine", Trumbo wrote from the perspective of a post-World War II Russian citizen. He argued that Russians were likely fearful of the mass of U.S. military power that surrounded them, at a time when any sympathetic view toward communist countries was viewed with suspicion. He ended the article by stating, "If I were a Russian ... I would be alarmed, and I would petition my government to take measures at once against what would seem an almost certain blow aimed at my existence. This is how it must appear in Russia today". He argued that the U.S. was a "menace" to Russia, rather than the more popular American view of Russia as the "red menace". According to anti-communist author Kenneth Billingsley in 2000, Trumbo had written in "The Daily Worker" that communist influence in Hollywood had prevented films from being made from anti-communist books, such as Arthur Koestler's "Darkness at Noon" and "The Yogi and the Commissar".

On July 29, 1946, William R. Wilkerson, publisher and founder of "The Hollywood Reporter", published a "TradeView" column entitled "A Vote For Joe Stalin". It named Trumbo and several others as Communist sympathizers, the first persons identified on what became known as "Billy's Blacklist". In October 1947, drawing upon these names, the House Un-American Activities Committee (HUAC) summoned Trumbo and nine others to testify for their investigation as to whether Communist agents and sympathizers had surreptitiously planted propaganda in U.S. films. The writers refused to give information about their own or any other person's involvement and were convicted for contempt of Congress. They appealed the conviction to the Supreme Court on First Amendment grounds and lost. In 1950, Trumbo served eleven months in the federal penitentiary in Ashland, Kentucky. In the 1976 documentary "Hollywood On Trial", Trumbo said "As far as I was concerned, it was a completely just verdict. I had contempt for that Congress and have had contempt for several since. And on the basis of guilt or innocence, I could never really complain very much. That this was a crime or misdemeanor was the complaint, my complaint."

Meanwhile, the MPAA had issued a statement that Trumbo and his compatriots would not be permitted to work in the industry, unless they disavowed Communism under oath. After completing his sentence, Trumbo sold his ranch and moved with his family to Mexico City with Hugo Butler and his wife Jean Rouverol, who had also been blacklisted. In Mexico Trumbo wrote 30 scripts under pseudonyms, for B-movie studios such as King Brothers Productions. In the case of "Gun Crazy" (1950), adapted from a short story by MacKinlay Kantor, Kantor agreed to be the front for Trumbo's screenplay. Trumbo's role in the screenplay was not revealed until 1992.

During this blacklist period, Trumbo also wrote "The Brave One" (1956) for the King Brothers. Like "Roman Holiday", it received an Academy Award for Best Story he couldn't claim. The script was credited to Robert Rich, a name borrowed from a nephew of the producers. Trumbo recalled earning an average fee of $1,750 per film for 18 screenplays written in two years and said, "None was very good".

In 1956 he published "The Devil in the Book", an analysis of the conviction of 14 California Smith Act defendants. The statute set criminal penalties for advocating the overthrow of the U.S. government and required all non-citizen adult residents to register with the government.

Gradually the blacklist weakened. With the support of director Otto Preminger, Trumbo was credited for the screenplay of the 1960 film "Exodus", adapted from the novel of the same name by Leon Uris. Shortly thereafter, actor Kirk Douglas announced Trumbo had written the screenplay for Stanley Kubrick's film "Spartacus". With these actions, Preminger and Douglas helped end the power of the blacklist. Trumbo was reinstated into the Writers Guild of America, West and was credited on all subsequent scripts. Eventually in 2011, the Guild gave him full credit for the script of "Roman Holiday". In 1971, Trumbo directed the film adaptation of his novel "Johnny Got His Gun", which starred Timothy Bottoms, Diane Varsi, Jason Robards and Donald Sutherland. One of the last films Trumbo wrote, "Executive Action" (1973), was based on the Kennedy assassination. In 1975, the Academy officially recognized Trumbo as the winner of the Oscar for "The Brave One" and presented him with a statuette.

In 1938, Trumbo married Cleo Fincher. She was born in Fresno on July 17, 1916, and moved with her divorced mother and her brother and sister to Los Angeles. Cleo Trumbo died of natural causes at the age of 93 on October 9, 2009, in Los Altos. At the time she was living with her younger daughter Mitzi. The Trumbos had three children: the filmmaker and screenwriter Christopher Trumbo, who became an expert on the Hollywood blacklist; Melissa, known as Mitzi, a photographer; and Nikola Trumbo, a psychotherapist. His daughter Mitzi dated comedian Steve Martin when they were both in their early 20s, which is recounted in Martin's 2007 book "Born Standing Up". Martin wrote of her, "Mitzi became my official photographer, and she snapped dozens of rolls of film, all to find the perfect publicity photo."

A long-time cigar smoker, Trumbo died in Los Angeles of a heart attack at the age of 70 on September 10, 1976. He donated his body to scientific research.

In 1993, Trumbo was posthumously awarded the Academy Award for writing "Roman Holiday" (1953). The screen credit and award were previously given to Ian McLellan Hunter, who had been a front for Trumbo. A new statue was made for this award because Hunter's son refused to hand over the one his father had received.

In 2003, Christopher Trumbo mounted an Off-Broadway play based on his father's letters called "Trumbo: Red, White and Blacklisted", in which a wide variety of actors played his father during the run, including Nathan Lane, Tim Robbins, Brian Dennehy, Ed Harris, Chris Cooper and Gore Vidal. He adapted it as the film "Trumbo" (2007), which added documentary footage and new interviews.

A dramatization of Trumbo's life, also called "Trumbo", was released in November 2015. It starred Bryan Cranston as the screenwriter and was directed by Jay Roach. For his portrayal of Trumbo, Cranston was nominated for Best Actor at the 88th Academy Awards.

The moving image collection of Trumbo is held at the Academy Film Archive and consists primarily of extensive 35mm production materials relating to the 1971 anti-war film "Johnny Got His Gun".







</doc>
<doc id="7930" url="https://en.wikipedia.org/wiki?curid=7930" title="Delaware">
Delaware

Delaware () is one of the 50 states of the United States, in the Mid-Atlantic or Northeastern region. Is it bordered to the south and west by Maryland, to the north by Pennsylvania, and to the east by New Jersey and the Atlantic Ocean. The state takes its name from Thomas West, 3rd Baron De La Warr, an English nobleman and Virginia's first colonial governor.

Delaware occupies the northeastern portion of the Delmarva Peninsula. It is the second smallest and sixth least populous state, but the sixth most densely populated. Delaware is divided into three counties, the lowest number of any state. From north to south, they are New Castle County, Kent County, and Sussex County. While the southern two counties have historically been predominantly agricultural, New Castle County is more industrialized.

Before its coastline was explored by Europeans in the 16th century, Delaware was inhabited by several groups of Native Americans, including the Lenape in the north and Nanticoke in the south. It was initially colonized by Dutch traders at Zwaanendael, near the present town of Lewes, in 1631. Delaware was one of the 13 colonies participating in the American Revolution. On December 7, 1787, Delaware became the first state to ratify the Constitution of the United States, and has since been known as "The First State".

The state was named after the Delaware River, which in turn derived its name from Thomas West, 3rd Baron De La Warr (1577–1618) who was the ruling governor of the Colony of Virginia at the time Europeans first explored the river. The Delaware Indians, a name used by Europeans for Lenape people indigenous to the Delaware Valley, also derive their name from the same source.

The surname "de La Warr" comes from Sussex and is of Anglo-Norman origin. It came probably from a Norman lieu-dit "La Guerre". This toponymic could derive from the Latin word "ager", from the Breton "gwern" or from the Late Latin "varectum" (fallow). The toponyms Gara, Gare, Gaire (the sound [ä] often mutated in [æ]) also appear in old texts cited by Lucien Musset, where the word "ga(i)ra" means gore. It could also be linked with a patronymic from the Old Norse "verr".

Delaware is long and ranges from to across, totaling , making it the second-smallest state in the United States after Rhode Island. Delaware is bounded to the north by Pennsylvania; to the east by the Delaware River, Delaware Bay, New Jersey and the Atlantic Ocean; and to the west and south by Maryland. Small portions of Delaware are also situated on the eastern side of the Delaware River sharing land boundaries with New Jersey. The state of Delaware, together with the Eastern Shore counties of Maryland and two counties of Virginia, form the Delmarva Peninsula, which stretches down the Mid-Atlantic Coast.

The definition of the northern boundary of the state is unusual. Most of the boundary between Delaware and Pennsylvania was originally defined by an arc extending from the cupola of the courthouse in the city of New Castle. This boundary is often referred to as the Twelve-Mile Circle. This is the only nominally circular state boundary in the United States.

This border extends all the way east to the low-tide mark on the New Jersey shore, then continues south along the shoreline until it again reaches the 12-mile (19 km) arc in the south; then the boundary continues in a more conventional way in the middle of the main channel (thalweg) of the Delaware River. To the west, a portion of the arc extends past the easternmost edge of Maryland. The remaining western border runs slightly east of due south from its intersection with the arc. The Wedge of land between the northwest part of the arc and the Maryland border was claimed by both Delaware and Pennsylvania until 1921, when Delaware's claim was confirmed.

Delaware is on a level plain, with the lowest mean elevation of any state in the nation. Its highest elevation, located at Ebright Azimuth, near Concord High School, is less than above sea level. The northernmost part of the state is part of the Piedmont Plateau with hills and rolling surfaces. The Atlantic Seaboard fall line approximately follows the Robert Kirkwood Highway between Newark and Wilmington; south of this road is the Atlantic Coastal Plain with flat, sandy, and, in some parts, swampy ground. A ridge about in elevation extends along the western boundary of the state and separates the watersheds that feed Delaware River and Bay to the east and the Chesapeake Bay to the west.

Since almost all of Delaware is a part of the Atlantic coastal plain, the effects of the ocean moderate its climate. The state lies in the humid subtropical climate zone. Despite its small size (roughly from its northernmost to southernmost points), there is significant variation in mean temperature and amount of snowfall between Sussex County and New Castle County. Moderated by the Atlantic Ocean and Delaware Bay, the southern portion of the state has a milder climate and a longer growing season than the northern portion of the state. Delaware's all-time record high of was recorded at Millsboro on July 21, 1930. The all-time record low of was also recorded at Millsboro on January 17, 1893.

The transitional climate of Delaware supports a wide variety of vegetation. In the northern third of the state are found Northeastern coastal forests and mixed oak forests typical of the northeastern United States. In the southern two-thirds of the state are found Middle Atlantic coastal forests. Trap Pond State Park, along with areas in other parts of Sussex County, for example, support the northernmost stands of bald cypress trees in North America.

Delaware provides government subsidy support for the clean-up of property "lightly contaminated" by hazardous waste, the proceeds for which come from a tax on wholesale petroleum sales.

Before Delaware was settled by European colonists, the area was home to the Eastern Algonquian tribes known as the Unami Lenape, or Delaware, who lived mostly along the coast, and the Nanticoke who occupied much of the southern Delmarva Peninsula. John Smith also shows two Iroquoian tribes, the Kuskarawock & Tockwogh, living north of the Nanticoke & they may have held small portions of land in the western part of the state before migrating across the Chesapeake Bay. The Kuskarawocks were most likely the Tuscarora.

The Unami Lenape in the Delaware Valley were closely related to Munsee Lenape tribes along the Hudson River. They had a settled hunting and agricultural society, and they rapidly became middlemen in an increasingly frantic fur trade with their ancient enemy, the Minqua or Susquehannock. With the loss of their lands on the Delaware River and the destruction of the Minqua by the Iroquois of the Five Nations in the 1670s, the remnants of the Lenape who wished to remain identified as such left the region and moved over the Alleghany Mountains by the mid-18th century. Generally, those who did not relocate out of the state of Delaware were baptized, became Christian and were grouped together with other persons of color in official records and in the minds of their non-Native American neighbors.

The Dutch were the first Europeans to settle in present-day Delaware in the middle region by establishing a trading post at Zwaanendael, near the site of Lewes in 1631. Within a year all the settlers were killed in a dispute with area Native American tribes. In 1638 New Sweden, a Swedish trading post and colony, was established at Fort Christina (now in Wilmington) by Peter Minuit at the head of a group of Swedes, Finns and Dutch. The colony of New Sweden lasted for 17 years. In 1651 the Dutch, reinvigorated by the leadership of Peter Stuyvesant, established a fort at present-day New Castle, and in 1655 they conquered the New Sweden colony, annexing it into the Dutch New Netherland. Only nine years later, in 1664, the Dutch were conquered by a fleet of English ships by Sir Robert Carr under the direction of James, the Duke of York. Fighting off a prior claim by Cecil Calvert, 2nd Baron Baltimore, Proprietor of Maryland, the Duke passed his somewhat dubious ownership on to William Penn in 1682. Penn strongly desired access to the sea for his Pennsylvania province and leased what then came to be known as the "Lower Counties on the Delaware" from the Duke.

Penn established representative government and briefly combined his two possessions under one General Assembly in 1682. However, by 1704 the Province of Pennsylvania had grown so large that their representatives wanted to make decisions without the assent of the Lower Counties and the two groups of representatives began meeting on their own, one at Philadelphia, and the other at New Castle. Penn and his heirs remained proprietors of both and always appointed the same person Governor for their Province of Pennsylvania and their territory of the Lower Counties. The fact that Delaware and Pennsylvania shared the same governor was not unique. From 1703 to 1738 New York and New Jersey shared a governor. Massachusetts and New Hampshire also shared a governor for some time.

Dependent in early years on indentured labor, Delaware imported more slaves as the number of English immigrants decreased with better economic conditions in England. The colony became a slave society and cultivated tobacco as a cash crop, although English immigrants continued to arrive.

Like the other middle colonies, the Lower Counties on the Delaware initially showed little enthusiasm for a break with Britain. The citizenry had a good relationship with the Proprietary government, and generally were allowed more independence of action in their Colonial Assembly than in other colonies. Merchants at the port of Wilmington had trading ties with the British.

So it was that New Castle lawyer Thomas McKean denounced the Stamp Act in the strongest terms, and Kent County native John Dickinson became the "Penman of the Revolution." Anticipating the Declaration of Independence, Patriot leaders Thomas McKean and Caesar Rodney convinced the Colonial Assembly to declare itself separated from British and Pennsylvania rule on June 15, 1776. The person best representing Delaware's majority, George Read, could not bring himself to vote for a Declaration of Independence. Only the dramatic overnight ride of Caesar Rodney gave the delegation the votes needed to cast Delaware's vote for independence.

Initially led by John Haslet, Delaware provided one of the premier regiments in the Continental Army, known as the "Delaware Blues" and nicknamed the "Blue Hen's Chicks." In August 1777 General Sir William Howe led a British army through Delaware on his way to a victory at the Battle of Brandywine and capture of the city of Philadelphia. The only real engagement on Delaware soil was the Battle of Cooch's Bridge, fought on September 3, 1777, at Cooch's Bridge in New Castle County, although there was a minor Loyalist rebellion in 1778.

Following the Battle of Brandywine, Wilmington was occupied by the British, and State President John McKinly was taken prisoner. The British remained in control of the Delaware River for much of the rest of the war, disrupting commerce and providing encouragement to an active Loyalist portion of the population, particularly in Sussex County. Because the British promised slaves of rebels freedom for fighting with them, escaped slaves flocked north to join their lines.

Following the American Revolution, statesmen from Delaware were among the leading proponents of a strong central United States with equal representation for each state.

Many colonial settlers came to Delaware from Maryland and Virginia, which had been experiencing a population boom. The economies of these colonies were chiefly based on tobacco culture and were increasingly dependent on slave labor for its intensive cultivation. Most of the English colonists arrived as indentured servants, under contracts to work as laborers for a fixed period to pay for their passage. In the early years the line between indentured servants and African slaves or laborers was fluid, and the working classes often lived closely together. Most of the free African-American families in Delaware before the Revolution had migrated from Maryland to find more affordable land. They were descendants chiefly of relationships or marriages between white servant women and enslaved, servant or free African or African-American men. As the flow of indentured laborers to the colony decreased with improving economic conditions in England, more slaves were imported for labor and the caste lines hardened.

At the end of the colonial period, the number of enslaved people in Delaware began to decline. Shifts in the agriculture economy from tobacco to mixed farming created less need for slaves' labor. Local Methodists and Quakers encouraged slaveholders to free their slaves following the American Revolution, and many did so in a surge of individual manumissions for idealistic reasons. By 1810 three-quarters of all blacks in Delaware were free. When John Dickinson freed his slaves in 1777, he was Delaware's largest slave owner with 37 slaves. By 1860, the largest slaveholder owned 16 slaves.

Although attempts to abolish slavery failed by narrow margins in the legislature, in practical terms, the state had mostly ended the practice. By the 1860 census on the verge of the Civil War, 91.7% of the black population were free; 1,798 were slaves, as compared to 19,829 "free colored persons".

The independent black denomination was chartered by freed slave Peter Spencer in 1813 as the "Union Church of Africans". This followed the 1793 establishment of the African Methodist Episcopal Church in Philadelphia, which had ties to the Methodist Episcopal Church until 1816. Spencer built a church in Wilmington for the new denomination.

This was renamed the African Union First Colored Methodist Protestant Church and Connection, more commonly known as the A.U.M.P. Church. Begun by Spencer in 1814, the annual gathering of the Big August Quarterly still draws people together in a religious and cultural festival, the oldest such cultural festival in the nation.

Delaware voted against secession on January 3, 1861, and so remained in the Union. While most Delaware citizens who fought in the war served in the regiments of the state, some served in companies on the Confederate side in Maryland and Virginia Regiments. Delaware is notable for being the only slave state from which no Confederate regiments or militia groups were assembled. Delaware essentially freed the few slaves that were still in bondage shortly after the Civil War, but rejected the 13th, 14th, and 15th Amendments to the Constitution; the 13th Amendment was rejected on February 8, 1865, the 14th Amendment was rejected on February 8, 1867, and the 15th Amendment was rejected on March 18, 1869. Delaware officially ratified the 13th, 14th, and 15th amendments on February 12, 1901.

The United States Census Bureau estimates that the population of Delaware was 952,065 people on July 1, 2016, a 6.0% increase since the 2010 United States Census.

According to the 2010 United States Census, Delaware had a population of 897,934 people. The racial composition of the state was:

Ethnically, Hispanics and Latinos of any race made up 8.2% of the population.

Delaware is the sixth most densely populated state, with a population density of 442.6 people per square mile, 356.4 per square mile more than the national average, and ranking 45th in population. Delaware is one of five states that do not have a single city with a population over 100,000 as of the 2010 census, the other four being West Virginia, Vermont, Maine and Wyoming. The center of population of Delaware is in New Castle County, in the town of Townsend.

As of 2011, 49.7% of Delaware's population younger than one year of age belonged to minority groups (i.e., did not have two parents of non-Hispanic white ancestry). In 2000 approximately 19% of the population were African-American and 5% of the population is Hispanic (mostly of Puerto Rican or Mexican ancestry).

"Note: Births in table don't add up because Hispanics are counted both by their ethnicity and by their race, giving a higher overall number."

As of 2000 91% of Delaware residents age 5 and older speak only English at home; 5% speak Spanish. French is the third most spoken language at 0.7%, followed by Chinese at 0.5% and German at 0.5%.

Legislation had been proposed in both the House and the Senate in Delaware to designate English as the official language. Neither bill was passed in the legislature.

As of the year 2010, The Association of Religion Data Archives reported that the three largest denominational groups in Delaware by number of adherents are the Catholic Church at 182,532 adherents, the United Methodist Church with 53,656 members reported, and non-denominational Evangelical Protestant with 22,973 adherents reported. The religious body with the largest number of congregations is the United Methodist Church (with 158 congregations) followed by non-denominational Evangelical Protestant (with 106 congregations), then the Catholic Church (with 45 congregations).

The Roman Catholic Diocese of Wilmington and the Episcopal Diocese of Delaware oversee the parishes within their denominations. The A.U.M.P. Church, the oldest African-American denomination in the nation, was founded in Wilmington. It still has a substantial presence in the state. Reflecting new immigrant populations, an Islamic mosque has been built in the Ogletown area, and a Hindu temple in Hockessin.

Delaware is home to an Amish community that resides to the west of Dover in Kent County, consisting of 9 church districts and between 1,200 and 1,500 people. The Amish first settled in Kent County in 1915. In recent years, increasing development has led to the decline in the number of Amish living in the community.

A 2012 survey of religious attitudes in the United States found that 34% of Delaware residents considered themselves "moderately religious," 33% "very religious," and 33% as "non-religious."

A 2012 Gallup poll found that Delaware's proportion of lesbian, gay, bisexual, and transgender adults stood at 3.4 percent of the population. This constitutes a total LGBT adult population estimate of 23,698 people. The number of same-sex couple households in 2010 stood at 2,646. This grew by 41.65% from a decade earlier. On July 1, 2013, same-sex marriage was legalized, and all civil unions would be converted into marriages.

According to a 2013 study by Phoenix Marketing International, Delaware had the ninth-largest number of millionaires per capita in the United States, with a ratio of 6.20 percent.

Delaware's agricultural output consists of poultry, nursery stock, soybeans, dairy products and corn.

As of October 2015, the state's unemployment rate was 5.1%.

The state's largest employers are:

Dover Air Force Base, located next to the state capital of Dover, is one of the largest Air Force bases in the country and is a major employer in Delaware. In addition to its other responsibilities in the United States Air Force Air Mobility Command, this air base serves as the entry point and mortuary for American military personnel and some U.S. government civilians who die overseas.

Since the mid-2000s, Delaware has seen the departure of the state's automotive manufacturing industry (General Motors Wilmington Assembly and Chrysler Newark Assembly), the corporate buyout of a major bank holding company (MBNA), the departure of the state's steel industry (Evraz Claymont Steel), the bankruptcy of a fiber mill (National Vulcanized Fibre), and the diminishing presence of Astra Zeneca in Wilmington.

In late 2015, DuPont announced that 1,700 employees, nearly a third of its footprint in Delaware, would be laid off in early 2016. The merger of E.I. du Pont de Nemours & Co. and Dow Chemical Company into DowDuPont took place on September 1, 2017.

More than 50% of all U.S. publicly traded companies and 63% of the Fortune 500 are incorporated in Delaware. The state's attractiveness as a corporate haven is largely because of its business-friendly corporation law. Franchise taxes on Delaware corporations supply about one-fifth of its state revenue. Although "USA (Delaware)" ranked as the world's most opaque jurisdiction on the Tax Justice Network's 2009 Financial Secrecy Index, the same group's 2011 Index ranks the USA fifth and does not specify Delaware. In Delaware, there are more than a million registered corporations, meaning there are more corporations than people.

 stipulates that alcoholic liquor only be sold in specifically licensed establishments, and only between 9:00 am and 1:00 am. Until 2003, Delaware was among the several states enforcing blue laws and banned the sale of liquor on Sunday.

The transportation system in Delaware is under the governance and supervision of the Delaware Department of Transportation, also known as "DelDOT". Funding for DelDOT projects is drawn, in part, from the Delaware Transportation Trust Fund, established in 1987 to help stabilize transportation funding; the availability of the Trust led to a gradual separation of DelDOT operations from other Delaware state operations. DelDOT manages programs such as a Delaware Adopt-a-Highway program, major road route snow removal, traffic control infrastructure (signs and signals), toll road management, Delaware Division of Motor Vehicles, the Delaware Transit Corporation (branded as "DART First State", the state government public transportation organization), among others. In 2009, DelDOT maintained 13,507 lane miles of roads, totaling 89 percent of the state's public roadway system; the remaining public road miles are under the supervision of individual municipalities. This far exceeds the United States national average of 20 percent for state department of transportation maintenance responsibility.

The "DART First State" public transportation system was named "Most Outstanding Public Transportation System" in 2003 by the American Public Transportation Association. Coverage of the system is broad within northern New Castle County with close association to major highways in Kent and Sussex counties. The system includes bus, subsidized passenger rail operated by Philadelphia transit agency SEPTA, and subsidized taxi and paratransit modes. The paratransit system, consisting of a statewide door-to-door bus service for the elderly and disabled, has been described by a Delaware state report as "the most generous paratransit system in the United States." , fees for the paratransit service have not changed since 1988.

One major branch of the U.S. Interstate Highway System, Interstate 95 (I-95), crosses Delaware southwest-to-northeast across New Castle County. In addition to I-95, there are six U.S. highways that serve Delaware: U.S. Route 9 (US 9), US 13, US 40, US 113, US 202, and US 301. There are also several state highways that cross the state of Delaware; a few of them include Delaware Route 1 (DE 1), DE 9, and DE 404. US 13 and DE 1 are primary north-south highways connecting Wilmington and Pennsylvania with Maryland, with DE 1 serving as the main route between Wilmington and the Delaware beaches. DE 9 is a north-south highway connecting Dover and Wilmington via a scenic route along the Delaware Bay. US 40, is a primary east-west route, connecting Maryland with New Jersey. DE 404 is another primary east-west highway connecting the Chesapeake Bay Bridge in Maryland with the Delaware beaches. The state also operates two toll highways, the Delaware Turnpike, which is I-95, between Maryland and New Castle and the Korean War Veterans Memorial Highway, which is DE 1, between Wilmington and Dover.

A bicycle route, Delaware Bicycle Route 1, spans the north-south length of the state from the Maryland border in Fenwick Island to the Pennsylvania border north of Montchanin. It is the first of several signed bike routes planned in Delaware.

Delaware has around 1,450 bridges, 95 percent of which are under the supervision of DelDOT. About 30 percent of all Delaware bridges were built before 1950, and about 60 percent of the number are included in the National Bridge Inventory. Some bridges not under DelDOT supervision includes the four bridges on the Chesapeake and Delaware Canal, which are under the jurisdiction of the U.S. Army Corps of Engineers, and the Delaware Memorial Bridge, which is under the bi-state Delaware River and Bay Authority.

It has been noted that the tar and chip composition of secondary roads in Sussex County make them more prone to deterioration than asphalt roadways found in almost the rest of the state. Among these roads, Sussex (county road) 236 is among the most problematic.

There are three ferries that operate in the state of Delaware:

Amtrak has two stations in Delaware along the Northeast Corridor; the relatively quiet Newark Rail Station in Newark, and the busier Wilmington Rail Station in Wilmington. The Northeast Corridor is also served by SEPTA's Wilmington/Newark Line of Regional Rail, which serves Claymont, Wilmington, Churchmans Crossing, and Newark.

Two Class I railroads, Norfolk Southern and CSX, provide freight rail service in northern New Castle County. Norfolk Southern provides freight service along the Northeast Corridor and to industrial areas in Edgemoor, New Castle, and Delaware City. CSX's Philadelphia Subdivision passes through northern New Castle County parallel to the Amtrak Northeast Corridor. Multiple short-line railroads provide freight service in Delaware. The Delmarva Central Railroad operates the most trackage of the short-line railroads, running from an interchange with Norfolk Southern in Porter south through Dover, Harrington, and Seaford to Delmar, with another line running from Harrington to Frankford. The Delmarva Central Railroad connects with two shortline railroads, the Delaware Coast Line Railroad and the Maryland and Delaware Railroad, which serve local customers in Sussex County. CSX connects with the freight/heritage operation, the Wilmington and Western Railroad, based in Wilmington and the East Penn Railroad, which operates a line from Wilmington to Coatesville, Pennsylvania.

The last north-south passenger train through the main part of Delaware was the Pennsylvania Railroad's "The Cavalier," which ended service from Philadelphia through the state's interior in 1951.

, there is no scheduled air service from any Delaware airport, as has been the case in various years since 1991. Various airlines had served Wilmington Airport, with the latest departure being Frontier Airlines in April 2015.

Delaware is centrally situated in the Northeast megalopolis region of cities along I-95. Therefore, Delaware commercial airline passengers most frequently use Philadelphia International Airport (PHL), Baltimore-Washington International Thurgood Marshall Airport (BWI) and Washington Dulles International Airport (IAD) for domestic and international transit. Residents of Sussex County will also use Wicomico Regional Airport (SBY), as it is located less than from the Delaware border. Atlantic City International Airport (ACY), Newark Liberty International Airport (EWR), and Ronald Reagan Washington National Airport (DCA) are also within a radius of New Castle County.

The Dover Air Force Base of the Air Mobility Command is in the central part of the state, and it is the home of the 436th Airlift Wing and the 512th Airlift Wing.

Other general aviation airports in Delaware include Summit Airport near Middletown, Delaware Airpark near Cheswold, and Delaware Coastal Airport near Georgetown.

Delaware's fourth and current constitution, adopted in 1897, provides for executive, judicial and legislative branches.

The Delaware General Assembly consists of a House of Representatives with 41 members and a Senate with 21 members. It sits in Dover, the state capital. Representatives are elected to two-year terms, while senators are elected to four-year terms. The Senate confirms judicial and other nominees appointed by the governor.

Delaware's U.S. Senators are Tom Carper (Democrat) and Chris Coons (Democrat). Delaware's single U.S. Representative is Lisa Blunt Rochester (Democrat).

The Delaware Constitution establishes a number of courts:

Minor non-constitutional courts include the Justice of the Peace Courts and Aldermen's Courts.

Significantly, Delaware has one of the few remaining Courts of Chancery in the nation, which has jurisdiction over equity cases, the vast majority of which are corporate disputes, many relating to mergers and acquisitions. The Court of Chancery and the Delaware Supreme Court have developed a worldwide reputation for rendering concise opinions concerning corporate law which generally (but not always) grant broad discretion to corporate boards of directors and officers. In addition, the Delaware General Corporation Law, which forms the basis of the Courts' opinions, is widely regarded as giving great flexibility to corporations to manage their affairs. For these reasons, Delaware is considered to have the most business-friendly legal system in the United States; therefore a great number of companies are incorporated in Delaware, including 60% of the companies listed on the New York Stock Exchange. Delaware was the last U.S. state to use judicial corporal punishment, in 1952.

The executive branch is headed by the Governor of Delaware. The present governor is John Carney (Democrat), who took office January 17, 2017. The lieutenant governor is Bethany Hall-Long. The governor presents a "State of the State" speech to a joint session of the Delaware legislature annually.

Delaware is subdivided into three counties; from north to south they are New Castle, Kent and Sussex. This is the fewest among all states. Each county elects its own legislative body (known in New Castle and Sussex counties as County Council, and in Kent County as Levy Court), which deal primarily in zoning and development issues. Most functions which are handled on a county-by-county basis in other states – such as court and law enforcement – have been centralized in Delaware, leading to a significant concentration of power in the Delaware state government. The counties were historically divided into hundreds, which were used as tax reporting and voting districts until the 1960s, but now serve no administrative role, their only current official legal use being in real-estate title descriptions.

The Democratic Party holds a plurality of registrations in Delaware. Until the 2000 presidential election, the state tended to be a Presidential bellwether, sending its three electoral votes to the winning candidate since 1952. This trend ended in 2000 when Delaware's electoral votes went to Al Gore. In 2004 John Kerry won Delaware by eight percentage points. In 2008 Democrat Barack Obama defeated Republican John McCain in Delaware 62.63% to 37.37%. Obama's running mate was Joe Biden, who had represented Delaware in the United States Senate since 1973. Obama carried Delaware again in 2012. In 2016, Delaware's electoral votes went to Hillary Clinton.

Delaware's swing to the Democrats is in part due to a strong Democratic trend in New Castle County, home to 55 percent of Delaware's population (the two smaller counties have only 359,000 people between them to New Castle's 535,000). New Castle has not voted Republican in a presidential election since 1988. In 1992, 2000, 2004, and 2016, the Republican presidential candidate carried both Kent and Sussex but lost by double-digits each time in New Castle, which was a large enough margin to swing the state to the Democrats. New Castle also elects a substantial majority of the legislature; 27 of the 41 state house districts and 14 of the 21 state senate districts are based in New Castle.

The Democrats have held the governorship since 1993, having won the last six gubernatorial elections in a row. Democrats presently hold seven of the nine statewide elected offices, while the Republicans hold only two statewide offices, State Auditor and State Treasurer.

Each of the 50 states of the United States has passed some form of freedom of information legislation, which provides a mechanism for the general public to request information of the government. In 2011 Delaware passed legislation placing a 15 business day time limit on addressing freedom-of-information requests, to either produce information or an explanation of why such information would take longer than this time to produce.

Delaware has six different income tax brackets, ranging from 2.2% to 5.95%. The state does not assess sales tax on consumers. The state does, however, impose a tax on the gross receipts of most businesses. Business and occupational license tax rates range from 0.096% to 1.92%, depending on the category of business activity.

Delaware does not assess a state-level tax on real or personal property. Real estate is subject to county property taxes, school district property taxes, vocational school district taxes, and, if located within an incorporated area, municipal property taxes.

Gambling provides significant revenue to the state. For instance, the casino at Delaware Park Racetrack provided more than $100 million USD to the state in 2010.

In June 2018, Delaware became the first US state to legalize sports betting following the Supreme Court ruling to repeal The Professional and Amateur Sports Protection Act (PASPA).

Wilmington is the state's largest city and its economic hub. It is located within commuting distance of both Philadelphia and Baltimore. All regions of Delaware are enjoying phenomenal growth, with Dover and the beach resorts expanding at a rapid rate.






Delaware was the origin of "Belton v. Gebhart", one of the four cases which were combined into "Brown v. Board of Education", the Supreme Court of the United States decision that led to the end of officially segregated public schools. Significantly, "Belton" was the only case in which the state court found for the plaintiffs, thereby ruling that segregation was unconstitutional.

Unlike many states, Delaware's educational system is centralized in a state Superintendent of Education, with local school boards retaining control over taxation and some curriculum decisions.

, the Delaware Department of Education had authorized the founding of 25 charter schools in the state, one of them being all-girls.

All teachers in the State's public school districts are unionized. , none of the State's charter schools are members of a teachers union. One of the State's teachers' unions is Delaware State Education Association (DSEA), whose President as of January 2012 is Frederika Jenner.


Delaware's sister state in Japan is Miyagi Prefecture.

The northern part of the state is served by network stations in Philadelphia and the southern part by network stations in Baltimore and Salisbury, Maryland. Philadelphia's ABC affiliate, WPVI-TV, maintains a news bureau in downtown Wilmington. Salisbury's ABC affiliate, WMDT covers Sussex and lower Kent County; while CBS affiliate, WBOC-TV, maintains bureaus in Dover and Milton.

Few television stations are based solely in Delaware; the local PBS station from Philadelphia (but licensed to Wilmington), WHYY-TV, maintains a studio and broadcasting facility in Wilmington and Dover, Ion Television affiliate WPPX is licensed to Wilmington but maintains their offices in Philadelphia and their digital transmitter outside of that city and an analog tower in New Jersey, and MeTV affiliate KJWP is licensed to Wilmington but maintains their offices in New Jersey and their transmitter is located at the antenna farm in Philadelphia.

In April 2014, it was revealed that Rehoboth Beach's WRDE-LD would affiliate with NBC, becoming the first major network-affiliated station in Delaware.

In addition to First State National Historical Park, Delaware has several museums, , , , , and other .

Rehoboth Beach, together with the towns of Lewes, Dewey Beach, Bethany Beach, South Bethany, and Fenwick Island, comprise Delaware's beach resorts. Rehoboth Beach often bills itself as "The Nation's Summer Capital" because it is a frequent summer vacation destination for Washington, D.C. residents as well as visitors from Maryland, Virginia, and in lesser numbers, Pennsylvania. Vacationers are drawn for many reasons, including the town's charm, artistic appeal, nightlife, and tax free shopping. According to SeaGrant Delaware, the Delaware Beaches generate $6.9 billion annually and over $711 million in tax revenue.

Delaware is home to several festivals, fairs, and events. Some of the more notable festivals are the Riverfest held in Seaford, the World Championship Punkin Chunkin formerly held at various locations throughout the state since 1986, the Rehoboth Beach Chocolate Festival, the Bethany Beach Jazz Funeral to mark the end of summer, the Apple Scrapple Festival held in Bridgeville, the Clifford Brown Jazz Festival in Wilmington, the Rehoboth Beach Jazz Festival, the Sea Witch Halloween Festival and Parade in Rehoboth Beach, the Rehoboth Beach Independent Film Festival, the Nanticoke Indian Pow Wow in Oak Orchard, Firefly Music Festival, and the Return Day Parade held after every election in Georgetown.

In 2015, tourism in Delaware generated $3.1 billion, which makes up of 5 percent of the state's GDP. Delaware saw 8.5 million visitors in 2015, with the tourism industry employing 41,730 people, making it the 4th largest private employer in the state. Major origin markets for Delaware tourists include Philadelphia, Baltimore, New York City, Washington, D.C., and Harrisburg, with 97% of tourists arriving to the state by car and 75% of tourists coming from or less.

As Delaware has no franchises in the major American professional sports leagues, many Delawareans follow either Philadelphia or Baltimore teams. In the WNBA, the Washington Mystics enjoy a major following due to the presence of Wilmington native and University of Delaware product Elena Delle Donne. The University of Delaware's football team has a large following throughout the state with the Delaware State University and Wesley College teams also enjoying a smaller degree of support.

Delaware is home to Dover International Speedway and Dover Downs. DIS, also known as the "Monster Mile", hosts two NASCAR race weekends each year, one in the late spring and one in the early fall. Dover Downs is a popular harness racing facility. It is the only co-located horse and car-racing facility in the nation, with the Dover Downs track inside the DIS track.

Delaware is represented in USA Rugby League by 2015 expansion club, the Delaware Black Foxes.

Delaware has been home to professional wrestling outfit Combat Zone Wrestling (CZW). CZW has been affiliated with the annual Tournament of Death and ECWA with its annual Super 8 Tournament.

Delaware's official state sport is bicycling.

Delaware is also the name of a Native American group (called in their own language Lenni Lenape) that was influential in the colonial period of the United States and is today headquartered in Cheswold, Kent County, Delaware. A band of the Nanticoke tribe of American Indians today resides in Sussex County and is headquartered in Millsboro, Sussex County, Delaware.


History
General


</doc>
<doc id="7931" url="https://en.wikipedia.org/wiki?curid=7931" title="Dictionary">
Dictionary

A dictionary, sometimes known as a wordbook, is a collection of words in one or more specific languages, often arranged alphabetically (or by radical and stroke for ideographic languages), which may include information on definitions, usage, etymologies, pronunciations, translation, etc. or a book of words in one language with their equivalents in another, sometimes known as a lexicon. It is a lexicographical product which shows inter-relationships among the data.

A broad distinction is made between general and specialized dictionaries. Specialized dictionaries include words in specialist fields, rather than a complete range of words in the language. Lexical items that describe concepts in specific fields are usually called terms instead of words, although there is no consensus whether lexicology and terminology are two different fields of study. In theory, general dictionaries are supposed to be semasiological, mapping word to definition, while specialized dictionaries are supposed to be onomasiological, first identifying concepts and then establishing the terms used to designate them. In practice, the two approaches are used for both types. There are other types of dictionaries that do not fit neatly into the above distinction, for instance bilingual (translation) dictionaries, dictionaries of synonyms (thesauri), and rhyming dictionaries. The word dictionary (unqualified) is usually understood to refer to a general purpose monolingual dictionary.

There is also a contrast between "prescriptive" or "descriptive" dictionaries; the former reflect what is seen as correct use of the language while the latter reflect recorded actual use. Stylistic indications (e.g. "informal" or "vulgar") in many modern dictionaries are also considered by some to be less than objectively descriptive.

Although the first recorded dictionaries date back to Sumerian times (these were bilingual dictionaries), the systematic study of dictionaries as objects of scientific interest themselves is a 20th-century enterprise, called lexicography, and largely initiated by Ladislav Zgusta. The birth of the new discipline was not without controversy, the practical dictionary-makers being sometimes accused by others of "astonishing" lack of method and critical-self reflection.

The oldest known dictionaries were Akkadian Empire cuneiform tablets with bilingual Sumerian–Akkadian wordlists, discovered in Ebla (modern Syria) and dated roughly 2300 BCE. The early 2nd millennium BCE "Urra=hubullu" glossary is the canonical Babylonian version of such bilingual Sumerian wordlists. A Chinese dictionary, the c. 3rd century BCE "Erya", was the earliest surviving monolingual dictionary; although some sources cite the c. 800 BCE Shizhoupian as a "dictionary", modern scholarship considers it a calligraphic compendium of Chinese characters from Zhou dynasty bronzes. Philitas of Cos (fl. 4th century BCE) wrote a pioneering vocabulary "Disorderly Words" (Ἄτακτοι γλῶσσαι, "") which explained the meanings of rare Homeric and other literary words, words from local dialects, and technical terms. Apollonius the Sophist (fl. 1st century CE) wrote the oldest surviving Homeric lexicon. The first Sanskrit dictionary, the Amarakośa, was written by Amara Sinha c. 4th century CE. Written in verse, it listed around 10,000 words. According to the "Nihon Shoki", the first Japanese dictionary was the long-lost 682 CE "Niina" glossary of Chinese characters. The oldest existing Japanese dictionary, the c. 835 CE "Tenrei Banshō Meigi", was also a glossary of written Chinese. In "Frahang-i Pahlavig", Aramaic heterograms are listed together with their translation in Middle Persian language and phonetic transcription in Pazand alphabet. A 9th-century CE Irish dictionary, Sanas Cormaic, contained etymologies and explanations of over 1,400 Irish words. In India around 1320, Amir Khusro compiled the Khaliq-e-bari which mainly dealt with Hindustani and Persian words.
Arabic dictionaries were compiled between the 8th and 14th centuries CE, organizing words in rhyme order (by the last syllable), by alphabetical order of the radicals, or according to the alphabetical order of the first letter (the system used in modern European language dictionaries). The modern system was mainly used in specialist dictionaries, such as those of terms from the Qur'an and hadith, while most general use dictionaries, such as the "Lisan al-`Arab" (13th century, still the best-known large-scale dictionary of Arabic) and "al-Qamus al-Muhit" (14th century) listed words in the alphabetical order of the radicals. The "Qamus al-Muhit" is the first handy dictionary in Arabic, which includes only words and their definitions, eliminating the supporting examples used in such dictionaries as the "Lisan" and the "Oxford English Dictionary".
In medieval Europe, glossaries with equivalents for Latin words in vernacular or simpler Latin were in use (e.g. the Leiden Glossary). The "Catholicon" (1287) by Johannes Balbus, a large grammatical work with an alphabetical lexicon, was widely adopted. It served as the basis for several bilingual dictionaries and was one of the earliest books (in 1460) to be printed. In 1502 Ambrogio Calepino's "Dictionarium" was published, originally a monolingual Latin dictionary, which over the course of the 16th century was enlarged to become a multilingual glossary. In 1532 Robert Estienne published the "Thesaurus linguae latinae" and in 1572 his son Henri Estienne published the "Thesaurus linguae graecae", which served up to the 19th century as the basis of Greek lexicography. The first monolingual dictionary written in Europe was the Spanish, written by Sebastián Covarrubias' "Tesoro de la lengua castellana o española", published in 1611 in Madrid, Spain. In 1612 the first edition of the "Vocabolario dell'Accademia della Crusca", for Italian, was published. It served as the model for similar works in French and English. In 1690 in Rotterdam was published, posthumously, the "Dictionnaire Universel" by Antoine Furetière for French. In 1694 appeared the first edition of the "Dictionnaire de l'Académie française". Between 1712 and 1721 was published the "Vocabulario portughez e latino" written by Raphael Bluteau. The Real Academia Española published the first edition of the "Diccionario de la lengua española" in 1780, but their "Diccionario de Autoridades", which included quotes taken from literary works, was published in 1726. The "Totius Latinitatis lexicon" by Egidio Forcellini was firstly published in 1777; it has formed the basis of all similar works that have since been published.

The first edition of "A Greek-English Lexicon" by Henry George Liddell and Robert Scott appeared in 1843; this work remained the basic dictionary of Greek until the end of the 20th century. And in 1858 was published the first volume of the Deutsches Wörterbuch by the Brothers Grimm; the work was completed in 1961. Between 1861 and 1874 was published the "Dizionario della lingua italiana" by Niccolò Tommaseo. Between 1862 and 1874 was published the six volumes of "A magyar nyelv szótára" (Dictionary of Hungarian Language) by Gergely Czuczor and János Fogarasi. Émile Littré published the Dictionnaire de la langue française between 1863 and 1872. In the same year 1863 appeared the first volume of the "Woordenboek der Nederlandsche Taal" which was completed in 1998. Also in 1863 Vladimir Ivanovich Dahl published the "Explanatory Dictionary of the Living Great Russian Language". The Duden dictionary dates back to 1880, and is currently the prescriptive source for the spelling of German. The decision to start work on the "Svenska Akademiens ordbok" was taken in 1787.

The earliest dictionaries in the English language were glossaries of French, Spanish or Latin words along with their definitions in English. The word "dictionary" was invented by an Englishman called John of Garland in 1220 — he had written a book "Dictionarius" to help with Latin "diction". An early non-alphabetical list of 8000 English words was the "Elementarie", created by Richard Mulcaster in 1582.

The first purely English alphabetical dictionary was "A Table Alphabeticall", written by English schoolteacher Robert Cawdrey in 1604. The only surviving copy is found at the Bodleian Library in Oxford. This dictionary, and the many imitators which followed it, was seen as unreliable and nowhere near definitive. Philip Stanhope, 4th Earl of Chesterfield was still lamenting in 1754, 150 years after Cawdrey's publication, that it is "a sort of disgrace to our nation, that hitherto we have had no… standard of our language; our dictionaries at present being more properly what our neighbors the Dutch and the Germans call theirs, word-books, than dictionaries in the superior sense of that title." 

In 1616, John Bullokar described the history of the dictionary with his "English Expositor". "Glossographia" by Thomas Blount, published in 1656, contains more than 10,000 words along with their etymologies or histories. Edward Phillips wrote another dictionary in 1658, entitled "The New World of English Words: Or a General Dictionary" which boldly plagiarized Blount's work, and the two denounced each other. This created more interest in the dictionaries. John Wilkins' 1668 essay on philosophical language contains a list of 11,500 words with careful distinctions, compiled by William Lloyd. Elisha Coles published his "English Dictionary" in 1676.

It was not until Samuel Johnson's "A Dictionary of the English Language" (1755) that a more reliable English dictionary was produced. Many people today mistakenly believe that Johnson wrote the first English dictionary: a testimony to this legacy. By this stage, dictionaries had evolved to contain textual references for most words, and were arranged alphabetically, rather than by topic (a previously popular form of arrangement, which meant all animals would be grouped together, etc.). Johnson's masterwork could be judged as the first to bring all these elements together, creating the first "modern" dictionary.

Johnson's dictionary remained the English-language standard for over 150 years, until the Oxford University Press began writing and releasing the "Oxford English Dictionary" in short fascicles from 1884 onwards. It took nearly 50 years to complete this huge work, and they finally released the complete "OED" in twelve volumes in 1928. It remains the most comprehensive and trusted English language dictionary to this day, with revisions and updates added by a dedicated team every three months. One of the main contributors to this modern dictionary was an ex-army surgeon, William Chester Minor, a convicted murderer who was confined to an asylum for the criminally insane.

In 1806, American Noah Webster published his first dictionary, "". In 1807 Webster began compiling an expanded and fully comprehensive dictionary, "An American Dictionary of the English Language;" it took twenty-seven years to complete. To evaluate the etymology of words, Webster learned twenty-six languages, including Old English (Anglo-Saxon), German, Greek, Latin, Italian, Spanish, French, Hebrew, Arabic, and Sanskrit.

Webster completed his dictionary during his year abroad in 1825 in Paris, France, and at the University of Cambridge. His book contained seventy thousand words, of which twelve thousand had never appeared in a published dictionary before. As a spelling reformer, Webster believed that English spelling rules were unnecessarily complex, so his dictionary introduced American English spellings, replacing "colour" with "color", substituting "wagon" for "waggon", and printing "center" instead of "centre". He also added American words, like "skunk" and "squash", that did not appear in British dictionaries. At the age of seventy, Webster published his dictionary in 1828; it sold 2500 copies. In 1840, the second edition was published in two volumes.

In a general dictionary, each word may have multiple meanings. Some dictionaries include each separate meaning in the order of most common usage while others list definitions in historical order, with the oldest usage first.

In many languages, words can appear in many different forms, but only the undeclined or unconjugated form appears as the headword in most dictionaries. Dictionaries are most commonly found in the form of a book, but some newer dictionaries, like StarDict and the "New Oxford American Dictionary" are dictionary software running on PDAs or computers. There are also many online dictionaries accessible via the Internet.

According to the "Manual of Specialized Lexicographies", a specialized dictionary, also referred to as a technical dictionary, is a dictionary that focuses upon a specific subject field. Following the description in "The Bilingual LSP Dictionary", lexicographers categorize specialized dictionaries into three types: A multi-field dictionary broadly covers several subject fields (e.g. a business dictionary), a single-field dictionary narrowly covers one particular subject field (e.g. law), and a sub-field dictionary covers a more specialized field (e.g. constitutional law). For example, the 23-language Inter-Active Terminology for Europe is a multi-field dictionary, the American National Biography is a single-field, and the African American National Biography Project is a sub-field dictionary. In terms of the coverage distinction between "minimizing dictionaries" and "maximizing dictionaries", multi-field dictionaries tend to minimize coverage across subject fields (for instance, "Oxford Dictionary of World Religions" and "Yadgar Dictionary of Computer and Internet Terms") whereas single-field and sub-field dictionaries tend to maximize coverage within a limited subject field ("The Oxford Dictionary of English Etymology").

Another variant is the glossary, an alphabetical list of defined terms in a specialized field, such as medicine (medical dictionary).

The simplest dictionary, a defining dictionary, provides a core glossary of the simplest meanings of the simplest concepts. From these, other concepts can be explained and defined, in particular for those who are first learning a language. In English, the commercial defining dictionaries typically include only one or two meanings of under 2000 words. With these, the rest of English, and even the 4000 most common English idioms and metaphors, can be defined.

Lexicographers apply two basic philosophies to the defining of words: "prescriptive" or "descriptive". Noah Webster, intent on forging a distinct identity for the American language, altered spellings and accentuated differences in meaning and pronunciation of some words. This is why American English now uses the spelling "color" while the rest of the English-speaking world prefers "colour". (Similarly, British English subsequently underwent a few spelling changes that did not affect American English; see further at American and British English spelling differences.)

Large 20th-century dictionaries such as the "Oxford English Dictionary" (OED) and "Webster's Third" are descriptive, and attempt to describe the actual use of words. Most dictionaries of English now apply the descriptive method to a word's definition, and then, outside of the definition itself, and information alerting readers to attitudes which may influence their choices on words often considered vulgar, offensive, erroneous, or easily confused. "Merriam-Webster" is subtle, only adding italicized notations such as, "sometimes offensive" or "stand" (nonstandard). "American Heritage" goes further, discussing issues separately in numerous "usage notes." "Encarta" provides similar notes, but is more prescriptive, offering warnings and admonitions against the use of certain words considered by many to be offensive or illiterate, such as, "an offensive term for..." or "a taboo term meaning...".

Because of the widespread use of dictionaries in schools, and their acceptance by many as language authorities, their treatment of the language does affect usage to some degree, with even the most descriptive dictionaries providing conservative continuity. In the long run, however, the meanings of words in English are primarily determined by usage, and the language is being changed and created every day. As Jorge Luis Borges says in the prologue to "El otro, el mismo": ""It is often forgotten that (dictionaries) are artificial repositories, put together well after the languages they define. The roots of language are irrational and of a magical nature.""

Sometimes the same dictionary can be descriptive in some domains and prescriptive in others. For example, according to Ghil'ad Zuckermann, the "Oxford English-Hebrew Dictionary" is "at war with itself": whereas its coverage (lexical items) and glosses (definitions) are descriptive and colloquial, its vocalization is prescriptive. This internal conflict results in absurd sentences such as "hi taharóg otí kshetiré me asíti lamkhonít" (she'll tear me apart when she sees what I've done to the car). Whereas "hi taharóg otí", literally 'she will kill me', is colloquial, me (a variant of ma 'what') is archaic, resulting in a combination that is unutterable in real life.

A historical dictionary is a specific kind of descriptive dictionary which describes the development of words and senses over time, usually using citations to original source material to support its conclusions.

In contrast to traditional dictionaries, which are designed to be used by human beings, dictionaries for natural language processing (NLP) are built to be used by computer programs. The final user is a human being but the direct user is a program. Such a dictionary does not need to be able to be printed on paper. The structure of the content is not linear, ordered entry by entry but has the form of a complex network (see Diathesis alternation). Because most of these dictionaries are used to control machine translations or cross-lingual information retrieval (CLIR) the content is usually multilingual and usually of huge size. In order to allow formalized exchange and merging of dictionaries, an ISO standard called Lexical Markup Framework (LMF) has been defined and used among the industrial and academic community.


In many languages, such as the English language, the pronunciation of some words is not consistently apparent from their spelling. In these languages, dictionaries usually provide the pronunciation. For example, the definition for the word "dictionary" might be followed by the International Phonetic Alphabet spelling . American English dictionaries often use their own pronunciation respelling systems with diacritics, for example "dictionary" is respelled as "dĭk′shə-nĕr′ē" in the American Heritage Dictionary. The IPA is more commonly used within the British Commonwealth countries. Yet others use their own pronunciation respelling systems without diacritics: for example, "dictionary" may be respelled as . Some online or electronic dictionaries provide audio recordings of words being spoken.

Histories and descriptions of the dictionaries of other languages include:


The age of the Internet brought online dictionaries to the desktop and, more recently, to the smart phone. David Skinner in 2013 noted that "Among the top ten lookups on Merriam-Webster Online at this moment are 'holistic, pragmatic, caveat, esoteric' and 'bourgeois.' Teaching users about words they don’t already know has been, historically, an aim of lexicography, and modern dictionaries do this well."
There exist a number of websites which operate as online dictionaries, usually with a specialized focus. Some of them have exclusively user driven content, often consisting of neologisms. Some of the more notable examples include:





</doc>
<doc id="7935" url="https://en.wikipedia.org/wiki?curid=7935" title="David D. Friedman">
David D. Friedman

David Director Friedman (born February 12, 1945) is an American economist, physicist, legal scholar, and libertarian theorist. He is known for his textbook writings on microeconomics and the libertarian theory of anarcho-capitalism, which is the subject of his most popular book, "The Machinery of Freedom". Besides "The Machinery of Freedom", he has authored several other books and articles, including "Price Theory: An Intermediate Text" (1986), "Law's Order: What Economics Has to Do with Law and Why It Matters" (2000), "Hidden Order: The Economics of Everyday Life" (1996), and "Future Imperfect" (2008).

David Friedman is the son of economists Rose and Milton Friedman. He graduated "magna cum laude" from Harvard University in 1965, with a bachelor's degree in chemistry and physics. He later earned a master's (1967) and a Ph.D. (1971) in theoretical physics from the University of Chicago. Despite his later career, he never took a class for credit in either economics or law. He is currently a professor of law at Santa Clara University, and a contributing editor for "Liberty" magazine. He is an atheist. His son, Patri Friedman, has also written about libertarian theory and market anarchism, particularly seasteading.

In his book "The Machinery of Freedom" (1973), Friedman sketched a form of anarcho-capitalism where all goods and services including law itself can be produced by the free market. This differs from the version proposed by Murray Rothbard, where a legal code would first be consented to by the parties involved in setting up the anarcho-capitalist society. Friedman advocates an incrementalist approach to achieve anarcho-capitalism by gradual privatization of areas that government is involved in, ultimately privatizing law and order itself. In the book, he states his opposition to violent anarcho-capitalist revolution.

He advocates a consequentialist version of anarcho-capitalism, arguing for anarchism on a cost-benefit analysis of state versus no state. It is contrasted with the natural-rights approach as propounded most notably by economist and libertarian theorist Murray Rothbard.

Friedman is a longtime member of the Society for Creative Anachronism, where he is known as "Duke Cariadoc of the Bow". He is known throughout the worldwide society for his articles on the philosophy of recreationism and practical historical recreations, especially those relating to the medieval Middle East. His work is compiled in the popular "Cariadoc's Miscellany". He is sometimes credited with founding the largest and longest-running SCA event, the Pennsic War; as king of the Middle Kingdom he challenged the East Kingdom, and later as king of the East accepted the challenge...and lost.

He is a long-time science fiction fan, and has written two fantasy novels, "Harald" (Baen Books, 2006) and "Salamander" (2011).

He has spoken in favor of a non-interventionist foreign policy.





</doc>
<doc id="7938" url="https://en.wikipedia.org/wiki?curid=7938" title="Diatomic molecule">
Diatomic molecule

Diatomic molecules are molecules composed of only two atoms, of the same or different chemical elements. The prefix "di-" is of Greek origin, meaning "two". If a diatomic molecule consists of two atoms of the same element, such as hydrogen (H) or oxygen (O), then it is said to be homonuclear. Otherwise, if a diatomic molecule consists of two different atoms, such as carbon monoxide (CO) or nitric oxide (NO), the molecule is said to be heteronuclear.

The only chemical elements that form stable homonuclear diatomic molecules at standard temperature and pressure (STP) (or typical laboratory conditions of 1 bar and 25 °C) are the gases hydrogen (H), nitrogen (N), oxygen (O), fluorine (F), and chlorine (Cl).

The noble gases (helium, neon, argon, krypton, xenon, and radon) are also gases at STP, but they are monatomic. The homonuclear diatomic gases and noble gases together are called "elemental gases" or "molecular gases", to distinguish them from other gases that are chemical compounds.

At slightly elevated temperatures, the halogens bromine (Br) and iodine (I) also form diatomic gases. All halogens have been observed as diatomic molecules, except for astatine, which is uncertain.

The mnemonics BrINClHOF, pronounced "Brinklehof", and HONClBrIF, pronounced "Honkelbrif", and HOFBrINCl (pronouced as Hofbrinkle) have been coined to aid recall of the list of diatomic elements.

Other elements form diatomic molecules when evaporated, but these diatomic species repolymerize when cooled. Heating ("cracking") elemental phosphorus gives diphosphorus, P. Sulfur vapor is mostly disulfur (S). Dilithium (Li) is known in the gas phase. Ditungsten (W) and dimolybdenum (Mo) form with sextuple bonds in the gas phase. The bond in a homonuclear diatomic molecule is non-polar. Dirubidium is diatomic.

All other diatomic molecules are chemical compounds of two different elements. Many elements can combine to form heteronuclear diatomic molecules, depending on temperature and pressure.

Common examples include the gases carbon monoxide (CO), nitric oxide (NO), and hydrogen chloride (HCl).

Many 1:1 binary compounds are not normally considered diatomic because they are polymeric at room temperature, but they form diatomic molecules when evaporated, for example gaseous MgO, SiO, and many others.

Hundreds of diatomic molecules have been identified in the environment of the Earth, in the laboratory, and in interstellar space. About 99% of the Earth's atmosphere is composed of two species of diatomic molecules: nitrogen (78%) and oxygen (21%). The natural abundance of hydrogen (H) in the Earth's atmosphere is only of the order of parts per million, but H is the most abundant diatomic molecule in the universe. The interstellar medium is, indeed, dominated by hydrogen atoms.

Diatomic elements played an important role in the elucidation of the concepts of element, atom, and molecule in the 19th century, because some of the most common elements, such as hydrogen, oxygen, and nitrogen, occur as diatomic molecules. John Dalton's original atomic hypothesis assumed that all elements were monatomic and that the atoms in compounds would normally have the simplest atomic ratios with respect to one another. For example, Dalton assumed water's formula to be HO, giving the atomic weight of oxygen as eight times that of hydrogen, instead of the modern value of about 16. As a consequence, confusion existed regarding atomic weights and molecular formulas for about half a century.

As early as 1805, Gay-Lussac and von Humboldt showed that water is formed of two volumes of hydrogen and one volume of oxygen, and by 1811 Amedeo Avogadro had arrived at the correct interpretation of water's composition, based on what is now called Avogadro's law and the assumption of diatomic elemental molecules. However, these results were mostly ignored until 1860, partly due to the belief that atoms of one element would have no chemical affinity toward atoms of the same element, and also partly due to apparent exceptions to Avogadro's law that were not explained until later in terms of dissociating molecules.

At the 1860 Karlsruhe Congress on atomic weights, Cannizzaro resurrected Avogadro's ideas and used them to produce a consistent table of atomic weights, which mostly agree with modern values. These weights were an important prerequisite for the discovery of the periodic law by Dmitri Mendeleev and Lothar Meyer.

Diatomic molecules are normally in their lowest or ground state, which conventionally is also known as the formula_1 state. When a gas of diatomic molecules is bombarded by energetic electrons, some of the molecules may be excited to higher electronic states, as occurs, for example, in the natural aurora; high-altitude nuclear explosions; and rocket-borne electron gun experiments. Such excitation can also occur when the gas absorbs light or other electromagnetic radiation. The excited states are unstable and naturally relax back to the ground state. Over various short time scales after the excitation (typically a fraction of a second, or sometimes longer than a second if the excited state is metastable), transitions occur from higher to lower electronic states and ultimately to the ground state, and in each transition results a photon is emitted. This emission is known as fluorescence. Successively higher electronic states are conventionally named formula_2, formula_3, formula_4, etc. (but this convention is not always followed, and sometimes lower case letters and alphabetically out-of-sequence letters are used, as in the example given below). The excitation energy must be greater than or equal to the energy of the electronic state in order for the excitation to occur.

In quantum theory, an electronic state of a diatomic molecule is represented by

where formula_6 is the total electronic spin quantum number, formula_7 is the total electronic angular momentum quantum number along the internuclear axis, and formula_8 is the vibrational quantum number. formula_7 takes on values 0, 1, 2, …, which are represented by the electronic state symbols formula_10, formula_11, formula_12,….
For example, the following table lists the common electronic states (without vibrational quantum numbers) along with the energy of the lowest vibrational level (formula_13) of diatomic nitrogen (N), the most abundant gas in the Earth's atmosphere. In the table, the subscripts and superscripts after formula_7 give additional quantum mechanical details about the electronic state.

Note: The "energy" units in the above table are actually the reciprocal of the wavelength of a photon emitted in a transition to the lowest energy state. The actual energy can be found by multiplying the given statistic by the product of "c" (the speed of light) and "h" (Planck's constant), i.e., about 1.99 × 10 Joule metres, and then multiplying by a further factor of 100 to convert from cm to m.

The aforementioned fluorescence occurs in distinct regions of the electromagnetic spectrum, called "emission bands": each band corresponds to a particular transition from a higher electronic state and vibrational level to a lower electronic state and vibrational level (typically, many vibrational levels are involved in an excited gas of diatomic molecules). For example, N formula_2-formula_1 emission bands (a.k.a. Vegard-Kaplan bands) are present in the spectral range from 0.14 to 1.45 μm (micrometres). A given band can be spread out over several nanometers in electromagnetic wavelength space, owing to the various transitions that occur in the molecule's rotational quantum number, formula_17. These are classified into distinct sub-band branches, depending on the change in formula_17. The formula_19 branch corresponds to formula_20, the formula_21 branch to formula_22, and the formula_23 branch to formula_24. Bands are spread out even further by the limited spectral resolution of the spectrometer that is used to measure the spectrum. The spectral resolution depends on the instrument's point spread function.

The molecular term symbol is a shorthand expression of the angular momenta that characterize the electronic quantum states of a diatomic molecule, which are eigenstates of the electronic molecular Hamiltonian. It is also convenient, and common, to represent a diatomic molecule as two point masses connected by a massless spring. The energies involved in the various motions of the molecule can then be broken down into three categories: the translational, rotational, and vibrational energies.

The translational energy of the molecule is given by the kinetic energy expression:

where formula_26 is the mass of the molecule and formula_8 is its velocity.

Classically, the kinetic energy of rotation is

For microscopic, atomic-level systems like a molecule, angular momentum can only have specific discrete values given by

Also, for a diatomic molecule the moment of inertia is

So, substituting the angular momentum and moment of inertia into E, the rotational energy levels of a diatomic molecule are given by:

Another type of motion of a diatomic molecule is for each atom to oscillate—or vibrate—along the line connecting the two atoms. The vibrational energy is approximately that of a quantum harmonic oscillator:

The spacing, and the energy of a typical spectroscopic transition, between vibrational energy levels is about 100 times greater than that of a typical transition between rotational energy levels.

The good quantum numbers for a diatomic molecule, as well as good approximations of rotational energy levels, can be obtained by modeling the molecule using Hund's cases.





</doc>
<doc id="7939" url="https://en.wikipedia.org/wiki?curid=7939" title="Duopoly">
Duopoly

A duopoly (from Greek δύο, "duo" (two) + πωλεῖν, "polein" (to sell)) is a form of oligopoly where only two sellers exist in one market. In practice, the term is also used where two firms have dominant control over a market. It is the most commonly studied form of oligopoly due to its simplicity.

There are two principal duopoly models, Cournot duopoly and Bertrand duopoly:


Modern American politics, in particular the electoral college system has been described as duopolistic since the Republican and Democratic parties have dominated and framed policy debate as well as the public discourse on matters of national concern for about a century and a half. Third Parties have encountered various blocks in getting onto ballots at different levels of government as well as other electoral obstacles, such as denial of access to general election debates.

A commonly cited example of a duopoly is that involving Visa and MasterCard, who between them control a large proportion of the electronic payment processing market. In 2000 they were the defendants in a U.S. Department of Justice antitrust lawsuit. An appeal was upheld in 2004.

Examples where two companies control a large proportion of a market are:

In Finland, the state-owned broadcasting company Yleisradio and the private broadcaster Mainos-TV had a legal duopoly (in the economists' sense of the word) from the 1950s to 1993. No other broadcasters were allowed. Mainos-TV operated by leasing air time from Yleisradio, broadcasting in reserved blocks between Yleisradio's own programming on its two channels. This was a unique phenomenon in the world. Between 1986 and 1992 there was an independent third channel but it was jointly owned by Yle and MTV; only in 1993 did MTV get its own channel.

In the United Kingdom, the BBC and ITV formed an effective duopoly (with Channel 4 originally being economically dependent on ITV) until the development of multichannel from the 1990s onwards.

Safaricom mobile service provider and Airtel in Kenya are perfect examples of Duopoly market in the African telecommunication industry.

Duopoly is also used in the United States broadcast television and radio industry to refer to a single company owning two outlets in the same city.

This usage is technically incompatible with the normal definition of the word and may lead to confusion, inasmuch as there are generally more than two owners of broadcast television stations in markets with broadcast duopolies. In Canada, this definition is therefore more commonly called a "twinstick".



</doc>
<doc id="7940" url="https://en.wikipedia.org/wiki?curid=7940" title="Dungeons &amp; Dragons">
Dungeons &amp; Dragons

Dungeons & Dragons (abbreviated as D&D or DnD) is a fantasy tabletop role-playing game (RPG) originally designed by Gary Gygax and Dave Arneson. It was first published in 1974 by Tactical Studies Rules, Inc. (TSR). The game has been published by Wizards of the Coast (now a subsidiary of Hasbro) since 1997. It was derived from miniature wargames with a variation of Chainmail serving as the initial rule system. "D&D" publication is commonly recognized as the beginning of modern role-playing games and the role-playing game industry.

"D&D" departs from traditional wargaming and assigns each player a specific character to play instead of a military formation. These characters embark upon imaginary adventures within a fantasy setting. A Dungeon Master serves as the game's referee and storyteller while maintaining the setting in which the adventures occur, and playing the role of the inhabitants. The characters form a party that interacts with the setting's inhabitants, and each other. Together they solve dilemmas, engage in battles, and gather treasure and knowledge. In the process the characters earn experience points in order to rise in levels, and become increasingly powerful over a series of sessions.

The early success of "Dungeons & Dragons" led to a proliferation of similar game systems. Despite the competition, "D&D" has remained as the market leader in the role-playing game industry. In 1977, the game was split into two branches: the relatively rules-light game system of basic "Dungeons & Dragons" and the more structured, rules-heavy game system of "Advanced Dungeons & Dragons" (abbreviated as "AD&D"). "AD&D" 2nd Edition was published in 1989. In 2000, a new system was released as "Dungeons & Dragons" 3rd edition. These rules formed the basis of the d20 System which is available under the Open Game License (OGL) for use by other publishers. "Dungeons & Dragons" version 3.5 was released in June 2003, with a (non-OGL) 4th edition in June 2008. A 5th edition was released during the second half of 2014.

, "Dungeons & Dragons" remained the best-known and best-selling role-playing game, with an estimated 20 million people having played the game and more than US$1 billion in book and equipment sales. The game has been supplemented by many pre-made adventures as well as commercial campaign settings suitable for use by regular gaming groups. "Dungeons & Dragons" is known beyond the game for other "D&D"-branded products, references in popular culture, and some of the controversies that have surrounded it, particularly a moral panic in the 1980s falsely linking it to Satanism and suicide. The game has won multiple awards and has been translated into many languages beyond the original English.

"Dungeons & Dragons" is a structured yet open-ended role-playing game. It is normally played indoors with the participants seated around a tabletop. Typically, each player controls only a single character, which represents an individual in a fictional setting. When working together as a group, these player characters (PCs) are often described as a "party" of adventurers, with each member often having their own area of specialty which contributes to the success of the whole. During the course of play, each player directs the actions of their character and their interactions with other characters in the game. This activity is performed through the verbal impersonation of the characters by the players, while employing a variety of social and other useful cognitive skills, such as logic, basic mathematics and imagination. A game often continues over a series of meetings to complete a single adventure, and longer into a series of related gaming adventures, called a "campaign".

The results of the party's choices and the overall storyline for the game are determined by the Dungeon Master (DM) according to the rules of the game and the DM's interpretation of those rules. The DM selects and describes the various non-player characters (NPCs) that the party encounters, the settings in which these interactions occur, and the outcomes of those encounters based on the players' choices and actions. Encounters often take the form of battles with "monsters" – a generic term used in "D&D" to describe potentially hostile beings such as animals, aberrant beings, or mythical creatures. The game's extensive rules – which cover diverse subjects such as social interactions, magic use, combat, and the effect of the environment on PCs – help the DM to make these decisions. The DM may choose to deviate from the published rules or make up new ones if they feel it is necessary.

The most recent versions of the game's rules are detailed in three core rulebooks: The "Player's Handbook", the "Dungeon Master's Guide" and the "Monster Manual".

The only items required to play the game are the rulebooks, a character sheet for each player, and a number of polyhedral dice. Many players also use miniature figures on a grid map as a visual aid, particularly during combat. Some editions of the game presume such usage. Many optional accessories are available to enhance the game, such as expansion rulebooks, pre-designed adventures and various campaign settings.

Before the game begins, each player creates their player character and records the details (described below) on a character sheet. First, a player determines their character's ability scores, which consist of Strength, Constitution, Dexterity, Intelligence, Wisdom, and Charisma. Each edition of the game has offered differing methods of determining these statistics. The player then chooses a race (species) such as human or elf, a character class (occupation) such as fighter or wizard, an alignment (a moral and ethical outlook), and other features to round out the character's abilities and backstory, which have varied in nature through differing editions.

During the game, players describe their PC's intended actions, such as punching an opponent or picking a lock, and converse with the DM, who then describes the result or response. Trivial actions, such as picking up a letter or opening an unlocked door, are usually automatically successful. The outcomes of more complex or risky actions are determined by rolling dice. Factors contributing to the outcome include the character's ability scores, skills and the difficulty of the task. In circumstances where a character does not have control of an event, such as when a trap or magical effect is triggered or a spell is cast, a saving throw can be used to determine whether the resulting damage is reduced or avoided. In this case the odds of success are influenced by the character's class, levels and ability scores.

As the game is played, each PC changes over time and generally increases in capability. Characters gain (or sometimes lose) experience, skills and wealth, and may even alter their alignment or gain additional character classes. The key way characters progress is by earning experience points (XP), which happens when they defeat an enemy or accomplish a difficult task. Acquiring enough XP allows a PC to advance a level, which grants the character improved class features, abilities and skills. XP can be lost in some circumstances, such as encounters with creatures that drain life energy, or by use of certain magical powers that come with an XP cost.

Hit points (HP) are a measure of a character's vitality and health and are determined by the class, level and constitution of each character. They can be temporarily lost when a character sustains wounds in combat or otherwise comes to harm, and loss of HP is the most common way for a character to die in the game. Death can also result from the loss of key ability scores or character levels. When a PC dies, it is often possible for the dead character to be resurrected through magic, although some penalties may be imposed as a result. If resurrection is not possible or not desired, the player may instead create a new PC to resume playing the game.

A typical "Dungeons & Dragons" game consists of an "adventure", which is roughly equivalent to a single story. The DM can either design an adventure on their own, or follow one of the many pre-made adventures (also known as "modules") that have been published throughout the history of "Dungeons & Dragons". Published adventures typically include a background story, illustrations, maps and goals for PCs to achieve. Some include location descriptions and handouts. Although a small adventure entitled "Temple of the Frog" was included in the "Blackmoor" rules supplement in 1975, the first stand-alone "D&D" module published by TSR was 1978's "Steading of the Hill Giant Chief", written by Gygax.

A linked series of adventures is commonly referred to as a "campaign". The locations where these adventures occur, such as a city, country, planet or an entire fictional universe, are referred to as "campaign settings" or "world". "D&D" settings are based in various fantasy genres and feature different levels and types of magic and technology. Popular commercially published campaign settings for "Dungeons & Dragons" include Greyhawk, Dragonlance, Forgotten Realms, Mystara, Spelljammer, Ravenloft, Dark Sun, Planescape, Birthright, and Eberron. Alternatively, DMs may develop their own fictional worlds to use as campaign settings.

The wargames from which "Dungeons & Dragons" evolved used miniature figures to represent combatants. "D&D" initially continued the use of miniatures in a fashion similar to its direct precursors. The original "D&D" set of 1974 required the use of the "Chainmail" miniatures game for combat resolution. By the publication of the 1977 game editions, combat was mostly resolved verbally. Thus miniatures were no longer required for game play, although some players continued to use them as a visual reference.

In the 1970s, numerous companies began to sell miniature figures specifically for "Dungeons & Dragons" and similar games. Licensed miniature manufacturers who produced official figures include Grenadier Miniatures (1980–1983), Citadel Miniatures (1984–1986), Ral Partha, and TSR itself. Most of these miniatures used the 25 mm scale.

Periodically, "Dungeons & Dragons" has returned to its wargaming roots with supplementary rules systems for miniatures-based wargaming. Supplements such as "Battlesystem" (1985 & 1989) and a new edition of "Chainmail" (2001) provided rule systems to handle battles between armies by using miniatures.

An immediate predecessor of "Dungeons & Dragons" was a set of medieval miniature rules written by Jeff Perren. These were expanded by Gary Gygax, whose additions included a fantasy supplement, before the game was published as "Chainmail". When Dave Wesely entered the Army in 1970, his friend and fellow Napoleonics wargamer Dave Arneson began a medieval variation of Wesely's Braunstein games, where players control individuals instead of armies. Arneson used "Chainmail" to resolve combat. As play progressed, Arneson added such innovations as character classes, experience points, level advancement, armor class, and others. Having partnered previously with Gygax on "Don't Give Up the Ship!", Arneson introduced Gygax to his Blackmoor game and the two then collaborated on developing "The Fantasy Game", the role-playing game (RPG) that became "Dungeons & Dragons", with the final writing and preparation of the text being done by Gygax. The name was chosen by Gygax's two-year-old daughter Cindy — upon being presented with a number of choices of possible names, she exclaimed, "Oh Daddy, I like Dungeons and Dragons best!", although less prevalent versions of the story gave credit to his wife Mary Jo.

Many "Dungeons & Dragons" elements appear in hobbies of the mid-to-late 20th century. For example, character-based role playing can be seen in improvisational theatre. Game-world simulations were well developed in wargaming. Fantasy milieux specifically designed for gaming could be seen in Glorantha's board games among others. Ultimately, however, "Dungeons & Dragons" represents a unique blending of these elements.

The world of "D&D" was influenced by world mythology, history, pulp fiction, and contemporary fantasy novels. The importance of J. R. R. Tolkien's "The Lord of the Rings" and "The Hobbit" as an influence on "D&D" is controversial. The presence in the game of halflings, elves, half-elves, dwarves, orcs, rangers, and the like, draw comparisons to these works. The resemblance was even closer before the threat of copyright action from Tolkien Enterprises prompted the name changes of hobbit to 'halfling', ent to 'treant', and balrog to 'balor'. For many years, Gygax played down the influence of Tolkien on the development of the game. However, in an interview in 2000, he acknowledged that Tolkien's work had a "strong impact".

The "D&D" magic system, in which wizards memorize spells that are used up once cast and must be re-memorized the next day, was heavily influenced by the "Dying Earth" stories and novels of Jack Vance. The original alignment system (which grouped all characters and creatures into 'Law', 'Neutrality' and 'Chaos') was derived from the novel "Three Hearts and Three Lions" by Poul Anderson. A troll described in this work influenced the "D&D" definition of that monster.

Other influences include the works of Robert E. Howard, Edgar Rice Burroughs, A. Merritt, H. P. Lovecraft, Fritz Leiber, L. Sprague de Camp, Fletcher Pratt, Roger Zelazny, and Michael Moorcock. Monsters, spells, and magic items used in the game have been inspired by hundreds of individual works such as A. E. van Vogt's "Black Destroyer", Coeurl (the Displacer Beast), Lewis Carroll's "Jabberwocky" (vorpal sword) and the Book of Genesis (the clerical spell 'Blade Barrier' was inspired by the "flaming sword which turned every way" at the gates of Eden).

"Dungeons & Dragons" has gone through several revisions. Parallel versions and inconsistent naming practices can make it difficult to distinguish between the different editions.

The original "Dungeons & Dragons", now referred to as "OD&D", was a small box set of three booklets published in 1974. It was amateurish in production and assumed the player was familiar with wargaming. Nevertheless, it grew rapidly in popularity, first among wargamers and then expanding to a more general audience of college and high school students. Roughly 1,000 copies of the game were sold in the first year followed by 3,000 in 1975, and much more in the following years. This first set went through many printings and was supplemented with several official additions, such as the original Greyhawk and Blackmoor supplements (both 1975), as well as magazine articles in TSR's official publications and many fanzines.

In early 1977, TSR created the first element of a two-pronged strategy that would divide "D&D" for nearly two decades. A "Dungeons & Dragons Basic Set" boxed edition was introduced that cleaned up the presentation of the essential rules, made the system understandable to the general public, and was sold in a package that could be stocked in toy stores. Later in 1977, the first part of "Advanced Dungeons & Dragons" ("AD&D") was published, which brought together the various published rules, options and corrections, then expanded them into a definitive, unified game for hobbyist gamers. TSR marketed them as an introductory game for new players and a more complex game for experienced ones; the "Basic Set" directed players who exhausted the possibilities of that game to switch to the advanced rules.

As a result of this parallel development, the basic game included many rules and concepts which contradicted comparable ones in "AD&D". John Eric Holmes, the editor of the basic game, preferred a lighter tone with more room for personal improvisation. "AD&D", on the other hand, was designed to create a tighter, more structured game system than the loose framework of the original game. Between 1977 and 1979, three hardcover rulebooks, commonly referred to as the "core rulebooks", were released: the "Player's Handbook" (PHB), the "Dungeon Master's Guide" (DMG), and the "Monster Manual" (MM). Several supplementary books were published throughout the 1980s, notably "Unearthed Arcana" (1985) that included a large number of new rules. Confusing matters further, the original "D&D" boxed set remained in publication until 1979, since it remained a healthy seller for TSR.

In the 1980s, the rules for "Advanced Dungeons & Dragons" and "basic" "Dungeons & Dragons" remained separate, each developing along different paths.

In 1981, the basic version of "Dungeons & Dragons" was revised by Tom Moldvay to make it even more novice-friendly. It was promoted as a continuation of the original "D&D" tone, whereas "AD&D" was promoted as advancement of the mechanics. An accompanying "Expert Set", originally written by David "Zeb" Cook, allowed players to continue using the simpler ruleset beyond the early levels of play. In 1983, revisions of those sets by Frank Mentzer were released, revising the presentation of the rules to a more tutorial format. These were followed by "Companion" (1983), "Master" (1985), and "Immortals" (1986) sets. Each set covered game play for more powerful characters than the previous. The first four sets were compiled in 1991 as a single hardcover book, the "Dungeons & Dragons Rules Cyclopedia", which was released alongside a new introductory boxed set.

"Advanced Dungeons & Dragons 2nd Edition" was published in 1989, again as three core rulebooks; the primary designer was David "Zeb" Cook. The "Monster Manual" was replaced by the "Monstrous Compendium", a loose-leaf binder that was subsequently replaced by the hardcover "Monstrous Manual" in 1993. In 1995, the core rulebooks were slightly revised, although still referred to by TSR as the 2nd Edition, and a series of "Player's Option" manuals were released as optional rulebooks.

The release of "AD&D 2nd Edition" deliberately excluded some aspects of the game that had attracted negative publicity. References to demons and devils, sexually suggestive artwork, and playable, evil-aligned character types – such as assassins and half-orcs – were removed. The edition moved away from a theme of 1960s and 1970s "sword and sorcery" fantasy fiction to a mixture of medieval history and mythology. The rules underwent minor changes, including the addition of non-weapon proficiencies – skill-like abilities that originally appeared in 1st Edition supplements. The game's magic spells were divided into schools and spheres. A major difference was the promotion of various game settings beyond that of traditional fantasy. This included blending fantasy with other genres, such as horror (Ravenloft), science fiction (Spelljammer), and apocalyptic (Dark Sun), as well as alternative historical and non-European mythological settings.

In 1997, a near-bankrupt TSR was purchased by Wizards of the Coast. Following three years of development, "Dungeons & Dragons" 3rd edition was released in 2000. The new release folded the Basic and Advanced lines back into a single unified game. It was the largest revision of the "D&D" rules to date, and served as the basis for a multi-genre role-playing system designed around 20-sided dice, called the d20 System. The 3rd Edition rules were designed to be internally consistent and less restrictive than previous editions of the game, allowing players more flexibility to create the characters they wanted to play. Skills and feats were introduced into the core rules to encourage further customization of characters. The new rules standardized the mechanics of action resolution and combat. In 2003, "Dungeons & Dragons v.3.5" was released as a revision of the 3rd Edition rules. This release incorporated hundreds of rule changes, mostly minor, and expanded the core rulebooks.

In early 2005, Wizards of the Coast's R&D team started to develop "Dungeons & Dragons 4th Edition", prompted mainly by the feedback obtained from the "D&D" playing community and a desire to make the game faster, more intuitive, and with a better play experience than under the 3rd Edition. The new game was developed through a number of design phases spanning from May 2005 until its release. "Dungeons & Dragons 4th Edition" was announced at Gen Con in August 2007, and the initial three core books were released June 6, 2008. 4th Edition streamlined the game into a simplified form and introduced numerous rules changes. Many character abilities were restructured into "Powers". These altered the spell-using classes by adding abilities that could be used at will, per encounter, or per day. Likewise, non-magic-using classes were provided with parallel sets of options. Software tools, including player character and monster building programs, became a major part of the game.

On January 9, 2012, Wizards of the Coast announced that it was working on a 5th edition of the game. The company planned to take suggestions from players and let them playtest the rules. Public playtesting began on May 24, 2012. At Gen Con 2012 in August, Mike Mearls, lead developer for 5th Edition, said that Wizards of the Coast had received feedback from more than 75,000 playtesters, but that the entire development process would take two years, adding, "I can't emphasize this enough ... we're very serious about taking the time we need to get this right." The release of the 5th Edition, coinciding with "D&D"s 40th anniversary, occurred in the second half of 2014.

The game had more than three million players around the world by 1981, and copies of the rules were selling at a rate of about 750,000 per year by 1984. Beginning with a French language edition in 1982, "Dungeons & Dragons" has been translated into many languages beyond the original English. By 2004, consumers had spent more than US$1 billion on "Dungeons & Dragons" products and the game had been played by more than 20 million people. As many as six million people played the game in 2007.

The various editions of "Dungeons & Dragons" have won many Origins Awards, including "All Time Best Roleplaying Rules of 1977", "Best Roleplaying Rules of 1989", and "Best Roleplaying Game of 2000" for the three flagship editions of the game. Both "Dungeons & Dragons" and "Advanced Dungeons & Dragons" are Origins Hall of Fame Games inductees as they were deemed sufficiently distinct to merit separate inclusion on different occasions. The independent "Games" magazine placed "Dungeons & Dragons" on their "Games 100" list from 1980 through 1983, then entered the game into the magazine's Hall of Fame in 1984. "Advanced Dungeons & Dragons" was ranked 2nd in the 1996 reader poll of "Arcane" magazine to determine the 50 most popular roleplaying games of all time.

Eric Goldberg reviewed "Dungeons & Dragons" in "Ares Magazine" #1, rating it a 6 out of 9. Goldberg commented that ""Dungeons and Dragons" is an impressive achievement based on the concept alone, and also must be credited with cementing the marriage between the fantasy genre and gaming."

"Dungeons & Dragons" was the first modern role-playing game and it established many of the conventions that have dominated the genre. Particularly notable are the use of dice as a game mechanic, character record sheets, use of numerical attributes and gamemaster-centered group dynamics. Within months of "Dungeons & Dragons"'s release, new role-playing game writers and publishers began releasing their own role-playing games, with most of these being in the fantasy genre. Some of the earliest other role-playing games inspired by "D&D" include "Tunnels & Trolls" (1975), "Empire of the Petal Throne" (1975), and "Chivalry & Sorcery" (1976).

The role-playing movement initiated by "D&D" would lead to release of the science fiction game "Traveller" (1977), the fantasy game "RuneQuest" (1978), and subsequent game systems such as Chaosium's "Call of Cthulhu" (1981), "Champions" (1982), "GURPS" (1986), and "" (1991). "Dungeons & Dragons" and the games it influenced fed back into the genre's origin – miniatures wargames – with combat strategy games like "Warhammer Fantasy Battles". "D&D" also had a large impact on modern video games.

Director Jon Favreau credits "Dungeons & Dragons" with giving him "... a really strong background in imagination, storytelling, understanding how to create tone and a sense of balance."

Early in the game's history, TSR took no action against small publishers' production of "D&D" compatible material, and even licensed Judges Guild to produce "D&D" materials for several years, such as "City State of the Invincible Overlord." This attitude changed in the mid-1980s when TSR took legal action to try to prevent others from publishing compatible material. This angered many fans and led to resentment by the other gaming companies. Although TSR took legal action against several publishers in an attempt to restrict third-party usage, it never brought any court cases to completion, instead settling out of court in every instance. TSR itself ran afoul of intellectual property law in several cases.

With the launch of "Dungeons & Dragons"'s 3rd Edition, Wizards of the Coast made the d20 System available under the Open Game License (OGL) and d20 System trademark license. Under these licenses, authors were free to use the d20 System when writing games and game supplements. The OGL and d20 Trademark License made possible new games, some based on licensed products like "Star Wars", and new versions of older games, such as "Call of Cthulhu".

With the release of the fourth edition, Wizards of the Coast introduced its Game System License, which represented a significant restriction compared to the very open policies embodied by the OGL. In part as a response to this, some publishers (such as Paizo Publishing with its "Pathfinder Roleplaying Game") who previously produced materials in support of the "D&D" product line, decided to continue supporting the 3rd Edition rules, thereby competing directly with Wizards of the Coast. Others, such as Kenzer & Company, are returning to the practice of publishing unlicensed supplements and arguing that copyright law does not allow Wizards of the Coast to restrict third-party usage.

During the 2000s, there has been a trend towards reviving and recreating older editions of "D&D", known as the Old School Revival. Game systems based on earlier editions of "D&D". "Castles & Crusades" (2004), by Troll Lord Games, is a reimagining of early editions by streamlining rules from OGL. This in turn inspired the creation of "retro-clones", games which more closely recreate the original rule sets, using material placed under the OGL along with non-copyrightable mechanical aspects of the older rules to create a new presentation of the games.

Alongside the publication of the fifth edition, Wizards of the Coast established a two-pronged licensing approach. The core of the fifth edition rules have been made available under the OGL, while publishers and independent creators have also been given the opportunity to create licensed materials directly for Dungeons & Dragons and associated properties like the Forgotten Realms under a program called the DM's Guild. The DM's Guild does not function under the OGL, but uses a community agreement intended to foster liberal cooperation among content creators.

At various times in its history, "Dungeons & Dragons" has received negative publicity, in particular from some Christian groups, for alleged promotion of such practices as devil worship, witchcraft, suicide, and murder, and for the presence of naked breasts in drawings of female humanoids in the original "AD&D" manuals (mainly monsters such as harpies, succubi, etc.). These controversies led TSR to remove many potentially controversial references and artwork when releasing the 2nd Edition of "AD&D". Many of these references, including the use of the names "devils" and "demons", were reintroduced in the 3rd edition. The moral panic over the game led to problems for fans of "D&D" who faced social ostracism, unfair treatment, and false association with the occult and Satanism, regardless of an individual fan's actual religious affiliation and beliefs.

"Dungeons & Dragons" has been the subject of rumors regarding players having difficulty separating fantasy from reality, even leading to psychotic episodes. The most notable of these was the saga of James Dallas Egbert III, the facts of which were fictionalized in the novel "Mazes and Monsters" and later made into a TV movie in 1982 starring Tom Hanks. The game was blamed for some of the actions of Chris Pritchard, who was convicted in 1990 of murdering his stepfather. Research by various psychologists, starting with Armando Simon, has concluded that no harmful effects are related to the playing of "D&D".

The game's commercial success was a factor that led to lawsuits regarding distribution of royalties between original creators Gygax and Arneson. Gygax later became embroiled in a political struggle for control of TSR which culminated in a court battle and Gygax's decision to sell his ownership interest in the company in 1985.

"D&D"'s commercial success has led to many other related products, including "Dragon" Magazine, "Dungeon" Magazine, an animated television series, a film series, an official role-playing soundtrack, novels, and computer games such as the MMORPG "Dungeons & Dragons Online". Hobby and toy stores sell dice, miniatures, adventures, and other game aids related to "D&D" and its game offspring.

"D&D" grew in popularity through the late 1970s and 1980s. Numerous games, films, and cultural references based on "D&D" or "D&D"-like fantasies, characters or adventures have been ubiquitous since the end of the 1970s. "D&D" players are (sometimes pejoratively) portrayed as the epitome of geekdom, and have become the basis of much geek and gamer humor and satire. Famous "D&D" players include Pulitzer Prize winning author Junot Díaz, professional basketball player Tim Duncan, comedian Stephen Colbert, and actors Vin Diesel and Robin Williams. "D&D" and its fans have been the subject of spoof films, including "Fear of Girls" and "".







</doc>
<doc id="7941" url="https://en.wikipedia.org/wiki?curid=7941" title="Double jeopardy">
Double jeopardy

Double jeopardy is a procedural defence that prevents an accused person from being tried again on the same (or similar) charges and on the same facts, following a valid acquittal or conviction. As described by the U.S. Supreme Court in its unanimous decision one of its earliest cases dealing with double jeopardy, "the prohibition is not against being twice punished, but against being twice put in jeopardy; and the accused, whether convicted or acquitted, is equally put in jeopardy at the first trial." 

If this issue is raised, evidence will be placed before the court, which will normally rule as a preliminary matter whether the plea is substantiated; if it is, the projected trial will be prevented from proceeding. In some countries, including Canada, Mexico and the United States, the guarantee against being "twice put in jeopardy" is a constitutional right. In other countries, the protection is afforded by statute.

In common law countries, a defendant may enter a peremptory plea of "autrefois acquit" (formerly acquitted) or "autrefois convict" (formerly convicted), with the same effect.

The doctrine appears to have originated in Roman law, in the principle "non bis in idem" ("an issue once decided must not be raised again").

The 72 signatories and 166 parties to the International Covenant on Civil and Political Rights recognise, under Article 14 (7): "No one shall be liable to be tried or punished again for an offence for which he has already been finally convicted or acquitted in accordance with the law and penal procedure of each country."

All members of the Council of Europe (which includes nearly all European countries and every member of the European Union) have adopted the European Convention on Human Rights. The optional Protocol No. 7 to the Convention, Article 4, protects against double jeopardy: "No one shall be liable to be tried or punished again in criminal proceedings under the jurisdiction of the same State for an offence for which he or she has already been finally acquitted or convicted in accordance with the law and penal procedure of that State."

This optional protocol has been ratified by all EU states except two: Germany, and the Netherlands. In those member states, national rules governing double jeopardy may or may not comply with the provision cited above.

Member states may, however, implement legislation which allows reopening of a case in the event that new evidence is found or if there was a fundamental defect in the previous proceedings:

The provisions of the preceding paragraph shall not prevent the reopening of the case in accordance with the law and penal procedure of the State concerned, if there is evidence of new or newly discovered facts, or if there has been a fundamental defect in the previous proceedings, which could affect the outcome of the case.

In many European countries, the prosecution may appeal an acquittal to a higher court. This is not regarded as double jeopardy, but as a continuation of the same case. The European Convention on Human Rights permits this by using the phrase ""finally" acquitted or convicted" (emphasis added) as the trigger for prohibiting subsequent prosecution.

In contrast to other common law nations, Australian double jeopardy law has been held to further prevent the prosecution for perjury following a previous acquittal where a finding of perjury would controvert the acquittal. This was confirmed in the case of "R v Carroll", where the police found new evidence convincingly disproving Carroll's sworn alibi two decades after he had been acquitted of murder charges in the death of Ipswich child Deidre Kennedy, and successfully prosecuted him for perjury. Public outcry following the overturn of his conviction (for perjury) by the High Court has led to widespread calls for reform of the law along the lines of the England and Wales legislation.

During a Council of Australian Governments (COAG) meeting of 2007, model legislation to rework double jeopardy laws was drafted, but there was no formal agreement for each state to introduce it. All states have now chosen to introduce legislation that mirrors COAG's recommendations on "fresh and compelling" evidence.

In New South Wales, retrials of serious cases with a minimum sentence of 20 years or more are now possible, whether or not the original trial preceded the 2006 reform. On 17 October 2006, the New South Wales Parliament passed legislation abolishing the rule against double jeopardy in cases where:

On 30 July 2008, South Australia also introduced legislation to scrap parts of its double jeopardy law, legalising retrials for serious offences with "fresh and compelling" evidence, or if the acquittal was tainted.

In Western Australia, on 8 September 2011 amendments were introduced that would allow also retrial if "new and compelling" evidence was found. It would apply to serious offences where the penalty was life imprisonment or imprisonment for 14 years or more. Acquittal because of tainting (witness intimidation, jury tampering, or perjury) would also allow retrial.

In Tasmania, on 19 August 2008, amendments were introduced to allow retrial in serious cases, if there is "fresh and compelling" evidence.

In Victoria on 21 December 2011, legislation was passed allowing new trials where there is "fresh and compelling DNA evidence, where the person acquitted subsequently admits to the crime, or where it becomes clear that key witnesses have given false evidence". Retrial applications however could only be made for serious offences such as murder, manslaughter, arson causing death, serious drug offences and aggravated forms of rape and armed robbery.

In Queensland on 18 October 2007, the double jeopardy laws were modified to allow a retrial where fresh and compelling evidence becomes available after an acquittal for murder or a "tainted acquittal" for a crime carrying a 25-year or more sentence. A "tainted acquittal" requires a conviction for an administration of justice offence, such as perjury, that led to the original acquittal. Unlike reforms in the United Kingdom, New South Wales, Tasmania, Victoria, South Australia, Western Australia, this law does not have a retrospective effect, which is unpopular with some advocates of the reform.

The Canadian Charter of Rights and Freedoms includes provisions such as section 11(h) prohibiting double jeopardy. However, this prohibition applies only after an accused person has been "finally" convicted or acquitted. Canadian law allows the prosecution to appeal an acquittal: if the acquittal is thrown out, the new trial is not considered to be double jeopardy, as the verdict of the first trial would have been annulled. In rare circumstances, a court of appeal might also substitute a conviction for an acquittal. This is not considered to be double jeopardy, either – in this case, the appeal and subsequent conviction are deemed to be a continuation of the original trial.

For an appeal from an acquittal to be successful, the Supreme Court of Canada requires that the Crown show that an error in law was made during the trial and that the error contributed to the verdict. It has been suggested that this test is unfairly beneficial to the prosecution. For instance, lawyer Martin Friedland, in his book "My Life in Crime and Other Academic Adventures", contends that the rule should be changed so that a retrial is granted only when the error is shown to be "responsible" for the verdict, not just a factor.

A notable example of this is Guy Paul Morin, who was wrongfully convicted in his second trial after the acquittal in his first trial was vacated by the Supreme Court of Canada.

In the Guy Turcotte case, for instance, the Quebec Court of Appeal overturned Turcotte's not criminally responsible verdict and ordered a second trial after it found that the judge committed an error in the first trial while giving instructions to the jury. Turcotte was later convicted of second-degree murder in the second trial.

Once all appeals have been exhausted on a case, the judgement is final and the action of the prosecution is closed (code of penal procedure, art. 6), except if the final ruling was forged. Prosecution for a crime already judged is impossible even if incriminating evidence has been found. However, a person who has been convicted may request another trial on grounds of new exculpating evidence through a procedure known as "révision".

The Basic Law ("Grundgesetz") for the Federal Republic of Germany does provide protection against double jeopardy if a final verdict is pronounced. A verdict is final if nobody appeals against it.

However, each trial party can appeal against a verdict in the first instance. This means the prosecution and/or the defendants can appeal against a judgement if they do not agree with it. In this case the trial starts again in the second instance, the court of appeal ("Berufungsgericht"), which considers the facts and reasons again and delivers the final judgement then.

If one of the parties disagrees with the judgement of the second instance, he or she can appeal it, but only on formal judicial reasons. The case will checked in the third instance ("Revisionsgericht"), whether all laws are applied correctly.

The rule applies to the whole "historical event, which is usually considered a single historical course of actions the separation of which would seem unnatural". This is true even if new facts occur that indicate other and/or much serious crimes.

The Penal Procedural Code ("Strafprozessordnung") permits a retrial ("Wiederaufnahmeverfahren"), if it is in favor of the defendant or if following events had happened:

In the case of an order of summary punishment, which can be issued by the court without a trial for lesser misdemeanours, there is a further exception:

In Germany, a felony is defined by § 12 (1) StGB as a crime which has a minimum of one year of imprisonment.

A partial protection against double jeopardy is a Fundamental Right guaranteed under Article 20 (2) of the Constitution of India, which states "No person shall be prosecuted and punished for the same offence more than once". This provision enshrines the concept of "autrefois convict", that no one convicted of an offence can be tried or punished a second time. However, it does not extend to "autrefois acquit", and so if a person is acquitted of a crime he can be retried. In India, protection against "autrefois acquit" is a statutory right, not a fundamental one. Such protection is provided by provisions of the Code of Criminal Procedure rather than by the Constitution.

The Constitution of Japan states in Article 39 that

In practice, however, if someone is acquitted in a lower District Court, then the prosecutor can appeal to the High Court, and then to the Supreme Court. Only the acquittal in the Supreme Court is the final acquittal which prevents any further retrial. This process sometimes takes decades.

The above is not considered a violation of the constitution. Because of Supreme Court precedent, this process is all considered part of a single proceeding.

In the Netherlands, the state prosecution can appeal a not-guilty verdict at the bench. New evidence can be brought to bear during a retrial at a district court. Thus one can be tried twice for the same alleged crime. If one is convicted at the district court, the defence can make an appeal on procedural grounds to the supreme court. The supreme court might admit this complaint, and the case will be reopened yet again, at another district court. Again, new evidence might be introduced by the prosecution.

On 9 April 2013 the Dutch senate voted 36 "yes" versus 35 "no" in favor of a new law that allows the prosecutor to re-try a person who was found not guilty in court. This new law is limited to crimes where someone died and new evidence must have been gathered. The new law also works retroactively.

Article 13 of the Constitution of Pakistan protects a person from being punished or prosecuted more than once for the same offence.

This principle is incorporated into the Constitution of the Republic of Serbia and further elaborated in its Criminal Procedure Act.

The Bill of Rights in the Constitution of South Africa forbids a retrial when there has already been an acquittal or a conviction.
Article 13 of the South Korean constitution provides that no citizen shall be placed in double jeopardy.

Double jeopardy has been permitted in England and Wales in certain (exceptional) circumstances since the Criminal Justice Act 2003.

The doctrines of "autrefois acquit" and "autrefois convict" persisted as part of the common law from the time of the Norman conquest of England; they were regarded as essential elements for protection of the subject's liberty and respect for due process of law in that there should be finality of proceedings. There were only three exceptions, all relatively recent, to the rules:

In Connelly v DPP ([1964] AC 1254), the Law Lords ruled that a defendant could not be tried for any offence arising out of substantially the same set of facts relied upon in a previous charge of which he had been acquitted, unless there are "special circumstances" proven by the prosecution. There is little case law on the meaning of "special circumstances", but it has been suggested that the emergence of new evidence would suffice.

A defendant who had been convicted of an offence could be given a second trial for an aggravated form of that offence if the facts constituting the aggravation were discovered after the first conviction. By contrast, a person who had been acquitted of a lesser offence could not be tried for an aggravated form even if new evidence became available.

Following the murder of Stephen Lawrence, the Macpherson Report recommended that the double jeopardy rule should be abrogated in murder cases, and that it should be possible to subject an acquitted murder suspect to a second trial if "fresh and viable" new evidence later came to light. The Law Commission later added its support to this in its report "Double Jeopardy and Prosecution Appeals" (2001). A parallel report into the criminal justice system by Lord Justice Auld, a past Senior Presiding Judge for England and Wales, had also commenced in 1999 and was published as the Auld Report six months after the Law Commission report. It opined that the Law Commission had been unduly cautious by limiting the scope to murder and that "the exceptions should [...] extend to other grave offences punishable with life and/or long terms of imprisonment as Parliament might specify."

Both Jack Straw (then Home Secretary) and William Hague (then Leader of the Opposition) favoured this measure. These recommendations were implemented—not uncontroversially at the time—within the Criminal Justice Act 2003, and this provision came into force in April 2005. It opened certain serious crimes (including murder, manslaughter, kidnapping, rape, armed robbery, and serious drug crimes) to a retrial, regardless of when committed, with two conditions: the retrial must be approved by the Director of Public Prosecutions, and the Court of Appeal must agree to quash the original acquittal due to "new and compelling evidence". Pressure by Ann Ming, the mother of 1989 murder victim Julie Hogg—whose killer, William Dunlop, was initially acquitted in 1991 and subsequently confessed—also contributed to the demand for legal change.

On 11 September 2006, Dunlop became the first person to be convicted of murder following a prior acquittal for the same crime, in his case his 1991 acquittal of Julie Hogg's murder. Some years later he had confessed to the crime, and was convicted of perjury, but was unable to be retried for the killing itself. The case was re-investigated in early 2005, when the new law came into effect, and his case was referred to the Court of Appeal in November 2005 for permission for a new trial, which was granted. Dunlop pleaded guilty to murdering Julie Hogg and was sentenced to life imprisonment, with a recommendation he serve no less than 17 years.

On 13 December 2010, Mark Weston became the first person to be retried and found guilty of murder by a jury (Dunlop having confessed). In 1996 Weston had been acquitted of the murder of Vikki Thompson at Ascott-under-Wychwood on 12 August 1995, but following the discovery in 2009 of compelling new evidence (Thompson's blood on Weston's boots) he was arrested and tried for a second time. He was sentenced to life imprisonment, to serve a minimum of 13 years.

The double jeopardy rule no longer applies absolutely in Scotland since the Double Jeopardy (Scotland) Act 2011 came into force on 28 November 2011. The Act introduced three broad exceptions to the rule: where the acquittal had been tainted by an attempt to pervert the course of justice; where the accused admitted his guilt after acquittal; and where there was new evidence.

In Northern Ireland the Criminal Justice Act 2003, effective 18 April 2005, makes certain "qualifying offence" (including murder, rape, kidnapping, specified sexual acts with young children, specified drug offences, defined acts of terrorism, as well as in certain cases attempts or conspiracies to commit the foregoing) subject to retrial after acquittal (including acquittals obtained before passage of the Act) if there is a finding by the Court of Appeal that there is "new and compelling evidence."

The ancient protection of the Common Law against double jeopardy is maintained in its full rigor in the United States, beyond reach of any change save that of a Constitutional Amendment. The Fifth Amendment to the United States Constitution provides:

Conversely, double jeopardy comes with a key exception. Under the dual sovereignty doctrine, multiple sovereigns can indict a defendant for the same crime. The federal and state governments can have overlapping criminal laws, so a criminal offender may be convicted in individual states and federal courts for exactly the same crime or for different crimes arising out of the same facts. However, in 2016, the Supreme Court held that Puerto Rico is not a separate sovereign for purposes of the Double Jeopardy Clause. The dual sovereignty doctrine has been the subject of substantial scholarly criticism.

The Double Jeopardy Clause encompasses four distinct prohibitions: subsequent prosecution after acquittal, subsequent prosecution after conviction, subsequent prosecution after certain mistrials, and multiple punishment in the same indictment. Jeopardy "attaches" when the jury is empanelled, the first witness is sworn, or a plea is accepted.

With two exceptions, the government is not permitted to appeal or retry the defendant once jeopardy attaches to a trial unless the case does not conclude. Conditions which constitute "conclusion" of a case include
In these cases the trial is concluded and the prosecution is precluded from appealing or retrying the defendant over the offense to which they were acquitted.

This principle does not prevent the government from appealing a pre-trial motion to dismiss or other non-merits dismissal, or a directed verdict after a jury conviction, nor does it prevent the trial judge from entertaining a motion for reconsideration of a directed verdict, if the jurisdiction has so provided by rule or statute. Nor does it prevent the government from retrying the defendant after an appellate reversal other than for sufficiency, including "habeas corpus", or "thirteenth juror" appellate reversals notwithstanding sufficiency on the principle that jeopardy has not "terminated."

The "dual sovereignty" doctrine allows a federal prosecution of an offense to proceed regardless of a previous state prosecution for that same offense and vice versa because "an act denounced as a crime by both national and state sovereignties is an offense against the peace and dignity of both and may be punished by each." The doctrine is solidly entrenched in the law, but there has been a traditional reluctance in the federal executive branch to gratuitously wield the power it grants, due to public opinion being generally hostile to such action.

The first exception to a ban on retrying a defendant is if, in a trial, the defendant bribed the judge into acquitting him or her, since the defendant was not in jeopardy 

The other exception to a ban on retrying a defendant is that the perpetrator can be retried by court martial in a military court, if they have been previously acquitted by a civilian court, and are members of the military.

In "Blockburger v. United States" (1932), the Supreme Court announced the following test: the government may separately try and punish the defendant for two crimes if each crime contains an element that the other does not. "Blockburger" is the default rule, unless the governing statute legislatively intends to depart; for example, Continuing Criminal Enterprise (CCE) may be punished separately from its predicates, as can conspiracy.

The "Blockburger" test, originally developed in the multiple punishments context, is also the test for prosecution after conviction. In "Grady v. Corbin" (1990), the Court held that a double jeopardy violation could lie even where the "Blockburger" test was not satisfied, but "Grady" was overruled in "United States v. Dixon" (1993).

The rule for mistrials depends upon who sought the mistrial. If the defendant moves for a mistrial, there is no bar to retrial, unless the prosecutor acted in "bad faith," i.e. goaded the defendant into moving for a mistrial because the government specifically wanted a mistrial. If the prosecutor moves for a mistrial, there is no bar to retrial if the trial judge finds "manifest necessity" for granting the mistrial. The same standard governs mistrials granted sua sponte.

Retrials are not common, due to the legal expenses to the government. However, in the mid-1980s Georgia antiques dealer James Arthur Williams was tried a record four times for the murder of Danny Hansford and (after three mistrials) was finally acquitted on the grounds of self-defense. The case is recounted in the book "Midnight in the Garden of Good and Evil" which was adapted into a film directed by Clint Eastwood (the movie omits the first three murder trials). 




Research and Notes produced for the UK Parliament, summarising the history of legal change, views and responses, and analyses:




</doc>
<doc id="7942" url="https://en.wikipedia.org/wiki?curid=7942" title="Disbarment">
Disbarment

Disbarment is the removal of a lawyer from a bar association or the practice of law, thus revoking his or her law license or admission to practice law. Disbarment is usually a punishment for unethical or criminal conduct. Procedures vary depending on the law society.

In Germany, the "Berufsverbot" is a ban on practicing a profession, which can be issued to a lawyer for misconduct, Volksverhetzung or for serious mismanagement of personal finances.

In April 1933, the Nazi government issued a "Berufsverbot" forbidding the practice of law by Jews, communists, and other political opponents, except for those protected by the Frontkämpferprivileg.

Generally disbarment is imposed as a sanction for conduct indicating that an attorney is not fit to practice law, willfully disregarding the interests of a client, or engaging in fraud which impedes the administration of justice. In addition, any lawyer who is convicted of a felony is automatically disbarred in most jurisdictions, a policy that, although opposed by the American Bar Association, has been described as a convicted felon's just deserts.

In the United States legal system, disbarment is specific to regions; one can be disbarred from some courts, while still being a member of the bar in another jurisdiction. However, under the American Bar Association's Model Rules of Professional Conduct, which have been adopted in most states, disbarment in one state or court is grounds for disbarment in a jurisdiction which has adopted the Model Rules.

Disbarment is quite rare (in 2011, only 1,046 lawyers were disbarred). Instead, lawyers are usually sanctioned by their own clients through civil malpractice proceedings, or via fine, censure, suspension, or other punishments from the disciplinary boards. To be disbarred is considered a great embarrassment and shame, even if one no longer wishes to pursue a career in the law.

Because disbarment rules vary by area, different rules can apply depending on where a lawyer is disbarred. Notably, the majority of US states have no procedure for permanently disbarring a person. Depending on the jurisdiction, a lawyer may reapply to the bar immediately, after five to seven years, or be banned for life.

The 20th and the 21st centuries have seen one former U.S. president and one former U.S. vice president disbarred, and another president suspended from one bar and caused to resign from another bar rather than face disbarment.

Former Vice President Spiro Agnew, having pleaded no contest (which subjects a person to the same criminal penalties as a guilty plea, but is not an admission of guilt for a civil suit) to charges of bribery and tax evasion, was disbarred from Maryland, the state of which he had previously been governor.

Former President Richard Nixon was disbarred from New York in 1976. for obstruction of justice related to the Watergate scandal. He had attempted to resign from the New York bar, as he had done with California and the Supreme Court, but his resignation was not accepted as he would not acknowledge that he was unable to defend himself from the charges brought against him.

In 2001, following a 5-year suspension by the Arkansas bar, the United States Supreme Court disbarred Bill Clinton, providing 40 days for him to contest the action. He resigned before the end of the 40 day period, avoiding permanent disbarment hearings.

Alger Hiss was disbarred for a felony conviction, but later became the first person reinstated to the bar in Massachusetts after disbarment.

In 2007, Mike Nifong, the District Attorney of Durham County, North Carolina who presided over the 2006 Duke University lacrosse case, was disbarred for prosecutorial misconduct related to his handling of the case.

In April 2012, a three-member panel appointed by the Arizona Supreme Court voted unanimously to disbar Andrew Thomas, former County Attorney of Maricopa County, Arizona, and a former close confederate of Maricopa County Sheriff Joe Arpaio. According to the panel, Thomas "outrageously exploited power, flagrantly fostered fear, and disgracefully misused the law" while serving as Maricopa County Attorney. The panel found "clear and convincing evidence" that Thomas brought unfounded and malicious criminal and civil charges against political opponents, including four state judges and the state attorney general. "Were this a criminal case," the panel concluded, "we are confident that the evidence would establish this conspiracy beyond a reasonable doubt."

Jack Thompson, the Florida lawyer noted for his activism against video games, rap music, and Howard Stern was permanently disbarred for various charges of misconduct. The action was the result of several grievances claiming that Thompson had made defamatory, false statements and attempted to humiliate, embarrass, harass or intimidate his opponents. The order was made on September 25, 2008, effective October 25. However, Thompson attempted to appeal to the higher courts in order to avoid the penalty actually taking effect. Neither the US District court, nor the US Supreme Court would hear his appeal, rendering the judgment of the Florida Supreme Court final.

Ed Fagan, a New York lawyer who prominently represented Holocaust victims against Swiss banks, was disbarred in New York (in 2008) and New Jersey (in 2009) for failing to pay court fines and fees; and for misappropriating client and escrow trust funds.

F. Lee Bailey, noted criminal defense attorney, was disbarred by the state of Florida in 2001, with reciprocal disbarment in Massachusetts in 2002. The Florida disbarment was the result of his handling of stock in the DuBoc marijuana case. Bailey was found guilty of 7 counts of attorney misconduct by the Florida Supreme Court. Bailey had transferred a large portion of DuBoc's assets into his own accounts, using the interest gained on those assets to pay for personal expenses. In March 2005, Bailey filed to regain his law license in Massachusetts. The book "Florida Pulp Nonfiction" details the peculiar facts of the DuBoc case along with extended interviews with Bailey that include his own defense. Bailey is also best known for infamously representing murder suspect O. J. Simpson in 1994.


</doc>
<doc id="7946" url="https://en.wikipedia.org/wiki?curid=7946" title="Dog tag">
Dog tag

"Dog tag" is an informal but common term for the type of identification tag worn by military personnel. The tags are primarily used for the identification of dead and wounded soldiers; they have personal information about the soldiers and convey essential basic medical information, such as blood type and history of inoculations. The tags often indicate religious preference as well. Dog tags are usually fabricated from a corrosion-resistant metal. They commonly contain two copies of the information, either in the form of a single tag that can be broken in half or two identical tags on the same chain. This duplication allows one tag (or half-tag) to be collected from a soldier's body for notification and the second to remain with the corpse when battle conditions prevent it from being immediately recovered. The term "dog tags" arose because of their resemblance to animal registration tags.

The earliest mention of an identification tag for soldiers comes in Polyaenus (Stratagems 1.17) where the Spartans wrote their names on sticks tied to their left wrists. A type of dog tag ("signaculum"), was given to the Roman legionnaire at the moment of enrollment. The legionnaire "signaculum" was a lead disk with a leather string, worn around the neck, with the name of the recruit and the indication of the legion of which the recruit was part. 
This procedure, together with enrolment in the list of recruits, was made at the beginning of a four-month probatory period ("probatio"). The recruit got the military status only after the oath of allegiance ("sacramentum"), at the end of "probatio", meaning that from a legal point of view the "signaculum" was given to a subject who was no longer a civilian, but not yet in the military.

In more recent times, dog tags were provided to Chinese soldiers as early as the mid-19th century. During the Taiping revolt (1851–66), both the Imperialists (i.e., the Chinese Imperial Army regular servicemen) and those Taiping rebels wearing a uniform wore a wooden dog tag at the belt, bearing the soldier's name, age, birthplace, unit, and date of enlistment.

During the American Civil War from 1861–1865, some soldiers pinned paper notes with their name and home address to the backs of their coats. Other soldiers stenciled identification on their knapsacks or scratched it in the soft lead backing of their army belt buckle.

Manufacturers of identification badges recognized a market and began advertising in periodicals. Their pins were usually shaped to suggest a branch of service, and engraved with the soldier's name and unit. Machine-stamped tags were also made of brass or lead with a hole and usually had (on one side) an eagle or shield, and such phrases as "War for the Union" or "Liberty, Union, and Equality". The other side had the soldier's name and unit, and sometimes a list of battles in which he had participated.

On a volunteer basis Prussian soldiers had decided to wear identification tags in the Austro-Prussian War of 1866. However, many rejected dog tags as a bad omen for their lives. So until eight months after the Battle of Königgrätz, with almost 8,900 Prussian casualties, only 429 of them could be identified. With the formation of the North German Confederation in 1867 Prussian military regulations became binding for the militaries of all North German member states. With the Prussian "Instruktion über das Sanitätswesen der Armee im Felde" (i.e., instruction on the medical corps organisation of the army afield) issued on 29 April 1869 the identification tags (then called "Recognitionsmarke"; i.e. literally recognition mark) were obligatorily to be handed out to each soldier before marching afield. The Prussian Army issued identification tags for its troops at the beginning of the Franco-Prussian War in 1870. They were nicknamed "Hundemarken" (the German equivalent of "dog tags") and compared to a similar identification system instituted by the dog licence fee, adding tags to collars of those dogs whose owners paid the fee, in the Prussian capital city of Berlin at around the same time period.

The British Army introduced identity discs in place of identity cards in 1907, in the form of aluminium discs, typically made at Regimental depots using machines similar to those common at fun fairs, the details being pressed into the thin metal one letter at a time.

Army Order 287 of September 1916 required the British Army provide all soldiers with two official tags, both made of vulcanised asbestos fibre (which were more comfortable to wear in hot climates) carrying identical details, again impressed one character at a time. The first tag, an octagonal green disc, was attached to a long cord around the neck. The second tag, a circular red disc, was threaded on a 6-inch cord suspended from the first tag. The first tag was intended to remain on the body for future identification, while the second tag could be taken to record the death.

British and Empire/Commonwealth forces (Australia, Canada, and New Zealand) were issued essentially identical identification discs of basic pattern during the Great War, Second World War and Korea, though official identity discs were frequently supplemented by private-purchase items such as identity bracelets, particularly favoured by sailors who rightly believed the official discs were unlikely to survive long immersion in water.

The U.S. Army first authorized identification tags in War Department General Order No. 204, dated December 20, 1906, which essentially prescribes the Kennedy identification tag:

An aluminum identification tag, the size of a silver half dollar and of suitable thickness, stamped with the name, rank, company, regiment, or corps of the wearer, will be worn by each officer and enlisted man of the Army whenever the field kit is worn, the tag to be suspended from the neck, underneath the clothing, by a cord or thong passed through a small hole in the tab. It is prescribed as a part of the uniform and when not worn as directed herein will be habitually kept in the possession of the owner. The tag will be issued by the Quartermaster's Department gratuitously to enlisted men and at cost price to officers.

The army changed regulations on July 6, 1916, so that all soldiers were issued two tags: one to stay with the body and the other to go to the person in charge of the burial for record-keeping purposes. In 1918, the army adopted and allotted the serial number system, and name and serial numbers were ordered stamped on the identification tags. (Serial number 1 was assigned to enlisted man Arthur B. Crane of Chicago in the course of his fifth enlistment period.)

There is a recurring myth about the notch situated in one end of the dog tags issued to United States Army personnel during World War II, and up until the Vietnam War era. It was rumored that the notch's purpose was that, if a soldier found one of his comrades on the battlefield, he could take one tag to the commanding officer and stick the other between the teeth of the soldier to ensure that the tag would remain with the body and be identified.

In reality, the notch was used with the Model 70 Addressograph Hand Identification Imprinting Machine (a pistol-type imprinter used primarily by the Medical Department during World War II). American dogtags of the 1930s through 1970s were produced using a Graphotype machine, in which characters are debossed into metal plates. Some tags are still debossed, using earlier equipment, and some are embossed (with raised letters) on computer-controlled equipment.

In the Graphotype process, commonly used commercially from the early 1900s through the 1980s, a debossing machine was used to stamp characters into metal plates; the plates could then be used to repetitively stamp such things as addresses onto paper in the same way that a typewriter functions, except that a single stroke of the printer could produce a block of text, rather than requiring each character to be printed individually. The debossing process creates durable, easily legible metal plates, well-suited for military identification tags, leading to adoption of the system by the American military. It was also realized that debossed tags can function the same way the original Graphotype plates do.

The Model 70 took advantage of this fact, and was intended to rapidly print all of the information from a soldier's dogtag directly onto medical and personnel forms, with a single squeeze of the trigger. However, this requires that the tag being inserted with the proper orientation (stamped characters facing down), and it was believed that battlefield stress could lead to errors. To force proper orientation of the tags, the tags are produced with a notch, and there is a locator tab inside the Model 70 which prevents the printer from operating if the tag is inserted with the notch in the wrong place (as it is if the tag is upside down).

This feature was not as useful in the field as had been hoped, however, due to adverse conditions such as weather, dirt and dust, water, etc. In addition, the Model 70 resembled a pistol, thus attracting the attention of snipers (who might assume that a man carrying a pistol was an officer). As a result, use of the Model 70 hand imprinter by field medics was rapidly abandoned (as were most of the Model 70s themselves), and eventually the specification that tags include the locator notch was removed from production orders. Existing stocks of tags were used until depleted, and in the 1960s it was not uncommon for a soldier to be issued one tag with the notch and one tag without. Notched tags are still in production, to satisfy the needs of hobbyists, film production, etc., while the Model 70 imprinter has become a rare collector's item.

It appears instructions that would confirm the notch's mythical use were issued at least unofficially by the Graves Registration Service during the Vietnam War to Army troops headed overseas.

Dog tags are traditionally part of the makeshift battlefield memorials soldiers created for their fallen comrades. The casualty's rifle with bayonet affixed is stood vertically atop the empty boots, with the helmet over the rifle's stock. The dog tags hang from the rifle's handle or trigger guard.

Some tags (along with similar items such as MedicAlert bracelets) are used also by civilians to identify their wearers and specify them as having health problems that may <br>
"(a)" suddenly incapacitate their wearers and render them incapable of providing treatment guidance (as in the cases of heart problems, epilepsy, diabetic coma, accident or major trauma) and/or <br> 
"(b)" interact adversely with medical treatments, especially standard or "first-line" ones (as in the case of an allergy to common medications) and/or <br>
"(c)" provide in case of emergency ("ICE") contact information and/or <br> 
"(d)" state a religious, moral, or other objection to artificial resuscitation, if a first responder attempts to administer such treatment when the wearer is non-responsive and thus unable to warn against doing so. A DNR Signed by a physician is still required in some states. 

Military personnel in some jurisdictions may wear a supplementary medical information tag.

Dog tags have recently found their way into youth fashion by way of military chic. Originally worn as a part of a military uniform by youth wishing to present a tough or militaristic image, dog tags have since seeped out into wider fashion circles. They may be inscribed with a person's details, their beliefs or tastes, a favorite quote, or may bear the name or logo of a band or performer.

Since the late 1990s, custom dog tags have been fashionable amongst musicians (particularly rappers), and as a marketing give-away item. Numerous companies offer customers the opportunity to create their own personalized dog tags with their own photos, logos, and text. Even high-end jewelers have featured gold and silver dog tags encrusted with diamonds and other jewels.

The Austrian Bundesheer utilized a single long, rectangular tag, with oval ends, stamped with blood group & Rh factor at the end, with ID number underneath. Two slots and a hole stamped beneath the nunicew the tag to be broken in half, and the long bottom portion has both the ID number and a series of holes which allows the tag to be inserted into a dosimeter. This has been replaced with a more conventional, wider and rounded rectangle which can still be halved, but lacks the dosimeter reading holes.

The Australian Defence Force issues soldiers two tags of different shapes, one octagonal and one circular, containing the following information:

The information is printed exactly the same on both discs. In the event of a casualty, the circular tag is removed from the body.

Belgian Forces identity tags are, like the Canadian and Norwegian, designed to be broken in two in case of fatality; the lower half is returned to Belgian Defence tail, while the upper half remains on the body. The tags contain the following information:


Canadian Forces identity discs (abbreviated "I discs") are designed to be broken in two in the case of fatality; the lower half is returned to National Defence Headquarters with the member's personal documents, while the upper half remains on the body. The tags contain the following information:

Before the Service Number was introduced in the 1990s, military personnel were identified on the I discs (as well as other documents) by their Social Insurance Number.

The People's Liberation Army issues two long, rectangular tags. All information is stamped in Simplified Chinese:
PLA is introducing a two-dimensional matrix code on the second tag, the matrix code contains a link to the official database. This allows the inquirer get more details about the military personnel.

The Ejército Nacional de Colombia uses long, rectangular metal tags with oval ends tags stamped with the following information:

Duplicate tags are issued. Often, tags are issued with a prayer inscribed on the reverse.

In Cyprus, identification tags include the following information:

The military of Denmark use dog tags made from small, rectangular metal plates. The tag is designed to be broken into two pieces each with the following information stamped onto it:
Additionally, the right hand side of each half-tag is engraved 'DANMARK', .
Starting in 1985, the individual's service number (which is the same as the social security number) is included on the tag. In case the individual dies, the lower half-tag is supposed to be collected, while the other will remain with the corpse. In the army, navy, and air force but not in the national guard, the individual's blood type is indicated on the lower half-tag only, since this information becomes irrelevant if the individual dies. In 2009, Danish dog tags were discontinued for conscripts.

The Nationale Volksarmee used a tag nearly identical to that used by both the Wehrmacht and the West German Bundeswehr. The oval aluminum tag was stamped "DDR" (Deutsche Demokratische Republik) above the personal ID number; this information was repeated on the bottom half, which was intended to be broken off in case of death. Oddly, the tag was not worn, but required to be kept in a plastic sleeve in the back of the WDA identity booklet.

The "Placas de identificación de campaña" consists of two long, rectangular steel or aluminum tags with rounded corners and a single hole punched in one end. It is suspended by a US-type ball chain, with a shorter chain for the second tag. The information on the tag is:

Estonian dog tags are designed to be broken in two. The dog tag is a metallic rounded rectangle suspended by a ball chain. Information consists of four fields: 

Example: 

In the Finnish Defence Forces, "tunnuslevy" or WWII term "tuntolevy" (Finnish for "Identification plate") is made of stainless steel and designed to be broken in two; however, the only text on it is the personal identification number and the letters "SF" (rarely FI), which stands for Suomi Finland, within a tower stamped atop of the upper half.

France issues either a metallic rounded rectangle (army) or disk (navy), designed to be broken in half, bearing family name & first name above the ID number.

German Bundeswehr ID tags are an oval-shaped disc designed to be broken in half. The two sides contain different information which are mirrored upside-down on the lower half of the ID tag. They feature the following information on segmented and numbered fields:

On the front:


On the back:


In Greece, identification tags include the following information:

The Hungarian army dog tag is made out of steel, forming a 25×35 mm tag designed to split diagonally. Both sides contain the same information: the soldier's personal identity code, blood group and the word HUNGARIA. Some may not have the blood group on them. These are only issued to soldiers who are serving outside of the country. If the soldier should die, one side is removed and kept for the army's official records, while the other side is left attached to the body.

The Saddam-era Iraqi Army utilized a single, long, rectangular metal tag with oval ends, inscribed (usually by hand) with Name and Number or Unit, and occasionally Blood Type.

Dog tags of the Israel Defense Forces are designed to be broken in two. The information appears in three lines (twice):

Another two dog tags are kept inside each military boot in order to identify dead soldiers.

Originally the IDF issued two circular aluminum tags (1948 – late 1950s) stamped in three lines with serial number, family name, and first name. The tags were threaded together through a single hole onto a cord worn around the neck.

Japan follows a similar system to the US Army for its Self Defence Force personnel, and the appearance of the tags is similar, although laser etched. The exact information order is as follows.


Malaysian Armed Forces have two identical oval tags with this information:

If more information needed, another two oval wrist tags are provided. The term "wrist tags" can be used to refer to the bracelet-like wristwatch. The additional tags only need to be worn on the wrist, with the main tags still on the neck. All personnel are allowed to attach a small religious pendant or locket; this makes a quick identifiable reference for their funeral services.

The Ejército de Mexico uses a single long, rectangular metal tag with oval ends, embossed with Name, serial number, and blood type plus Rh factor.

Military of the Netherlands identity tags, like the Canadian and Norwegian ones, are designed to be broken in two in case of a fatality; the lower end is returned to Dutch Defence Headquarters, while the upper half remains on the body. There is a difference in the Army and Airforce service number and the Navy service number:

The tags contain the following information:

Norwegian dog tags are designed to be broken in two like the Canadian version:

The first dog tags were issued in Poland following the order of the General Staff of December 12, 1920. The earliest design (dubbed "kapala" in Polish, more properly called "kapsel legitymacyjny" - meaning "identification cap") consisted of a tin-made 30×50 mm rectangular frame and a rectangular cap fitting into the frame. Soldiers' details were filled in a small ID card placed inside the frame, as well as on the inside of the frame itself. The dog tag was similar to the tags used by the Austro-Hungarian Army during World War I. In case the soldier died, the frame was left with his body, while the lid was returned to his unit together with a note on his death. The ID card was handed over to the chaplain or the rabbi.

In 1928, a new type of dog tag was proposed by gen. bryg. Stanisław Rouppert, Poland's representative at the International Red Cross. It was slightly modified and adopted in 1931 under the name of Nieśmiertelnik wz. 1931 (literally, Immortalizer mark 1931). The new design consisted of an oval piece of metal (ideally steel, but in most cases aluminum alloy was used), roughly 40 by 50 millimeters. There were two notches on both sides of the tag, as well as two rectangular holes in the middle to allow for easier breaking of the tag in two halves. The halves contained the same set of data and were identical, except the upper half had two holes for a string or twine to go through. The data stamped on the dog tag from 2008 (wz. 2008) included:

with the name of Polish Army "Siły Zbrojne RP" and Polish Emblem.

The former Republic of Rhodesia used two WW2 British-style compressed asbestos fiber tags, a No. 1 octagonal (green) tag and a No. 2 circular (red) tag, stamped with identical information. The red tag was supposedly fireproof and the green tag rotproof. The following information was stamped on the tags: Number, Name, Initials, & Religion; Blood Type was stamped on reverse. The air force and BSAP often stamped their service on the reverse side above the blood group.

Many soldiers state they were issued blank tags and told to punch the information in themselves. 

The Russian Armed Forces use oval metal tags, similar to the dog tags of the Soviet Army. Each tag contains the title " () and the individual's alphanumeric number, as shown on the photo.

The Singapore Armed Forces-issued dog tags are inscribed (not embossed) with up to four items:

The dog tags consist of two metal pieces, one oval with two holes and one round with one hole. A synthetic lanyard is threaded through both holes in the oval piece and tied around the wearer's neck. The round piece is tied to the main loop on a shorter loop.

The former South African Defense Force used two long, rectangular stainless steel tags with oval ends, stamped with serial number, name and initials, religion, and blood type.

The South Korean Army issues two long, rectangular tags with oval ends, stamped (in Korean lettering). The tags are worn on the neck with a ball chain. The tags contain the information listed below:

The South Vietnamese Army used two American-style dog tags. Some tags added religion, e.g., Công Giáo for Catholic. They were stamped or inscribed with: 

During World War II, the Red Army did not issue metal dog tags to its troops. They were issued small ebony cylinders containing a slip of paper with a soldier's particulars written on it. These do not hold up as well as metal dog tags.
After World War II, the Soviet Army used oval metal tags, similar to today's dog tags of the Russian Armed forces. Each tag contains the title " () and the individual's alphanumeric number.

Issues a single metal oval, worn vertically, stamped "" above and below the 3-slot horizontal break line. It is stamped in 4 lines with:

Swedish identification tags are designed to be able to break apart. The information on them was prior to 2010:

Swedish dog tags issued to Armed Forces personnel after 2010 are, for personal security reasons, only marked with a personal identity number.

During the Cold War, dog tags were issued to everyone, often soon after birth, since the threat of total war also meant the risk of severe civilian casualties. However, in 2010, the Government decided that the dog tags were not needed anymore.

Swiss Armed Forces ID tag is an oval shaped non reflective plaque, containing the following information:
On the back side the letters CH standing for (Confoederatio Helvetica) are engraved next to a Swiss cross.

Tags are properly known as identification tags; the term "dog tags" has never been used in regulations.

A persistent rumor is that debossed (imprinted with stamped in letters) dog tags were issued from World War II till the end of the Vietnam War and that currently the U.S. Armed Forces is issuing embossed (imprinted with raised letters) dog tags. In actuality, the U.S. Armed Forces issues dog tags with both types of imprinting, depending on the machine used at a given facility. The military issued 95% of their identification tags up until recently (within the past 10 years) with debossed text.

The U.S. Armed Forces typically carry two identical oval dog tags containing:

During World War II, an American dog tag could indicate only one of three religions through the inclusion of one letter: "P" for Protestant, "C" for Catholic, or "H" for Jewish (from the word, "Hebrew"), or (according to at least one source) "NO" to indicate no religious preference. Army regulations (606-5) soon included X and Y in addition to P, C, and H: the X indicating any religion not included in the first three, and the Y indicating either no religion or a choice not to list religion.
By the time of the Vietnam War, some IDs spelled out the broad religious choices such as PROTESTANT and CATHOLIC, rather than using initials, and also began to show individual denominations such as "METHODIST" or "BAPTIST." Tags did vary by service, however, such as the use of "CATH," not "CATHOLIC" on some Navy tags. For those with no religious affiliation and those who chose not to list an affiliation, either the space for religion was left blank or the words "NO PREFERENCE" or "NO RELIGIOUS PREF" (or the abbreviation "NO PREF") were included.

Although American dog tags currently include the recipient's religion as a way of ensuring that religious needs will be met, some personnel have them reissued without religious affiliation listed—or keep two sets, one with the designation and one without—out of fear that identification as a member of a particular religion could increase the danger to their welfare or their lives if they fell into enemy hands. Some Jewish personnel avoided flying over German lines during WWII with ID tags that indicated their religion, and some Jewish personnel avoid the religious designation today out of concern that they could be captured by extremists who are anti-Semitic. Additionally, when American troops were first sent to Saudi Arabia during the Gulf War there were allegations that some U.S. military authorities were pressuring Jewish military personnel to avoid listing their religions on their ID tags.




</doc>
<doc id="7950" url="https://en.wikipedia.org/wiki?curid=7950" title="Drum">
Drum

The drum is a member of the percussion group of musical instruments. In the Hornbostel-Sachs classification system, it is a membranophone. Drums consist of at least one membrane, called a drumhead or drum skin, that is stretched over a shell and struck, either directly with the player's hands, or with a drum stick, to produce sound. There is usually a "resonance head" on the underside of the drum, typically tuned to a slightly lower pitch than the top drumhead. Other techniques have been used to cause drums to make sound, such as the thumb roll. Drums are the world's oldest and most ubiquitous musical instruments, and the basic design has remained virtually unchanged for thousands of years.

Drums may be played individually, with the player using a single drum, and some drums such as the djembe are almost always played in this way. Others are normally played in a set of two or more, all played by the one player, such as bongo drums and timpani. A number of different drums together with cymbals form the basic modern drum kit.

Drums are usually played by striking with the hand, or with one or two sticks. A wide variety of sticks are used, including wooden sticks and sticks with soft beaters of felt on the end. In jazz, some In many traditional cultures, drums have a symbolic function and are used in religious ceremonies. Drums are often used in music therapy, especially hand drums, because of their tactile nature and easy use by a wide variety of people.

In popular music and jazz, "drums" usually refers to a drum kit or a set of drums (with some cymbals, or in the case of harder rock music genres, many cymbals), and "drummer" to the person who plays them.

Drums acquired even divine status in places such as Burundi, where the "karyenda" was a symbol of the power of the king.

The shell almost invariably has a circular opening over which the drumhead is stretched, but the shape of the remainder of the shell varies widely. In the western musical tradition, the most usual shape is a cylinder, although timpani, for example, use bowl-shaped shells. Other shapes include a frame design (tar, Bodhrán), truncated cones (bongo drums, Ashiko), goblet shaped (djembe), and joined truncated cones (talking drum).

Drums with cylindrical shells can be open at one end (as is the case with timbales), or can have two drum heads, one head on each end. Single-headed drums typically consist of a skin stretched over an enclosed space, or over one of the ends of a hollow vessel. Drums with two heads covering both ends of a cylindrical shell often have a small hole somewhat halfway between the two heads; the shell forms a resonating chamber for the resulting sound. Exceptions include the African slit drum, also known as a log drum as it is made from a hollowed-out tree trunk, and the Caribbean steel drum, made from a metal barrel. Drums with two heads can also have a set of wires, called snares, held across the bottom head, top head, or both heads, hence the name snare drum. On some drums with two heads, a hole or bass reflex port may be cut or installed onto one head, as with some 2010s era bass drums in rock music.

On modern band and orchestral drums, the drumhead is placed over the opening of the drum, which in turn is held onto the shell by a "counterhoop" (or "rim"), which is then held by means of a number of tuning screws called "tension rods" that screw into lugs placed evenly around the circumference. The head's tension can be adjusted by loosening or tightening the rods. Many such drums have six to ten tension rods. The sound of a drum depends on many variables—including shape, shell size and thickness, shell materials, counterhoop material, drumhead material, drumhead tension, drum position, location, and striking velocity and angle.

Prior to the invention of tension rods, drum skins were attached and tuned by rope systems—as on the Djembe—or pegs and ropes such as on Ewe Drums. These methods are rarely used today, though sometimes appear on regimental marching band snare drums. The head of a talking drum, for example, can be temporarily tightened by squeezing the ropes that connect the top and bottom heads. Similarly, the tabla is tuned by hammering a disc held in place around the drum by ropes stretching from the top to bottom head. Orchestral timpani can be quickly tuned to precise pitches by using a foot pedal.

Several factors determine the sound a drum produces, including the type, shape and construction of the drum shell, the type of drum heads it has, and the tension of these drumheads. Different drum sounds have different uses in music. Take, for example, the modern Tom-tom drum. A jazz drummer may want drums that are high pitched, resonant and quiet whereas a rock drummer may prefer drums that are loud, dry and low-pitched. Since these drummers want different sounds, their drums are constructed and tuned differently.

The drum head has the most effect on how a drum sounds. Each type of drum head serves its own musical purpose and has its own unique sound. Double-ply drumheads dampen high frequency harmonics because they are heavier and they are suited to heavy playing. Drum heads with a white, textured coating on them muffle the overtones of the drum head slightly, producing a less diverse pitch. Drum heads with central silver or black dots tend to muffle the overtones even more. And drum heads with perimeter sound rings mostly eliminate overtones (Howie 2005). Some jazz drummers avoid using thick drum heads, preferring single ply drum heads or drum heads with no muffling. Rock drummers often prefer the thicker or coated drum heads.

The second biggest factor that affects drum sound is head tension against the shell. When the hoop is placed around the drum head and shell and tightened down with tension rods, the tension of the head can be adjusted. When the tension is increased, the amplitude of the sound is reduced and the frequency is increased, making the pitch higher and the volume lower.

The type of shell also affects the sound of a drum. Because the vibrations resonate in the shell of the drum, the shell can be used to increase the volume and to manipulate the type of sound produced. The larger the diameter of the shell, the lower the pitch. The larger the depth of the drum, the louder the volume. Shell thickness also determines the volume of drums. Thicker shells produce louder drums. Mahogany raises the frequency of low pitches and keeps higher frequencies at about the same speed. When choosing a set of shells, a jazz drummer may want smaller maple shells, while a rock drummer may want larger birch shells. For more information about tuning drums or the physics of a drum, visit the external links listed below.

Drums made with alligator skins have been found in Neolithic cultures located in China, dating to a period of 5500–2350 BC. In literary records, drums manifested shamanistic characteristics were often used in ritual ceremonies.

The bronze Dong Son drum was fabricated by the Bronze Age Dong Son culture of northern Vietnam. They include the ornate Ngoc Lu drum.

Macaque monkeys drum objects in a rhythmic way to show social dominance and this has been shown to be processed in a similar way in their brains to vocalizations suggesting an evolutionary origin to drumming as part of social communication. Other primates make drumming sounds by chest beating or hand clapping, and rodents such as kangaroo rats also make similar sounds using their paws on the ground.

Drums are used not only for their musical qualities, but also as a means of communication over great distances. The talking drums of Africa are used to imitate the tone patterns of spoken language. Throughout Sri Lankan history drums have been used for communication between the state and the community, and Sri Lankan drums have a history stretching back over 2500 years.

Drumming may be a purposeful expression of emotion for entertainment, spiritualism and communication. Many cultures practice drumming as a spiritual or religious passage and interpret drummed rhythm similarly to spoken language or prayer. Drumming has developed over millennia to be a powerful art form. Drumming is commonly viewed as the root of music and is sometimes performed as a kinesthetic dance. As a discipline, drumming concentrates on training the body to punctuate, convey and interpret musical rhythmic intention to an audience and to the performer.

Chinese troops used tàigǔ drums to motivate troops, to help set a marching pace, and to call out orders or announcements. For example, during a war between Qi and Lu in 684 BC, the effect of drum on soldier's morale is employed to change the result of a major battle. Fife-and-drum corps of Swiss mercenary foot soldiers also used drums. They used an early version of the snare drum carried over the player's right shoulder, suspended by a strap (typically played with one hand using traditional grip). It is to this instrument that the English word "drum" was first used. Similarly, during the English Civil War rope-tension drums would be carried by junior officers as a means to relay commands from senior officers over the noise of battle. These were also hung over the shoulder of the drummer and typically played with two drum sticks. Different regiments and companies would have distinctive and unique drum beats only they recognized. In the mid-19th century, the Scottish military started incorporating pipe bands into their Highland Regiments.

During pre-Columbian warfare, Aztec nations were known to have used drums to send signals to the battling warriors. The Nahuatl word for drum is roughly translated as huehuetl.

The Rig Veda, one of the oldest religious scriptures in the world, contain several references to the use of Dundhubi (war drum). Arya tribes charged into battle to the beating of the war drum and chanting of a hymn that appears in Book VI of the Rig Veda and also the Atharva Veda where it is referred to as the "Hymn to the battle drum".






</doc>
<doc id="7951" url="https://en.wikipedia.org/wiki?curid=7951" title="Delphi">
Delphi

Delphi (; ) is famous as the ancient sanctuary that grew rich as the seat of Pythia, the oracle who was consulted about important decisions throughout the ancient classical world. Moreover, the Greeks considered Delphi the navel (or centre) of the world, as represented by the stone monument known as the Omphalos of Delphi.

It occupies an impressive site on the south-western slope of Mount Parnassus, overlooking the coastal plain to the south and the valley of Phocis. It is now an extensive archaeological site with a modern town of the same name nearby. It is recognised by UNESCO as a World Heritage Site in having had a phenomenal influence in the ancient world, as evidenced by the rich monuments built there by most of the important ancient Greek city-states, demonstrating their fundamental Hellenic unity.

Delphi is located in upper central Greece, on multiple plateaux along the slope of Mount Parnassus, and includes the Sanctuary of Apollo, the site of the ancient Oracle. This semicircular spur is known as Phaedriades, and overlooks the Pleistos Valley.

In myths dating to the classical period of Ancient Greece (510-323 BC), Zeus determined the site of Delphi when he sought to find the centre of his "Grandmother Earth" (Gaia). He sent two eagles flying from the eastern and western extremities, and the path of the eagles crossed over Delphi where the omphalos, or navel of Gaia was found.

Earlier myths include traditions that Pythia, or the Delphic oracle, already was the site of an important oracle in the pre-classical Greek world (as early as 1400 BC) and, rededicated from about 800 BC, when it served as the major site during classical times for the worship of the god Apollo. Apollo was said to have slain Python, a "drako" a serpent or a dragon who lived there and protected the navel of the Earth. "Python" (derived from the verb πύθω ("pythō"), "to rot") is claimed by some to be the original name of the site in recognition of Python which Apollo defeated. The Homeric Hymn to Delphic Apollo recalled that the ancient name of this site had been "Krisa". Others relate that it was named Pytho and that Pythia, the priestess serving as the oracle, was chosen from their ranks by a group of priestesses who officiated at the temple.

Excavation at Delphi, which was a post-Mycenaean settlement of the late 9th century, has uncovered artifacts increasing steadily in volume beginning with the last quarter of the 8th century BC. Pottery and bronze as well as tripod dedications continue in a steady stream, in contrast to Olympia. Neither the range of objects nor the presence of prestigious dedications proves that Delphi was a focus of attention for a wide range of worshippers, but the large quantity of valuable goods, found in no other mainland sanctuary, encourages that view.

Apollo's sacred precinct in Delphi was a panhellenic sanctuary, where every four years, starting in 586 BC athletes from all over the Greek world competed in the Pythian Games, one of the four Panhellenic Games, precursors of the Modern Olympics. The victors at Delphi were presented with a laurel crown ("stephanos") which was ceremonially cut from a tree by a boy who re-enacted the slaying of the Python. (These competitions are also called stephantic games, after the crown.) Delphi was set apart from the other games sites because it hosted the mousikos agon, musical competitions.

These Pythian Games rank second among the four stephanitic games chronologically and in importance. These games, though, were different from the games at Olympia in that they were not of such vast importance to the city of Delphi as the games at Olympia were to the area surrounding Olympia. Delphi would have been a renowned city whether or not it hosted these games; it had other attractions that led to it being labeled the "omphalos" (navel) of the earth, in other words, the centre of the world.

In the inner "hestia" (hearth) of the Temple of Apollo, an eternal flame burned. After the battle of Plataea, the Greek cities extinguished their fires and brought new fire from the hearth of Greece, at Delphi; in the foundation stories of several Greek colonies, the founding colonists were first dedicated at Delphi.

The name "Delphoi" comes from the same root as δελφύς "delphys", "womb" and may indicate archaic veneration of Gaia at the site. Apollo is connected with the site by his epithet Δελφίνιος "Delphinios", "the Delphinian". The epithet is connected with dolphins (Greek δελφίς,-ῖνος) in the Homeric "Hymn to Apollo" (line 400), recounting the legend of how Apollo first came to Delphi in the shape of a dolphin, carrying Cretan priests on his back. The Homeric name of the oracle is "Pytho" ("Πυθώ"). Another legend held that Apollo walked to Delphi from the north and stopped at Tempe, a city in Thessaly, to pick laurel (also known as bay tree) which he considered to be a sacred plant. In commemoration of this legend, the winners at the Pythian Games received a wreath of laurel picked in the Temple.

Delphi became the site of a major temple to Phoebus Apollo, as well as the Pythian Games and the famous prehistoric oracle. Even in Roman times, hundreds of votive statues remained, described by Pliny the Younger and seen by Pausanias. Carved into the temple were three phrases: ("gnōthi seautón" = "know thyself") and ("mēdén ágan" = "nothing in excess"), and ("engýa pára d'atē" = "make a pledge and mischief is nigh"), In antiquity, the origin of these phrases was attributed to one or more of the Seven Sages of Greece by authors such as
Plato and Pausanias. Additionally, according to Plutarch's essay on the meaning of the ""E at Delphi""—the only literary source for the inscription—there was also inscribed at the temple a large letter E. Among other things epsilon signifies the number 5. However, ancient as well as modern scholars have doubted the legitimacy of such inscriptions. According to one pair of scholars, "The actual authorship of the three maxims set up on the Delphian temple may be left uncertain. Most likely they were popular proverbs, which tended later to be attributed to particular sages."

According to the Homeric-hymn to the Pythian Apollo, Apollo shot his first arrow as an infant which effectively slew the serpent Pytho, the son of Gaia, who guarded the spot. To atone the murder of Gaia's son, Apollo was forced to fly and spend eight years in menial service before he could return forgiven. A festival, the Septeria, was held every year, at which the whole story was represented: the slaying of the serpent, and the flight, atonement, and return of the god.

The Pythian Games took place every four years to commemorate Apollo's victory. Another regular Delphi festival was the "Theophania" (Θεοφάνεια), an annual festival in spring celebrating the return of Apollo from his winter quarters in Hyperborea. The culmination of the festival was a display of an image of the gods, usually hidden in the sanctuary, to worshippers.

The "Theoxenia" was held each summer, centred on a feast for "gods and ambassadors from other states". Myths indicate that Apollo killed the chthonic serpent Python, Pythia in older myths, but according to some later accounts his wife, Pythia, who lived beside the Castalian Spring. Some sources say it is because Python had attempted to rape Leto while she was pregnant with Apollo and Artemis.

This spring flowed toward the temple but disappeared beneath, creating a cleft which emitted chemical vapors that caused the Oracle at Delphi to reveal her prophecies. Apollo killed Python but had to be punished for it, since she was a child of Gaia. The shrine dedicated to Apollo was originally dedicated to Gaia and shared with Poseidon. The name Pythia remained as the title of the Delphic Oracle.

Erwin Rohde wrote that the Python was an earth spirit, who was conquered by Apollo, and buried under the Omphalos, and that it is a case of one deity setting up a temple on the grave of another. Another view holds that Apollo was a fairly recent addition to the Greek pantheon coming originally from Lydia. The Etruscans coming from northern Anatolia also worshipped Apollo, and it may be that he was originally identical with Mesopotamian Aplu, an Akkadian title meaning "son", originally given to the plague God Nergal, son of Enlil. Apollo Smintheus (Greek ), the mouse killer eliminates mice, a primary cause of disease, hence he promotes preventive medicine.

Delphi is perhaps best known for its oracle, the Pythia, the sibyl or priestess at the sanctuary dedicated to Apollo. According to Aeschylus in the prologue of the "Eumenides", the oracle had origins in prehistoric times and the worship of Gaea, a view echoed by H.W. Parke.

One tale of the sanctuary's discovery states that a goatherd, who grazed his flocks on Parnassus, one day observed his goats playing with great agility upon nearing a chasm in the rock; the goatherd noticing this held his head over the chasm causing the fumes to go to his brain; throwing him into a strange trance.

Apollo spoke through his oracle. She had to be an older woman of blameless life chosen from among the peasants of the area. Alone in an enclosed inner sanctum (Ancient Greek "adyton" - "do not enter") she sat on a tripod seat over an opening in the earth (the "chasm"). According to legend, when Apollo slew Python its body fell into this fissure and fumes arose from its decomposing body. Intoxicated by the vapours, the sibyl would fall into a trance, allowing Apollo to possess her spirit. In this state she prophesied. The oracle could not be consulted during the winter months, for this was traditionally the time when Apollo would live among the Hyperboreans. Dionysus would inhabit the temple during his absence.

The time to consult pythia for an oracle during the year is determined from astronomical and geological grounds related to the constellations of Lyra and Cygnus but the hydrocarbon vapours emitted from the chasm. Similar practice was followed in other Apollo oracles too.

While in a trance the Pythia "raved" – probably a form of ecstatic speech – and her ravings were "translated" by the priests of the temple into elegant hexameters. It has been speculated that the ancient writers, including Plutarch who had worked as a priest at Delphi, were correct in attributing the oracular effects to the sweet-smelling "pneuma" (Ancient Greek for breath, wind or vapour) escaping from the chasm in the rock. That exhalation could have been high in the known anaesthetic and sweet-smelling ethylene or other hydrocarbons such as ethane known to produce violent trances. Though this theory remains debatable the authors put up a detailed answer to their critics.

Ancient sources describe the priestess using “laurel” to inspire her prophecies. Several alternative plant candidates have been suggested including Cannabis, Hyoscyamus, Rhododendron and Oleander. Harissis claims that a review of contemporary toxicological literature indicates that oleander causes symptoms similar to those shown by the Pythia, and his study of ancient texts shows that oleander was often included under the term “laurel”. The Pythia may have chewed oleander leaves and inhaled their smoke prior to her oracular pronouncements and sometimes dying from the toxicity. The toxic substances of oleander resulted in symptoms similar to those of epilepsy, the “sacred disease,” which may have been seen as the possession of the Pythia by the spirit of Apollo.

The Delphic Oracle exerted considerable influence throughout the Greek world, and she was consulted before all major undertakings including wars and the founding of colonies. She also was respected by the Greek-influenced countries around the periphery of the Greek world, such as Lydia, Caria, and even Egypt.

The oracle was also known to the early Romans. Rome's seventh and last king, Lucius Tarquinius Superbus, after witnessing a snake near his palace, sent a delegation including two of his sons to consult the oracle.

In 83 BC a Thracian tribe raided Delphi, burned the temple, plundered the sanctuary and stole the "unquenchable fire" from the altar. During the raid, part of the temple roof collapsed. The same year, the Temple was severely damaged by an earthquake. Thus the Oracle fell in decay and the surrounding area became impoverished. The sparse local population led to difficulties in filling the posts required. The Oracle's credibility waned due to doubtful predictions.

The oracle flourished again in the second century AD during the rule of emperor Hadrian, who is believed to have visited the oracle twice and offered complete autonomy to the city.
By the 4th century, Delphi had acquired the status of a city. 

Despite the rise of Christianity across the Roman Empire, the oracle remained a religious centre throughout the 4th century, and the Pythian Games continued to be held at least until 424 AD; however, the decline continued. The attempt of the emperor Julian to revive polytheism did not survive his reign. Excavations have revealed a large three-aisled basilica in the city, as well as traces of a church building in the sanctuary's gymnasium. The site was abandoned in the 6th or 7th centuries, although a single bishop of Delphi is attested in an episcopal list of the late 8th/early 9th centuries.

Delphi was since ancient times a place of worship for Gaia, the mother goddess connected with fertility. The town started to gain pan-Hellenic relevance as both a shrine and an oracle in the 7th century BC. Initially under the control of Phocaean settlers based in nearby Kirra (currently Itea), Delphi was reclaimed by the Athenians during the First Sacred War (597–585 BC). The conflict resulted in the consolidation of the Amphictyonic League, which had both a military and a religious function revolving around the protection of the Temple of Apollo. This shrine was destroyed by fire in 548 BC and then fell under the control of the Alcmaeonids banned from Athens. In 449–448 BC, the Second Sacred War (fought in the wider context of the First Peloponnesian War between the Peloponnesian League led by Sparta and the Delian-Attic League led by Athens) resulted in the Phocians gaining control of Delphi and the management of the Pythian Games.

In 356 BC the Phocians under Philomelos captured and sacked Delphi, leading to the Third Sacred War (356–346 BC), which ended with the defeat of the former and the rise of Macedon under the reign of Philip II. This led to the Fourth Sacred War (339 BC), which culminated in the Battle of Chaeronea (338 BC) and the establishment of Macedonian rule over Greece.
In Delphi, Macedonian rule was superseded by the Aetolians in 279 BC, when a Gallic invasion was repelled, and by the Romans in 191 BC. The site was sacked by Lucius Cornelius Sulla in 86 BC, during the Mithridatic Wars, and by Nero in 66 AD. Although subsequent Roman emperors of the Flavian dynasty contributed towards to the restoration of the site, it gradually lost importance. In the course of the 3rd century mystery cults became more popular than the traditional Greek pantheon. Christianity, which started as yet one more mystery cult, soon gained ground, and this eventually resulted in the persecution of pagans in the late Roman Empire. The anti-pagan legislation of the Flavian dynasty deprived ancient sanctuaries of their assets. The emperor Julian attempted to reverse this religious climate, yet his "pagan revival" was particularly short-lived. When the doctor Oreibasius visited the oracle of Delphi, in order to question the fate of paganism,he received a pessimistic answer:

[Tell the king that the flute has fallen to the ground. Phoebus does not have a home any more, neither an oracular laurel, nor a speaking fountain, because the talking water has dried out.]

It was shut down during the persecution of pagans in the late Roman Empire by Theodosius I in 381 AD.

Time and natural disasters added to the picture of desolation of the once glorious place, and during the Ottoman period the village of Kastri was founded on the site.

Occupation of the site at Delphi can be traced back to the Neolithic period with extensive occupation and use beginning in the Mycenaean period (1600–1100 BC). Most of the ruins that survive today date from the most intense period of activity at the site in the 6th century BC.

The ruins of the Temple of Delphi visible today date from the 4th century BC, and are of a peripteral Doric building. It was erected by Spintharus, Xenodoros, and Agathon on the remains of an earlier temple, dated to the 6th century BC which itself was erected on the site of a 7th-century BC construction attributed to the architects Trophonios and Agamedes.

The Amphictyonic Council was a council of representatives from six Greek tribes that controlled Delphi and also the quadrennial Pythian Games. They met biannually and came from Thessaly and central Greece. Over time, the town of Delphi gained more control of itself and the council lost much of its influence.

From the entrance of the site, continuing up the slope almost to the temple itself, are a large number of votive statues, and numerous so-called treasuries. These were built by many of the Greek city states to commemorate victories and to thank the oracle for her advice which was thought to have contributed to those victories. These buildings held the rich offerings made to Apollo; these were frequently a "tithe" or tenth of the spoils of a battle. The most impressive is the now-restored Athenian Treasury, built to commemorate their victory at the Battle of Marathon in 490 BC.

The Siphnian Treasury was dedicated by the city of Siphnos whose citizens gave a tithe of the yield from their silver mines until the mines came to an abrupt end when the sea flooded the workings.

One of the largest of the treasuries was that of Argos. Built in the late Doric period, the Argives took great pride in establishing their place amongst the other city states. Completed in 380 BC, the treasury draws inspiration mostly from the Temple of Hera located in the Argolis, the acropolis of the city. However, recent analysis of the Archaic elements of the treasury suggest that its founding preceded this.

Other identifiable treasuries are those of the Sikyonians, the Boeotians and the Thebans.

Located in front of the Temple of Apollo, the main altar of the sanctuary was paid for and built by the people of Chios. It is dated to the 5th century BC by the inscription on its cornice. Made entirely of black marble, except for the base and cornice, the altar would have made a striking impression. It was restored in 1920.

The stoa leads off north-east from the main sanctuary. It was built in the Ionic order and consists of seven fluted columns, unusually carved from single pieces of stone (most columns were constructed from a series of discs joined together). The inscription on the stylobate indicates that it was built by the Athenians after their naval victory over the Persians in 478 BC, to house their war trophies. The stoa was attached to the existing Polygonal Wall.

The Sibyl rock is a pulpit-like outcrop of rock between the Athenian Treasury and the Stoa of the Athenians upon the sacred way which leads up to the temple of Apollo in the archaeological area of Delphi. It is claimed to be where an ancient Sibyl pre-dating the Pythia of Apollo sat to deliver her prophecies.

The ancient theatre at Delphi was built further up the hill from the Temple of Apollo giving spectators a view of the entire sanctuary and the valley below. It was originally built in the 4th century BC but was remodeled on several occasions, particularly in 160/159 B.C. at the expenses of king Eumenes II of Pergamon and in 67 A.D. on the occasion of emperor Nero's visit.
The koilon (cavea) leans against the natural slope of the mountain whereas its eastern part overrides a little torrent which led the water of the fountain Cassotis right underneath the temple of Apollo. The orchestra was initially a full circle with a diameter measuring 7 meters. The rectangular scene building ended up in two arched openings, of which the foundations are preserved today. Access to the theatre was possible through the parodoi, i.e. the side corridors. On the support walls of the parodoi are engraved large numbers of manumission inscriptions recording fictitious sales of the slaves to the god.
The koilon was divided horizontally in two zones via a corridor called diazoma. The lower zone had 27 rows of seats and the upper one only 8. Six radially arranged stairs divided the lower part of the koilon in seven tiers. The theatre could accommodate about 4,500 spectators.

On the occasion of Nero's visit to Greece in 67 A.D. various alterations took place. The orchestra was paved and delimited by a parapet made of stone. The proscenium was replaced by a low pedestal, the pulpitum; its façade was decorated with scenes from Hercules' myth in relief. Further repairs and transformations took place in the 2nd century A.D. Pausanias mentions that these were carried out under the auspices of Herod Atticus.
In antiquity, the theatre was used for the vocal and musical contests which formed part of the programme of the Pythian Games in the late Hellenistic and Roman period. The theatre was abandoned when the sanctuary declined in Late Antiquity. After its excavation and initial restoration it hosted theatrical performances during the Delphic Festivals organized by A. Sikelianos and his wife, Eva Palmer, in 1927 and in 1930. It has recently been restored again as the serious landslides posed a grave threat for its stability for decades.

The Tholos at the sanctuary of Athena Pronoia (Ἀθηνᾶ Πρόνοια, "Athena of forethought") is a circular building that was constructed between 380 and 360 BC. It consisted of 20 Doric columns arranged with an exterior diameter of 14.76 meters, with 10 Corinthian columns in the interior.

The Tholos is located approximately a half a mile (800 m) from the main ruins at Delphi (at ). Three of the Doric columns have been restored, making it the most popular site at Delphi for tourists to take photographs.

The architect of the "vaulted temple at Delphi" is named by Vitruvius, in "De architectura" Book VII, as Theodorus Phoceus (not Theodorus of Samos, whom Vitruvius names separately).

The gymnasium, which is half a mile away from the main sanctuary, was a series of buildings used by the youth of Delphi. The building consisted of two levels: a stoa on the upper level providing open space, and a palaestra, pool and baths on lower floor. These pools and baths were said to have magical powers, and imparted the ability to communicate to Apollo himself.

The stadium is located further up the hill, beyond the "via sacra" and the theatre. It was originally built in the 5th century BC but was altered in later centuries. The last major remodelling took place in the 2nd century AD under the patronage of Herodes Atticus when the stone seating was built and (arched) entrance. It could seat 6500 spectators and the track was 177 metres long and 25.5 metres wide.

It was at the Pythian games that prominent political leaders, such as Cleisthenes, tyrant of Sikyon, and Hieron, tyrant of Syracuse, competed with their chariots. The hippodrome where these events took place was referred to by Pindar, and this monument was sought by archaeologists for over two centuries.

Its traces have recently been found at Gonia in the plain of Krisa in the place where the original stadium was sited.

The retaining wall was built to support the terrace housing the construction of the second temple of Apollo in 548 BC. Its name is taken from the polygonal masonry of which it is constructed. At a later date, from 200 BC onwards, the stones were inscribed with the manumission contracts of slaves who were consecrated to Apollo. Approximately a thousand manumissions are recorded on the wall.

The sacred spring of Delphi lies in the ravine of the Phaedriades. The preserved remains of two monumental fountains that received the water from the spring date to the Archaic period and the Roman, with the latter cut into the rock.

Delphi is famous for its many preserved athletic statues. It is known that Olympia originally housed far more of these statues, but time brought ruin to many of them, leaving Delphi as the main site of athletic statues. Kleobis and Biton, two brothers renowned for their strength, are modeled in two of the earliest known athletic statues at Delphi. The statues commemorate their feat of pulling their mother's cart several miles to the Sanctuary of Hera in the absence of oxen. The neighbors were most impressed and their mother asked Hera to grant them the greatest gift. When they entered Hera's temple, they fell into a slumber and never woke, dying at the height of their admiration, the perfect gift.

The Charioteer of Delphi is another ancient relic that has withstood the centuries. It is one of the best known statues from antiquity. The charioteer has lost many features, including his chariot and his left arm, but he stands as a tribute to athletic art of antiquity.

Ancient tradition accounted for four temples that successively occupied the site before the 548/7 BC fire, following which the Alcmaeonids built a fifth. The poet Pindar celebrated the Alcmaeonid's temple in "Pythian" 7.8-9 and he also provided details of the third building ("Paean" 8. 65-75). Other details are given by Pausanias (10.5.9-13) and the Homeric Hymn to Apollo (294 ff.). The first temple was said to have been constructed out of olive branches from Tempe. The second was made by bees out of wax and wings but was miraculously carried off by a powerful wind and deposited among the Hyperboreans. The third, as described by Pindar, was created by the gods Hephaestus and Athena, but its architectural details included Siren-like figures or 'Enchantresses', whose baneful songs eventually provoked the Olympian gods to bury the temple in the earth (according to Pausanias, it was destroyed by earthquake and fire). In Pindar's words, addressed to the Muses:

<br>
The fourth temple was said to have been constructed from stone by Trophonius and Agamedes.

The Delphi Archaeological Museum is at the foot of the main archaeological complex, on the east side of the village, and on the north side of the main road. The museum houses an impressive collection associated with ancient Delphi, including the earliest known notation of a melody, the famous Charioteer, golden treasures discovered beneath the Sacred Way, and fragments of reliefs from the Siphnian Treasury. Immediately adjacent to the exit (and overlooked by most tour guides) is the inscription that mentions the Roman proconsul Gallio.

Entries to the museum and to the main complex are separate and chargeable, and a reduced rate ticket gets entry to both. There is a small cafe, and a post office by the museum.

The site had been occupied by the village of Kastri since medieval times. Before a systematic excavation of the site could be undertaken, the village had to be relocated but the residents resisted. The opportunity to relocate the village occurred when it was substantially damaged by an earthquake, with villagers offered a completely new village in exchange for the old site. In 1893 the French Archaeological School removed vast quantities of soil from numerous landslides to reveal both the major buildings and structures of the sanctuary of Apollo and of Athena Pronoia along with thousands of objects, inscriptions and sculptures.

The site is now an archaeological one, and a very popular tourist destination. It is easily accessible from Athens as a day trip, and is often combined with the winter sports facilities available on Mount Parnassus, as well as the beaches and summer sports facilities of the nearby coast of Phocis.

The site is also protected as a site of extraordinary natural beauty, and the views from it are also protected: no industrial artefacts are to be seen from Delphi other than roads and traditional architecture residences (for example high voltage power lines and the like are routed so as to be invisible from the area of the sanctuary).

During the Great Excavation were discovered architectural members from a 5th-century Christian basilica, when Delphi were a bishopric. Other important Late Roman buildings are the Eastern Baths, the house with the peristyle, the Roman Agora, the large cistern usw. At the outskirts of the city there were located late Roman cemeteries. 
To the Southeast of the precinct of Apollo lay the so-called Southeastern Mansion, a very large building with a 65 meters-long façade, spread over four levels, with four triclinia and private baths. Large storage jars kept the provisions, whereas other pottery vessels and luxury items were discovered in the rooms. Among the finds stands out a tiny leopard made of mother of pearl, possibly of Sassanian origin, on display in the ground floor gallery of the Delphi Archaeological Museum. 
The mansion dates to the beginning of the 5th century and functioned as a private house until 580, later however it was transformed into a potters' workshop. It is only then, in the beginning of the 6th century, that the city seems to decline: its size is reduced and its trade contacts seem to be drastically diminished. Local pottery production is produced in large quantities: it is coarser and made of reddish clay, aiming at satisfying the needs of the inhabitants.

The Sacred Way remained the main street of the settlement, transformed, however, into a street with commercial and industrial use. Around the agora were built workshops as well as the only intra muros early Christian basilica. The domestic area spread mainly in the western part of the settlement. The houses were rather spacious and two large cisterns provided running water to them.

From the 16th century onwards, West Europe developed an interest in Delphi. In the mid-15th century Strabo was first translated in Latin. The earliest depictions of Delphi were totally imaginary, created by the German N. Gerbel, who published in 1545 a text based on the map of Greece by N. Sofianos. The ancient sanctuary was depicted as a fortified city.
The first travelers with archaeological interests, apart from the precursor Cyriacus of Ancona, were the British George Wheler and the French Jacob Spon, who visited Greece in a joint expedition in 1675-76. They published their impressions separately. In Wheler's "Journey into Greece", published in 1682, a sketch of the region of Dephi appeared, where the settlement of Kastri and some ruins were depicted. The illustrations in Spon's publication "Voyage d'Italie, de Dalmatie, de Grèce et du Levant, 1678" are considered original and groundbreaking.

Travelers continued to visit Delphi throughout the 19th century and published their books which contained diaries, sketches, views of the site as well as pictures of coins. The illustrations often reflected the spirit of romanticism, as evident by the works of Otto Magnus von Stackelberg, where, apart from the landscapes ("La Grèce. Vues pittoresques et topographiques", Paris 1834) are depicted also human types ("Costumes et usages des peuples de la Grèce moderne dessinés sur les lieux", Paris 1828). The philhellene painter W. Williams has comprised the landscape of Delphi in his themes (1829). important personalities such as F.Ch.-H.-L. Pouqueville, W.M. Leake, Chr. Wordsworth and Lord Byron are amongst the most important visitors of Delphi.
After the foundation of the modern Greek state, the press becomes also interested in these travelers. Thus "Ephemeris" writes (17/03/1889):
“In the "Revues des Deux Mondes" Paul Lefaivre published his memoirs from an excursion to Delphi. The French author relates in a charming style his adventures on the road, praising particularly the ability of an old woman to put back in its place the dismantled arm of one of his foreign traveling companions, who had fallen off the horse. In Arachova the Greek type is preserved intact. The men are rather athletes than farmers, built for running and wrestling, particularly elegant and slender under their mountain gear. Only briefly does he refer to the antiquities of Delphi, but he refers to a pelasgian wall 80 meters long, on which innumerable inscriptions are carved, decrees, conventions, manumissions".

Gradually the first travelling guides appeared. The revolutionary "pocket" books invented by Karl Baedeker, accompanied by maps useful for visiting archaeological sites such as Delphi (1894) and the informed plans, the guides became practical and popular. The photographic lens revolutionized the way of depicting the landscape and the antiquities, particularly from 1893 onwards, when the systematic excavations of the French Archaeological School started. However, artists such as Vera Willoughby, continued to be inspired by the landscape.

Delphic themes inspired several graphic artists. Besides the landscape, Pythia/Sibylla become an illustration subject even on Tarot cards. A famous example constitutes Michelangelo's Delphic Sibyl (1509), the 19th-century German engraving Oracle of Apollo at Delphi, as well as the most recent The Oracle of Delphi, inc on paper, by the Swedish Malin Lind.
Modern artists are inspired also by the Delphic Maxims. Examples of such works are displayed in the "Sculpture park of the European Cultural Center of Delphi" and in exhibitions taking place at the Archaeological Museum of Delphi.

Delphi inspired literature as well. In 1814 W. Haygarth, friend of Lord Byron, refers to Delphi in his work "Greece, a Poem". In 1888 Charles Marie René Leconte de Lisle published his lyric drama L’Apollonide, accompanied by music by Franz Servais. More recent French authors used Delphi as a source of inspiration such as Yves Bonnefoy (Delphes du second jour) or Jean Sullivan (nickname of Joseph Lemarchand) in L'Obsession de Delphes (1967), but also Rob MacGregor's Indiana Jones and the Peril at Delphi (1991).

The presence of Delphi in Greek literature is very intense. Poets such as Kostis Palamas (The Delphic Hymn, 1894), Kostas Karyotakis (Delphic festival, 1927), Nikephoros Vrettakos (return from Delphi, 1957), Yannis Ritsos (Delphi, 1961–62) and Kiki Dimoula (Gas omphalos and Appropriate terrain 1988), to mention only the most renowned ones. Angelos Sikelianos wrote The Dedication (of the Delphic speech) (1927), the Delphic Hymn (1927) and the tragedy Sibylla (1940), whereas in the context of the Delphic idea and the Delphic festivals he published an essay titled "The Delphic union" (1930). The nobelist George Seferis wrote an essay under the title "Delphi", comprised in the book "Dokimes".

The importance of Delphi for the Greeks is significant. The site has been recorded on the collective memory and have been expressed through tradition. Nikolaos Politis, the famous Greek ethnographer, in his Studies on the life and language of the Greek people - part A, offers two examples from Delphi:
a) the priest of Apollo (176)
When Christ was born a priest of Apollo was sacrificing below the monastery of Panayia, on the road of Livadeia, on a site called Logari. Suddenly he abandoned the sacrifice and says to the people: "in this moment was born the son of God, who will be very powerful, like Apollo, but then Apollo will beat him". He didn't have time to finish his speech and a thunder came down and burnt him, opening the rock nearby into two. [p. 99]
b)The Mylords (108)
The Mylords are not Christians, because nobody ever saw them cross themselves. They originate from the old pagan inhabitants of Delphi who kept their property in castle called Adelphi, named after the two brother princes who built it. When Christ and his mother came to the site, and all people around converted to Christianity they thought that they should better leave; thus the Mylords left for the West and took all their belongings with them. The Mylords come here now and worship these stones. [p. 59]







</doc>
<doc id="7952" url="https://en.wikipedia.org/wiki?curid=7952" title="Digital Equipment Corporation">
Digital Equipment Corporation

Digital Equipment Corporation, also known as DEC and using the trademark Digital, was a major American company in the computer industry from the 1950s to the 1990s.

DEC was a leading vendor of computer systems, including computers, software, and peripherals. Their PDP and successor VAX products were the most successful of all minicomputers in terms of sales.

DEC was acquired in June 1998 by Compaq, in what was at that time the largest merger in the history of the computer industry. At the time, Compaq was focused on the enterprise market and had recently purchased several other large vendors. DEC was a major player overseas where Compaq had less presence. However, Compaq had little idea what to do with its acquisitions, and soon found itself in financial difficulty of its own. The company subsequently merged with Hewlett-Packard (HP) in May 2002. some of DEC's product lines were still produced under the HP name.

From 1957 until 1992, DEC's headquarters were located in a former wool mill in Maynard, Massachusetts (since renamed Clock Tower Place, and now home to many companies). DEC was acquired in June 1998 by Compaq, which subsequently merged with Hewlett-Packard (HP) in May 2002. Some parts of DEC, notably the compiler business and the Hudson, Massachusetts facility, were sold to Intel.

Initially focusing on the small end of the computer market allowed DEC to grow without its potential competitors making serious efforts to compete with them. Their PDP series of machines became popular in the 1960s, especially the PDP-8, widely considered to be the first successful minicomputer. Looking to simplify and update their line, DEC replaced most of their smaller machines with the PDP-11 in 1970, eventually selling over 600,000 units and cementing DEC's position in the industry.

Originally designed as a follow-on to the PDP-11, DEC's VAX-11 series was the first widely used 32-bit minicomputer, sometimes referred to as "superminis". These systems were able to compete in many roles with larger mainframe computers, such as the IBM System/370. The VAX was a best-seller, with over 400,000 sold, and its sales through the 1980s propelled the company into the second largest computer company in the industry. At its peak, DEC was the second largest employer in Massachusetts, second only to the Massachusetts State Government.

The rapid rise of the business microcomputer in the late 1980s, and especially the introduction of powerful 32-bit systems in the 1990s, quickly eroded the value of DEC's systems. DEC's last major attempt to find a space in the rapidly changing market was the DEC Alpha 64-bit RISC instruction set architecture. DEC initially started work on Alpha as a way to re-implement their VAX series, but also employed it in a range of high-performance workstations. Although the Alpha processor family met both of these goals, and, for most of its lifetime, was the fastest processor family on the market, extremely high asking prices were outsold by lower priced x86 chips from Intel and clones such as AMD.

DEC was acquired in June 1998 by Compaq, in what was at that time the largest merger in the history of the computer industry. At the time, Compaq was focused on the enterprise market and had recently purchased several other large vendors. DEC was a major player overseas where Compaq had less presence. However, Compaq had little idea what to do with its acquisitions, and soon found itself in financial difficulty of its own. The company subsequently merged with Hewlett-Packard (HP) in May 2002. , some of DEC's product lines were still produced under the HP name.

Beyond DECsystem-10/20, PDP, VAX and Alpha, DEC was well respected for its communication subsystem designs, such as Ethernet, DNA (DIGITAL Network Architecture: predominantly DECnet products), DSA (Digital Storage Architecture: disks/tapes/controllers), and its "dumb terminal" subsystems including VT100 and DECserver products.

DEC's Research Laboratories (or Research Labs, as they were commonly known) conducted DEC's corporate research. Some of them were operated by Compaq and are still operated by Hewlett-Packard. The laboratories were:


Some of the former employees of DEC's Research Labs or DEC's R&D in general include:

Some of the former employees of Digital Equipment Corp who were responsible for developing Alpha and StrongARM:


Some of the work of the Research Labs was published in the "Digital Technical Journal", which was in published from 1985 until 1998.

DEC supported the ANSI standards, especially the ASCII character set, which survives in Unicode and the ISO 8859 character set family. DEC's own Multinational Character Set also had a large influence on ISO 8859-1 (Latin-1) and, by extension, Unicode.

The first versions of the C language and the Unix operating system ran on DEC's PDP series of computers (first on a PDP-7, then the PDP-11's), which were among the first commercially viable minicomputers, although for several years DEC itself did not encourage the use of Unix.

DEC produced widely used and influential interactive operating systems, including OS-8, TOPS-10, TOPS-20, RSTS/E, RSX-11, RT-11, and OpenVMS. PDP computers, in particular the PDP-11 model, inspired a generation of programmers and software developers. Some PDP-11 systems more than 25 years old (software and hardware) are still being used to control and monitor factories, transportation systems and nuclear plants. DEC was an early champion of time-sharing systems.

The command-line interfaces found in DEC's systems, eventually codified as DCL, would look familiar to any user of modern microcomputer CLIs; those used in earlier systems, such as CTSS, IBM's JCL, or Univac's time-sharing systems, would look utterly alien. Many features of the CP/M and MS-DOS CLI show a recognizable family resemblance to DEC's OSes, including command names such as DIR and HELP and the "name-dot-extension" file naming conventions.

The MUMPS programming language, with its built-in database, was developed on the PDP-7, 9, and 15 series machines. MUMPS is still widely used in medical informations systems, such as those provide by Meditech and Epic Systems.

VAX and MicroVAX computers (very widespread in the 1980s) running VMS formed one of the most important proprietary networks, DECnet, which linked business and research facilities. The DECnet protocols formed one of the first peer-to-peer networking standards, with DECnet phase I being released in the mid-1970s. Email, file sharing, and distributed collaborative projects existed within the company long before their value was recognized in the market.

DEC, Intel and Xerox through their collaboration to create the DIX standard, were champions of Ethernet, but DEC is the company that made Ethernet commercially successful. Initially, Ethernet-based DECnet and LAT protocols interconnected VAXes with DECserver terminal servers. Starting with the Unibus to Ethernet adapter, multiple generations of Ethernet hardware from DEC were the de facto standard. The CI "computer interconnect" adapter was the industry's first network interface controller to use separate transmit and receive "rings".

DEC also invented clustering, an operating system technology that treated multiple machines as one logical entity. Clustering permitted sharing of pooled disk and tape storage via the HSC50/70/90 and later series of Hierarchical Storage Controllers (HSC). The HSCs delivered the first hardware RAID 0 and RAID 1 capabilities and the first serial interconnects of multiple storage technologies. This technology was the forerunner to architectures such as Network of Workstations which are used for massively cooperative tasks such as web-searches and drug research.

The LA36 and LA120 dot matrix printers became industry standards and may have hastened the demise of the Teletype Corporation.

The VT100 computer terminal became the industry standard, implementing a useful subset of the ANSI X3.64 standard, and even today terminal emulators such as HyperTerminal, PuTTY and Xterm still emulate a VT100 (or its more capable successor, the VT220).

The X Window System, the network transparent window system used on Unix and Linux, and also available on other operating systems, was developed at MIT jointly between Project Athena and the Laboratory for Computer Science. DEC was the primary sponsor for this project, which was a contemporary of the GNU Project but not associated with it.

In the period 1994–99 Linus Torvalds developed versions of Linux on early AlphaServer systems made available to him by the engineering department. Compaq software engineers developed special Linux kernel modules. A well-known Linux distribution that ran on AlphaServer systems was Red Hat 7.2. Another distribution that ran on Alpha was Gentoo Linux.

Notes-11 and its follow-on product, VAX Notes, were two of the first examples of online collaboration software, a category that has become to be known as groupware. Len Kawell, one of the original Notes-11 developers later joined Lotus Development Corporation and contributed to their Lotus Notes product.

DEC was one of the first businesses connected to the Internet, with "dec.com", registered in 1985, being one of the first of the now ubiquitous ".com" domains. DEC's "gatekeeper.dec.com" was a well-known software repository during the pre-World Wide Web days, and DEC was also the first computer vendor to open a public website, on 1 October 1993. The popular AltaVista, created by DEC, was one of the first comprehensive Internet search engines. (Although Lycos was earlier, it was much more limited.)

DEC invented Digital Linear Tape (DLT), formerly known as CompacTape, which began as a compact backup medium for MicroVAX systems, and later grew to capacities of 800 gigabytes.

Work on the first hard-disk-based MP3-player, the Personal Jukebox, started at the DEC Systems Research Center. (The project was started about a month before the merger into Compaq was completed.)

DEC's Western Research Lab created the Itsy Pocket Computer. This was developed into the Compaq iPaq line of PDAs, which replaced the Compaq Aero PDA.

Digital Federal Credit Union (DCU) is a credit union which was chartered in 1979 for employees of DEC. Today its field of membership is open to existing family members, over 900 different sponsors, several communities in Massachusetts and several organizations. DCU has over 700 different sponsors, including the companies that acquired pieces of DEC.

DEC once held the Class A IP address block 16.0.0.0/8.

Originally the users' group was called DECUS (Digital Equipment Computer User Society) during the 1960s to 1990s. When Compaq acquired DEC in 1998, the users group was renamed CUO, the Compaq Users' Organisation. When HP acquired Compaq in 2002, CUO became HP-Interex, although there are still DECUS groups in several countries. In the United States, the organization is represented by the Encompass organization; currently Connect.



</doc>
<doc id="7954" url="https://en.wikipedia.org/wiki?curid=7954" title="Dead Kennedys">
Dead Kennedys

Dead Kennedys are an American punk rock band that formed in San Francisco, California, in 1978. The band was one of the first American hardcore bands to make a significant impact in the United Kingdom.

Dead Kennedys' lyrics were usually political in nature, satirizing establishment political figures (liberal and conservative) and authority in general, as well as popular culture and even the punk movement itself. During their initial incarnation between 1978 and 1986, they attracted considerable controversy for their provocative lyrics and artwork. Several stores refused to stock their recordings, provoking debate about censorship in rock music; in the mid 1980s, vocalist and primary lyricist Jello Biafra became an active campaigner against the Parents Music Resource Center. This culminated in an obscenity trial between 1985 and 1986, which resulted in a hung jury.

The group released a total of four studio albums and one EP before disbanding in 1986. Following the band's dissolution, Biafra continued to collaborate and record with other artists including D.O.A., NoMeansNo and his own bands Lard and the Guantanamo School of Medicine, as well as releasing several spoken word performances.

In 2000 (upheld on appeal in 2003), Jello Biafra lost an acrimonious legal case initiated by his former Dead Kennedys band mates over songwriting credits and unpaid royalties. In 2001, the band reformed without Biafra; various singers have since been recruited for vocal duties.

Dead Kennedys formed in June 1978 in San Francisco, California, when East Bay Ray (Raymond Pepperell) advertised for bandmates in the newspaper "The Recycler", after seeing a ska-punk show at Mabuhay Gardens in San Francisco. The original band lineup consisted of Jello Biafra (Eric Reed Boucher) on vocals, East Bay Ray on guitar, Klaus Flouride (Geoffrey Lyall) on bass, 6025 (Carlos Cadona) on rhythm guitar and Ted (Bruce Slesinger) on drums and percussion. This lineup recorded their first demos. Their first live show was on July 19, 1978 at Mabuhay Gardens in San Francisco, California. They were the opening act on a bill that included DV8 and Negative Trend with The Offs headlining.

Dead Kennedys played numerous shows at local venues afterwards. Due to the provocative name of the band, they sometimes played under pseudonyms, including "The DK's", "The Sharks", "The Creamsicles" and "The Pink Twinkies". The band's real name generated controversy. "San Francisco Chronicle" columnist Herb Caen wrote in November 1978, "Just when you think tastelessness has reached its nadir, along comes a punk rock group called 'The Dead Kennedys', which will play at Mabuhay Gardens on Nov. 22, the 15th anniversary of John F. Kennedy's assassination." Despite mounting protests, the owner of Mabuhay declared, "I can't cancel them NOW—there's a contract. Not, apparently, the kind of contract some people have in mind." However, despite popular belief, the name was not meant to insult the Kennedy family, but according to Biafra, "to bring attention to the end of the American Dream".

6025 left the band in March 1979 under somewhat unclear circumstances, generally considered to be musical differences. In June, the band released their first single, "California Über Alles", on Biafra and East Bay Ray's independent label, Alternative Tentacles. The band followed with a poorly attended East Coast tour, being a new and fairly unknown band at the time, without a full album release.

In early 1980, they recorded and released the single "Holiday in Cambodia". In June, the band recorded their debut album, "Fresh Fruit for Rotting Vegetables", released in September of that year. The album reached number 33 on the UK Albums Chart.

On March 25, 1980, Dead Kennedys were invited to perform at the Bay Area Music Awards in an effort to give the event some "new wave credibility", in the words of the organizers. The day of the performance was spent practicing the song they were asked to play, the underground hit "California Über Alles". The band became the talking point of the ceremony when after about 15 seconds into the song, Biafra stopped the band—in a manner reminiscent of Elvis Costello's "Saturday Night Live" appearance—and said, "Hold it! We've gotta prove that we're adults now. We're not a punk rock band, we're a new wave band." The band, all wearing white shirts with a big, black S painted on the front, pulled black ties from around the backs of their necks to form a dollar sign, then started playing a new song titled "Pull My Strings", a barbed, satirical attack on the ethics of the mainstream music industry, which contained the lyrics, "Is my cock big enough, is my brain small enough, for you to make me a star?". The song also referenced The Knack's song "My Sharona". "Pull My Strings" was never recorded for a studio release, though the performance at the Bay Area Music Awards, which was the only time the song was ever performed, was released on the band's compilation album "Give Me Convenience or Give Me Death".

In January 1981, Ted announced that he wanted to leave to pursue a career in architecture and would help look for a replacement. He played his last concert in February 1981. His replacement was D.H. Peligro (Darren Henley). Around the same time, East Bay Ray had tried to pressure the rest of the band to sign to the major record label Polydor Records; Biafra stated that he was prepared to leave the group if the rest of the band wanted to sign to the label, though East Bay Ray asserts that he recommended against signing with Polydor. Polydor decided not to sign the band after they learned that Dead Kennedys' next single was to be entitled "Too Drunk to Fuck".

When "Too Drunk to Fuck" came out in May 1981, the song caused much controversy in the UK as the BBC feared the single would reach the Top 30; this would require a mention of the song on "Top of the Pops". It was never played although it was called "'Too Drunk' by the Kennedys" by presenter Tony Blackburn.

After Peligro joined the band, the extended play "In God We Trust, Inc." (1981) saw them move toward a more aggressive hardcore/thrash sound. In addition to the EP's controversial artwork depicting a gold Christ figure on a cross of dollar bills, the lyrics contained Biafra's most biting social and political commentary yet, and songs such as "Moral Majority", "Nazi Punks Fuck Off!" and "We've Got a Bigger Problem Now" placed Dead Kennedys as the spokesmen of social protest, while "Dog Bite", a cover version of "Rawhide" and various joke introductions showed a much more whimsical side. In 1982, they released their second studio album, "Plastic Surgery Disasters". The album's cover features a withered starving African child's hand being held and dwarfed by a white man's hand, a picture that had won the World Press Photo award in 1980, taken in Karamoja district in Uganda by Mike Wells.

The band's music had evolved considerably in a short time, moving away from hardcore formulae toward a more innovative jazz-informed style, featuring musicianship and dynamics far beyond other bands in the genre (thus effectively removing the music from that genre). By now the group had become a de facto political force, pitting itself against rising elements of American social and political life such as the religious right, Ronald Reagan and the idle rich. The band continued touring all over the United States, as well as Europe and Australia, and gained a large underground following. While they continued to play live shows during 1983 and 1984, they took a break from releasing new records to concentrate on the Alternative Tentacles record label, which would become synonymous with DIY alternative culture. The band continued to write and perform new material during this time, which would appear on their next album (some of these early performances can be seen in the "DMPO's on Broadway" video, originally released by Dirk Dirksen and later reissued on Rhino).

The release of the album "Frankenchrist" in 1985 showed the band had grown in musical proficiency and lyrical maturity. While there were still a number of loud/fast songs, much of the music featured an eclectic mix of instruments including trumpets and synthesizers. Around this time Klaus Flouride released the similarly experimental solo EP "Cha Cha Cha With Mr. Flouride". Lyrically, the band continued their trademark social commentary, with songs such as "MTV Get Off The Air" and "Jock-O-Rama (Invasion of the Beef Patrol)" poking fun at mainstream America.

However, the controversy that erupted over H.R. Giger's "Penis Landscape", included as an insert with the album, dwarfed the notoriety of its music. The artwork caused a furor with the newly formed Parents Music Resource Center (PMRC). In December 1985 a teenage girl purchased the album at the Wherehouse Records store in Los Angeles County. The girl's mother wrote letters of complaint to the California Attorney General and to Los Angeles prosecutors. In 1986, members of the band, along with other parties involved in the distribution of "Frankenchrist", were charged criminally with distribution of harmful matter to minors. The store where the teen actually purchased the album was never named in the lawsuit. The criminal charges focused on an illustration by H.R. Giger, titled "Work 219: Landscape XX" (also known as "Penis Landscape"). Included as a poster with the album, "Penis Landscape" depicts nine copulating penises and vaginas.

Members of the band and others in the chain of distribution were charged with violating the California Penal Code on a misdemeanor charge carrying a maximum penalty of up to a year in county jail and a base fine of up to $2,000. Biafra says that during this time government agents invaded and searched his home. The prosecution tried to present the poster to the jury in isolation for consideration as obscene material, but Judge Susan Isacoff ruled that the poster must be considered along with the music and lyrics. The charges against three of the original defendants, Ruth Schwartz (owner of Mordam Records), Steve Boudreau (a distributor involved in supplying "Frankenchrist" to the Los Angeles Wherehouse store), and Salvatore Alberti (owner of the factory where the record was pressed), were dismissed for lack of evidence.

In August 1987, the case went to the jury with two remaining defendants: Jello Biafra and Michael Bonanno (former Alternative Tentacles label manager). However, the criminal trial ended with a hung jury, split 7 to 5 in favor of acquittal. District Attorneys Michael Guarino and Ira Riener made a motion for a retrial which was denied by Judge Isacoff, Superior Court Judge for the County of Los Angeles. The album, however, was banned from many record stores nationwide.

After the break up of the band, Jello Biafra brought up the court case on "The Oprah Winfrey Show". Biafra was on the show with Tipper Gore as part of a panel discussion on the issues of "controversial music lyrics" and censorship.

In addition to the obscenity lawsuit, the band became increasingly disillusioned with the underground scene as well. The hardcore scene, which had been a haven for free-thinking intellectuals and downtrodden nonconformists, was attracting a more violent audience that imposed an increasing level of brutality on other concertgoers and began to alienate many of the bands and individuals who had helped pioneer the movement in the early 1980s. In earlier years the band had criticized neo-Nazi skinheads for trying to ruin the punk scene, but just as big a problem was the popularity of increasingly macho hardcore bands, which brought the group (and their genre) an audience that had little to do with the ideas/ideals they stood for. In January 1986, frustrated and alienated from their own scene, Dead Kennedys decided to break up to pursue other interests and played their last concert on February 21. The band continued to work on songs, with Biafra penning songs such as "Chickenshit Conformist" and "Anarchy for Sale", which articulated their feelings about the "dumbing down" of punk rock.

During the summer they recorded these songs for their final album, "Bedtime for Democracy", which was released in November. The artwork, depicting a defaced Statue of Liberty overrun with Nazis, media, opportunists, Klan members, corrupt government officials, and religious zombies, echoed the idea that neither America itself or the punk scene were safe havens anymore for "your tired, your poor, your huddled masses yearning to breathe free". The album contains a number of fast/short songs interspersed with jazz ("D.M.S.O."), spoken word ("A Commercial") and psychedelia ("Cesspools In Eden"). The lyrical focus is more introspective and earnest ("Where Do Ya Draw The Line?"), with an anti-war, anti-violence ("Rambozo The Clown") bent, moving away from the violent imagery of their early records, while remaining as subversive as ever ("I Spy", "D.M.S.O."). In December, the band announced their split. Biafra went on to speak about his political beliefs on numerous television shows and he released a number of spoken-word albums. Ray, Flouride, and Peligro also went on to solo careers.

In 2001, Ray, Peligro, and Flouride reformed the Dead Kennedys, with former Dr. Know singer Brandon Cruz replacing Biafra on vocals. The band played under the name "DK Kennedys" for a few concerts, but later reverted to "Dead Kennedys" permanently. They played across the continental United States, Europe, Asia, South America, and Russia. Brandon Cruz left the band in May 2003 and was replaced by Jeff Penalty. The band has released two live albums of archival performances on Manifesto Records: "Mutiny on the Bay", compiled from various live shows including a recording from their last show with Biafra in 1986, and "Live at the Deaf Club", a recording of a 1979 performance at the Deaf Club in San Francisco which was greeted with more enthusiasm.

On October 9, 2007, a best of album titled "Milking the Sacred Cow" was released. It includes two previously unreleased live versions of "Soup Is Good Food" and "Jock-O-Rama", originally found on "Frankenchrist".

Jeff Penalty left the band in March 2008 in what he describes as a "not amicable split." In a statement released, Jeff said that, following a series of disputes, the band had secretly recruited a new singer and played a gig in his neighbourhood, although he also stated he was "really proud of what we were able to accomplish with Dead Kennedys". He was replaced by former Wynona Riders singer Ron "Skip" Greer. D.H. also left the band to "take some personal time off". He was replaced for a tour by Translator drummer Dave Scheff.

On August 21, 2008, the band announced an extended break from touring due to the health-related issues of Flouride and Peligro. They stated their plans to collaborate on new projects. The band performed a gig in Santa Rosa, California in June 2009, with Peligro returning to the drum kit.

In August 2010, Dead Kennedys announced plans for a short East Coast tour. The lineup assembled for this tour contained East Bay Ray, Peligro, Greer, and bassist Greg Reeves replacing Flouride, who was taking "personal time off" from the band. The tour dates included performances in Philadelphia, New York City, Boston, Washington, D.C., Portland, Maine and Hawaii. The band has played a reworked version of their song "MTV Get Off the Air", re-titled "MP3 Get Off the Web", with lyrics criticizing music piracy during their October 16, 2010, concert at the Rock and Roll Hotel in Washington, D.C..

Dead Kennedys had world tours in 2013 and in 2014, the latter mostly in North American cities. In 2015 and 2016 they toured again, including South America, where they haven't played since 2001.

In 2017, East Bay Ray revealed that the band and Jello Biafra had been approached by the Punk-oriented music festival Riot Fest about a potential reunion. While Ray and the rest of the band expressed interest in the concept, Biafra refused.

In the late 1990s, former band members discovered problems with the payment of royalties they had received from Alternative Tentacles. East Bay Ray, Klaus Flouride, and D. H. Peligro claimed that Jello Biafra had conspired to pay lower royalty rates. Although both sides agreed that the failure to pay these royalties was an accounting mistake, the other members were upset that Biafra failed to inform them of the mistake after he and his co-workers discovered it and additionally claimed he intended to use the money against them.

A jury found Biafra and Alternative Tentacles "guilty of malice, oppression and fraud". Malice was defined for the jury as "conduct which is intended to cause injury or despicable conduct which is carried with a willful and conscious disregard for the rights of others". Biafra's appeal was denied; he had to pay the outstanding royalties and punitive damages, and was forced to hand over the rights to the majority of Dead Kennedys' back catalogue to the Decay Music partnership. The jury and judges also noted, in their words, that Biafra "lacked credibility" on the songwriting issue and found from evidence presented by both sides that the songwriting credits were due to the entire band.

This dispute was hotly contested by all concerned who felt passionately for their cause, and the case caused minor waves within punk circles. Biafra claims that East Bay Ray had long expressed displeasure with Alternative Tentacles and with the amount of money he received from them, thus the original incentive for the discovery of the back payments. It was found out that Alternative Tentacles was paying Dead Kennedys less per CD than all the other bands, including Biafra himself, and not informing his other bandmates, which was the fraud. Biafra accused the band of wanting to license the famous Dead Kennedys song "Holiday in Cambodia" for use in a Levi's jeans commercial, which the band denied.

Biafra's former bandmates maintain that they sued because of Jello Biafra's deliberate withholding of money, though when pressed they have acknowledged that the payment was an accounting mistake, but insist that Biafra was wrong in failing to inform the band directly. Details about this issue remain scarce. The band also maintains that the Levi's story was completely fictitious and invented by Biafra to discredit them.

Matters were stirred up even further when the three bandmates invited Jello Biafra to "bury the hatchet" in the form of a band reunion. Jello Biafra felt it was unprofessional because no one contacted him directly. In addition, Biafra was disdainful of the reunion, and having long expressed his disdain for nostalgia and rock reunion/oldies tours in particular, argued that the whole affair was motivated by greed.

Several DVDs, re-issues, and live albums have been released since the departure of Biafra most recently on Manifesto Records. According to Biafra, the live albums are "cash-ins" on Dead Kennedys' name and his music. Biafra also accused the releases of the new live material of having poor sound quality. Furthermore he has stated he is not receiving any royalties from the sale of any Manifesto Records releases. Consequently, he has discouraged fans from buying any Dead Kennedy reissues. The other band members denied Biafra's accusations regarding the live releases, and have defended the mixes as an effort of hard work. Biafra dismissed the new group as "the world's greediest karaoke band." Nevertheless, in 2003, Klaus Flouride said of performances without the band's former frontman: "There hasn't been a show yet that people didn't really like."

Biafra further criticized them for advertising shows using his own image taken from the original 1980s incarnation of the band, which he labeled as false advertising. He attacked the reformed Dead Kennedys in a song called "Those Dumb Punk Kids (Will Buy Anything)", which appears on his second collaboration with sludge metal band the Melvins, "Sieg Howdy!".

Biafra told an audience at a speaking gig in Trenton, New Jersey, that the remaining Dead Kennedys have licensed their single "Too Drunk to Fuck" to be used in a rape scene in a Robert Rodriguez movie. The reference is to a lounge cover of the song, recorded by the band Nouvelle Vague, played during a scene in the "Planet Terror" segment of "Grindhouse", although no rape takes place, and in fact the would-be rapist is killed by the would-be victim. The scene in "Planet Terror" has would-be rapist, "Rapist No. 1" (Quentin Tarantino) order one-legged stripper "Cherry Darlin" (Rose McGowan) to get up off the floor and dance. At this point Tarantino hits play on a cassette recorder and Nouvelle Vague's cover of "Too Drunk To Fuck" plays.

Jello, clearly disapproving of the situation, later wrote, "This is their lowest point since Levi's... This goes against everything the Dead Kennedys stands for in spades... The terrified woman later 'wins' by killing Tarantino, but that excuse does not rescue this at all. I wrote every note of that song and this is not what it was meant for... Some people will do anything for money. I can't help but think back to how prudish Klaus Flouride was when he objected to H.R. Giger's painting on the "Frankenchrist" (sic) poster, saying he couldn't bear to show it to his parents. I'd sure love to be a fly on the wall when he tries to explain putting a song in a rape scene for money to his teenage daughter... The deal was pushed through by a new business manager the other three hired."

The reformed Dead Kennedys followed their court victory by releasing reissues of all Dead Kennedys albums (except "Fresh Fruit for Rotting Vegetables", to which they did not have the rights until 2005), releasing several new archival concert DVDs, and licensing several songs to "The Manchurian Candidate" remake and the Tony Hawk Pro Skater video game. East Bay Ray claims he received a fax from Alternative Tentacles purporting Biafra approved the licensing for the game.

The band claims on their website that they still pay close attention to an anti-corporate ideology, despite performing on September 5, 2003 at a festival in Turkey that was sponsored by Coca-Cola, noting that they have since pulled out of a show in Los Angeles when they found that it was being sponsored by Coors. However, Biafra claims the above mentioned licensing deals prove otherwise. Some have found difficulty reconciling this claim when Biafra also licensed to major corporations, approving with the other band members use of Dead Kennedys’ songs in major studio film releases such as "Neighbors", "Freddy Got Fingered", and "Fear and Loathing in Las Vegas".

The original logo was created by Winston Smith. He later contributed artwork for the covers of "In God We Trust, Inc.", "Plastic Surgery Disasters", "Frankenchrist", "Bedtime for Democracy", "Give Me Convenience or Give Me Death", the back cover of the "Kill the Poor" single and the Alternative Tentacles logo. When asked about the "DK" logo in an interview, Jello Biafra explained, "...I wanted to make sure it was something simple and easy to spray-paint so people would graffiti it all over the place, and then I showed it to Winston Smith. He played around with it, came back with a bunch of designs that had the circle and slightly 3-D looking letters and he had ones with different patterns behind it. I liked the one with bricks, but ultimately I thought simple red behind it was the boldest and the best."

Dead Kennedys have been described as a hardcore punk band. Dead Kennedys were noted for the harshness of their lyrics, which generally combined biting Juvenalian social satire while expressing a staunchly left-wing view of contemporary America. Unlike other leftist punk bands who use more direct sloganeering, Dead Kennedys' lyrics were often snide. For example, "Holiday in Cambodia" is a multi-layered satire targeting both yuppies and Cambodia's recently deposed Khmer Rouge regime.

Current members
Former members
Timeline






</doc>
<doc id="7955" url="https://en.wikipedia.org/wiki?curid=7955" title="DNA">
DNA

Deoxyribonucleic acid (; DNA) is a molecule composed of two chains (made of nucleotides) which coil around each other to form a double helix carrying the genetic instructions used in the growth, development, functioning and reproduction of all known living organisms and many viruses. DNA and ribonucleic acid (RNA) are nucleic acids; alongside proteins, lipids and complex carbohydrates (polysaccharides), nucleic acids are one of the four major types of macromolecules that are essential for all known forms of life.

The two DNA strands are also known as polynucleotides since they are composed of simpler monomeric units called nucleotides. Each nucleotide is composed of one of four nitrogen-containing nucleobases (cytosine [C], guanine [G], adenine [A] or thymine [T]), a sugar called deoxyribose, and a phosphate group. The nucleotides are joined to one another in a chain by covalent bonds between the sugar of one nucleotide and the phosphate of the next, resulting in an alternating sugar-phosphate backbone. The nitrogenous bases of the two separate polynucleotide strands are bound together, according to base pairing rules (A with T and C with G), with hydrogen bonds to make double-stranded DNA.

The complementary nitrogenous bases are divided into two groups, pyrimidines and purines. In DNA, the pyrimidines are thymine and cytosine; the purines are adenine and guanine.

DNA stores biological information. The DNA backbone is resistant to cleavage, and both strands of the double-stranded structure store the same biological information. This information is replicated as and when the two strands separate. A large part of DNA (more than 98% for humans) is non-coding, meaning that these sections do not serve as patterns for protein sequences.

The two strands of DNA run in opposite directions to each other and are thus antiparallel. Attached to each sugar is one of four types of nucleobases (informally, "bases"). It is the sequence of these four nucleobases along the backbone that encodes genetic information. RNA strands are created using DNA strands as a template in a process called transcription. Under the genetic code, these RNA strands are translated to specify the sequence of amino acids within proteins in a process called translation.

Within eukaryotic cells, DNA is organized into long structures called chromosomes. Before typical cell division these chromosomes are duplicated in the process of DNA replication, providing a complete set of chromosomes for each daughter cell. Eukaryotic organisms (animals, plants, fungi and protists) store most of their DNA inside the cell nucleus and some of their DNA in organelles, such as mitochondria or chloroplasts. In contrast, prokaryotes (bacteria and archaea) store their DNA only in the cytoplasm. Within eukaryotic chromosomes, chromatin proteins such as histones compact and organize DNA. These compact structures guide the interactions between DNA and other proteins, helping control which parts of the DNA are transcribed.

DNA was first isolated by Friedrich Miescher in 1869. Its molecular structure was first identified by James Watson and Francis Crick at the Cavendish Laboratory within the University of Cambridge in 1953, whose model-building efforts were guided by X-ray diffraction data acquired by Raymond Gosling, who was a post-graduate student of Rosalind Franklin. DNA is used by researchers as a molecular tool to explore physical laws and theories, such as the ergodic theorem and the theory of elasticity. The unique material properties of DNA have made it an attractive molecule for material scientists and engineers interested in micro- and nano-fabrication. Among notable advances in this field are DNA origami and DNA-based hybrid materials.

DNA is a long polymer made from repeating units called nucleotides. The structure of DNA is dynamic along its length, being capable of coiling into tight loops, and other shapes. In all species it is composed of two helical chains, bound to each other by hydrogen bonds. Both chains are coiled round the same axis, and have the same pitch of 34 ångströms (3.4 nanometres). The pair of chains has a radius of 10 ångströms (1.0 nanometre). According to another study, when measured in a different solution, the DNA chain measured 22 to 26 ångströms wide (2.2 to 2.6 nanometres), and one nucleotide unit measured 3.3 Å (0.33 nm) long. Although each individual nucleotide repeating unit is very small, DNA polymers can be very large molecules containing millions to hundreds of millions of nucleotides. For instance, the DNA in the largest human chromosome, chromosome number 1, consists of approximately 220 million base pairs and would be 85 mm long if straightened.

In living organisms, DNA does not usually exist as a single molecule, but instead as a pair of molecules that are held tightly together. These two long strands entwine like vines, in the shape of a double helix. The nucleotide contains both a segment of the backbone of the molecule (which holds the chain together) and a nucleobase (which interacts with the other DNA strand in the helix). A nucleobase linked to a sugar is called a nucleoside and a base linked to a sugar and one or more phosphate groups is called a nucleotide. A polymer comprising multiple linked nucleotides (as in DNA) is called a polynucleotide.

The backbone of the DNA strand is made from alternating phosphate and sugar residues. The sugar in DNA is 2-deoxyribose, which is a pentose (five-carbon) sugar. The sugars are joined together by phosphate groups that form phosphodiester bonds between the third and fifth carbon atoms of adjacent sugar rings, which are known as the 3′ and 5′ carbons, the prime symbol being used to distinguish these carbon atoms from those of the base to which the deoxyribose forms a glycosidic bond. When imagining DNA, each phosphoryl is normally considered to "belong" to the nucleotide whose 5′ carbon forms a bond therewith. Any DNA strand therefore normally has one end at which there is a phosphoryl attached to the 5′ carbon of a ribose (the 5′ phosphoryl) and another end at which there is a free hydroxyl attached to the 3′ carbon of a ribose (the 3′ hydroxyl). The orientation of the 3′ and 5′ carbons along the sugar-phosphate backbone confers directionality (sometimes called polarity) to each DNA strand. In a double helix, the direction of the nucleotides in one strand is opposite to their direction in the other strand: the strands are "antiparallel". The asymmetric ends of DNA strands are said to have a directionality of "five prime" (5′) and "three prime" (3′), with the 5′ end having a terminal phosphate group and the 3′ end a terminal hydroxyl group. One major difference between DNA and RNA is the sugar, with the 2-deoxyribose in DNA being replaced by the alternative pentose sugar ribose in RNA.

The DNA double helix is stabilized primarily by two forces: hydrogen bonds between nucleotides and base-stacking interactions among aromatic nucleobases. In the aqueous environment of the cell, the conjugated bonds of nucleotide bases align perpendicular to the axis of the DNA molecule, minimizing their interaction with the solvation shell. The four bases found in DNA are adenine (A), cytosine (C), guanine (G) and thymine (T). These four bases are attached to the sugar-phosphate to form the complete nucleotide, as shown for adenosine monophosphate. Adenine pairs with thymine and guanine pairs with cytosine. It was represented by A-T base pairs and G-C base pairs.

The nucleobases are classified into two types: the purines, A and G, being fused five- and six-membered heterocyclic compounds, and the pyrimidines, the six-membered rings C and T. A fifth pyrimidine nucleobase, uracil (U), usually takes the place of thymine in RNA and differs from thymine by lacking a methyl group on its ring. In addition to RNA and DNA, many artificial nucleic acid analogues have been created to study the properties of nucleic acids, or for use in biotechnology.

Uracil is not usually found in DNA, occurring only as a breakdown product of cytosine. However, in several bacteriophages, "Bacillus subtilis" bacteriophages PBS1 and PBS2 and "Yersinia" bacteriophage piR1-37, thymine has been replaced by uracil. Another phage - Staphylococcal phage S6 - has been identified with a genome where thymine has been replaced by uracil.

5-hydroxymethyldeoxyuridine,(hm5dU) is also known to replace thymidine in several genomes including the "Bacillus" phages SPO1, ϕe, SP8, H1, 2C and SP82. Another modified uracil - 5-dihydroxypentauracil – has also been described.

Base J (beta-d-glucopyranosyloxymethyluracil), a modified form of uracil, is also found in several organisms: the flagellates "Diplonema" and "Euglena", and all the kinetoplastid genera. Biosynthesis of J occurs in two steps: in the first step, a specific thymidine in DNA is converted into hydroxymethyldeoxyuridine; in the second, HOMedU is glycosylated to form J. Proteins that bind specifically to this base have been identified. These proteins appear to be distant relatives of the Tet1 oncogene that is involved in the pathogenesis of acute myeloid leukemia. J appears to act as a termination signal for RNA polymerase II.

In 1976 a bacteriophage - S-2L - which infects species of the genus "Synechocystis" was found to have all the adenosine bases within its genome replaced by 2,6-diaminopurine. In 2016 deoxyarchaeosine was found to be present in the genomes of several bacteria and the "Escherichia" phage 9g.

Modified bases also occur in DNA. The first of these recognised was 5-methylcytosine which was found in the genome of "Mycobacterium tuberculosis" in 1925. The complete replacement of cytosine by 5-glycosylhydroxymethylcytosine in T even phages (T2, T4 and T6) was observed in 1953 In the genomes of Xanthomonas oryzae bacteriophage Xp12 and halovirus FH the full complement of cystosine has been replaced by 5-methylcytosine. 6N-methyadenine was discovered to be present in DNA in 1955. N6-carbamoyl-methyladenine was described in 1975. 7-methylguanine was described in 1976. N4-methylcytosine in DNA was described in 1983. In 1985 5-hydroxycytosine was found in the genomes of the Rhizobium phages RL38JI and N17. α-putrescinylthymine occurs in both the genomes of the "Delftia" phage ΦW-14 and the "Bacillus" phage SP10. α-glutamylthymidine is found in the Bacillus phage SP01 and 5-dihydroxypentyluracil is found in the Bacillus phage SP15.

The reason for the presence of these non canonical bases in DNA is not known. It seems likely that at least part of the reason for their presence in bacterial viruses (phages) is to avoid the restriction enzymes present in bacteria. This enzyme system acts at least in part as a molecular immune system protecting bacteria from infection by viruses.

This does not appear to be the entire story. Four modifications to the cytosine residues in human DNA have been reported. These modifications are the addition of methyl (CH)-, hydroxymethyl (CHOH)-, formyl (CHO)- and carboxyl (COOH)- groups. These modifications are thought to have regulatory functions.

Uracil is found in the centromeric regions of at least two human chromosomes (6 and 11).

Seventeen non canonical bases are known to occur in DNA. Most of these are modifications of the canonical bases plus uracil.


Twin helical strands form the DNA backbone. Another double helix may be found tracing the spaces, or grooves, between the strands. These voids are adjacent to the base pairs and may provide a binding site. As the strands are not symmetrically located with respect to each other, the grooves are unequally sized. One groove, the major groove, is 22 Å wide and the other, the minor groove, is 12 Å wide. The width of the major groove means that the edges of the bases are more accessible in the major groove than in the minor groove. As a result, proteins such as transcription factors that can bind to specific sequences in double-stranded DNA usually make contact with the sides of the bases exposed in the major groove. This situation varies in unusual conformations of DNA within the cell "(see below)", but the major and minor grooves are always named to reflect the differences in size that would be seen if the DNA is twisted back into the ordinary B form.

In a DNA double helix, each type of nucleobase on one strand bonds with just one type of nucleobase on the other strand. This is called complementary base pairing. Here, purines form hydrogen bonds to pyrimidines, with adenine bonding only to thymine in two hydrogen bonds, and cytosine bonding only to guanine in three hydrogen bonds. This arrangement of two nucleotides binding together across the double helix is called a Watson-Crick base pair. Another type of base pairing is Hoogsteen base pairing where two hydrogen bonds form between guanine and cytosine. As hydrogen bonds are not covalent, they can be broken and rejoined relatively easily. The two strands of DNA in a double helix can thus be pulled apart like a zipper, either by a mechanical force or high temperature. As a result of this base pair complementarity, all the information in the double-stranded sequence of a DNA helix is duplicated on each strand, which is vital in DNA replication. This reversible and specific interaction between complementary base pairs is critical for all the functions of DNA in living organisms.

The two types of base pairs form different numbers of hydrogen bonds, AT forming two hydrogen bonds, and GC forming three hydrogen bonds (see figures, right).
DNA with high GC-content is more stable than DNA with low GC-content.
As noted above, most DNA molecules are actually two polymer strands, bound together in a helical fashion by noncovalent bonds; this double-stranded (dsDNA) structure is maintained largely by the intrastrand base stacking interactions, which are strongest for G,C stacks. The two strands can come apart – a process known as melting – to form two single-stranded DNA (ssDNA) molecules. Melting occurs at high temperature, low salt and high pH (low pH also melts DNA, but since DNA is unstable due to acid depurination, low pH is rarely used).

The stability of the dsDNA form depends not only on the GC-content (% G,C basepairs) but also on sequence (since stacking is sequence specific) and also length (longer molecules are more stable). The stability can be measured in various ways; a common way is the "melting temperature", which is the temperature at which 50% of the ds molecules are converted to ss molecules; melting temperature is dependent on ionic strength and the concentration of DNA.
As a result, it is both the percentage of GC base pairs and the overall length of a DNA double helix that determines the strength of the association between the two strands of DNA. Long DNA helices with a high GC-content have stronger-interacting strands, while short helices with high AT content have weaker-interacting strands. In biology, parts of the DNA double helix that need to separate easily, such as the TATAAT Pribnow box in some promoters, tend to have a high AT content, making the strands easier to pull apart.

In the laboratory, the strength of this interaction can be measured by finding the temperature necessary to break the hydrogen bonds, their melting temperature (also called "T" value). When all the base pairs in a DNA double helix melt, the strands separate and exist in solution as two entirely independent molecules. These single-stranded DNA molecules have no single common shape, but some conformations are more stable than others.

A DNA sequence is called "sense" if its sequence is the same as that of a messenger RNA copy that is translated into protein. The sequence on the opposite strand is called the "antisense" sequence. Both sense and antisense sequences can exist on different parts of the same strand of DNA (i.e. both strands can contain both sense and antisense sequences). In both prokaryotes and eukaryotes, antisense RNA sequences are produced, but the functions of these RNAs are not entirely clear. One proposal is that antisense RNAs are involved in regulating gene expression through RNA-RNA base pairing.

A few DNA sequences in prokaryotes and eukaryotes, and more in plasmids and viruses, blur the distinction between sense and antisense strands by having overlapping genes. In these cases, some DNA sequences do double duty, encoding one protein when read along one strand, and a second protein when read in the opposite direction along the other strand. In bacteria, this overlap may be involved in the regulation of gene transcription, while in viruses, overlapping genes increase the amount of information that can be encoded within the small viral genome.

DNA can be twisted like a rope in a process called DNA supercoiling. With DNA in its "relaxed" state, a strand usually circles the axis of the double helix once every 10.4 base pairs, but if the DNA is twisted the strands become more tightly or more loosely wound. If the DNA is twisted in the direction of the helix, this is positive supercoiling, and the bases are held more tightly together. If they are twisted in the opposite direction, this is negative supercoiling, and the bases come apart more easily. In nature, most DNA has slight negative supercoiling that is introduced by enzymes called topoisomerases. These enzymes are also needed to relieve the twisting stresses introduced into DNA strands during processes such as transcription and DNA replication.

DNA exists in many possible conformations that include A-DNA, B-DNA, and Z-DNA forms, although, only B-DNA and Z-DNA have been directly observed in functional organisms. The conformation that DNA adopts depends on the hydration level, DNA sequence, the amount and direction of supercoiling, chemical modifications of the bases, the type and concentration of metal ions, and the presence of polyamines in solution.

The first published reports of A-DNA X-ray diffraction patterns—and also B-DNA—used analyses based on Patterson transforms that provided only a limited amount of structural information for oriented fibers of DNA. An alternative analysis was then proposed by Wilkins "et al.", in 1953, for the "in vivo" B-DNA X-ray diffraction-scattering patterns of highly hydrated DNA fibers in terms of squares of Bessel functions. In the same journal, James Watson and Francis Crick presented their molecular modeling analysis of the DNA X-ray diffraction patterns to suggest that the structure was a double-helix.

Although the "B-DNA form" is most common under the conditions found in cells, it is not a well-defined conformation but a family of related DNA conformations that occur at the high hydration levels present in living cells. Their corresponding X-ray diffraction and scattering patterns are characteristic of molecular paracrystals with a significant degree of disorder.

Compared to B-DNA, the A-DNA form is a wider right-handed spiral, with a shallow, wide minor groove and a narrower, deeper major groove. The A form occurs under non-physiological conditions in partly dehydrated samples of DNA, while in the cell it may be produced in hybrid pairings of DNA and RNA strands, and in enzyme-DNA complexes. Segments of DNA where the bases have been chemically modified by methylation may undergo a larger change in conformation and adopt the Z form. Here, the strands turn about the helical axis in a left-handed spiral, the opposite of the more common B form. These unusual structures can be recognized by specific Z-DNA binding proteins and may be involved in the regulation of transcription.

For many years exobiologists have proposed the existence of a shadow biosphere, a postulated microbial biosphere of Earth that uses radically different biochemical and molecular processes than currently known life. One of the proposals was the existence of lifeforms that use arsenic instead of phosphorus in DNA. A report in 2010 of the possibility in the bacterium GFAJ-1, was announced, though the research was disputed, and evidence suggests the bacterium actively prevents the incorporation of arsenic into the DNA backbone and other biomolecules.

At the ends of the linear chromosomes are specialized regions of DNA called telomeres. The main function of these regions is to allow the cell to replicate chromosome ends using the enzyme telomerase, as the enzymes that normally replicate DNA cannot copy the extreme 3′ ends of chromosomes. These specialized chromosome caps also help protect the DNA ends, and stop the DNA repair systems in the cell from treating them as damage to be corrected. In human cells, telomeres are usually lengths of single-stranded DNA containing several thousand repeats of a simple TTAGGG sequence.

These guanine-rich sequences may stabilize chromosome ends by forming structures of stacked sets of four-base units, rather than the usual base pairs found in other DNA molecules. Here, four guanine bases form a flat plate and these flat four-base units then stack on top of each other, to form a stable G-quadruplex structure. These structures are stabilized by hydrogen bonding between the edges of the bases and chelation of a metal ion in the centre of each four-base unit. Other structures can also be formed, with the central set of four bases coming from either a single strand folded around the bases, or several different parallel strands, each contributing one base to the central structure.

In addition to these stacked structures, telomeres also form large loop structures called telomere loops, or T-loops. Here, the single-stranded DNA curls around in a long circle stabilized by telomere-binding proteins. At the very end of the T-loop, the single-stranded telomere DNA is held onto a region of double-stranded DNA by the telomere strand disrupting the double-helical DNA and base pairing to one of the two strands. This triple-stranded structure is called a displacement loop or D-loop.

In DNA, fraying occurs when non-complementary regions exist at the end of an otherwise complementary double-strand of DNA. However, branched DNA can occur if a third strand of DNA is introduced and contains adjoining regions able to hybridize with the frayed regions of the pre-existing double-strand. Although the simplest example of branched DNA involves only three strands of DNA, complexes involving additional strands and multiple branches are also possible. Branched DNA can be used in nanotechnology to construct geometric shapes, see the section on uses in technology below.

The expression of genes is influenced by how the DNA is packaged in chromosomes, in a structure called chromatin. Base modifications can be involved in packaging, with regions that have low or no gene expression usually containing high levels of methylation of cytosine bases. DNA packaging and its influence on gene expression can also occur by covalent modifications of the histone protein core around which DNA is wrapped in the chromatin structure or else by remodeling carried out by chromatin remodeling complexes (see Chromatin remodeling). There is, further, crosstalk between DNA methylation and histone modification, so they can coordinately affect chromatin and gene expression.

For one example, cytosine methylation produces 5-methylcytosine, which is important for X-inactivation of chromosomes. The average level of methylation varies between organisms – the worm "Caenorhabditis elegans" lacks cytosine methylation, while vertebrates have higher levels, with up to 1% of their DNA containing 5-methylcytosine. Despite the importance of 5-methylcytosine, it can deaminate to leave a thymine base, so methylated cytosines are particularly prone to mutations. Other base modifications include adenine methylation in bacteria, the presence of 5-hydroxymethylcytosine in the brain, and the glycosylation of uracil to produce the "J-base" in kinetoplastids.

DNA can be damaged by many sorts of mutagens, which change the DNA sequence. Mutagens include oxidizing agents, alkylating agents and also high-energy electromagnetic radiation such as ultraviolet light and X-rays. The type of DNA damage produced depends on the type of mutagen. For example, UV light can damage DNA by producing thymine dimers, which are cross-links between pyrimidine bases. On the other hand, oxidants such as free radicals or hydrogen peroxide produce multiple forms of damage, including base modifications, particularly of guanosine, and double-strand breaks. A typical human cell contains about 150,000 bases that have suffered oxidative damage. Of these oxidative lesions, the most dangerous are double-strand breaks, as these are difficult to repair and can produce point mutations, insertions, deletions from the DNA sequence, and chromosomal translocations. These mutations can cause cancer. Because of inherent limits in the DNA repair mechanisms, if humans lived long enough, they would all eventually develop cancer. DNA damages that are naturally occurring, due to normal cellular processes that produce reactive oxygen species, the hydrolytic activities of cellular water, etc., also occur frequently. Although most of these damages are repaired, in any cell some DNA damage may remain despite the action of repair processes. These remaining DNA damages accumulate with age in mammalian postmitotic tissues. This accumulation appears to be an important underlying cause of aging.

Many mutagens fit into the space between two adjacent base pairs, this is called "intercalation". Most intercalators are aromatic and planar molecules; examples include ethidium bromide, acridines, daunomycin, and doxorubicin. For an intercalator to fit between base pairs, the bases must separate, distorting the DNA strands by unwinding of the double helix. This inhibits both transcription and DNA replication, causing toxicity and mutations. As a result, DNA intercalators may be carcinogens, and in the case of thalidomide, a teratogen. Others such as benzo["a"]pyrene diol epoxide and aflatoxin form DNA adducts that induce errors in replication. Nevertheless, due to their ability to inhibit DNA transcription and replication, other similar toxins are also used in chemotherapy to inhibit rapidly growing cancer cells.

DNA usually occurs as linear chromosomes in eukaryotes, and circular chromosomes in prokaryotes. The set of chromosomes in a cell makes up its genome; the human genome has approximately 3 billion base pairs of DNA arranged into 46 chromosomes. The information carried by DNA is held in the sequence of pieces of DNA called genes. Transmission of genetic information in genes is achieved via complementary base pairing. For example, in transcription, when a cell uses the information in a gene, the DNA sequence is copied into a complementary RNA sequence through the attraction between the DNA and the correct RNA nucleotides. Usually, this RNA copy is then used to make a matching protein sequence in a process called translation, which depends on the same interaction between RNA nucleotides. In alternative fashion, a cell may simply copy its genetic information in a process called DNA replication. The details of these functions are covered in other articles; here the focus is on the interactions between DNA and other molecules that mediate the function of the genome.

Genomic DNA is tightly and orderly packed in the process called DNA condensation, to fit the small available volumes of the cell. In eukaryotes, DNA is located in the cell nucleus, with small amounts in mitochondria and chloroplasts. In prokaryotes, the DNA is held within an irregularly shaped body in the cytoplasm called the nucleoid. The genetic information in a genome is held within genes, and the complete set of this information in an organism is called its genotype. A gene is a unit of heredity and is a region of DNA that influences a particular characteristic in an organism. Genes contain an open reading frame that can be transcribed, and regulatory sequences such as promoters and enhancers, which control transcription of the open reading frame.

In many species, only a small fraction of the total sequence of the genome encodes protein. For example, only about 1.5% of the human genome consists of protein-coding exons, with over 50% of human DNA consisting of non-coding repetitive sequences. The reasons for the presence of so much noncoding DNA in eukaryotic genomes and the extraordinary differences in genome size, or "C-value", among species, represent a long-standing puzzle known as the "C-value enigma". However, some DNA sequences that do not code protein may still encode functional non-coding RNA molecules, which are involved in the regulation of gene expression.
Some noncoding DNA sequences play structural roles in chromosomes. Telomeres and centromeres typically contain few genes but are important for the function and stability of chromosomes. An abundant form of noncoding DNA in humans are pseudogenes, which are copies of genes that have been disabled by mutation. These sequences are usually just molecular fossils, although they can occasionally serve as raw genetic material for the creation of new genes through the process of gene duplication and divergence.

A gene is a sequence of DNA that contains genetic information and can influence the phenotype of an organism. Within a gene, the sequence of bases along a DNA strand defines a messenger RNA sequence, which then defines one or more protein sequences. The relationship between the nucleotide sequences of genes and the amino-acid sequences of proteins is determined by the rules of translation, known collectively as the genetic code. The genetic code consists of three-letter 'words' called "codons" formed from a sequence of three nucleotides (e.g. ACT, CAG, TTT).

In transcription, the codons of a gene are copied into messenger RNA by RNA polymerase. This RNA copy is then decoded by a ribosome that reads the RNA sequence by base-pairing the messenger RNA to transfer RNA, which carries amino acids. Since there are 4 bases in 3-letter combinations, there are 64 possible codons (4 combinations). These encode the twenty standard amino acids, giving most amino acids more than one possible codon. There are also three 'stop' or 'nonsense' codons signifying the end of the coding region; these are the TAA, TGA, and TAG codons.

Cell division is essential for an organism to grow, but, when a cell divides, it must replicate the DNA in its genome so that the two daughter cells have the same genetic information as their parent. The double-stranded structure of DNA provides a simple mechanism for DNA replication. Here, the two strands are separated and then each strand's complementary DNA sequence is recreated by an enzyme called DNA polymerase. This enzyme makes the complementary strand by finding the correct base through complementary base pairing and bonding it onto the original strand. As DNA polymerases can only extend a DNA strand in a 5′ to 3′ direction, different mechanisms are used to copy the antiparallel strands of the double helix. In this way, the base on the old strand dictates which base appears on the new strand, and the cell ends up with a perfect copy of its DNA.

Naked extracellular DNA (eDNA), most of it released by cell death, is nearly ubiquitous in the environment. Its concentration in soil may be as high as 2 μg/L, and its concentration in natural aquatic environments may be as high at 88 μg/L. Various possible functions have been proposed for eDNA: it may be involved in horizontal gene transfer; it may provide nutrients; and it may act as a buffer to recruit or titrate ions or antibiotics. Extracellular DNA acts as a functional extracellular matrix component in the biofilms of several bacterial species. It may act as a recognition factor to regulate the attachment and dispersal of specific cell types in the biofilm; it may contribute to biofilm formation; and it may contribute to the biofilm's physical strength and resistance to biological stress.

Cell-free fetal DNA is found in the blood of the mother, and can be sequenced to determine a great deal of information about the developing fetus.

All the functions of DNA depend on interactions with proteins. These protein interactions can be non-specific, or the protein can bind specifically to a single DNA sequence. Enzymes can also bind to DNA and of these, the polymerases that copy the DNA base sequence in transcription and DNA replication are particularly important.

Structural proteins that bind DNA are well-understood examples of non-specific DNA-protein interactions. Within chromosomes, DNA is held in complexes with structural proteins. These proteins organize the DNA into a compact structure called chromatin. In eukaryotes, this structure involves DNA binding to a complex of small basic proteins called histones, while in prokaryotes multiple types of proteins are involved. The histones form a disk-shaped complex called a nucleosome, which contains two complete turns of double-stranded DNA wrapped around its surface. These non-specific interactions are formed through basic residues in the histones, making ionic bonds to the acidic sugar-phosphate backbone of the DNA, and are thus largely independent of the base sequence. Chemical modifications of these basic amino acid residues include methylation, phosphorylation, and acetylation. These chemical changes alter the strength of the interaction between the DNA and the histones, making the DNA more or less accessible to transcription factors and changing the rate of transcription. Other non-specific DNA-binding proteins in chromatin include the high-mobility group proteins, which bind to bent or distorted DNA. These proteins are important in bending arrays of nucleosomes and arranging them into the larger structures that make up chromosomes.

A distinct group of DNA-binding proteins is the DNA-binding proteins that specifically bind single-stranded DNA. In humans, replication protein A is the best-understood member of this family and is used in processes where the double helix is separated, including DNA replication, recombination, and DNA repair. These binding proteins seem to stabilize single-stranded DNA and protect it from forming stem-loops or being degraded by nucleases.
In contrast, other proteins have evolved to bind to particular DNA sequences. The most intensively studied of these are the various transcription factors, which are proteins that regulate transcription. Each transcription factor binds to one particular set of DNA sequences and activates or inhibits the transcription of genes that have these sequences close to their promoters. The transcription factors do this in two ways. Firstly, they can bind the RNA polymerase responsible for transcription, either directly or through other mediator proteins; this locates the polymerase at the promoter and allows it to begin transcription. Alternatively, transcription factors can bind enzymes that modify the histones at the promoter. This changes the accessibility of the DNA template to the polymerase.

As these DNA targets can occur throughout an organism's genome, changes in the activity of one type of transcription factor can affect thousands of genes. Consequently, these proteins are often the targets of the signal transduction processes that control responses to environmental changes or cellular differentiation and development. The specificity of these transcription factors' interactions with DNA come from the proteins making multiple contacts to the edges of the DNA bases, allowing them to "read" the DNA sequence. Most of these base-interactions are made in the major groove, where the bases are most accessible.

Nucleases are enzymes that cut DNA strands by catalyzing the hydrolysis of the phosphodiester bonds. Nucleases that hydrolyse nucleotides from the ends of DNA strands are called exonucleases, while endonucleases cut within strands. The most frequently used nucleases in molecular biology are the restriction endonucleases, which cut DNA at specific sequences. For instance, the EcoRV enzyme shown to the left recognizes the 6-base sequence 5′-GATATC-3′ and makes a cut at the horizontal line. In nature, these enzymes protect bacteria against phage infection by digesting the phage DNA when it enters the bacterial cell, acting as part of the restriction modification system. In technology, these sequence-specific nucleases are used in molecular cloning and DNA fingerprinting.

Enzymes called DNA ligases can rejoin cut or broken DNA strands. Ligases are particularly important in lagging strand DNA replication, as they join together the short segments of DNA produced at the replication fork into a complete copy of the DNA template. They are also used in DNA repair and genetic recombination.

Topoisomerases are enzymes with both nuclease and ligase activity. These proteins change the amount of supercoiling in DNA. Some of these enzymes work by cutting the DNA helix and allowing one section to rotate, thereby reducing its level of supercoiling; the enzyme then seals the DNA break. Other types of these enzymes are capable of cutting one DNA helix and then passing a second strand of DNA through this break, before rejoining the helix. Topoisomerases are required for many processes involving DNA, such as DNA replication and transcription.

Helicases are proteins that are a type of molecular motor. They use the chemical energy in nucleoside triphosphates, predominantly adenosine triphosphate (ATP), to break hydrogen bonds between bases and unwind the DNA double helix into single strands. These enzymes are essential for most processes where enzymes need to access the DNA bases.

Polymerases are enzymes that synthesize polynucleotide chains from nucleoside triphosphates. The sequence of their products is created based on existing polynucleotide chains—which are called "templates". These enzymes function by repeatedly adding a nucleotide to the 3′ hydroxyl group at the end of the growing polynucleotide chain. As a consequence, all polymerases work in a 5′ to 3′ direction. In the active site of these enzymes, the incoming nucleoside triphosphate base-pairs to the template: this allows polymerases to accurately synthesize the complementary strand of their template. Polymerases are classified according to the type of template that they use.

In DNA replication, DNA-dependent DNA polymerases make copies of DNA polynucleotide chains. To preserve biological information, it is essential that the sequence of bases in each copy are precisely complementary to the sequence of bases in the template strand. Many DNA polymerases have a proofreading activity. Here, the polymerase recognizes the occasional mistakes in the synthesis reaction by the lack of base pairing between the mismatched nucleotides. If a mismatch is detected, a 3′ to 5′ exonuclease activity is activated and the incorrect base removed. In most organisms, DNA polymerases function in a large complex called the replisome that contains multiple accessory subunits, such as the DNA clamp or helicases.

RNA-dependent DNA polymerases are a specialized class of polymerases that copy the sequence of an RNA strand into DNA. They include reverse transcriptase, which is a viral enzyme involved in the infection of cells by retroviruses, and telomerase, which is required for the replication of telomeres. For example, HIV reverse transcriptase is an enzyme for AIDS virus replication. Telomerase is an unusual polymerase because it contains its own RNA template as part of its structure. It synthesizes telomeres at the ends of chromosomes. Telomeres prevent fusion of the ends of neighboring chromosomes and protect chromosome ends from damage.

Transcription is carried out by a DNA-dependent RNA polymerase that copies the sequence of a DNA strand into RNA. To begin transcribing a gene, the RNA polymerase binds to a sequence of DNA called a promoter and separates the DNA strands. It then copies the gene sequence into a messenger RNA transcript until it reaches a region of DNA called the terminator, where it halts and detaches from the DNA. As with human DNA-dependent DNA polymerases, RNA polymerase II, the enzyme that transcribes most of the genes in the human genome, operates as part of a large protein complex with multiple regulatory and accessory subunits.

A DNA helix usually does not interact with other segments of DNA, and in human cells, the different chromosomes even occupy separate areas in the nucleus called "chromosome territories". This physical separation of different chromosomes is important for the ability of DNA to function as a stable repository for information, as one of the few times chromosomes interact is in chromosomal crossover which occurs during sexual reproduction, when genetic recombination occurs. Chromosomal crossover is when two DNA helices break, swap a section and then rejoin.

Recombination allows chromosomes to exchange genetic information and produces new combinations of genes, which increases the efficiency of natural selection and can be important in the rapid evolution of new proteins. Genetic recombination can also be involved in DNA repair, particularly in the cell's response to double-strand breaks.

The most common form of chromosomal crossover is homologous recombination, where the two chromosomes involved share very similar sequences. Non-homologous recombination can be damaging to cells, as it can produce chromosomal translocations and genetic abnormalities. The recombination reaction is catalyzed by enzymes known as recombinases, such as RAD51. The first step in recombination is a double-stranded break caused by either an endonuclease or damage to the DNA. A series of steps catalyzed in part by the recombinase then leads to joining of the two helices by at least one Holliday junction, in which a segment of a single strand in each helix is annealed to the complementary strand in the other helix. The Holliday junction is a tetrahedral junction structure that can be moved along the pair of chromosomes, swapping one strand for another. The recombination reaction is then halted by cleavage of the junction and re-ligation of the released DNA. Only strands of like polarity exchange DNA during recombination. There are two types of cleavage: east-west cleavage and north-south cleavage. The north-south cleavage nicks both strands of DNA, while the east-west cleavage has one strand of DNA intact. The formation of a Holliday junction during recombination makes it possible for genetic diversity, genes to exchange on chromosomes, and expression of wild-type viral genomes.

DNA contains the genetic information that allows all modern living things to function, grow and reproduce. However, it is unclear how long in the 4-billion-year history of life DNA has performed this function, as it has been proposed that the earliest forms of life may have used RNA as their genetic material. RNA may have acted as the central part of early cell metabolism as it can both transmit genetic information and carry out catalysis as part of ribozymes. This ancient RNA world where nucleic acid would have been used for both catalysis and genetics may have influenced the evolution of the current genetic code based on four nucleotide bases. This would occur, since the number of different bases in such an organism is a trade-off between a small number of bases increasing replication accuracy and a large number of bases increasing the catalytic efficiency of ribozymes. However, there is no direct evidence of ancient genetic systems, as recovery of DNA from most fossils is impossible because DNA survives in the environment for less than one million years, and slowly degrades into short fragments in solution. Claims for older DNA have been made, most notably a report of the isolation of a viable bacterium from a salt crystal 250 million years old, but these claims are controversial.

Building blocks of DNA (adenine, guanine, and related organic molecules) may have been formed extraterrestrially in outer space. Complex DNA and RNA organic compounds of life, including uracil, cytosine, and thymine, have also been formed in the laboratory under conditions mimicking those found in outer space, using starting chemicals, such as pyrimidine, found in meteorites. Pyrimidine, like polycyclic aromatic hydrocarbons (PAHs), the most carbon-rich chemical found in the universe, may have been formed in red giants or in interstellar cosmic dust and gas clouds.

Methods have been developed to purify DNA from organisms, such as phenol-chloroform extraction, and to manipulate it in the laboratory, such as restriction digests and the polymerase chain reaction. Modern biology and biochemistry make intensive use of these techniques in recombinant DNA technology. Recombinant DNA is a man-made DNA sequence that has been assembled from other DNA sequences. They can be transformed into organisms in the form of plasmids or in the appropriate format, by using a viral vector. The genetically modified organisms produced can be used to produce products such as recombinant proteins, used in medical research, or be grown in agriculture.

Forensic scientists can use DNA in blood, semen, skin, saliva or hair found at a crime scene to identify a matching DNA of an individual, such as a perpetrator. This process is formally termed DNA profiling, but may also be called "genetic fingerprinting". In DNA profiling, the lengths of variable sections of repetitive DNA, such as short tandem repeats and minisatellites, are compared between people. This method is usually an extremely reliable technique for identifying a matching DNA. However, identification can be complicated if the scene is contaminated with DNA from several people. DNA profiling was developed in 1984 by British geneticist Sir Alec Jeffreys, and first used in forensic science to convict Colin Pitchfork in the 1988 Enderby murders case.

The development of forensic science and the ability to now obtain genetic matching on minute samples of blood, skin, saliva, or hair has led to re-examining many cases. Evidence can now be uncovered that was scientifically impossible at the time of the original examination. Combined with the removal of the double jeopardy law in some places, this can allow cases to be reopened where prior trials have failed to produce sufficient evidence to convince a jury. People charged with serious crimes may be required to provide a sample of DNA for matching purposes. The most obvious defense to DNA matches obtained forensically is to claim that cross-contamination of evidence has occurred. This has resulted in meticulous strict handling procedures with new cases of serious crime.
DNA profiling is also used successfully to positively identify victims of mass casualty incidents, bodies or body parts in serious accidents, and individual victims in mass war graves, via matching to family members.

DNA profiling is also used in DNA paternity testing to determine if someone is the biological parent or grandparent of a child with the probability of parentage is typically 99.99% when the alleged parent is biologically related to the child. Normal DNA sequencing methods happen after birth, but there are new methods to test paternity while a mother is still pregnant.

Deoxyribozymes, also called DNAzymes or catalytic DNA, are first discovered in 1994. They are mostly single stranded DNA sequences isolated from a large pool of random DNA sequences through a combinatorial approach called in vitro selection or systematic evolution of ligands by exponential enrichment (SELEX). DNAzymes catalyze variety of chemical reactions including RNA-DNA cleavage, RNA-DNA ligation, amino acids phosphorylation-dephosphorylation, carbon-carbon bond formation, and etc. DNAzymes can enhance catalytic rate of chemical reactions up to 100,000,000,000-fold over the uncatalyzed reaction. The most extensively studied class of DNAzymes is RNA-cleaving types which have been used to detect different metal ions and designing therapeutic agents. Several metal-specific DNAzymes have been reported including the GR-5 DNAzyme (lead-specific), the CA1-3 DNAzymes (copper-specific), the 39E DNAzyme (uranyl-specific) and the NaA43 DNAzyme (sodium-specific). The NaA43 DNAzyme, which is reported to be more than 10,000-fold selective for sodium over other metal ions, was used to make a real-time sodium sensor in living cells.

Bioinformatics involves the development of techniques to store, data mine, search and manipulate biological data, including DNA nucleic acid sequence data. These have led to widely applied advances in computer science, especially string searching algorithms, machine learning, and database theory. String searching or matching algorithms, which find an occurrence of a sequence of letters inside a larger sequence of letters, were developed to search for specific sequences of nucleotides. The DNA sequence may be aligned with other DNA sequences to identify homologous sequences and locate the specific mutations that make them distinct. These techniques, especially multiple sequence alignment, are used in studying phylogenetic relationships and protein function. Data sets representing entire genomes' worth of DNA sequences, such as those produced by the Human Genome Project, are difficult to use without the annotations that identify the locations of genes and regulatory elements on each chromosome. Regions of DNA sequence that have the characteristic patterns associated with protein- or RNA-coding genes can be identified by gene finding algorithms, which allow researchers to predict the presence of particular gene products and their possible functions in an organism even before they have been isolated experimentally. Entire genomes may also be compared, which can shed light on the evolutionary history of particular organism and permit the examination of complex evolutionary events.

DNA nanotechnology uses the unique molecular recognition properties of DNA and other nucleic acids to create self-assembling branched DNA complexes with useful properties. DNA is thus used as a structural material rather than as a carrier of biological information. This has led to the creation of two-dimensional periodic lattices (both tile-based and using the "DNA origami" method) and three-dimensional structures in the shapes of polyhedra. Nanomechanical devices and algorithmic self-assembly have also been demonstrated, and these DNA structures have been used to template the arrangement of other molecules such as gold nanoparticles and streptavidin proteins.

Because DNA collects mutations over time, which are then inherited, it contains historical information, and, by comparing DNA sequences, geneticists can infer the evolutionary history of organisms, their phylogeny. This field of phylogenetics is a powerful tool in evolutionary biology. If DNA sequences within a species are compared, population geneticists can learn the history of particular populations. This can be used in studies ranging from ecological genetics to anthropology.

In a paper published in "Nature" in January 2013, scientists from the European Bioinformatics Institute and Agilent Technologies proposed a mechanism to use DNA's ability to code information as a means of digital data storage. The group was able to encode 739 kilobytes of data into DNA code, synthesize the actual DNA, then sequence the DNA and decode the information back to its original form, with a reported 100% accuracy. The encoded information consisted of text files and audio files. A prior experiment was published in August 2012. It was conducted by researchers at Harvard University, where the text of a 54,000-word book was encoded in DNA.

Moreover, in living cells, the storage can be turned active by enzymes. Light-gated protein domains fused to DNA processing enzymes are suitable for that task "in vitro". Fluorescent exonucleases can transmit the output according to the nucleotide they have read.

DNA was first isolated by the Swiss physician Friedrich Miescher who, in 1869, discovered a microscopic substance in the pus of discarded surgical bandages. As it resided in the nuclei of cells, he called it "nuclein". In 1878, Albrecht Kossel isolated the non-protein component of "nuclein", nucleic acid, and later isolated its five primary nucleobases. 

In 1909, Phoebus Levene identified the base, sugar, and phosphate nucleotide unit of the RNA (then named "yeast nucleic acid"). In 1929, Levene identified deoxyribose sugar in "thymus nucleic acid" (DNA). Levene suggested that DNA consisted of a string of four nucleotide units linked together through the phosphate groups ("tetranucleotide hypothesis"). Levene thought the chain was short and the bases repeated in a fixed order.
In 1927, Nikolai Koltsov proposed that inherited traits would be inherited via a "giant hereditary molecule" made up of "two mirror strands that would replicate in a semi-conservative fashion using each strand as a template". In 1928, Frederick Griffith in his experiment discovered that traits of the "smooth" form of "Pneumococcus" could be transferred to the "rough" form of the same bacteria by mixing killed "smooth" bacteria with the live "rough" form. This system provided the first clear suggestion that DNA carries genetic information.

In 1933, while studying virgin sea urchin eggs, Jean Brachet suggested that DNA is found in cell nucleus and that RNA is present exclusively in the cytoplasm. At the time, "yeast nucleic acid" (RNA) was thought to occur only in plants, while "thymus nucleic acid" (DNA) only in animals. The latter was thought to be a tetramer, with the function of buffering cellular pH.

In 1937, William Astbury produced the first X-ray diffraction patterns that showed that DNA had a regular structure.

In 1943, Oswald Avery, along with coworkers Colin MacLeod and Maclyn McCarty, identified DNA as the transforming principle, supporting Griffith's suggestion (Avery–MacLeod–McCarty experiment). DNA's role in heredity was confirmed in 1952 when Alfred Hershey and Martha Chase in the Hershey–Chase experiment showed that DNA is the genetic material of the T2 phage.
Late in 1951, Francis Crick started working with James Watson at the Cavendish Laboratory within the University of Cambridge. In 1953, Watson and Crick suggested what is now accepted as the first correct double-helix model of DNA structure in the journal "Nature". Their double-helix, molecular model of DNA was then based on one X-ray diffraction image (labeled as "Photo 51") taken by Rosalind Franklin and Raymond Gosling in May 1952, and the information that the DNA bases are paired. On 28 February 1953 Crick interrupted patrons' lunchtime at The Eagle pub in Cambridge to announce that he and Watson had "discovered the secret of life". 

Months earlier, in February 1953, Linus Pauling and Robert Corey proposed a model for nucleic acids containing three intertwined chains, with the phosphates near the axis, and the bases on the outside. Experimental evidence supporting the Watson and Crick model was published in a series of five articles in the same issue of "Nature". Of these, Franklin and Gosling's paper was the first publication of their own X-ray diffraction data and original analysis method that partly supported the Watson and Crick model; this issue also contained an article on DNA structure by Maurice Wilkins and two of his colleagues, whose analysis and "in vivo" B-DNA X-ray patterns also supported the presence "in vivo" of the double-helical DNA configurations as proposed by Crick and Watson for their double-helix molecular model of DNA in the prior two pages of "Nature". In 1962, after Franklin's death, Watson, Crick, and Wilkins jointly received the Nobel Prize in Physiology or Medicine. Nobel Prizes are awarded only to living recipients. A debate continues about who should receive credit for the discovery.

In an influential presentation in 1957, Crick laid out the central dogma of molecular biology, which foretold the relationship between DNA, RNA, and proteins, and articulated the "adaptor hypothesis". Final confirmation of the replication mechanism that was implied by the double-helical structure followed in 1958 through the Meselson–Stahl experiment. Further work by Crick and coworkers showed that the genetic code was based on non-overlapping triplets of bases, called codons, allowing Har Gobind Khorana, Robert W. Holley, and Marshall Warren Nirenberg to decipher the genetic code. These findings represent the birth of molecular biology.



</doc>
<doc id="7957" url="https://en.wikipedia.org/wiki?curid=7957" title="Kennedy family">
Kennedy family

The Kennedy family is an American political family that has long been prominent in American politics, public service, and business. The first Kennedy was elected to public office in 1884, 35 years after the family's arrival from Ireland. At least one Kennedy family member held federal elective office in every year since 1947, a span of time comprising more than a quarter of the United States' existence.

The descendants of Joseph P. Kennedy Sr. and Rose Kennedy include a president of the United States (who had also served in both houses of Congress), a U.S. attorney general, four other members of the United States House of Representatives or Senate, and two U.S. ambassadors, a lieutenant governor, three state legislators (one of whom went on to the U.S. House of Representatives), and one mayor.

In addition, Joseph Sr. and Rose's daughter, Eunice, founded the National Institute of Child Health and Human Development (a part of the National Institutes of Health), and founded the Special Olympics. A granddaughter, Maria Shriver, was married to Arnold Schwarzenegger, the actor and bodybuilder who later served as California governor; as First Lady of California, she was instrumental in creating the California Museum for History, Women and the Arts (now the California Museum).. Other descendants of Joseph and Rose Kennedy have been active as lawyers, authors, and activists on behalf of those with physical and intellectual challenges.

According to genealogist Brian Kennedy in his work "JFK's Irish O'Kennedy Ancestors", the Kennedys who would go on to play a significant role in the United States of America originated from the Ó Cinnéide Fionn (one of the three Irish Gaelic Ó Cinnéide clans who ruled the kingdom of Ormond, along with the Ó Cinnéide Donn and Ó Cinnéide Ruadh). Their progenitor, Diarmaid Ó Cinnéide Fionn held Knigh Castle close to what is today Puckane, County Tipperary in 1546. From there, having lost out to the New English order in the Kingdom of Ireland, they ended up in Dunganstown, New Ross, County Wexford by 1740. It is here that Patrick Kennedy was born in 1823.

The first Kennedys to reside in the United States were Patrick Kennedy (1823–1858) and Bridget Murphy (1824–1888), who sailed from Ireland to East Boston in 1849; Patrick worked in East Boston as a barrel maker, or cooper. Patrick and Bridget had five children; their youngest, Patrick Joseph "P. J." Kennedy, went into business and served in the state Legislature from 1884-1895.

P.J. and Mary Augusta Hickey, were the parents of four children. Their oldest was Joseph Patrick "Joe" Kennedy Sr., who amassed a fortune in banking and securities trading, which he further expanded by investing in other growing industries. Joseph Sr. was appointed by President Franklin D. Roosevelt as the first chairman of the Securities and Exchange Commission, chairman of the Maritime Commission, and U.S. Ambassador to the United Kingdom in the lead-up to World War II. He served on The Hoover Commission, officially named the Commission on Organization of the Executive Branch of the Government, from 1947–1949; the commission was appointed by President Harry S Truman to recommend administrative changes in the federal government.

Joseph Sr. and Rose Elizabeth Fitzgerald, were the parents of nine children: Joseph Jr., John, Rosemary, Kathleen, Eunice, Patricia, Robert, Jean, and Ted. John served as the President of the United States, while Robert and Ted both became prominent senators. Every Kennedy to hold elective office has served as a Democrat, while other members of the family have worked for the Democratic Party or held Cabinet posts in Democratic administrations. Many have attended Harvard University, and the family has contributed greatly to that university's John F. Kennedy School of Government.

Joseph Sr. expected his eldest son, Joseph Jr., to go into politics and to ultimately be elected President. Joseph Jr. was elected as a delegate to the 1940 Democratic National Convention, and enlisted in the Navy after the U.S. entered World War II. Joseph Jr. was killed in 1944 when the bomber he was piloting exploded in flight. It then fell upon John, who had considered a career as a journalist — he had authored a book and did some reporting for Hearst Newspapers — to fulfill his father's desire to see the family involved in politics and government. After returning from Navy service, John served in the U.S. House of Representatives from 1947 to 1953, and then as U.S. Senator until his election as President in 1960. 

During John's administration, Robert served as attorney general; brother-in-law, Sargent Shriver, served as director of the new Peace Corps, and Ted was elected to the Senate. Among the Kennedy administration's accomplishments are the Alliance for Progress, the Peace Corps, peaceful resolution to the Cuban Missile Crisis, the Nuclear Test Ban Treaty of 1963, the 24th Amendment ending the poll tax, and the Civil Rights Act of 1964. The family was the subject of intense media coverage during and after the Kennedy presidency, often emphasizing their relative youth, allure, education, and future in politics. Ted served in the Senate with his brother Robert, and was serving in the Senate when his nephew, Joseph P. II, and son, Patrick J., served in the U.S. House of Representatives. 

The family suffered many tragedies, which contributed to the idea of the "Kennedy curse". In 1941, Rosemary underwent a lobotomy intended to curb her behavioral and emotional issues, but the operation left her incapacitated; Joseph Jr. died in 1944 when the Navy bomber he was piloting during World War II exploded in flight; Kathleen died in a plane crash in France in 1948; John and Robert were assassinated in 1963 and 1968; and Ted was the driver of his car that accidentally went off a bridge and into a channel in 1969, resulting in the drowning death of his passenger, Mary Jo Kopechne. Of Joseph Sr.'s grandchildren, David died of a drug overdose in 1984; Michael died from injuries sustained in a skiing accident in 1997; John Jr. died in a plane crash in 1999; and Kara died of a heart attack in 2011.

"Italics" denote members who married into the family. Only members who held political office are shown below.


Since John F. Kennedy's election to the U.S. House of Representatives in 1946, there have been very few times in which a Kennedy was not holding public office — first from December 22, 1960 until January 20, 1961 (from Kennedy's resignation from the Senate to his taking office as President) and next from Patrick J. Kennedy's departure from the House on January 3, 2011 until Joseph P. Kennedy III's election to the House on January 3, 2013.

In 1961, John F. Kennedy was presented with a grant of arms for all the descendants of Patrick Kennedy (1823–1858) from the Chief Herald of Ireland. The design of the arms strongly alludes to symbols in the coats of arms of the O'Kennedys of Ormonde and the FitzGeralds of Desmond, from whom the family is believed to be descended. The crest is an armored hand holding four arrows between two olive branches, elements taken from the coat of arms of the United States of America and also symbolic of Kennedy and his brothers.



</doc>
<doc id="7958" url="https://en.wikipedia.org/wiki?curid=7958" title="Deflation (disambiguation)">
Deflation (disambiguation)

Deflation refers to a decrease in the general price level, the opposite of inflation.

Deflation may also refer to:



</doc>
<doc id="7959" url="https://en.wikipedia.org/wiki?curid=7959" title="Democracy">
Democracy

Democracy ( "", literally "rule by people"), in modern usage, has three senses - all for a system of government where the citizens exercise power by voting. In a direct democracy, the citizens as a whole form a governing body, and vote directly on each issue, e.g. on the passage of a particular tax law. In a representative democracy the citizens elect representatives from among themselves. These representatives meet to form a governing body, such as a legislature. In a constitutional democracy the powers of the majority are exercised within the framework of a representative democracy, but the constitution limits the majority and protects the minority, usually through the enjoyment by all of certain individual rights, e.g. freedom of speech, or freedom of association. "Rule of the majority" is sometimes referred to as democracy. Democracy is a system of processing conflicts in which outcomes depend on what participants do, but no single force controls what occurs and its outcomes. 

The uncertainty of outcomes is inherent in democracy, which makes all forces struggle repeatedly for the realization of their interests, being the devolution of power from a group of people to a set of rules. Western democracy, as distinct from that which existed in pre-modern societies, is generally considered to have originated in city-states such as Classical Athens and the Roman Republic, where various schemes and degrees of enfranchisement of the free male population were observed before the form disappeared in the West at the beginning of late antiquity. The English word dates to the 16th century, from the older Middle French and Middle Latin equivalents.

According to American political scientist Larry Diamond, democracy consists of four key elements: a political system for choosing and replacing the government through free and fair elections; the active participation of the people, as citizens, in politics and civic life; protection of the human rights of all citizens; a rule of law, in which the laws and procedures apply equally to all citizens.

The term appeared in the 5th century BC to denote the political systems then existing in Greek city-states, notably Athens, to mean "rule of the people", in contrast to aristocracy (, ""), meaning "rule of an elite". While theoretically these definitions are in opposition, in practice the distinction has been blurred historically. The political system of Classical Athens, for example, granted democratic citizenship to free men and excluded slaves and women from political participation. In virtually all democratic governments throughout ancient and modern history, democratic citizenship consisted of an elite class, until full enfranchisement was won for all adult citizens in most modern democracies through the suffrage movements of the 19th and 20th centuries.

Democracy contrasts with forms of government where power is either held by an individual, as in an absolute monarchy, or where power is held by a small number of individuals, as in an oligarchy. Nevertheless, these oppositions, inherited from Greek philosophy, are now ambiguous because contemporary governments have mixed democratic, oligarchic and monarchic elements. Karl Popper defined democracy in contrast to dictatorship or tyranny, thus focusing on opportunities for the people to control their leaders and to oust them without the need for a revolution.

No consensus exists on how to define democracy, but legal equality, political freedom and rule of law have been identified as important characteristics. These principles are reflected in all eligible citizens being equal before the law and having equal access to legislative processes. For example, in a representative democracy, every vote has equal weight, no unreasonable restrictions can apply to anyone seeking to become a representative, and the freedom of its eligible citizens is secured by legitimised rights and liberties which are typically protected by a constitution. Other uses of "democracy" include that of direct democracy.

One theory holds that democracy requires three fundamental principles: upward control (sovereignty residing at the lowest levels of authority), political equality, and social norms by which individuals and institutions only consider acceptable acts that reflect the first two principles of upward control and political equality.

The term "democracy" is sometimes used as shorthand for liberal democracy, which is a variant of representative democracy that may include elements such as political pluralism; equality before the law; the right to petition elected officials for redress of grievances; due process; civil liberties; human rights; and elements of civil society outside the government. Roger Scruton argues that democracy alone cannot provide personal and political freedom unless the institutions of civil society are also present.

In some countries, notably in the United Kingdom which originated the Westminster system, the dominant principle is that of parliamentary sovereignty, while maintaining judicial independence. In the United States, separation of powers is often cited as a central attribute. In India, parliamentary sovereignty is subject to the Constitution of India which includes judicial review. Though the term "democracy" is typically used in the context of a political state, the principles also are applicable to private organisations.

Majority rule is often listed as a characteristic of democracy. Hence, democracy allows for political minorities to be oppressed by the "tyranny of the majority" in the absence of legal protections of individual or group rights. An essential part of an "ideal" representative democracy is competitive elections that are substantively and procedurally "fair," i.e., just and equitable. In some countries, freedom of political expression, freedom of speech, freedom of the press, and internet democracy are considered important to ensure that voters are well informed, enabling them to vote according to their own interests.

It has also been suggested that a basic feature of democracy is the capacity of all voters to participate freely and fully in the life of their society. With its emphasis on notions of social contract and the collective will of all the voters, democracy can also be characterised as a form of political collectivism because it is defined as a form of government in which all eligible citizens have an equal say in lawmaking.

While representative democracy is sometimes equated with the republican form of government, the term "republic" classically has encompassed both democracies and aristocracies. Many democracies are constitutional monarchies, such as the United Kingdom.

The term "democracy" first appeared in ancient Greek political and philosophical thought in the city-state of Athens during classical antiquity. The word comes from "demos", "common people" and "kratos", strength. Led by Cleisthenes, Athenians established what is generally held as the first democracy in 508–507 BC. Cleisthenes is referred to as "the father of Athenian democracy."

Athenian democracy took the form of a direct democracy, and it had two distinguishing features: the random selection of ordinary citizens to fill the few existing government administrative and judicial offices, and a legislative assembly consisting of all Athenian citizens. All eligible citizens were allowed to speak and vote in the assembly, which set the laws of the city state. However, Athenian citizenship excluded women, slaves, foreigners (μέτοικοι / "métoikoi"), non-landowners, and men under 20 years of age.
The exclusion of large parts of the population from the citizen body is closely related to the ancient understanding of citizenship. In most of antiquity the benefit of citizenship was tied to the obligation to fight war campaigns.

Athenian democracy was not only "direct" in the sense that decisions were made by the assembled people, but also the "most direct" in the sense that the people through the assembly, boule and courts of law controlled the entire political process and a large proportion of citizens were involved constantly in the public business. Even though the rights of the individual were not secured by the Athenian constitution in the modern sense (the ancient Greeks had no word for "rights"), the Athenians enjoyed their liberties not in opposition to the government but by living in a city that was not subject to another power and by not being subjects themselves to the rule of another person.

Range voting appeared in Sparta as early as 700 BC. The Apella was an assembly of the people, held once a month, in which every male citizen of at least 30 years of age could participate. In the Apella, Spartans elected leaders and cast votes by range voting and shouting. Aristotle called this "childish", as compared with the stone voting ballots used by the Athenians. Sparta adopted it because of its simplicity, and to prevent any bias voting, buying, or cheating that was predominant in the early democratic elections.

Even though the Roman Republic contributed significantly to many aspects of democracy, only a minority of Romans were citizens with votes in elections for representatives. The votes of the powerful were given more weight through a system of gerrymandering, so most high officials, including members of the Senate, came from a few wealthy and noble families. In addition, the Roman Republic was the first government in the western world to have a Republic as a nation-state, although it didn't have much of a democracy. The Romans invented the concept of classics and many works from Ancient Greece were preserved. Additionally, the Roman model of governance inspired many political thinkers over the centuries, and today's modern representative democracies imitate more the Roman than the Greek models because it was a state in which supreme power was held by the people and their elected representatives, and which had an elected or nominated leader. Other cultures, such as the Iroquois Nation in the Americas between around 1450 and 1600 AD also developed a form of democratic society before they came in contact with the Europeans. This indicates that forms of democracy may have been invented in other societies around the world.

During the Middle Ages, there were various systems involving elections or assemblies, although often only involving a small part of the population. These included:
Most regions in medieval Europe were ruled by clergy or feudal lords.

The Kouroukan Fouga divided the Mali Empire into ruling clans (lineages) that were represented at a great assembly called the "Gbara". However, the charter made Mali more similar to a constitutional monarchy than a democratic republic. 
The Parliament of England had its roots in the restrictions on the power of kings written into Magna Carta (1215), which explicitly protected certain rights of the King's subjects and implicitly supported what became the English writ of habeas corpus, safeguarding individual freedom against unlawful imprisonment with right to appeal. The first representative national assembly in England was Simon de Montfort's Parliament in 1265. The emergence of petitioning is some of the earliest evidence of parliament being used as a forum to address the general grievances of ordinary people. However, the power to call parliament remained at the pleasure of the monarch.

In 17th century England, there was renewed interest in Magna Carta. The Parliament of England passed the Petition of Right in 1628 which established certain liberties for subjects. The English Civil War (1642–1651) was fought between the King and an oligarchic but elected Parliament, during which the idea of a political party took form with groups debating rights to political representation during the Putney Debates of 1647. Subsequently, the Protectorate (1653-59) and the English Restoration (1660) restored more autocratic rule, although Parliament passed the Habeas Corpus Act in 1679 which strengthened the convention that forbade detention lacking sufficient cause or evidence. After the Glorious Revolution of 1688, the Bill of Rights was enacted in 1689 which codified certain rights and liberties, and is still in effect. The Bill set out the requirement for regular elections, rules for freedom of speech in Parliament and limited the power of the monarch, ensuring that, unlike much of Europe at the time, royal absolutism would not prevail.

In the Cossack republics of Ukraine in the 16th and 17th centuries, the Cossack Hetmanate and Zaporizhian Sich, the holder of the highest post of Hetman was elected by the representatives from the country's districts.

In North America, representative government began in Jamestown, Virginia, with the election of the House of Burgesses (forerunner of the Virginia General Assembly) in 1619. English Puritans who migrated from 1620 established colonies in New England whose local governance was democratic and which contributed to the democratic development of the United States; although these local assemblies had some small amounts of devolved power, the ultimate authority was held by the Crown and the English Parliament. The Puritans (Pilgrim Fathers), Baptists, and Quakers who founded these colonies applied the democratic organisation of their congregations also to the administration of their communities in worldly matters.

The first Parliament of Great Britain was established in 1707, after the merger of the Kingdom of England and the Kingdom of Scotland under the Acts of Union. Although the monarch increasingly became a figurehead,
only a small minority actually had a voice; Parliament was elected by only a few percent of the population (less than 3% as late as 1780). During the Age of Liberty in Sweden (1718–1772), civil rights were expanded and power shifted from the monarch to parliament. The taxed peasantry was represented in parliament, although with little influence, but commoners without taxed property had no suffrage.

The creation of the short-lived Corsican Republic in 1755 marked the first nation in modern history to adopt a democratic constitution (all men and women above age of 25 could vote). This Corsican Constitution was the first based on Enlightenment principles and included female suffrage, something that was not granted in most other democracies until the 20th century.

In the American colonial period before 1776, and for some time after, often only adult white male property owners could vote; enslaved Africans, most free black people and most women were not extended the franchise. On the American frontier, democracy became a way of life, with more widespread social, economic and political equality. Although not described as a democracy by the founding fathers, they shared a determination to root the American experiment in the principles of natural freedom and equality.

The American Revolution led to the adoption of the United States Constitution in 1787, the oldest surviving, still active, governmental codified constitution. The Constitution provided for an elected government and protected civil rights and liberties for some, but did not end slavery nor extend voting rights in the United States beyond white male property owners (about 6% of the population). The Bill of Rights in 1791 set limits on government power to protect personal freedoms but had little impact on judgements by the courts for the first 130 years after ratification.

In 1789, Revolutionary France adopted the Declaration of the Rights of Man and of the Citizen and, although short-lived, the National Convention was elected by all men in 1792. However, in the early 19th century, little of democracy – as theory, practice, or even as word – remained in the North Atlantic world.

During this period, slavery remained a social and economic institution in places around the world. This was particularly the case in the United States, and especially in the last fifteen slave states that kept slavery legal in the American South until the Civil War. A variety of organisations were established advocating the movement of black people from the United States to locations where they would enjoy greater freedom and equality.

The United Kingdom's Slave Trade Act 1807 banned the trade across the British Empire, which was enforced internationally by the Royal Navy under treaties Britain negotiated with other nations. As the voting franchise in the U.K. was increased, it also was made more uniform in a series of reforms beginning with the Reform Act 1832, although the United Kingdom did not manage to become a complete democracy well into the 20th century. In 1833, the United Kingdom passed the Slavery Abolition Act which took effect across the British Empire.

Universal male suffrage was established in France in March 1848 in the wake of the French Revolution of 1848. In 1848, several revolutions broke out in Europe as rulers were confronted with popular demands for liberal constitutions and more democratic government.

In the 1860 United States Census, the slave population in the United States had grown to four million, and in Reconstruction after the Civil War (late 1860s), the newly freed slaves became citizens with a nominal right to vote for men. Full enfranchisement of citizens was not secured until after the Civil Rights Movement gained passage by the United States Congress of the Voting Rights Act of 1965.

20th-century transitions to liberal democracy have come in successive "waves of democracy", variously resulting from wars, revolutions, decolonisation, and religious and economic circumstances. Global waves of "democratic regression" reversing democratization, have also occurred in the 1920s and 30s, in the 1960s and 1970s, and in the 2010s.

World War I and the dissolution of the Ottoman and Austro-Hungarian empires resulted in the creation of new nation-states from Europe, most of them at least nominally democratic.

In the 1920s democracy flourished and women's suffrage advanced, but the Great Depression brought disenchantment and most of the countries of Europe, Latin America, and Asia turned to strong-man rule or dictatorships. Fascism and dictatorships flourished in Nazi Germany, Italy, Spain and Portugal, as well as non-democratic governments in the Baltics, the Balkans, Brazil, Cuba, China, and Japan, among others.

World War II brought a definitive reversal of this trend in western Europe. The democratisation of the American, British, and French sectors of occupied Germany (disputed), Austria, Italy, and the occupied Japan served as a model for the later theory of government change. However, most of Eastern Europe, including the Soviet sector of Germany fell into the non-democratic Soviet bloc.

The war was followed by decolonisation, and again most of the new independent states had nominally democratic constitutions. India emerged as the world's largest democracy and continues to be so. Countries that were once part of the British Empire often adopted the British Westminster system.

By 1960, the vast majority of country-states were nominally democracies, although most of the world's populations lived in nations that experienced sham elections, and other forms of subterfuge (particularly in "Communist" nations and the former colonies.)

A subsequent wave of democratisation brought substantial gains toward true liberal democracy for many nations. Spain, Portugal (1974), and several of the military dictatorships in South America returned to civilian rule in the late 1970s and early 1980s (Argentina in 1983, Bolivia, Uruguay in 1984, Brazil in 1985, and Chile in the early 1990s). This was followed by nations in East and South Asia by the mid-to-late 1980s.

Economic malaise in the 1980s, along with resentment of Soviet oppression, contributed to the collapse of the Soviet Union, the associated end of the Cold War, and the democratisation and liberalisation of the former Eastern bloc countries. The most successful of the new democracies were those geographically and culturally closest to western Europe, and they are now members or candidate members of the European Union. In 1986, after the toppling of the most prominent Asian dictatorship, the only democratic state of its kind at the time emerged in the Philippines with the rise of Corazon Aquino, who would later be known as the Mother of Asian Democracy.

The liberal trend spread to some nations in Africa in the 1990s, most prominently in South Africa. Some recent examples of attempts of liberalisation include the Indonesian Revolution of 1998, the Bulldozer Revolution in Yugoslavia, the Rose Revolution in Georgia, the Orange Revolution in Ukraine, the Cedar Revolution in Lebanon, the Tulip Revolution in Kyrgyzstan, and the Jasmine Revolution in Tunisia.

According to Freedom House, in 2007 there were 123 electoral democracies (up from 40 in 1972). According to "World Forum on Democracy", electoral democracies now represent 120 of the 192 existing countries and constitute 58.2 percent of the world's population. At the same time liberal democracies i.e. countries Freedom House regards as free and respectful of basic human rights and the rule of law are 85 in number and represent 38 percent of the global population.

Most electoral democracies continue to exclude those younger than 18 from voting. The voting age has been lowered to 16 for national elections in a number of countries, including Brazil, Austria, Cuba, and Nicaragua. In California, a 2004 proposal to permit a quarter vote at 14 and a half vote at 16 was ultimately defeated. In 2008, the German parliament proposed but shelved a bill that would grant the vote to each citizen at birth, to be used by a parent until the child claims it for themselves.

In 2007 the United Nations declared September 15 the International Day of Democracy.

According to Freedom House, starting in 2005, there have been eleven consecutive years in which declines in political rights and civil liberties throughout the world have outnumbered improvements, as populist and nationalist political forces have gained ground everywhere from Poland (under the Law and Justice Party) to the Philippines (under Rodrigo Duterte).

In a Freedom House report released in 2018, Democracy Scores for most countries declined for the 12th consecutive year.

Several freedom indices are published by several organisations according to their own various definitions of the term:

Because democracy is an overarching concept that includes the functioning of diverse institutions which are not easy to measure, strong limitations exist in quantifying and econometrically measuring the potential effects of democracy or its relationship with other phenomena - whether inequality, poverty, education etc. Given the constraints in acquiring reliable data with within-country variation on aspects of democracy, academics have largely studied cross-country variations. Yet variations between democratic institutions are very large across countries which constrains meaningful comparisons using statistical approaches. Since democracy is typically measured aggregately as a macro variable using a single observation for each country and each year, studying democracy faces a range of econometric constraints and is limited to basic correlations. Cross-country comparison of a composite, comprehensive and qualitative concept like democracy may thus not always be, for many purposes, methodologically rigorous or useful.

Democracy has taken a number of forms, both in theory and practice. Some varieties of democracy provide better representation and more freedom for their citizens than others. However, if any democracy is not structured so as to prohibit the government from excluding the people from the legislative process, or any branch of government from altering the separation of powers in its own favour, then a branch of the system can accumulate too much power and destroy the democracy.

The following kinds of democracy are not exclusive of one another: many specify details of aspects that are independent of one another and can co-exist in a single system.

Several variants of democracy exist, but there are two basic forms, both of which concern how the whole body of all eligible citizens executes its will. One form of democracy is direct democracy, in which all eligible citizens have active participation in the political decision making, for example voting on policy initiatives directly. In most modern democracies, the whole body of eligible citizens remain the sovereign power but political power is exercised indirectly through elected representatives; this is called a representative democracy.

Direct democracy is a political system where the citizens participate in the decision-making personally, contrary to relying on intermediaries or representatives. The use of a lot system, a characteristic of Athenian democracy, is unique to direct democracies. In this system, important governmental and administrative tasks are performed by citizens picked from a lottery. A direct democracy gives the voting population the power to:


Within modern-day representative governments, certain electoral tools like referendums, citizens' initiatives and recall elections are referred to as forms of direct democracy. However, some advocates of direct democracy argue for local assemblies of face-to-face discussion. Direct democracy as a government system currently exists in the Swiss cantons of Appenzell Innerrhoden and Glarus, the Rebel Zapatista Autonomous Municipalities, communities affiliated with the CIPO-RFM, the Bolivian city councils of FEJUVE, and Kurdish cantons of Rojava.

Representative democracy involves the election of government officials by the people being represented. If the head of state is also democratically elected then it is called a democratic republic. The most common mechanisms involve election of the candidate with a majority or a plurality of the votes. Most western countries have representative systems.

Representatives may be elected or become diplomatic representatives by a particular district (or constituency), or represent the entire electorate through proportional systems, with some using a combination of the two. Some representative democracies also incorporate elements of direct democracy, such as referendums. A characteristic of representative democracy is that while the representatives are elected by the people to act in the people's interest, they retain the freedom to exercise their own judgement as how best to do so. Such reasons have driven criticism upon representative democracy, pointing out the contradictions of representation mechanisms with democracy

Parliamentary democracy is a representative democracy where government is appointed by, or can be dismissed by, representatives as opposed to a "presidential rule" wherein the president is both head of state and the head of government and is elected by the voters. Under a parliamentary democracy, government is exercised by delegation to an executive ministry and subject to ongoing review, checks and balances by the legislative parliament elected by the people.

Parliamentary systems have the right to dismiss a Prime Minister at any point in time that they feel he or she is not doing their job to the expectations of the legislature. This is done through a Vote of No Confidence where the legislature decides whether or not to remove the Prime Minister from office by a majority support for his or her dismissal. In some countries, the Prime Minister can also call an election whenever he or she so chooses, and typically the Prime Minister will hold an election when he or she knows that they are in good favour with the public as to get re-elected. In other parliamentary democracies extra elections are virtually never held, a minority government being preferred until the next ordinary elections. An important feature of the parliamentary democracy is the concept of the "loyal opposition". The essence of the concept is that the second largest political party (or coalition) opposes the governing party (or coalition), while still remaining loyal to the state and its democratic principles.

Presidential Democracy is a system where the public elects the president through free and fair elections. The president serves as both the head of state and head of government controlling most of the executive powers. The president serves for a specific term and cannot exceed that amount of time. Elections typically have a fixed date and aren't easily changed. The president has direct control over the cabinet, specifically appointing the cabinet members.

The president cannot be easily removed from office by the legislature, but he or she cannot remove members of the legislative branch any more easily. This provides some measure of separation of powers. In consequence however, the president and the legislature may end up in the control of separate parties, allowing one to block the other and thereby interfere with the orderly operation of the state. This may be the reason why presidential democracy is not very common outside the Americas, Africa, and Central and Southeast Asia.

A semi-presidential system is a system of democracy in which the government includes both a prime minister and a president. The particular powers held by the prime minister and president vary by country.

Some modern democracies that are predominantly representative in nature also heavily rely upon forms of political action that are directly democratic. These democracies, which combine elements of representative democracy and direct democracy, are termed "hybrid democracies", "semi-direct democracies" or "participatory democracies". Examples include Switzerland and some U.S. states, where frequent use is made of referendums and initiatives.

The Swiss confederation is a semi-direct democracy. At the federal level, citizens can propose changes to the constitution (federal popular initiative) or ask for a referendum to be held on any law voted by the parliament. Between January 1995 and June 2005, Swiss citizens voted 31 times, to answer 103 questions (during the same period, French citizens participated in only two referendums). Although in the past 120 years less than 250 initiatives have been put to referendum. The populace has been conservative, approving only about 10% of the initiatives put before them; in addition, they have often opted for a version of the initiative rewritten by government.

In the United States, no mechanisms of direct democracy exists at the federal level, but over half of the states and many localities provide for citizen-sponsored ballot initiatives (also called "ballot measures", "ballot questions" or "propositions"), and the vast majority of states allow for referendums. Examples include the extensive use of referendums in the US state of California, which is a state that has more than 20 million voters.

In New England, Town meetings are often used, especially in rural areas, to manage local government. This creates a hybrid form of government, with a local direct democracy and a state government which is representative. For example, most Vermont towns hold annual town meetings in March in which town officers are elected, budgets for the town and schools are voted on, and citizens have the opportunity to speak and be heard on political matters.

Many countries such as the United Kingdom, Spain, the Netherlands, Belgium, Scandinavian countries, Thailand, Japan and Bhutan turned powerful monarchs into constitutional monarchs with limited or, often gradually, merely symbolic roles. For example, in the predecessor states to the United Kingdom, constitutional monarchy began to emerge and has continued uninterrupted since the Glorious Revolution of 1688 and passage of the Bill of Rights 1689.

In other countries, the monarchy was abolished along with the aristocratic system (as in France, China, Russia, Germany, Austria, Hungary, Italy, Greece and Egypt). An elected president, with or without significant powers, became the head of state in these countries.

Elite upper houses of legislatures, which often had lifetime or hereditary tenure, were common in many nations. Over time, these either had their powers limited (as with the British House of Lords) or else became elective and remained powerful (as with the Australian Senate).

The term "republic" has many different meanings, but today often refers to a representative democracy with an elected head of state, such as a president, serving for a limited term, in contrast to states with a hereditary monarch as a head of state, even if these states also are representative democracies with an elected or appointed head of government such as a prime minister.

The Founding Fathers of the United States rarely praised and often criticised democracy, which in their time tended to specifically mean direct democracy, often without the protection of a constitution enshrining basic rights; James Madison argued, especially in "The Federalist" No. 10, that what distinguished a direct "democracy" from a "republic" was that the former became weaker as it got larger and suffered more violently from the effects of faction, whereas a republic could get stronger as it got larger and combats faction by its very structure.

What was critical to American values, John Adams insisted, was that the government be "bound by fixed laws, which the people have a voice in making, and a right to defend." As Benjamin Franklin was exiting after writing the U.S. constitution, a woman asked him "Well, Doctor, what have we got—a republic or a monarchy?". He replied "A republic—if you can keep it."

A liberal democracy is a representative democracy in which the ability of the elected representatives to exercise decision-making power is subject to the rule of law, and moderated by a constitution or laws that emphasise the protection of the rights and freedoms of individuals, and which places constraints on the leaders and on the extent to which the will of the majority can be exercised against the rights of minorities (see civil liberties).

In a liberal democracy, it is possible for some large-scale decisions to emerge from the many individual decisions that citizens are free to make. In other words, citizens can "vote with their feet" or "vote with their dollars", resulting in significant informal government-by-the-masses that exercises many "powers" associated with formal government elsewhere.

Socialist thought has several different views on democracy. Social democracy, democratic socialism, and the dictatorship of the proletariat (usually exercised through Soviet democracy) are some examples. Many democratic socialists and social democrats believe in a form of participatory, industrial, economic and/or workplace democracy combined with a representative democracy.

Within Marxist orthodoxy there is a hostility to what is commonly called "liberal democracy", which they simply refer to as parliamentary democracy because of its often centralised nature. Because of their desire to eliminate the political elitism they see in capitalism, Marxists, Leninists and Trotskyists believe in direct democracy implemented through a system of communes (which are sometimes called soviets). This system ultimately manifests itself as council democracy and begins with workplace democracy. (See Democracy in Marxism.)

Anarchists are split in this domain, depending on whether they believe that a majority-rule is tyrannic or not. The only form of democracy considered acceptable to many anarchists is direct democracy. Pierre-Joseph Proudhon argued that the only acceptable form of direct democracy is one in which it is recognised that majority decisions are not binding on the minority, even when unanimous. However, anarcho-communist Murray Bookchin criticised individualist anarchists for opposing democracy, and says "majority rule" is consistent with anarchism.

Some anarcho-communists oppose the majoritarian nature of direct democracy, feeling that it can impede individual liberty and opt in favour of a non-majoritarian form of consensus democracy, similar to Proudhon's position on direct democracy. Henry David Thoreau, who did not self-identify as an anarchist but argued for "a better government" and is cited as an inspiration by some anarchists, argued that people should not be in the position of ruling others or being ruled when there is no consent.

Sometimes called "democracy without elections", sortition chooses decision makers via a random process. The intention is that those chosen will be representative of the opinions and interests of the people at large, and be more fair and impartial than an elected official. The technique was in widespread use in Athenian Democracy and Renaissance Florence and is still used in modern jury selection.

A consociational democracy allows for simultaneous majority votes in two or more ethno-religious constituencies, and policies are enacted only if they gain majority support from both or all of them.

A consensus democracy, in contrast, would not be dichotomous. Instead, decisions would be based on a multi-option approach, and policies would be enacted if they gained sufficient support, either in a purely verbal agreement, or via a consensus vote—a multi-option preference vote. If the threshold of support were at a sufficiently high level, minorities would be as it were protected automatically. Furthermore, any voting would be ethno-colour blind.

Qualified majority voting is designed by the Treaty of Rome to be the principal method of reaching decisions in the European Council of Ministers. This system allocates votes to member states in part according to their population, but heavily weighted in favour of the smaller states. This might be seen as a form of representative democracy, but representatives to the Council might be appointed rather than directly elected.

Inclusive democracy is a political theory and political project that aims for direct democracy in all fields of social life: political democracy in the form of face-to-face assemblies which are confederated, economic democracy in a stateless, moneyless and marketless economy, democracy in the social realm, i.e. self-management in places of work and education, and ecological democracy which aims to reintegrate society and nature. The theoretical project of inclusive democracy emerged from the work of political philosopher Takis Fotopoulos in "Towards An Inclusive Democracy" and was further developed in the journal "Democracy & Nature" and its successor "The International Journal of Inclusive Democracy".

The basic unit of decision making in an inclusive democracy is the demotic assembly, i.e. the assembly of demos, the citizen body in a given geographical area which may encompass a town and the surrounding villages, or even neighbourhoods of large cities. An inclusive democracy today can only take the form of a confederal democracy that is based on a network of administrative councils whose members or delegates are elected from popular face-to-face democratic assemblies in the various demoi. Thus, their role is purely administrative and practical, not one of policy-making like that of representatives in representative democracy.

The citizen body is advised by experts but it is the citizen body which functions as the ultimate decision-taker . Authority can be delegated to a segment of the citizen body to carry out specific duties, for example to serve as members of popular courts, or of regional and confederal councils. Such delegation is made, in principle, by lot, on a rotation basis, and is always recallable by the citizen body. Delegates to regional and confederal bodies should have specific mandates.

A Parpolity or Participatory Polity is a theoretical form of democracy that is ruled by a Nested Council structure. The guiding philosophy is that people should have decision making power in proportion to how much they are affected by the decision. Local councils of 25–50 people are completely autonomous on issues that affect only them, and these councils send delegates to higher level councils who are again autonomous regarding issues that affect only the population affected by that council.

A council court of randomly chosen citizens serves as a check on the tyranny of the majority, and rules on which body gets to vote on which issue. Delegates may vote differently from how their sending council might wish, but are mandated to communicate the wishes of their sending council. Delegates are recallable at any time. Referendums are possible at any time via votes of most lower-level councils, however, not everything is a referendum as this is most likely a waste of time. A parpolity is meant to work in tandem with a participatory economy.

Cosmopolitan democracy, also known as "Global democracy" or "World Federalism", is a political system in which democracy is implemented on a global scale, either directly or through representatives. An important justification for this kind of system is that the decisions made in national or regional democracies often affect people outside the constituency who, by definition, cannot vote. By contrast, in a cosmopolitan democracy, the people who are affected by decisions also have a say in them.

According to its supporters, any attempt to solve global problems is undemocratic without some form of cosmopolitan democracy. The general principle of cosmopolitan democracy is to expand some or all of the values and norms of democracy, including the rule of law; the non-violent resolution of conflicts; and equality among citizens, beyond the limits of the state. To be fully implemented, this would require reforming existing international organisations, e.g. the United Nations, as well as the creation of new institutions such as a World Parliament, which ideally would enhance public control over, and accountability in, international politics.

Cosmopolitan Democracy has been promoted, among others, by physicist Albert Einstein, writer Kurt Vonnegut, columnist George Monbiot, and professors David Held and Daniele Archibugi. The creation of the International Criminal Court in 2003 was seen as a major step forward by many supporters of this type of cosmopolitan democracy.

Creative Democracy is advocated by American philosopher John Dewey. The main idea about Creative Democracy is that democracy encourages individual capacity building and the interaction among the society. Dewey argues that democracy is a way of life in his work of "Creative Democracy: The Task Before Us" and an experience built on faith in human nature, faith in human beings, and faith in working with others. Democracy, in Dewey's view, is a moral ideal requiring actual effort and work by people; it is not an institutional concept that exists outside of ourselves. "The task of democracy", Dewey concludes, "is forever that of creation of a freer and more humane experience in which all share and to which all contribute".

Guided democracy is a form of democracy which incorporates regular popular elections, but which often carefully "guides" the choices offered to the electorate in a manner which may reduce the ability of the electorate to truly determine the type of government exercised over them. Such democracies typically have only one central authority which is often not subject to meaningful public review by any other governmental authority. Russian-style democracy has often been referred to as a "Guided democracy." Russian politicians have referred to their government as having only one center of power/ authority, as opposed to most other forms of democracy which usually attempt to incorporate two or more naturally competing sources of authority within the same government.

Aside from the public sphere, similar democratic principles and mechanisms of voting and representation have been used to govern other kinds of groups. Many non-governmental organisations decide policy and leadership by voting. Most trade unions and cooperatives are governed by democratic elections. Corporations are controlled by shareholders on the principle of one share, one vote - sometimes supplemented by workplace democracy. Amitai Etzioni has postulated a system that fuses elements of democracy with sharia law, termed "islamocracy".

Aristotle contrasted rule by the many (democracy/timocracy), with rule by the few (oligarchy/aristocracy), and with rule by a single person (tyranny or today autocracy/absolute monarchy). He also thought that there was a good and a bad variant of each system (he considered democracy to be the degenerate counterpart to timocracy).

For Aristotle the underlying principle of democracy is freedom, since only in a democracy can the citizens have a share in freedom. In essence, he argues that this is what every democracy should make its aim. There are two main aspects of freedom: being ruled and ruling in turn, since everyone is equal according to number, not merit, and to be able to live as one pleases.

A common view among early and renaissance Republican theorists was that democracy could only survive in small political communities. Heeding the lessons of the Roman Republic's shift to monarchism as it grew larger, these Republican theorists held that the expansion of territory and population inevitably led to tyranny. Democracy was therefore highly fragile and rare historically, as it could only survive in small political units, which due to their size were vulnerable to conquest by larger political units. Montesquieu famously said, "if a republic is small, it is destroyed by an outside force; if it is large, it is destroyed by an internal vice." Rousseau asserted, "It is, therefore the natural property of small states to be governed as a republic, of middling ones to be subject to a monarch, and of large empires to be swayed by a despotic prince."

Among modern political theorists, there are three contending conceptions of the fundamental rationale for democracy: "aggregative democracy," "deliberative democracy," and "radical democracy."

The theory of "aggregative democracy" claims that the aim of the democratic processes is to solicit citizens' preferences and aggregate them together to determine what social policies society should adopt. Therefore, proponents of this view hold that democratic participation should primarily focus on voting, where the policy with the most votes gets implemented.

Different variants of aggregative democracy exist. Under "minimalism", democracy is a system of government in which citizens have given teams of political leaders the right to rule in periodic elections. According to this minimalist conception, citizens cannot and should not "rule" because, for example, on most issues, most of the time, they have no clear views or their views are not well-founded. Joseph Schumpeter articulated this view most famously in his book "Capitalism, Socialism, and Democracy". Contemporary proponents of minimalism include William H. Riker, Adam Przeworski, Richard Posner.

According to the theory of direct democracy, on the other hand, citizens should vote directly, not through their representatives, on legislative proposals. Proponents of direct democracy offer varied reasons to support this view. Political activity can be valuable in itself, it socialises and educates citizens, and popular participation can check powerful elites. Most importantly, citizens do not really rule themselves unless they directly decide laws and policies.

Governments will tend to produce laws and policies that are close to the views of the median voter—with half to their left and the other half to their right. This is not actually a desirable outcome as it represents the action of self-interested and somewhat unaccountable political elites competing for votes. Anthony Downs suggests that ideological political parties are necessary to act as a mediating broker between individual and governments. Downs laid out this view in his 1957 book "An Economic Theory of Democracy".

Robert A. Dahl argues that the fundamental democratic principle is that, when it comes to binding collective decisions, each person in a political community is entitled to have his/her interests be given equal consideration (not necessarily that all people are equally satisfied by the collective decision). He uses the term polyarchy to refer to societies in which there exists a certain set of institutions and procedures which are perceived as leading to such democracy. First and foremost among these institutions is the regular occurrence of free and open elections which are used to select representatives who then manage all or most of the public policy of the society. However, these polyarchic procedures may not create a full democracy if, for example, poverty prevents political participation. Similarly, Ronald Dworkin argues that "democracy is a substantive, not a merely procedural, ideal."

"Deliberative democracy" is based on the notion that democracy is government by deliberation. Unlike aggregative democracy, deliberative democracy holds that, for a democratic decision to be legitimate, it must be preceded by authentic deliberation, not merely the aggregation of preferences that occurs in voting. "Authentic deliberation" is deliberation among decision-makers that is free from distortions of unequal political power, such as power a decision-maker obtained through economic wealth or the support of interest groups. If the decision-makers cannot reach consensus after authentically deliberating on a proposal, then they vote on the proposal using a form of majority rule.
Many theorists is discussing the conception of Debliberative Democracy, considering specially the thought of Jürgen Habermas.

"Radical democracy" is based on the idea that there are hierarchical and oppressive power relations that exist in society. Democracy's role is to make visible and challenge those relations by allowing for difference, dissent and antagonisms in decision making processes.

Some economists have criticized the efficiency of democracy, citing the premise of the irrational voter, or a voter who makes decisions without all of the facts or necessary information in order to make a truly informed decision. Another argument is that democracy slows down processes because of the amount of input and participation needed in order to go forward with a decision. A common example often quoted to substantiate this point is the high economic development achieved by China (a non-democratic country) as compared to India (a democratic country). According to economists, the lack of democratic participation in countries like China allows for unfettered economic growth.

On the other hand, Socrates was of the belief that democracy without educated masses (educated in the more broader sense of being knowledgeable and responsible) would only lead to populism being the criteria to become an elected leader, and not competence. This would ultimately lead to a demise of the nation. This was quoted by Plato in book 10 of The Republic, in Socrates' conversation with Adimantus. Socrates was of the opinion that the right to vote must not be an indiscriminate right (for example by birth or citizenship), but must be given only to people who thought sufficiently of their choice.

The 20th-century Italian thinkers Vilfredo Pareto and Gaetano Mosca (independently) argued that democracy was illusory, and served only to mask the reality of elite rule. Indeed, they argued that elite oligarchy is the unbendable law of human nature, due largely to the apathy and division of the masses (as opposed to the drive, initiative and unity of the elites), and that democratic institutions would do no more than shift the exercise of power from oppression to manipulation. As Louis Brandeis once professed, "We may have democracy, or we may have wealth concentrated in the hands of a few, but we can't have both."

Plato's "The Republic" presents a critical view of democracy through the narration of Socrates: "Democracy, which is a charming form of government, full of variety and disorder, and dispensing a sort of equality to equals and unequaled alike." In his work, Plato lists 5 forms of government from best to worst. Assuming that "the Republic" was intended to be a serious critique of the political thought in Athens, Plato argues that only Kallipolis, an aristocracy led by the unwilling philosopher-kings (the wisest men), is a just form of government.

James Madison critiqued direct democracy (which he referred to simply as "democracy") in Federalist No. 10, arguing that representative democracy—which he described using the term "republic"—is a preferable form of government, saying: "... democracies have ever been spectacles of turbulence and contention; have ever been found incompatible with personal security or the rights of property; and have in general been as short in their lives as they have been violent in their deaths." Madison offered that republics were superior to democracies because republics safeguarded against tyranny of the majority, stating in Federalist No. 10: "the same advantage which a republic has over a democracy, in controlling the effects of faction, is enjoyed by a large over a small republic".

More recently, democracy is criticised for not offering enough political stability. As governments are frequently elected on and off there tends to be frequent changes in the policies of democratic countries both domestically and internationally. Even if a political party maintains power, vociferous, headline grabbing protests and harsh criticism from the popular media are often enough to force sudden, unexpected political change. Frequent policy changes with regard to business and immigration are likely to deter investment and so hinder economic growth. For this reason, many people have put forward the idea that democracy is undesirable for a developing country in which economic growth and the reduction of poverty are top priorities.

This opportunist alliance not only has the handicap of having to cater to too many ideologically opposing factions, but it is usually short lived since any perceived or actual imbalance in the treatment of coalition partners, or changes to leadership in the coalition partners themselves, can very easily result in the coalition partner withdrawing its support from the government.

Biased media has been accused of causing political instability, resulting in the obstruction of democracy, rather than its promotion.

In representative democracies, it may not benefit incumbents to conduct fair elections. A study showed that incumbents who rig elections stay in office 2.5 times as long as those who permit fair elections. Democracies in countries with high per capita income have been found to be less prone to violence, but in countries with low incomes the tendency is the reverse. Election misconduct is more likely in countries with low per capita incomes, small populations, rich in natural resources, and a lack of institutional checks and balances. Sub-Saharan countries, as well as Afghanistan, all tend to fall into that category.

Governments that have frequent elections tend to have significantly more stable economic policies than those governments who have infrequent elections. However, this trend does not apply to governments where fraudulent elections are common.

Democracy in modern times has almost always faced opposition from the previously existing government, and many times it has faced opposition from social elites. The implementation of a democratic government within a non-democratic state is typically brought about by democratic revolution.

Post-Enlightenment ideologies such as fascism, nazism, communism and neo-fundamentalism oppose democracy on different grounds, generally citing that the concept of democracy as a constant process is flawed and detrimental to a preferable course of development.

Several philosophers and researchers have outlined historical and social factors seen as supporting the evolution of democracy.
"Cultural factors" like Protestantism influenced the development of democracy, rule of law, human rights and political liberty (the faithful elected priests, religious freedom and tolerance has been practiced).

Other commentators have mentioned the influence of "wealth" (e.g. S. M. Lipset, 1959). In a related theory, Ronald Inglehart suggests that improved living-standards can convince people that they can take their basic survival for granted, leading to increased emphasis on self-expression values, which is highly correlated to democracy.

Carroll Quigley concludes that the characteristics of weapons are the main predictor of democracy: Democracy tends to emerge only when the best weapons available are easy for individuals to buy and use. By the 1800s, guns were the best personal weapons available, and in America, almost everyone could afford to buy a gun, and could learn how to use it fairly easily. Governments couldn't do any better: it became the age of mass armies of citizen soldiers with guns Similarly, Periclean Greece was an age of the citizen soldier and democracy.

Recent theories stress the relevance of "education" and of "human capital" – and within them of "cognitive ability" to increasing tolerance, rationality, political literacy and participation. Two effects of education and cognitive ability are distinguished: a cognitive effect (competence to make rational choices, better information-processing) and an ethical effect (support of democratic values, freedom, human rights etc.), which itself depends on intelligence.

Evidence that is consistent with conventional theories of why democracy emerges and is sustained has been hard to come by. Recent statistical analyses have challenged modernisation theory by demonstrating that there is no reliable evidence for the claim that democracy is more likely to emerge when countries become wealthier, more educated, or less unequal. Neither is there convincing evidence that increased reliance on oil revenues prevents democratisation, despite a vast theoretical literature on "the Resource Curse" that asserts that oil revenues sever the link between citizen taxation and government accountability, seen as the key to representative democracy. The lack of evidence for these conventional theories of democratisation have led researchers to search for the "deep" determinants of contemporary political institutions, be they geographical or demographic. More inclusive institutions lead to democracy because as people gain more power, they are able to demand more from the elites, who in turn have to concede more things to keep their position. This virtuous circle, may end up in democracy.

An example of this is the disease environment. Places with different mortality rates had different populations and productivity levels around the world. For example, in Africa, the Tsetse fly which is harmful to humans and livestock reduced the ability of the Africans to plow the land. This made Africa less settled. As a consequence, political power was less concentrated. This also affected the colonial institutions that where set in place by the European countries in Africa. If the colonial settlers could live or not in a place made them develop different institutions which led to different economic and social paths. This also affected the distribution of power and the collective actions people could take. As a result, some African countries ended up having democracies and others autocracies. Another example of geographical determinants for democracy is having access to coastal areas and rivers. This natural endowment has a positive relation with economic development thanks to the benefits of trade. Trade brought economic development, which in turn, broaden the power. If the ruler wanted to increase his revenues, he had to protect property rights to create incentives for people to invest. As more people had more power, more concessions had to be made by the ruler and in many places this process lead to democracy. These determinants defined the structur of the society moving the balance of political power.

In the 21st century, democracy has become such a popular method of reaching decisions that its application beyond politics to other areas such as entertainment, food and fashion, consumerism, urban planning, education, art, literature, science and theology has been criticised as "the reigning dogma of our time". The argument suggests that applying a populist or market-driven approach to art and literature (for example), means that innovative creative work goes unpublished or unproduced. In education, the argument is that essential but more difficult studies are not undertaken. Science, as a truth-based discipline, is particularly corrupted by the idea that the correct conclusion can be arrived at by popular vote. However, more recently, theorists have also advanced the concept epistemic democracy to assert that democracy actually does a good job tracking the truth.

Robert Michels asserts that although democracy can never be fully realised, democracy may be developed automatically in the act of striving for democracy: "The peasant in the fable, when on his death-bed, tells his sons that a treasure is buried in the field. After the old man's death the sons dig everywhere in order to discover the treasure. They do not find it. But their indefatigable labor improves the soil and secures for them a comparative well-being. The treasure in the fable may well symbolise democracy."

Dr. Harald Wydra, in his book "Communism and The Emergence of Democracy" (2007), maintains that the development of democracy should not be viewed as a purely procedural or as a static concept but rather as an ongoing "process of meaning formation". Drawing on Claude Lefort's idea of the empty place of power, that "power emanates from the people [...] but is the power of nobody", he remarks that democracy is reverence to a symbolic mythical authority as in reality, there is no such thing as the people or "demos". Democratic political figures are not supreme rulers but rather temporary guardians of an empty place. Any claim to substance such as the collective good, the public interest or the will of the nation is subject to the competitive struggle and times of for gaining the authority of office and government. The essence of the democratic system is an empty place, void of real people which can only be temporarily filled and never be appropriated. The seat of power is there, but remains open to constant change. As such, what "democracy" is or what is "democratic" progresses throughout history as a continual and potentially never ending process of social construction.



</doc>
<doc id="7960" url="https://en.wikipedia.org/wiki?curid=7960" title="Deduction and induction">
Deduction and induction

Deduction and induction may refer to:


</doc>
<doc id="7962" url="https://en.wikipedia.org/wiki?curid=7962" title="Logical disjunction">
Logical disjunction

In logic and mathematics, or is the truth-functional operator of (inclusive) disjunction, also known as alternation; the "or" of a set of operands is true if and only if "one or more" of its operands is true. The logical connective that represents this operator is typically written as ∨ or +.

formula_1 is true if formula_2 is true, or if formula_3 is true, or if both formula_2 and formula_3 are true.

In logic, "or" by itself means the "inclusive" "or", distinguished from an exclusive or, which is false when both of its arguments are true, while an "or" is true in that case.

An operand of a disjunction is called a disjunct.

Related concepts in other fields are:

Or is usually expressed with an infix operator: in mathematics and logic, ∨; in electronics, +; and in most programming languages, |, ||, or or. In Jan Łukasiewicz's prefix notation for logic, the operator is A, for Polish "alternatywa" (English: alternative).

Logical disjunction is an operation on two logical values, typically the values of two propositions, that has a value of "false" if and only if both of its operands are false. More generally, a disjunction is a logical formula that can have one or more literals separated only by 'or's. A single literal is often considered to be a degenerate disjunction.

The disjunctive identity is false, which is to say that the "or" of an expression with false has the same value as the original expression. In keeping with the concept of vacuous truth, when disjunction is defined as an operator or function of arbitrary arity, the empty disjunction (OR-ing over an empty set of operands) is generally defined as false.

The truth table of formula_1:

The following properties apply to disjunction:




The mathematical symbol for logical disjunction varies in the literature. In addition to the word "or", and the formula "A"pq"", the symbol "formula_15", deriving from the Latin word "vel" (“either”, “or”) is commonly used for disjunction. For example: ""A" formula_15 "B" " is read as ""A" or "B" ". Such a disjunction is false if both "A" and "B" are false. In all other cases it is true.

All of the following are disjunctions:

The corresponding operation in set theory is the set-theoretic union.

Operators corresponding to logical disjunction exist in most programming languages.

Disjunction is often used for bitwise operations. Examples:

The codice_1 operator can be used to set bits in a bit field to 1, by codice_1-ing the field with a constant field with the relevant bits set to 1. For example, codice_3 will force the final bit to 1 while leaving other bits unchanged.

Many languages distinguish between bitwise and logical disjunction by providing two distinct operators; in languages following C, bitwise disjunction is performed with the single pipe (codice_4) and logical disjunction with the double pipe (codice_5) operators.

Logical disjunction is usually short-circuited; that is, if the first (left) operand evaluates to codice_6 then the second (right) operand is not evaluated. The logical disjunction operator thus usually constitutes a sequence point.
In a parallel (concurrent) language, it is possible to short-circuit both sides: they are evaluated in parallel,
and if one terminates with value true, the other is interrupted. This operator is thus called the parallel or.

Although in most languages the type of a logical disjunction expression is boolean and thus can only have the value codice_6 or codice_8, in some (such as Python and JavaScript) the logical disjunction operator returns one of its operands: the first operand if it evaluates to a true value, and the second operand otherwise.

The Curry–Howard correspondence relates a constructivist form of disjunction to tagged union types.

The membership of an element of a union set in set theory is defined in terms of a logical disjunction: "x" ∈ "A" ∪ "B" if and only if ("x" ∈ "A") ∨ ("x" ∈ "B"). Because of this, logical disjunction satisfies many of the same identities as set-theoretic union, such as associativity, commutativity, distributivity, and de Morgan's laws, identifying logical conjunction with set intersection, logical negation with set complement.

As with other notions formalized in mathematical logic, the meaning of the natural-language coordinating conjunction "or" is closely related to but different from the logical "or". For example, "Please ring me or send an email" likely means "do one or the other, but not both". On the other hand, "Her grades are so good that either she's very bright or she studies hard" does not exclude the possibility of both. In other words, in ordinary language "or" (even if used with "either") can mean either the inclusive "or" [inclusive-]or the exclusive "or."






</doc>
<doc id="7963" url="https://en.wikipedia.org/wiki?curid=7963" title="Disjunctive syllogism">
Disjunctive syllogism

In classical logic, disjunctive syllogism (historically known as modus tollendo ponens (MTP), Latin for "mode that affirms by denying") is a valid argument form which is a syllogism having a disjunctive statement for one of its premises.

An example in English:

In propositional logic, disjunctive syllogism (also known as disjunction elimination and or elimination, or abbreviated ∨E), is a valid rule of inference. If we are told that at least one of two statements is true; and also told that it is not the former that is true; we can infer that it has to be the latter that is true. If "P" is true or "Q" is true and "P" is false, then "Q" is true. The reason this is called "disjunctive syllogism" is that, first, it is a syllogism, a three-step argument, and second, it contains a logical disjunction, which simply means an "or" statement. "P or Q" is a disjunction; P and Q are called the statement's "disjuncts". The rule makes it possible to eliminate a disjunction from a logical proof. It is the rule that:

where the rule is that whenever instances of "formula_2", and "formula_3" appear on lines of a proof, "formula_4" can be placed on a subsequent line.

Disjunctive syllogism is closely related and similar to hypothetical syllogism, in that it is also type of syllogism, and also the name of a rule of inference. It is also related to the law of noncontradiction and the law of excluded middle, two of the .

The "disjunctive syllogism" rule may be written in sequent notation:

where formula_6 is a metalogical symbol meaning that formula_4 is a syntactic consequence of formula_2, and formula_9 in some logical system;

and expressed as a truth-functional tautology or theorem of propositional logic:

where formula_11, and formula_4 are propositions expressed in some formal system.

Here is an example:

Here is another example:

Please observe that the disjunctive syllogism works whether 'or' is considered 'exclusive' or 'inclusive' disjunction. See below for the definitions of these terms.

There are two kinds of logical disjunction:


The widely used English language concept of "or" is often ambiguous between these two meanings, but the difference is pivotal in evaluating disjunctive arguments.

This argument:

is valid and indifferent between both meanings. However, only in the "exclusive" meaning is the following form valid:

However, if the fact is true it does not commit the fallacy.

With the "inclusive" meaning you could draw no conclusion from the first two premises of that argument. See affirming a disjunct.

Unlike "modus ponendo ponens" and "modus ponendo tollens", with which it should not be confused, disjunctive syllogism is often not made an explicit rule or axiom of logical systems, as the above arguments can be proven with a (slightly devious) combination of reductio ad absurdum and disjunction elimination.

Other forms of syllogism include:

Disjunctive syllogism holds in classical propositional logic and intuitionistic logic, but not in some paraconsistent logics.



</doc>
<doc id="7964" url="https://en.wikipedia.org/wiki?curid=7964" title="Definition">
Definition

A definition is a statement of the meaning of a term (a word, phrase, or other set of symbols). Definitions can be classified into two large categories, intensional definitions (which try to give the essence of a term) and extensional definitions (which proceed by listing the objects that a term describes). Another important category of definitions is the class of ostensive definitions, which convey the meaning of a term by pointing out examples. A term may have many different senses and multiple meanings, and thus require multiple definitions.

In mathematics, a definition is used to give a precise meaning to a new term, instead of describing a pre-existing term. Definitions and axioms are the basis on which all of modern mathematics is constructed.

In modern usage, a "definition" is something, typically expressed in words, that attaches a meaning to a word or group of words. The word or group of words that is to be defined is called the "definiendum", and the word, group of words, or action that defines it is called the "definiens". In the definition ""An elephant is a large gray animal native to Asia and Africa"", the word "elephant" is the "definiendum", and everything after the word "is" is the "definiens".

The "definiens" is not "the meaning" of the word defined, but is instead something that "conveys the same meaning" as that word.

There are many sub-types of definitions, often specific to a given field of knowledge or study. These include, among many others, lexical definitions, or the common dictionary definitions of words already in a language; demonstrative definitions, which define something by pointing to an example of it (""This," [said while pointing to a large grey animal], "is an Asian elephant.""); and precising definitions, which reduce the vagueness of a word, typically in some special sense (""'Large', among female Asian elephants, is any individual weighing over 5,500 pounds."").

An "intensional definition", also called a "connotative" definition, specifies the necessary and sufficient conditions for a thing being a member of a specific set. Any definition that attempts to set out the essence of something, such as that by genus and differentia, is an intensional definition.

An "extensional definition", also called a "denotative" definition, of a concept or term specifies its "extension". It is a list naming every object that is a member of a specific set.

Thus, the "seven deadly sins" can be defined "intensionally" as those singled out by Pope Gregory I as particularly destructive of the life of grace and charity within a person, thus creating the threat of eternal damnation. An "extensional" definition would be the list of wrath, greed, sloth, pride, lust, envy, and gluttony. In contrast, while an intensional definition of "Prime Minister" might be "the most senior minister of a cabinet in the executive branch of government in a parliamentary system", an extensional definition is not possible since it is not known who future prime ministers will be.

A genus–differentia definition is a type of intensional definition that takes a large category (the genus) and narrows it down to a smaller category by a distinguishing characteristic (i.e. the differentia).

More formally, a genus-differentia definition consists of:

For example, consider the following genus-differentia definitions:

Those definitions can be expressed as a genus ("a plane figure") and two differentiae ("that has three straight bounding sides" and "that has four straight bounding sides", respectively).

It is possible to have two different genus-differentia definitions that describe the same term, especially when the term describes the overlap of two large categories. For instance, both of these genus-differentia definitions of "square" are equally acceptable:

Thus, a "square" is a member of both the genus "rectangle" and the genus "rhombus".

One important form of the extensional definition is "ostensive definition". This gives the meaning of a term by pointing, in the case of an individual, to the thing itself, or in the case of a class, to examples of the right kind. So one can explain who "Alice" (an individual) is by pointing her out to another; or what a "rabbit" (a class) is by pointing at several and expecting another to understand. The process of ostensive definition itself was critically appraised by Ludwig Wittgenstein.

An "enumerative definition" of a concept or term is an "extensional definition" that gives an explicit and exhaustive listing of all the objects that fall under the concept or term in question. Enumerative definitions are only possible for finite sets and only practical for relatively small sets.

"Divisio" and "partitio" are classical terms for definitions. A "partitio" is simply an intensional definition. A "divisio" is not an extensional definition, but an exhaustive list of subsets of a set, in the sense that every member of the "divided" set is a member of one of the subsets. An extreme form of "divisio" lists all sets whose only member is a member of the "divided" set. The difference between this and an extensional definition is that extensional definitions list "members", and not subsets.

In classical thought, a definition was taken to be a statement of the essence of a thing. Aristotle had it that an object's essential attributes form its "essential nature", and that a definition of the object must include these essential attributes.

The idea that a definition should state the essence of a thing led to the distinction between "nominal" and "real" essence, originating with Aristotle. In a passage from the Posterior Analytics, he says that the meaning of a made-up name can be known (he gives the example "goat stag"), without knowing what he calls the "essential nature" of the thing that the name would denote, if there were such a thing. This led medieval logicians to distinguish between what they called the "quid nominis" or "whatness of the name", and the underlying nature common to all the things it names, which they called the "quid rei" or "whatness of the thing". (Early modern philosophers like Locke used the corresponding English terms "nominal essence" and "real essence"). The name "hobbit", for example, is perfectly meaningful. It has a "quid nominis". But one could not know the real nature of hobbits, even if there were such things, and so the real nature or "quid rei" of hobbits cannot be known. By contrast, the name "man" denotes real things (men) that have a certain "quid rei". The meaning of a name is distinct from the nature that thing must have in order that the name apply to it.

This leads to a corresponding distinction between "nominal" and "real" definitions. A nominal definition is the definition explaining what a word means, i.e. which says what the "nominal essence" is, and is definition in the classical sense as given above. A real definition, by contrast, is one expressing the real nature or "quid rei" of the thing.

This preoccupation with essence dissipated in much of modern philosophy. Analytic philosophy in particular is critical of attempts to elucidate the essence of a thing. Russell described essence as "a hopelessly muddle-headed notion".

More recently Kripke's formalisation of possible world semantics in modal logic led to a new approach to essentialism. Insofar as the essential properties of a thing are "necessary" to it, they are those things it possesses in all possible worlds. Kripke refers to names used in this way as rigid designators.
A definition may also be classified as an operational definition or theoretical definition.

A homonym is, in the strict sense, one of a group of words that share the same spelling and pronunciation but have different meanings. Thus homonyms are simultaneously homographs (words that share the same spelling, regardless of their pronunciation) "and" homophones (words that share the same pronunciation, regardless of their spelling). The state of being a homonym is called homonymy. Examples of homonyms are the pair "stalk" (part of a plant) and "stalk" (follow/harass a person) and the pair "left" (past tense of leave) and "left" (opposite of right). A distinction is sometimes made between "true" homonyms, which are unrelated in origin, such as "skate" (glide on ice) and "skate" (the fish), and polysemous homonyms, or polysemes, which have a shared origin, such as "mouth" (of a river) and "mouth" (of an animal).

Polysemy is the capacity for a sign (such as a word, phrase, or symbol) to have multiple meanings (that is, multiple semes or sememes and thus multiple senses), usually related by contiguity of meaning within a semantic field. It is thus usually regarded as distinct from homonymy, in which the multiple meanings of a word may be unconnected or unrelated.

In mathematics, definitions are generally not used to describe existing terms, but to give meaning to a new term. The meaning of a mathematical statement changes if definitions change. The precise meaning of a term given by a mathematical definition is often different than the English definition of the word used, which can lead to confusion for students who do not pay close attention to the definitions given.

Authors have used different terms to classify definitions used in formal languages like mathematics. Norman Swartz classifies a definition as "stipulative" if it is intended to guide a specific discussion. A stipulative definition might be considered a temporary, working definition, and can only be disproved by showing a logical contradiction. In contrast, a "descriptive" definition can be shown to be "right" or "wrong" with reference to general usage.

Swartz defines a "precising definition" as one that extends the descriptive dictionary definition (lexical definition) for a specific purpose by including additional criteria. A precising definition narrows the set of things that meet the definition.

C.L. Stevenson has identified "persuasive definition" as a form of stipulative definition which purports to state the "true" or "commonly accepted" meaning of a term, while in reality stipulating an altered use (perhaps as an argument for some specific belief). Stevenson has also noted that some definitions are "legal" or "coercive" – their object is to create or alter rights, duties, or crimes.

A recursive definition, sometimes also called an "inductive" definition, is one that defines a word in terms of itself, so to speak, albeit in a useful way. Normally this consists of three steps:

For instance, we could define a natural number as follows (after Peano): 

So "0" will have exactly one successor, which for convenience can be called "1". In turn, "1" will have exactly one successor, which could be called "2", and so on. Notice that the second condition in the definition itself refers to natural numbers, and hence involves self-reference. Although this sort of definition involves a form of circularity, it is not vicious, and the definition has been quite successful.

In the same way, we can define ancestor as follows:
Or simply: an ancestor is a parent or a parent of an ancestor.

In medical dictionaries, definitions should to the greatest extent possible be:

Certain rules have traditionally been given for definitions (in particular, genus-differentia definitions).

Given that a natural language such as English contains, at any given time, a finite number of words, any comprehensive list of definitions must either be circular or rely upon primitive notions. If every term of every "definiens" must itself be defined, "where at last should we stop?" A dictionary, for instance, insofar as it is a comprehensive list of lexical definitions, must resort to circularity.

Many philosophers have chosen instead to leave some terms undefined. The scholastic philosophers claimed that the highest genera (the so-called ten "generalissima") cannot be defined, since a higher genus cannot be assigned under which they may fall. Thus being, unity and similar concepts cannot be defined. Locke supposes in "An Essay Concerning Human Understanding" that the names of simple concepts do not admit of any definition. More recently Bertrand Russell sought to develop a formal language based on logical atoms. Other philosophers, notably Wittgenstein, rejected the need for any undefined simples. Wittgenstein pointed out in his "Philosophical Investigations" that what counts as a "simple" in one circumstance might not do so in another. He rejected the very idea that every explanation of the meaning of a term needed itself to be explained: "As though an explanation hung in the air unless supported by another one", claiming instead that explanation of a term is only needed to avoid misunderstanding.

Locke and Mill also argued that individuals cannot be defined. Names are learned by connecting an idea with a sound, so that speaker and hearer have the same idea when the same word is used. This is not possible when no one else is acquainted with the particular thing that has "fallen under our notice". Russell offered his theory of descriptions in part as a way of defining a proper name, the definition being given by a definite description that "picks out" exactly one individual. Saul Kripke pointed to difficulties with this approach, especially in relation to modality, in his book "Naming and Necessity".

There is a presumption in the classic example of a definition that the "definiens" can be stated. Wittgenstein argued that for some terms this is not the case. The examples he used include "game", "number" and "family". In such cases, he argued, there is no fixed boundary that can be used to provide a definition. Rather, the items are grouped together because of a family resemblance. For terms such as these it is not possible and indeed not necessary to state a definition; rather, one simply comes to understand the "use" of the term.





</doc>
<doc id="7965" url="https://en.wikipedia.org/wiki?curid=7965" title="Disruption">
Disruption

Disruption or Disruptive may refer to:




</doc>
<doc id="7966" url="https://en.wikipedia.org/wiki?curid=7966" title="Disco">
Disco

Disco is a music genre and subculture that emerged the mid 1960s and early 1970s from America's urban nightlife scene. It reached peak popularity between the mid-1970s and early 1980s. Disco started as a mélange of music from venues popular with African Americans, Hispanic and Latino Americans, Italian Americans, LGBT people (especially African-American and white gay men), and psychedelic hippies in Philadelphia and New York City during the late 1960s and early 1970s. Disco can be seen as a reaction to both the dominance of rock music and the stigmatization of dance music by the counterculture during this period. Several dance styles were also developed during this time, including the Bump and the Hustle.

The disco sound is typified by "four-on-the-floor" beats, syncopated basslines, and string sections, horns, electric piano, synthesizers, and electric rhythm guitars. Lead guitar features less frequently in disco than in rock. Well-known disco artists include Donna Summer, the Bee Gees, Gloria Gaynor, KC and the Sunshine Band, the Village People, Thelma Houston, and Chic, and many at the height of the genre's popularity, many non-disco artists recorded disco songs. While performers and singers garnered public attention, record producers working behind the scenes played an important role in developing the genre. Films such as "Saturday Night Fever" (1977) and "Thank God It's Friday" (1978) contributed to disco's mainstream popularity.

By the late 1970s, most major U.S. cities had thriving disco club scenes, and DJs would mix dance records at clubs such as Studio 54 in New York City, a venue popular among celebrities. Discothèque-goers often wore expensive, extravagant and sexy fashions. There was also a thriving drug subculture in the disco scene, particularly for drugs that would enhance the experience of dancing to the loud music and the flashing lights, such as cocaine and Quaaludes, the latter being so common in disco subculture that they were nicknamed "disco biscuits". Disco clubs were also associated with promiscuity.

Disco was the last popular music movement driven by the baby boom generation. It drastically declined in the United States in 1980, and by 1982 it had lost most of its popularity there. Disco Demolition Night, an anti-disco protest held in Chicago on July 12, 1979, remains the most well-known of several "backlash" incidents across the country that symbolized disco's declining fortune.

Disco was a key influence in the development of electronic dance music and house music. It has had several revivals, such as Madonna's highly successful 2005 album "Confessions on a Dance Floor", and again in the 2010s, entering the pop charts in the US and the UK.

The term disco is derived from "discothèque" (French for "library of phonograph records", but it was subsequently used as a term for nightclubs in Paris). By the early 1940s, the terms "disc jockey" and "DJ" were in use to describe radio presenters. During WWII, because of restrictions set in place by the Nazi occupiers, jazz dance halls in Occupied France played records instead of using live music. Eventually more than one of these jazz venues had the proper name "discothèque". By 1959, the term was used in Paris to describe any of these type of nightclubs. That year, a young reporter named Klaus Quirini started to select and introduce records at the Scotch-Club in Aachen, West Germany. By the following year the term was being used in the United States to describe that type of club, and a type of dancing in those clubs. By 1964, "discothèque" and the shorthand "disco" were used to describe a type of sleeveless dress worn when going out to nightclubs. In September 1964, "Playboy" magazine used the word "disco" as a shorthand for a discothèque-styled nightclub.

In 1974 there were an estimated 25,000 mobile discos and 40,000 professional disc jockeys in the United Kingdom. Mobile Discos referred to Disc Jockeys for hire that brought their own equipment to office parties, weddings and the like. "Disco Dance Music" referred to glam rock. Simon Reynolds has described Gary Glitter's Rock and Roll Part 2 as the first hybrid disco-rock song.

In Philadelphia, R&B musicians and audiences from the Black, Italian, and Latino communities adopted several traits from the hippie and psychedelia subcultures. They included using music venues with a loud, overwhelming sound, free-form dancing, trippy lighting, colorful costumes, and the use of hallucinogenic drugs. Psychedelic soul groups like the Chambers Brothers and especially Sly and the Family Stone influenced proto-disco acts such as Isaac Hayes, Willie Hutch and the soul style known as the Philadelphia Sound. In addition, the perceived positivity, lack of irony, and earnestness of the hippies informed proto-disco music like MFSB's album "Love Is the Message". To the mainstream public M.F.S.B. stood for "Mother Father Sister Brother"; to the tough areas where they came from it was understood to stand for "Mother Fuckin' Son of a Bitch", a reference to their playing skill and musical prowess.

A forerunner to disco-style clubs was the private dance parties held by New York City DJ David Mancuso in The Loft, a members-only club in his home in 1970. When Mancuso threw his first house parties, the gay community (members of whom comprised much of The Loft's attendee roster) was often harassed by police in New York gay bars and dance clubs. But at The Loft and many other early, private discotheques, men could dance together without fear of police action thanks to Mancuso's underground business model. The first article about disco was written in 1973 by Vince Aletti for "Rolling Stone" magazine. In 1974, New York City's WPIX-FM premiered the first disco radio show.

Philadelphia soul and New York soul were evolutions of the Motown sound, and were typified by the lavish percussion, lush string orchestra arrangements and expensive record production processes that became a prominent part of mid-1970s disco songs. Early songs with disco elements include "You Keep Me Hangin' On" (the Supremes, 1966), "Soul Makossa" (Manu Dibango, 1972), "Superstition" by Stevie Wonder (1972) Eddie Kendricks' "Keep on Truckin'" (1973) and "The Love I Lost" by Harold Melvin and the Blue Notes (1973). "Love Train" by the O'Jays (1972), with M.F.S.B. as the backup band, topped the "Billboard" pop chart in March 1973, and has been called "disco".

In 1972, The Four Seasons released ""The Night"", a song with disco elements which appealed to the Northern Soul scene .
Early disco was dominated by record producers and labels such as Salsoul Records (Ken, Stanley, and Joseph Cayre), West End Records (Mel Cheren), Casablanca (Neil Bogart), and Prelude (Marvin Schlachter), to name a few. The genre was also shaped by Tom Moulton, who wanted to extend the enjoyment of dance songs—thus creating the extended mix or "remix", going from a three-minute 45 rpm single to the much longer 12" record. Other influential DJs and remixers who helped to establish what became known as the "disco sound" included David Mancuso, Nicky Siano, Shep Pettibone, Larry Levan, Walter Gibbons, and Chicago-based Frankie Knuckles. Frankie Knuckles was not only an important disco DJ; he also helped to develop house music in the 1980s, a contribution which earned him the honorific title of "Godfather of House".

"The [disco] DJ was central to the ritual of 1970s dance culture, but the dancing crowd was no less important, and it was the combination of these two elements that created the conditions for the dance floor dynamic." In disco parties and clubs, a "...good DJ didn't only lead dancers...[to the dance floor,] but would also feel the mood of the dance floor and select records according to this energy (which could be communicated by the vigor of the dancing, or level of the crowd's screams, or sign language of dancers directed towards the booth)." Disco-era DJs would often remix (re-edit) existing songs using reel-to-reel tape machines, and add in percussion breaks, new sections, and new sounds. DJs would select songs and grooves according to what the dancers wanted, transitioning from one song to another with a DJ mixer and using a microphone to introduce songs and speak to the audiences. Other equipment was added to the basic DJ setup, providing unique sound manipulations, such as reverb, equalization, and echo effects unit. Using this equipment, a DJ could do effects such as cutting out all but the throbbing bassline of a song, and then slowly mixing in the beginning of another song using the DJ mixer's crossfader.

Disco hit the television airwaves with the music/dance variety show "Soul Train" in 1971 hosted by Don Cornelius, then Marty Angelo's "Disco Step-by-Step Television Show" in 1975, Steve Marcus' "Disco Magic/Disco 77", Eddie Rivera's "Soap Factory", and Merv Griffin's "Dance Fever", hosted by Deney Terrio, who is credited with teaching actor John Travolta to dance for his role in the film "Saturday Night Fever", as well as DANCE, based out of Columbia, South Carolina.

From 1974 to 1977, disco music continued to increase in popularity as many disco songs topped the charts. In 1974, "Love's Theme" by Barry White's Love Unlimited Orchestra became the second disco song to reach number one on the "Billboard" Hot 100, after "Love Train". MFSB also released "TSOP (The Sound of Philadelphia)", featuring vocals by the Three Degrees, and this was the third disco song to hit number one; "TSOP" was written as the theme song for "Soul Train". 

The Hues Corporation's 1974 "Rock the Boat", a US number-one single and million-seller, was one of the early disco songs to reach number one. The same year saw the release of "Kung Fu Fighting", performed by Carl Douglas and produced by Biddu, which reached number one in both the UK and US, and became the best-selling single of the year and one of the best-selling singles of all time with eleven million records sold worldwide, helping to popularize disco music to a great extent. Another notable chart-topping disco success that year was George McCrae's "Rock Your Baby".
In the northwestern sections of the United Kingdom, the Northern Soul explosion, which started in the late 1960s and peaked in 1974, made the region receptive to Disco, which the region's Disc Jockeys were bringing back from New York City. George McCrae's "Rock Your Baby" became the United Kingdom's first number one disco single.

Also in 1974, Gloria Gaynor released the first side-long disco mix vinyl album, which included a remake of the Jackson 5's "Never Can Say Goodbye" and two other songs, "Honey Bee" and her disco version of "Reach Out (I'll Be There)", first topped the Billboard disco/dance charts in November 1974. Gaynor's number-one disco song was "I Will Survive", released in 1978, which was seen as a symbol of female strength and a gay anthem. Also in the mix, Vincent Montana's Salsoul Orchestra contributed with their 1975 Latin-flavored orchestral dance song "Salsoul Hustle", reaching number four on the Billboard Dance Chart and the 1976 songs "Tangerine" and "Nice 'n' Naasty".

Formed by Harry Wayne Casey ("KC") and Richard Finch, Miami's KC and the Sunshine Band had a string of disco-definitive top-five singles between 1975 and 1977, including "Get Down Tonight", "That's the Way (I Like It)", "(Shake, Shake, Shake) Shake Your Booty", "I'm Your Boogie Man" and "Keep It Comin' Love". Electric Light Orchestra's 1975 song "Evil Woman", although described as Orchestral Rock, featured a violin sound that became a staple of disco. In 1979, however, ELO did release two "true" disco songs: "Last Train To London" and "Shine A Little Love".

In 1975, American singer and songwriter Donna Summer recorded a song which she brought to her producer Giorgio Moroder entitled "Love to Love You Baby" which contained a series of simulated orgasms. The song was never intended for release but when Moroder played it in the clubs it caused a sensation. Moroder released it and it went to number 2. It has been described as the arrival of the expression of raw female sexual desire in pop music. A 17-minute 12 inch single was released. The 12" single became and remains a standard in discos today.

In 1977 Summer released "I Feel Love", which combined disco with its subgenre Hi-NRG and electronic music, was a massive worldwide success. In 1978, her multi-million selling vinyl single disco version of "MacArthur Park" was number one on the "Billboard" Hot 100 chart for three weeks and was nominated for the Grammy Award for Best Female Pop Vocal Performance. Summer's recording, which was included as part of the "MacArthur Park Suite" on her double album Live and More, was eight minutes and forty seconds long on the album. The shorter seven-inch vinyl single version of the MacArthur Park was Summer's first single to reach number one on the Hot 100; it does not include the balladic second movement of the song, however. A 2013 remix of "Mac Arthur Park" by Summer topped the Billboard Dance Charts marking five consecutive decades with a number-one song on the charts. From 1978 to 1979, Summer continued to release singles such as "Last Dance", "Bad Girls", "Heaven Knows", "No More Tears (Enough Is Enough)", "Hot Stuff", "Dim All the Lights", and "On the Radio", all very successful songs, landing in the top five or better, on the Billboard pop charts.

The Bee Gees used Barry Gibb's falsetto to garner hits such as "You Should Be Dancing", "Stayin' Alive", "Night Fever", "More Than A Woman" and "Love You Inside Out". Andy Gibb, a younger brother to the Bee Gees, followed with similarly-styled solo singles such as "I Just Want to Be Your Everything", "(Love Is) Thicker Than Water" and "Shadow Dancing". In 1975, songs such as Van McCoy's "The Hustle" and Summer's version of "Could It Be Magic" brought disco further into the mainstream. Other notable early successful disco songs include The Hues Corporation's "Rock the Boat" (1974), Barry White's "You're the First, the Last, My Everything" (1974), Labelle's "Lady Marmalade" (1975), Silver Convention's "Fly Robin Fly" (1975) and Johnny Taylor's Disco Lady (1976).

In December 1977, the film "Saturday Night Fever" was released. It was a huge success and its soundtrack became one of the best-selling albums of all time. The idea for the film was sparked by a 1976 "New York" magazine article titled "Tribal Rites of the New Saturday Night" which supposedly chronicled the disco culture in mid-1970s New York City, but was later revealed to have been fabricated. Some critics said the film "mainstreamed" disco, making it more acceptable to heterosexual white males.

Chic was formed mainly by guitarist Nile Rodgers—a self-described "street hippie" from late 1960s New York—and bassist Bernard Edwards. "Le Freak" was a popular 1978 single of theirs that is regarded as an iconic song of the genre. Other successful songs by Chic include the often-sampled "Good Times" (1979) and "Everybody Dance" (1979). The group regarded themselves as the disco movement's rock band that made good on the hippie movement's ideals of peace, love, and freedom. Every song they wrote was written with an eye toward giving it "deep hidden meaning" or D.H.M.

Sylvester, a flamboyant and openly gay singer famous for his soaring falsetto voice, scored his biggest disco song in 1979 – "You Make Me Feel (Mighty Real)", and "Dance (Disco Heat)", his singing style was said to have influenced the singer Prince. At that time, disco was one of the forms of music most open to gay performers.

The Village People were a singing/dancing group created by Jacques Morali and Henri Belolo to target disco's gay audience. They were known for their onstage costumes of typically male-considered jobs and ethnic minorities and achieved mainstream success with their 1978 hit song "Macho Man". Other songs include "Y.M.C.A." (1979) and "In the Navy" (1979).

The Jacksons (previously "the Jackson 5") did many disco songs from 1975 to 1980, including "Shake Your Body (Down to the Ground)" (1978), "Blame it on the Boogie" (1978), "Lovely One" (1980), and "Can You Feel It" (1980)—all sung by Michael Jackson, whose 1979 solo album, "Off the Wall", included several disco hits, including the album's title song, "Rock with You", "Workin' Day and Night", and his second chart-topping solo hit in the disco genre, "Don't Stop 'Til You Get Enough".

At the height of its popularity, many non-disco artists recorded songs with disco elements, including Chaka Khan's "I'm Every Woman" (1978), Earth, Wind & Fire's "September" (1978) and "Boogie Wonderland" with the Emotions (1979), Blondie's "Heart of Glass" (1979), Cher's "Take Me Home" (1979), Barry Manilow's "Copacabana" (1978), David Bowie's "John I'm Only Dancing (Again)" (1979), Rod Stewart's "Da Ya Think I'm Sexy?" (1979), Frankie Valli's "Swearin' to God" (1975) and "Grease" (1978), George Benson's "Give Me the Night" (1980), Diana Ross's "Love Hangover" (1976), M's "Pop Muzik" (1979), Barbra Streisand's "The Main Event" (1979), Dan Hartman's "Instant Replay" (1978). The biggest hit by Ian Dury and the Blockheads, best known as a new wave band, was "Hit Me with Your Rhythm Stick" (1978), featuring a strong disco sound.

Even mainstream rock artists adopted elements of disco. Progressive rock group Pink Floyd used disco-like drums and guitar in their song "Another Brick in the Wall, Part 2" (1979), which became their only number-one single in both the US and UK. The Eagles referenced disco with "One of These Nights" (1975) and "Disco Strangler" (1979), Paul McCartney & Wings with "Goodnight Tonight" (1979), Queen with "Another One Bites the Dust" (1980), the Rolling Stones with "Miss You" (1978) and "Emotional Rescue" (1980), Electric Light Orchestra's "Shine a Little Love" and "Last Train to London" (both 1979), Chicago did "Street Player" (1979), the Kinks did "(Wish I Could Fly Like) Superman" (1979), the Grateful Dead did "Shakedown Street", and the J. Geils Band did "Come Back" (1980). Even hard rock group KISS jumped in with "I Was Made For Lovin' You" (1979). Ringo Starr's album "Ringo the 4th" (1978) features a strong disco influence.

Also noteworthy are John Paul Young's "Love Is in the Air" (1977), Patrick Hernandez's "Born to Be Alive" (1978), Cheryl Lynn's "Got to Be Real" (1978), Evelyn "Champagne" King's "Shame" (1978), Sister Sledge's "We Are Family" (1979), Anita Ward's "Ring My Bell" (1979), Lipps Inc.'s "Funkytown" (1979), Geraldine Hunt's "Can't Fake the Feeling" (1980), Alicia Bridges' "I Love the Nightlife" (1978) and Walter Murphy's various attempts to bring classical music to the mainstream, most notably his disco song "A Fifth of Beethoven" (1976), which was inspired by Beethoven's fifth symphony.
Pre-existing non-disco songs and standards would frequently be "disco-ized" in the 1970s. The rich orchestral accompaniment that became identified with the disco era conjured up the memories of the big band era—which brought out several artists that recorded and disco-ized some big band arrangements including Perry Como, who re-recorded his 1945 song "Temptation", in 1975, as well as Ethel Merman, who released an album of disco songs entitled "The Ethel Merman Disco Album" in 1979.

Myron Floren, second-in-command on "The Lawrence Welk Show", released a recording of the "Clarinet Polka" entitled "Disco Accordion." Similarly, Bobby Vinton adapted "The Pennsylvania Polka" into a song named "Disco Polka". Easy listening icon Percy Faith, in one of his last recordings, released an album entitled "Disco Party" (1975) and recorded a disco version of his famous "Theme from A Summer Place" in 1976. Classical music was even adapted for disco, notably Walter Murphy's "A Fifth of Beethoven" (1976, based on the first movement of Beethoven's 5th Symphony) and "Flight 76" (1976, based on Rimsky-Korsakov's "Flight of the Bumblebee"), and Louis Clark's "Hooked On Classics" series of albums and singles.

Notable disco songs based on film and television themes included a medley from "Star Wars", "Star Wars Theme/Cantina Band" (1977) by Meco, and "Twilight Zone/Twilight Tone" (1979) by the Manhattan Transfer. Even the "I Love Lucy" theme was not spared from being disco-ized. Many original television theme songs of the era also showed a strong disco influence, such as "Keep Your Eye On the Sparrow" (theme from "Baretta", performed by Sammy Davis, Jr. and later a successful single for Rhythm Heritage), "Theme from "S.W.A.T."" (from "S.W.A.T", original and single versions by Rhythm Heritage), and Mike Post's "Theme from "Magnum, P.I."".

Several parodies of the disco style were created. Rick Dees, at the time a radio DJ in Memphis, Tennessee, recorded "Disco Duck" (1976) and "Dis-Gorilla" (1977); Frank Zappa parodied the lifestyles of disco dancers in "Disco Boy" on his 1976 "Zoot Allures" album, and in "Dancin' Fool" on his 1979 "Sheik Yerbouti" album; "Weird Al" Yankovic's eponymous 1983 debut album includes a disco song called "Gotta Boogie", an extended pun on the similarity of the disco move to the American slang word "booger". Comedian Bill Cosby devoted his entire 1977 album "Disco Bill" to disco parodies.

By the mid-1970s, the United States was dealing with the aftermath of the Vietnam War and the Watergate scandal. Also by this time the economic prosperity of the previous decade had declined, and unemployment, inflation and crime rates had soared. Disco music and disco dancing provided an escape from these negative economic issues. As well, in the 1970s, the key counterculture of the 1960s, the hippie movement, was fading away. The disco movement was far more than just music. It was also a subculture based around nightclubs, dance clubs, and DJs. In "Beautiful Things in Popular Culture", Simon Frith highlights the sociability of disco and its roots in 1960s counterculture. "The driving force of the New York underground dance scene in which disco was forged was not simply that city's complex ethnic and sexual culture but also a 1960s notion of community, pleasure and generosity that can only be described as hippie," he says. "The best disco music contained within it a remarkably powerful sense of collective euphoria."

Film critic Roger Ebert called the popular embrace of disco's exuberant dance moves an escape from "the general depression and drabness of the political and musical atmosphere of the late seventies." Pauline Kael, writing about the disco-themed film "Saturday Night Fever", said the film and disco itself touched on "something deeply romantic, the need to move, to dance, and the need to be who you'd like to be. Nirvana is the dance; when the music stops, you return to being ordinary."

By the end of the 1970s, a strong anti-disco sentiment developed among rock fans and musicians, particularly in the United States. Disco was criticized as mindless, consumeristic, overproduced and escapist. The slogans "disco sucks" and "death to disco" became common. Rock artists such as Rod Stewart and David Bowie who added disco elements to their music were accused of being sell outs.

The punk subculture in the United States and United Kingdom was often hostile to disco (although in the UK, many early Sex Pistols fans such as the Bromley Contingent and Jordan quite liked disco, often congregating at nightclubs such as Louise's in Soho and the Sombrero in Kensington. The track "Love Hangover" by Diana Ross, the house anthem at the former, was cited as a particular favourite by many early UK Punks. Also, the film "The Great Rock 'n' Roll Swindle" and its soundtrack album contained a disco medley of Sex Pistols songs, entitled "Black Arabs" and credited to a group of the same name.) Jello Biafra of the Dead Kennedys, in the song "Saturday Night Holocaust", likened disco to the cabaret culture of Weimar-era Germany for its apathy towards government policies and its escapism. Mark Mothersbaugh of Devo said that disco was "like a beautiful woman with a great body and no brains", and a product of political apathy of that era. New Jersey rock critic Jim Testa wrote "Put a Bullet Through the Jukebox", a vitriolic screed attacking disco that was considered a punk call to arms. Steve Hillage, shortly prior to his transformation from a progressive rock musician into an electronic artist at the end of the 1970s with the inspiration of disco, disappointed his rockist fans by admitting his love for disco, with Hillage recalling "it's like I'd killed their pet cat."

Anti-disco sentiment was expressed in some television shows and films. A recurring theme on the show "WKRP in Cincinnati" was a hostile attitude towards disco music. In one scene of the 1980 comedy film "Airplane!", a wayward airplane slices a radio tower with its wing, takes out an all-disco radio station.

July 12, 1979, became known as "the day disco died" because of Disco Demolition Night, an anti-disco demonstration in a baseball double-header at Comiskey Park in Chicago. Rock-station DJs Steve Dahl and Garry Meier, along with Michael Veeck, son of Chicago White Sox owner Bill Veeck, staged the promotional event for disgruntled rock fans between the games of a White Sox doubleheader. The event, which involved exploding disco records, ended with a riot, during which the raucous crowd tore out seats and pieces of turf, and caused other damage. The Chicago Police Department made numerous arrests, and the extensive damage to the field forced the White Sox to forfeit the second game to the Detroit Tigers, who had won the first game. 

Six months prior to the chaotic event (in December 1978), popular progressive rock radio station WDAI (WLS-FM) had suddenly switched to an all-disco format, disenfranchising thousands of Chicago rock fans and leaving Dahl unemployed. WDAI, who despite surviving the backlash and still had good ratings at this point, continued to play Disco until it flipped to a short-lived hybrid Top 40/Rock format in May 1980. Another Disco outlet that also competed against WDAI at the time, WGCI-FM, would later incorporate R&B and Pop Songs into the format, eventually evolving into an Urban Contemporary outlet that it continues with today. The latter also helped bring the house music genre to the airwaves, ending the backlash somewhat with Chicago emerging as the birthplace of house.

On July 21, 1979, the top six records on the U.S. music charts were disco songs. By September 22, there were no disco songs in the US Top 10 chart, with the exception of Herb Alpert's instrumental "Rise," a smooth jazz composition with some disco overtones. Some in the media, in celebratory tones, declared disco "dead" and rock revived. Karen Mixon Cook, the first female disco DJ, stated that people still pause every July 12 for a moment of silence in honor of Disco. Dahl stated in a 2004 interview that disco was "probably on its way out. But I think it [Disco Demolition Night] hastened its demise".

The anti-disco backlash, combined with other societal and radio industry factors, changed the face of pop radio in the years following Disco Demolition Night. Starting in the 1980s, country music began a slow rise in American main pop charts. Emblematic of country music's rise to mainstream popularity was the commercially successful 1980 movie "Urban Cowboy". The continued popularity of power pop and the revival of oldies in the late 1970s was also related to the disco backlash; the 1978 film "Grease" was emblematic of this trend. Somewhat ironically, the star of both films was John Travolta, who in 1977 had starred in "Saturday Night Fever", which remains one of the most iconic disco films of the era.

During this period of decline in disco's popularity, several record companies folded, were reorganized, or were sold. In 1979, MCA Records purchased ABC Records, absorbed some of its artists, and then shut the label down. Midsong International Records ceased operations in 1980. RSO Records founder Robert Stigwood left the label in 1981 and TK Records closed in the same year. Salsoul Records continues to exist in the 2000s, but primarily is used as a reissue brand. Casablanca Records had been releasing fewer records in the 1980s, and was shut down in 1986 by parent company PolyGram.

Many groups that were popular during the disco period subsequently struggled to maintain their success—even those that tried to adapt to evolving musical tastes. The Bee Gees, for instance, had only one top-10 entry (1989's "One") and three more top-40 songs (despite recording and releasing far more than that and completely abandoning disco in their 1980s and 1990s songs) in the United States after the 1970s, even though numerous songs they wrote and had "other" artists perform were successful. Of the handful of groups "not" taken down by disco's fall from favor, Kool and the Gang, Donna Summer, the Jacksons—and Michael Jackson in particular—stand out: In spite of having helped define the disco sound early on, they continued to make popular and danceable, if more refined, songs for yet another generation of music fans in the 1980s and beyond. Earth, Wind & Fire also survived the disco backlash and continued to produce successful singles at roughly the same pace for several more years, in addition to an even longer string of R&B chart hits that lasted into the 1990s.

Factors that have been cited as leading to the decline of disco in the United States include economic and political changes at the end of the 1970s, as well as burnout from the hedonistic lifestyles led by participants. In the years since Disco Demolition Night, some social critics have described the backlash as implicitly macho and bigoted, and an attack on non-white and non-heterosexual cultures. The backlash also made its way into US politics with the election of conservative Ronald Reagan in 1980 which also led to Republican control of the United States Senate for the first time since 1954, plus the subsequent rise of the Religious Right around the same time.

In January 1979, rock critic Robert Christgau argued that homophobia, and most likely racism, were reasons behind the backlash, a conclusion seconded by John Rockwell. Craig Werner wrote: "The Anti-disco movement represented an unholy alliance of funkateers and feminists, progressives and puritans, rockers and reactionaries. Nonetheless, the attacks on disco gave respectable voice to the ugliest kinds of unacknowledged racism, sexism and homophobia." Legs McNeil, founder of the fanzine "Punk", was quoted in an interview as saying, "the hippies always wanted to be black. We were going, 'f**k the blues, f**k the black experience'." He also said that disco was the result of an "unholy" union between homosexuals and blacks.

Steve Dahl, who had spearheaded Disco Demolition Night, denied any racist or homophobic undertones to the promotion, saying, "It's really easy to look at it historically, from this perspective, and attach all those things to it. But we weren't thinking like that." It has been noted that British punk rock critics of disco were very supportive of the pro-black/anti-racist reggae genre as well as the more pro-gay new romantics movement. Christgau and Jim Testa have said that there were legitimate artistic reasons for being critical of disco.

In 1979, the music industry in the United States underwent its worst slump in decades, and disco, despite its mass popularity, was blamed. The producer-oriented sound was having difficulty mixing well with the industry's artist-oriented marketing system. Harold Childs, senior vice president at A&M Records, told the "Los Angeles Times" that "radio is really desperate for rock product" and "they're all looking for some white rock-n-roll". Gloria Gaynor argued that the music industry supported the destruction of disco because rock music producers were losing money and rock musicians were losing the spotlight. However, disco music remained relatively successful in the early 1980s, with songs like Irene Cara's "Flashdance... What a Feeling", Michael Jackson's "Thriller", K.C. and the Sunshine Band's last major single, "Give It Up", and Madonna's first album had strong disco influences. Record producer Giorgio Moroder's soundtracks to "American Gigolo", "Flashdance" and "Scarface" (which also had a heavy disco influence) proved that the style was still very much embraced. Queen's 1982 album, "Hot Space" was inspired by the genre as well.

In the 1990s, disco and its legacy became more accepted by music artists and listeners alike, as more songs and films were released that referenced disco. Examples of songs during this time that were influenced by disco included Deee-Lite's "Groove Is in the Heart" (1990), U2's "Lemon" (1993), Blur's "Girls & Boys" (1994) & "Entertain Me" (1995), Pulp's "Disco 2000" (1995), and Jamiroquai's "Canned Heat" (1999), while films such as "Boogie Nights" (1997) and "The Last Days of Disco" (1998) featured primarily disco soundtracks.

In the early 2000s, an updated genre of disco called "nu-disco" began breaking into the mainstream. A few examples like Daft Punk's "One More Time" and Kylie Minogue's "Love At First Sight" became club favorites and commercial successes. Several nu-disco songs were crossovers with funky house, such as Spiller's "Groovejet (If This Ain't Love)" and Modjo's "Lady (Hear Me Tonight)", both songs sampling older disco songs and both reaching number one on the UK Singles Chart in 2000. Robbie Williams' disco single "Rock DJ" was the UK's fourth best-selling single the same year. Rock band Manic Street Preachers released a disco song, "Miss Europa Disco Dancer", in 2001. The song's disco influence, which appears on "Know Your Enemy", was described as being "much-discussed". In 2005, Madonna immersed herself in the disco music of the 1970s, and released her album "Confessions on a Dance Floor" to rave reviews. In addition to that, her song "Hung Up" became a major top-10 song and club staple, and sampled ABBA's 1970s song "Gimme! Gimme! Gimme! (A Man After Midnight)". In addition to her disco-influenced attire to award shows and interviews, her Confessions Tour also incorporated various elements of the 1970s, such as disco balls, a mirrored stage design, and the roller derby.

The success of the "nu-disco" revival of the early 2000s was described by music critic Tom Ewing as more interpersonal than the pop music of the 1990s: "The revival of disco within pop put a spotlight on something that had gone missing over the 90s: a sense of music not just for dancing, but for dancing with someone. Disco was a music of mutual attraction: cruising, flirtation, negotiation. Its dancefloor is a space for immediate pleasure, but also for promises kept and otherwise. It’s a place where things start, but their resolution, let alone their meaning, is never clear. All of 2000s great disco number ones explore how to play this hand. Madison Avenue look to impose their will upon it, to set terms and roles. Spiller is less rigid. 'Groovejet' accepts the night’s changeability, happily sells out certainty for an amused smile and a few great one-liners."

In 2013, several 1970s-style disco and funk songs charted, and the pop charts had more dance songs than at any other point since the late 1970s. The biggest disco song of the year as of June was "Get Lucky" by Daft Punk, featuring Nile Rodgers on guitar. "Random Access Memories" also ended up winning Album of the Year at the 2014 Grammys. Other disco-styled songs that made it into the top 40 were Robin Thicke's "Blurred Lines" (number one), Justin Timberlake's "Take Back the Night" (number 29), Bruno Mars' "Treasure" (number five) and Michael Jackson's posthumous release "Love Never Felt So Good" (number nine). In addition, Arcade Fire's "Reflektor" featured strong disco elements. In 2014, disco music could be found in Lady Gaga's "Artpop" and Katy Perry's "Birthday". Other disco songs from 2014 include "I Want It All" By Karmin and 'Wrong Club" by the Ting Tings.

Other top-10 entries from 2015 like Mark Ronson's disco groove-infused "Uptown Funk", Maroon 5's "Sugar", the Weeknd's "Can't Feel My Face" and Jason Derulo's "Want To Want Me" also ascended the charts and have a strong disco influence. Disco mogul and producer Giorgio Moroder also re-appeared with his new album "Déjà Vu" in 2015 which has proved to be a modest success. Other songs from 2015 like "I Don't Like It, I Love It" by Flo Rida, "Adventure of a Lifetime" by Coldplay, "Back Together" by Robin Thicke and "Levels" by Nick Jonas feature disco elements as well. In 2016, disco songs or disco-styled pop songs are showing a strong presence on the music charts as a possible backlash to the 1980s-styled synthpop, electro house and dubstep that have been dominating the current charts. Justin Timberlake's 2016 song "Can't Stop the Feeling!", which shows strong elements of disco, became the 26th song to debut at number-one on the "Billboard" Hot 100 in the history of the chart. "The Martian", a 2015 film, extensively uses disco music as a soundtrack, although for the main character, astronaut Mark Watney, there's only one thing worse than being stranded on Mars: it's being stranded on Mars with nothing but disco music. "Kill the Lights", featured on an episode of the HBO television series "Vinyl" (2016) and with Nile Rodgers' guitar licks, hit number one on the US Dance chart in July 2016.

Diana Ross was one of the first Motown artists to embrace the disco sound with her successful 1976 outing "Love Hangover" from her self-titled album. Her 1980 dance classics "Upside Down" and "I'm Coming Out" were written and produced by Nile Rogers and Bernard Edwards of the group Chic. the Supremes, the group that made Ross famous, scored a handful of hits in the disco clubs without Ross, most notably 1976's "I'm Gonna Let My Heart Do the Walking" and, their last charted single before disbanding, 1977's "You're My Driving Wheel".

At the request of Motown that he produce songs in the disco genre, Marvin Gaye released "Got to Give It Up" in 1978, despite his dislike of disco. He vowed not to record any songs in the genre, and actually wrote the song as a parody. Stevie Wonder released the disco single "Sir Duke" in 1977 as a tribute to Duke Ellington, the influential jazz legend who had died in 1974. Smokey Robinson left the Motown group the Miracles for a solo career in 1972 and released his third solo album "A Quiet Storm" in 1975, which spawned and lent its name to the "Quiet Storm" musical programming format and subgenre of R&B. It contained the disco single "Baby That's Backatcha". Other Motown artists who scored disco hits include: Robinson's former group, the Miracles, with "Love Machine" (1975), Eddie Kendricks with "Keep On Truckin'" (1973), the Originals with "Down to Love Town" (1976) and Thelma Houston with her cover of the Harold Melvin and the Blue Notes song "Don't Leave Me This Way" (1976). The label continued to release successful disco songs into the 1980s with Rick James' "Super Freak" (1981), and the Commodores' "Lady (You Bring Me Up)" (1981). 

Several of Motown's solo artists who left the label went on to have successful disco songs. Mary Wells, Motown's first female superstar with her signature song "My Guy" (written by Smokey Robinson), abruptly left the label in 1964. She briefly reappeared on the charts with the disco song "Gigolo" in 1980. Jimmy Ruffin, the elder brother of the Temptations lead singer David Ruffin, was also signed to Motown, and released his most successful and well-known song "What Becomes of the Brokenhearted" as a single in 1966. Ruffin eventually left the record label in the mid-1970s, but saw success with the 1980 disco song "Hold On (To My Love)", which was written and produced by Robin Gibb of the Bee Gees, for his album "Sunrise". Edwin Starr, known for his Motown protest song "War" (1970), reentered the charts in 1979 with a pair of disco songs, "Contact" and "H.A.P.P.Y. Radio".

Kiki Dee became the first white British singer to sign with Motown in the US, and released one album, "Great Expectations" (1970), and two singles "The Day Will Come Between Sunday and Monday" (1970) and "Love Makes The World Go Round" (1971), the latter giving her first ever chart entry (number 87 on the US Chart). She soon left the company and signed with Elton John's The Rocket Record Company, and in 1976 had her biggest and best-known single, "Don't Go Breaking My Heart", a disco duet with John. The song was intended as an affectionate disco-style pastiche of the Motown sound, in particular the various duets recorded by Marvin Gaye with Tammi Terrell and Kim Weston. Michael Jackson released many successful solo singles under the Motown label, like "Got To Be There" (1971), "Ben" (1972) and a cover of Bobby Day's "Rockin' Robin" (1972). He went on to score hits in the disco genre with "Rock with You" (1979), "Don't Stop 'Til You Get Enough" (1979) and "Billie Jean" (1983) for Epic Records.

Many Motown groups who had left the record label charted with disco songs. Michael Jackson was the lead singer of the Jackson 5, one of Motown's premier acts in the early 1970s. They left the record company in 1975 (Jermaine Jackson, however, remained with the label) after successful songs like "I Want You Back" (1969) and "ABC" (1970), and even the disco song "Dancing Machine" (1974). Renamed as 'the Jacksons' (as Motown owned the name 'the Jackson 5'), they went on to find success with disco songs like "Blame It on the Boogie" (1978), "Shake Your Body (Down to the Ground)" (1979) and "Can You Feel It?" (1981) on the Epic label. the Isley Brothers, whose short tenure at the company had produced the song "This Old Heart of Mine (Is Weak for You)" in 1966, went on release successful disco songs like "That Lady" (1973) and "It's a Disco Night (Rock Don't Stop)" (1979). Gladys Knight and the Pips, who recorded the most successful version of "I Heard It Through the Grapevine" (1967) before Marvin Gaye, scored commercially successful singles such as "Baby, Don't Change Your Mind" (1977) and "Bourgie, Bourgie" (1980) in the disco era.

The Detroit Spinners were also signed to the Motown label and saw success with the Stevie Wonder-produced song "It's a Shame" in 1970. They left soon after, on the advice of fellow Detroit native Aretha Franklin, to Atlantic Records, and there had disco songs like "The Rubberband Man" (1976). In 1979, they released a successful cover of Elton John's "Are You Ready for Love", as well as a medley of the Four Seasons' song "Working My Way Back to You" and Michael Zager's "Forgive Me, Girl". The Four Seasons themselves were briefly signed to Motown's MoWest label, a short-lived subsidiary for R&B and soul artists based on the West Coast, and there the group produced one album, "Chameleon" (1972) – to little commercial success in the US. However, one single, "The Night", was released in Britain in 1975, and thanks to popularity from the Northern Soul circuit, reached number seven on the UK Singles Chart. The Four Seasons left Motown in 1974 and went on to have a disco hit with their song "December, 1963 (Oh, What a Night)" (1975) for Warner Curb Records.

Norman Whitfield was a producer at Motown, renowned for creating innovative "psychedelic soul" songs. The genre later developed into funk, and from there into disco. The Undisputed Truth, a Motown recording act assembled by Whitfield to experiment with his psychedelic soul production techniques, found success with their 1971 song "Smiling Faces Sometimes". The disco single "You + Me = Love" (number 43) in 1976, which also made number 2 on the US Dance Charts. In 1977, singer, songwriter and producer Willie Hutch signed with Whitfield's new label. He had been signed to Motown since 1970, scored a successful disco single with his song "In and Out". The group Rose Royce produced the to the 1976 film "Car Wash", which contained the commercially successful song of the same name.

Singer Stacy Lattisaw signed with Motown "after" achieving success in the disco genre. In 1980, she released her album "Let Me Be Your Angel", which spawned the disco singles "Dynamite" and "Jump to the Beat" on the Cotillion label. Lattisaw continued to enjoy success as a contemporary R&B/pop artist throughout the 1980s. She signed with Motown in 1986, and achieved most success when teaming up with Johnny Gill, releasing the 1989 song "Where Do We Go From Here?" from her last ever album, "What You Need", before retiring. In addition, her first ever single, back in 1979, was a disco cover of "When You're Young and in Love", which was most famously recorded by Motown female group the Marvelettes in 1967.

Additionally, the debut single of Shalamar, the group originally created as a disco-driven vehicle by "Soul Train" creator Don Cornelius, was "Uptown Festival" (1977), a medley of 10 classic Motown songs sung over a 1970s disco beat.

As disco's popularity sharply declined in the United States, abandoned by major U.S. record labels and producers, European disco continued evolving within the broad mainstream pop music scene. European acts Silver Convention, Love and Kisses, Munich Machine, and American acts Donna Summer and the Village People, were acts that defined the late 1970s Euro disco sound. Producers Giorgio Moroder, whom AllMusic described as "one of the principal architects of the disco sound" with the Donna Summer song "I Feel Love" (1977), and Jean-Marc Cerrone were involved with Euro disco. The German group Kraftwerk also had an influence on Euro disco.
By far the most successful Euro disco act was ABBA. This Swedish quartet, which sang in English, found success with singles such as "Waterloo" (1974), "Fernando" (1976), "Take a Chance on Me" (1978), "Gimme! Gimme! Gimme! (A Man After Midnight)" (1979), and their signature smash "Dancing Queen" (1976)—ranks as the eighth best-selling act of all time. Other prominent European pop and disco groups were Luv' from the Netherlands and Boney M., a group of four West Indian singers and dancers masterminded by West German record producer Frank Farian. Boney M. charted worldwide with such songs as "Daddy Cool", "Ma Baker" and "Rivers Of Babylon". Another Euro disco act was the French Amanda Lear, where Euro-disco sound is most heard in Enigma ("Give A Bit Of Mmh To Me") song (1978).

In France, Dalida released "J'attendrai" ("I Will Wait"), which also became successful in Canada, Europe and Japan. Dalida successfully adjusted herself to disco era and released at least a dozen of songs that charted among top number 10 in whole Europe and wider.
Claude François, who re-invented himself as the king of French disco, released "La plus belle chose du monde", a French version of the Bee Gees song "Massachusetts", which became successful in Canada and Europe and "Alexandrie Alexandra" was posthumously released on the day of his burial and became a worldwide success. Cerrone's early songs, "Love in C Minor", "Give Me Love" and "Supernature" were successful in the US and Europe.

In Italy Raffaella Carrà is the most successful disco act. Her greatest international single was "Tanti Auguri" ("Best Wishes"), which has become a popular song with gay audiences. The song is also known under its Spanish title "Para hacer bien el amor hay que venir al sur" (which refers to Southern Europe, since the song was recorded and taped in Spain). The Estonian version of the song "Jätke võtmed väljapoole" was performed by Anne Veski. "A far l'amore comincia tu" ("To make love, your move first") was another success for her internationally, known in Spanish as "En el amor todo es empezar", in German as "Liebelei", in French as "Puisque tu l'aimes dis le lui", and in English as "Do It, Do It Again". It was her only entry to the UK Singles Chart, reaching number 9, where she remains a one-hit wonder. In 1977, she recorded another successful single, "Fiesta" ("The Party" in English) originally in Spanish, but then recorded it in French and Italian after the song hit the charts. "A far l'amore comincia tu" has also been covered in Turkish by a Turkish popstar Ajda Pekkan as "Sakın Ha" in 1977.
Recently, Carrà has gained new attention for her appearance as the female dancing soloist in a 1974 TV performance of the experimental gibberish song "Prisencolinensinainciusol" (1973) by Adriano Celentano. A remixed video featuring her dancing went viral on the internet in 2008.
In 2008 a video of a performance of her only successful UK single, "Do It, Do It Again", was featured in the "Doctor Who" episode "Midnight". Rafaella Carrà worked with Bob Sinclar on the new single "Far l'Amore" which was released on YouTube on March 17, 2011. The song charted in different European countries.

The music typically layered soaring, often-reverberated vocals, often doubled by horns, over a background "pad" of electric pianos and "chicken-scratch" rhythm guitars played on an electric guitar. "The 'chicken scratch' sound is achieved by lightly pressing the strings against the fretboard and then quickly releasing them just enough to get a slightly muted scratching [sound] while constantly strumming very close to the bridge." Other backing keyboard instruments include the piano, electric organ (during early years), string synth, and electromechanical keyboards such as the Fender Rhodes electric piano, Wurlitzer electric piano, and Hohner Clavinet. Synthesizers are also fairly common in disco, especially in the late 1970s.

The rhythm is laid down by prominent, syncopated basslines (with heavy use of broken octaves, that is, octaves with the notes sounded one after the other) played on the bass guitar and by drummers using a drum kit, African/Latin percussion, and electronic drums such as Simmons and Roland drum modules. The sound was enriched with solo lines and harmony parts played by a variety of orchestral instruments, such as harp, violin, viola, cello, trumpet, saxophone, trombone, clarinet, flugelhorn, French horn, tuba, English horn, oboe, flute (sometimes especially the alto flute and occasionally bass flute), piccolo, timpani and synth strings, string section or a full string orchestra.

Most disco songs have a steady four-on-the-floor beat, a quaver or semi-quaver hi-hat pattern with an open hi-hat on the off-beat, and a heavy, syncopated bass line. Other Latin rhythms such as the rhumba, the samba and the cha-cha-cha are also found in disco recordings, and Latin polyrhythms, such as a rhumba beat layered over a merengue, are commonplace. The quaver pattern is often supported by other instruments such as the rhythm guitar and may be implied rather than explicitly present.

Songs often use syncopation, which is the accenting of unexpected beats. In general, the difference between a disco, or any dance song, and a rock or popular song is that in dance music the bass drum hits "four to the floor", at least once a beat (which in 4/4 time is 4 beats per measure), whereas in rock the bass hits on one and three and lets the snare take the lead on two and four (the "backbeat"). Disco is further characterized by a 16th note division of the quarter notes as shown in the second drum pattern below, after a typical rock drum pattern.

The orchestral sound usually known as "disco sound" relies heavily on string sections and horns playing linear phrases, in unison with the soaring, often reverberated vocals or playing instrumental fills, while electric pianos and chicken-scratch guitars create the background "pad" sound defining the harmony progression. Typically, all of the doubling of parts and use of additional instruments creates a rich "wall of sound". There are, however, more minimalistic flavors of disco with reduced, transparent instrumentation, pioneered by Chic.

In 1977, Giorgio Moroder again became responsible for a development in disco. Alongside Donna Summer and Pete Bellotte he wrote the song "I Feel Love" for Summer to perform. It became the first well-known disco song to have a completely synthesised backing track. The song is still considered to have been well ahead of its time. Other disco producers, most famously Tom Moulton, grabbed ideas and techniques from dub music (which came with the increased Jamaican migration to New York City in the 1970s) to provide alternatives to the "four on the floor" style that dominated. DJ Larry Levan utilized styles from dub and jazz and remixing techniques to create early versions of house music that sparked the genre.

The "disco sound" was much more costly to produce than many of the other popular music genres from the 1970s. Unlike the simpler, four-piece band sound of the funk, soul of the late 1960s, or the small jazz organ trios, disco music often included a large pop band, with several chordal instruments (guitar, keyboards, synthesizer), several drum or percussion instruments (drumkit, Latin percussion, electronic drums), a horn section, a string orchestra, and a variety of "classical" solo instruments (for example, flute, piccolo, and so on).

Disco songs were arranged and composed by experienced arrangers and orchestrators, and record producers added their creative touches to the overall sound using multitrack recording techniques and effects units. Recording complex arrangements with such a large number of instruments and sections required a team that included a conductor, copyists, record producers, and mixing engineers. Mixing engineers had an important role in the disco production process, because disco songs used as many as 64 tracks of vocals and instruments. Mixing engineers and record producers, under the direction of arrangers, compiled these tracks into a fluid composition of verses, bridges, and refrains, complete with orchestral builds and breaks. Mixing engineers and record producers helped to develop the "disco sound" by creating a distinctive-sounding, sophisticated disco mix.

Early records were the "standard" 3 minute version until Tom Moulton came up with a way to make songs longer. Moulton wanted to make longer songs, so that he could take a crowd of dancers at a club to another level and keep them dancing longer. He found that was impossible to make the 45-RPM vinyl discs of the time longer, as they could usually hold no more than 5 minutes of good-quality music. With the help of José Rodriguez, his remasterer/mastering engineer, he pressed a single on a 10" disc instead of 7". They cut the next single on a 12" disc, the same format as a standard album. Moulton and Rodriguez discovered that these larger records could have much longer songs and remixes. Twelve-inch records, even for singles, fast became the standard format for all DJs of the disco genre.

Because record sales were often dependent on dance floor play by DJs in leading nightclubs, DJs were also important to the development and popularization of disco music. By selecting and playing disco songs, DJs helped to make certain songs more or less popular. Notable disco DJs include the first female disco DJ in the US, Karen Mixon Cook, and many other DJs, such as Jim Burgess, Walter Gibbons, John "Jellybean" Benitez, Richie Kaczar of Studio 54, Rick Gianatos, Francis Grasso of Sanctuary, Larry Levan, Ian Levine and David Mancuso. DJs not only played songs in clubs; they also remixed, looped and live-mixed these songs from the DJ booth, changing the ways songs sounded. For example, a DJ might use the intro or bassline from a popular disco track and beatmatch and layer the vocals from a second song over top. As well, some DJs were also record producers who created and produced disco songs in the recording studio. Larry Levan, for example, is as well known for his prolific record producer work as for his contributions as a DJ.

By the late 1970s most major US cities had thriving disco club scenes, but the largest scenes were in San Francisco, Miami, and most notably New York City. The scene was centered on discotheques, nightclubs, and private loft parties where DJs would play disco tracks from discs and records through PA systems for the patrons who came to dance. Powerful, bass-heavy, hi-fi sound systems were viewed as a key part of the disco club experience. "Mancuso introduced the technologies of tweeter arrays (clusters of small loudspeakers, which emit high-end frequencies, positioned above the floor) and bass reinforcements (additional sets of subwoofers positioned at ground level) at the start of the 1970s in order to boost the treble and bass at opportune moments, and by the end of the decade sound engineers such as Richard Long had multiplied the effects of these innovations in venues such as the Garage." The DJs played "... a smooth mix of long single records to keep people 'dancing all night long'". Some of the most prestigious clubs had elaborate lighting systems that throbbed to the beat of the music.

In the 1970s, notable discos included "Artemis" in Philadelphia, "Studio One" in Los Angeles, "Leviticus" in New York, "Dugan's Bistro" in Chicago, and "The Library" in Atlanta. In the late 70s, Studio 54 in New York City was arguably the most well known nightclub in the world. This club played a major formative role in the growth of disco music and nightclub culture in general. The Copacabana, another New York nightclub dating to the 1940s, had a revival in the late 1970s when it embraced disco; it would become the setting of a Barry Manilow song of the same name.

In the early years, dancers in discos danced in a "hang loose" or "freestyle" approach. At first, many dancers improvised their own dance styles and dance steps. Later in the disco era, popular dance styles were developed, including the "Bump", "Penguin", "Boogaloo", "Watergate" and the "Robot". By October 1975 The Hustle reigned. It was highly stylized, sophisticated and overtly sexual. Variations included the Brooklyn Hustle, New York Hustle and Latin Hustle.

During the disco era, many nightclubs would commonly host disco dance competitions or offer free dance lessons. Some cities had disco dance instructors or dance schools, which taught people how to do popular disco dances such as ""touch dancing", ""the hustle", and "the cha cha". The pioneer of disco dance instruction was Karen Lustgarten in San Francisco in 1973. Her book "The Complete Guide to Disco Dancing" (Warner Books, 1978) was the first to name, break down and codify popular disco dances as dance forms and distinguish between disco freestyle, partner and line dances. The book topped the "New York Times" bestseller list for 13 weeks and was translated into Chinese, German and French.

In Chicago, the "Step By Step" disco dance TV show was launched with the sponsorship support of the Coca-Cola company. Produced in the same studio that Don Cornelius used for the nationally syndicated dance/music television show, "Soul Train", "Step by Step"'s audience grew and the show became a success. The dynamic dance duo of Robin and Reggie led the show. The pair spent the week teaching disco dancing to dancers in the disco clubs. The instructional show which aired on Saturday mornings had a following of dancers who would stay up all night on Fridays so they could be on the set the next morning, ready to return to the disco on Saturday night knowing with the latest personalized dance steps. The producers of the show, John Reid and Greg Roselli, routinely made appearances at disco functions with Robin and Reggie to scout out new dancing talent and promote upcoming events such as "Disco Night at White Sox Park".

Some notable professional dance troupes of the 1970s included Pan's People and Hot Gossip. For many dancers, a key source of inspiration for 1970s disco dancing was the film "Saturday Night Fever" (1977). This developed into the music and dance style of such films as "Fame" (1980), "Disco Dancer" (1982), "Flashdance" (1983), and "The Last Days of Disco" (1998). Interest in disco dancing also helped spawn dance competition TV shows such as "Dance Fever" (1979).

Disco fashions were very trendy in the late 1970s. Discothèque-goers often wore glamorous, expensive and extravagant fashions for nights out at their local disco club. Some women would wear sheer, flowing dresses, such as Halston dresses or loose, flared pants. Other women wore tight, revealing, sexy clothes, such as backless halter tops, "hot pants" or body-hugging spandex bodywear or "catsuits". Men would wear shiny polyester Qiana shirts with colourful patterns and pointy, extra wide collars, preferably open at the chest. Men often wore Pierre Cardin suits, three piece suits with a vest and double-knit polyester shirt jackets with matching trousers known as the leisure suit. Men's leisure suits were typically form-fitted in some parts of the body, such as the waist and bottom, but the lower part of the pants were flared in a bell bottom style, to permit freedom of movement.

During the disco era, men engaged in elaborate grooming rituals and spent time choosing fashion clothing, both activities that would have been considered "feminine" according to the gender stereotypes of the era. Women dancers wore glitter makeup, sequins or gold lamé clothing that would shimmer under the lights. Bold colours were popular for both genders. Platform shoes and boots for both genders and high heels for women were popular footwear. Necklaces and medallions were a common fashion accessory. Less commonly, some disco dancers wore outlandish costumes, dressed in drag, covered their bodies with gold or silver paint, or wore very skimpy outfits leaving them nearly nude; these uncommon get-ups were more likely to be seen at invitation-only New York City loft parties and disco clubs.

In addition to the dance and fashion aspects of the disco club scene, there was also a thriving club drug subculture, particularly for drugs that would enhance the experience of dancing to the loud, bass-heavy music and the flashing coloured lights, such as cocaine (nicknamed "blow"), amyl nitrite "poppers", and the "... other quintessential 1970s club drug Quaalude, which suspended motor coordination and gave the sensation that one's arms and legs had turned to "Jell-O." Quaaludes were so popular at disco clubs that the drug was nicknamed "disco biscuits".

Paul Gootenberg states that "[t]he relationship of cocaine to 1970s disco culture cannot be stressed enough..." During the 1970s, the use of cocaine by well-to-do celebrities led to its "glamorization" and to the widely held view that it was a "soft drug". Cocaine was also popular because its stimulating effect "...fueled all-night parties" at disco clubs. LSD, marijuana, and "speed" (amphetamines) were also popular in disco clubs, and the use of these drugs "...contributed to the hedonistic quality of the dance floor experience." Since disco dances were typically held in liquor licensed-nightclubs and dance clubs, alcoholic drinks were also consumed by dancers; some users intentionally combined alcohol with the consumption of other drugs, such as Quaaludes, for a stronger effect.

According to Peter Braunstein, the "massive quantities of drugs ingested in discothèques produced the next cultural phenomenon of the disco era: rampant promiscuity and public sex. While the dance floor was the central arena of seduction, actual sex usually took place in the nether regions of the disco: bathroom stalls, exit stairwells, and so on. In other cases the disco became a kind of 'main course' in a hedonist's menu for a night out." At The Saint nightclub, a high percentage of the gay male dancers and patrons would have sex in the club; they typically had unprotected sex, because in 1980, HIV-AIDS had not yet been identified. At The Saint, "...dancers would elope to an un[monitored] upstairs balcony to engage in sex." The promiscuity and public sex at discos was part of a broader trend towards exploring a freer sexual expression in the 1970s, an era that is also associated with "swingers clubs, hot tubs, [and] key parties."

Famous disco bars included the Paradise Garage and Crisco Disco as well as "... cocaine-filled celeb hangouts such as Manhattan's Studio 54," which was operated by Steve Rubell and Ian Schrager. Studio 54 was notorious for the hedonism that went on within; the balconies were known for sexual encounters, and drug use was rampant. Its dance floor was decorated with an image of the "Man in the Moon" that included an animated cocaine spoon.

The transition from the late-1970s disco styles to the early-1980s dance styles was marked primarily by the change from complex arrangements performed by large ensembles of studio session musicians (including a horn section and an orchestral string section), to a leaner sound, in which one or two singers would perform to the accompaniment of synthesizer keyboards and drum machines.

In addition, dance music during the 1981–83 period borrowed elements from blues and jazz, creating a style different from the disco of the 1970s. This emerging music was still known as disco for a short time, as the word had become associated with any kind of dance music played in discothèques. Examples of early-1980s' dance sound performers include D. Train, Kashif, and Patrice Rushen. These changes were influenced by some of the notable R&B and jazz musicians of the 1970s, such as Stevie Wonder, Kashif and Herbie Hancock, who had pioneered "one-man-band"-type keyboard techniques. Some of these influences had already begun to emerge during the mid-1970s, at the height of disco's popularity.

During the first years of the 1980s, the disco sound began to be phased out, and faster tempos and synthesized effects, accompanied by guitar and simplified backgrounds, moved dance music toward the funk and pop genres. This trend can be seen in singer Billy Ocean's recordings between 1979 and 1981. Whereas Ocean's 1979 song "American Hearts" was backed with an orchestral arrangement played by the Los Angeles Symphony Orchestra, his 1981 song ""One of Those Nights (Feel Like Gettin' Down)"" had a more bare, stripped-down sound, with no orchestration or symphonic arrangements. This drift from the original disco sound is called post-disco. In this music scene there are rooted subgenres, such as Italo disco, techno, house, dance-pop, boogie, and early alternative dance. During the early 1980s, dance music dropped the complicated song structure and orchestration that typified the disco sound.

During the 1970s, many TV theme songs were produced (or older themes updated) with disco influenced music. Examples include "S.W.A.T." (1975), "Wonder Woman" (1975), "Charlie's Angels" (1976), "NBC Saturday Night At The Movies" (1976), "The Love Boat" (1977), "The Donahue Show" (1977), "CHiPs" (1977), "The Professionals" (1977), "Three's Company" (1977), "Dallas" (1978), NBC Sports broadcasts (1978), "Kojak" (1977), "The Hollywood Squares" (1979). The British science fiction program "" (1975) also featured a soundtrack strongly influenced by disco, especially in the show's second season.

The rising popularity of disco came in tandem with developments in the role of the DJ. DJing developed from the use of multiple record turntables and DJ mixers to create a continuous, seamless mix of songs, with one song transitioning to another with no break in the music to interrupt the dancing. The resulting DJ mix differed from previous forms of dance music in the 1960s, which were oriented towards live performances by musicians. This in turn affected the arrangement of dance music, since songs in the disco era typically contained beginnings and endings marked by a simple beat or riff that could be easily used to transition to a new song. The development of DJing was also influenced by new turntablism techniques, such as beatmatching, a process facilitated by the introduction of new turntable technologies such as the Technics SL-1200 MK 2, first sold in 1978, which had a precise variable pitch control and a direct drive motor. DJs were often avid record collectors, who would hunt through used record stores for obscure soul records and vintage funk recordings. DJs helped to introduce rare records and new artists to club audiences.

In the 1970s, individual DJs became more prominent, and some DJs, such as Larry Levan, the resident at Paradise Garage, Jim Burgess, Tee Scott and Francis Grasso became famous in the disco scene. Levan, for example, developed a cult following amongst club-goers, who referred to his DJ sets as "Saturday Mass". Some DJs would use reel to reel tape recorders to make remixes and tape edits of songs. Some DJs who were making remixes made the transition from the DJ booth to becoming a record producer, notably Burgess. Scott developed several innovations. He was the first disco DJ to use three turntables as sound sources, the first to simultaneously play two beatmatched records, the first user of electronic effects units in his mixes and an innovator in mixing dialogue in from well-known movies into his mixes, typically over a percussion break. These mixing techniques were also applied to radio DJs, such as Ted Currier of WKTU and WBLS. Grasso is particularly notable for taking the DJ “profession out of servitude and [making] the DJ the musical head chef”. Once he entered the scene, the DJ was no longer responsible for waiting on the crowd hand and foot, meeting their every song request. Instead, with increased agency and visibility, the DJ was now able to use his own technical and creative skills to whip up a nightly special of innovative mixes, refining his personal sound and aesthetic, and building his own reputation. Known as the first DJ to create a take his audience on a narrative, musical journey, Grasso discovered that music could effectively shift the energy of the crowd, and even more, that he had all this power at his fingertips.

About five years after the disco era came to a close in the late 1970s, rave culture began to emerge from the acid house scene. Rave culture incorporated disco culture's same love of dance music played by DJs over powerful sound systems, recreational drug and club drug exploration, sexual promiscuity, and hedonism. Although disco culture started out underground, it eventually thrived in the mainstream by the late 1970s, and major labels commodified and packaged the music for mass consumption. In contrast, the rave culture started out underground and stayed underground. In part this was to avoid the animosity that was still surrounding disco and dance music. The rave scene also stayed underground to avoid law enforcement attention that was directed at the rave culture due to its use of secret, unauthorized warehouses for some dance events and its association with illegal club drugs like Ecstasy.

The disco sound had a strong influence on early hip hop. Most of the early hip hop songs were created by isolating existing disco bass-guitar lines and dubbing over them with MC rhymes. The Sugarhill Gang used Chic's "Good Times" as the foundation for their 1979 song "Rapper's Delight", generally considered to be the song that first popularized rap music in the United States and around the world. In 1982, Afrika Bambaataa released the single "Planet Rock", which incorporated electronica elements from Kraftwerk's "Trans-Europe Express" and "Numbers" as well as YMO's "Riot in Lagos".

The Planet Rock sound also spawned a hip hop electronic dance trend, electro music, which included songs such as Planet Patrol's "Play at Your Own Risk" (1982), C Bank's "One More Shot" (1982), Cerrone's "Club Underworld" (1984), Shannon's "Let the Music Play" (1983), Freeez's "I.O.U." (1983), Midnight Star's "Freak-a-Zoid" (1983), Chaka Khan's "I Feel For You" (1984).

House music is a genre of electronic dance music that originated in Chicago in the early 1980s. It was initially popularized in Chicago, circa 1984. House music quickly spread to other American cities such as Detroit, New York City, and Newark – all of which developed their own regional scenes. In the mid-to-late 1980s, house music became popular in Europe as well as major cities in South America, and Australia. Early house music commercial success in Europe saw songs such as "Pump Up The Volume" by MARRS (1987), "House Nation" by House Master Boyz and the Rude Boy of House (1987), "Theme from S'Express" by S'Express (1988) and "Doctorin' the House" by Coldcut (1988) in the pop charts. Since the early to mid-1990s, house music has been infused in mainstream pop and dance music worldwide.

Early house music was generally dance-based music characterized by repetitive four on the floor beats, rhythms mainly provided by drum machines, off-beat hi-hat cymbals, and synthesized basslines. While house displayed several characteristics similar to disco music, it was more electronic and minimalistic, and the repetitive rhythm of house was more important than the song itself. As well, house did not use the lush string sections that were a key part of the disco sound. House music in the 2010s, while keeping several of these core elements, notably the prominent kick drum on every beat, varies widely in style and influence, ranging from the soulful and atmospheric deep house to the more minimalistic microhouse. House music has also fused with several other genres creating fusion subgenres, such as euro house, tech house, electro house and jump house.

The post-punk movement that originated in the late 1970s both supported punk rock's rule breaking while rejecting its move back to raw rock music. Post-punk's mantra of constantly moving forward lent itself to both openness to and experimentation with elements of disco and other styles. Public Image Limited is considered the first post-punk group. The group's second album "Metal Box" fully embraced the "studio as instrument" methodology of disco. The group's founder John Lydon, the former lead singer for the Sex Pistols, told the press that disco was the only music he cared for at the time.

No wave was a subgenre of post-punk centered in New York City. For shock value, James Chance, a notable member of the no wave scene, penned an article in the "East Village Eye" urging his readers to move uptown and get "trancin' with some superadioactive disco voodoo funk". His band James White and the Blacks wrote a disco album "Off White". Their performances resembled those of disco performers (horn section, dancers and so on). In 1981 ZE Records led the transition from no wave into the more subtle mutant disco (post-disco/punk) genre. Mutant disco acts such as Kid Creole and the Coconuts, Was Not Was, ESG and Liquid Liquid influenced several British post-punk acts such as New Order, Orange Juice and A Certain Ratio.

In the early 2000s the dance-punk (new rave in the United Kingdom) emerged as a part of a broader post punk revival. It fused elements of punk-related rock with different forms of dance music including disco. Klaxons, LCD Soundsystem, Death From Above 1979, the Rapture and Shitdisco were among acts associated with the genre.

Nu-disco is a 21st-century dance music genre associated with the renewed interest in 1970s and early 1980s disco, mid-1980s Italo disco, and the synthesizer-heavy Euro disco aesthetics. The moniker appeared in print as early as 2002, and by mid-2008 was used by record shops such as the online retailers Juno and Beatport. These vendors often associate it with re-edits of original-era disco music, as well as with music from European producers who make dance music inspired by original-era American disco, electro and other genres popular in the late 1970s and early 1980s. It is also used to describe the music on several American labels that were previously associated with the genres electroclash and French house.





</doc>
<doc id="7970" url="https://en.wikipedia.org/wiki?curid=7970" title="Darwin">
Darwin

Darwin most often refers to:


Darwin may also refer to:












</doc>
<doc id="7973" url="https://en.wikipedia.org/wiki?curid=7973" title="Donegal fiddle tradition">
Donegal fiddle tradition

The Donegal fiddle tradition is the way of playing the fiddle that is traditional in County Donegal, Ireland. It is one of the distinct fiddle traditions within Irish traditional music.

The distinctness of the Donegal tradition developed due to the close relations between Donegal and Scotland, and the Donegal repertoire and style has influences from Scottish fiddle music. For example, in addition to the standard tune types such as Jigs and Reels, the Donegal tradition also has Highlands (influenced by the Scottish Strathspey). The distinctiveness of the Donegal tradition led to some conflict between Donegal players and representatives of the mainstream tradition when Irish traditional music was organised in the 1960s.

The tradition has several distinguishing traits compared to other fiddle traditions such as the Sliabh Luachra style of southern ireland, most of which involves styles of bowing and the ornamentation of the music, and rhythm. Due to the frequency of double stops and the strong bowing it is often compared to the Cape Breton tradition. Another characteristic of the style is the rapid pace at which it tends to proceed. Modern players, such as the fiddle group Altan, continue to be popular due to a variety of reasons.

Among the most famous Donegal style players are John Doherty from the early twentieth century and James Byrne, Paddy Glackin, Tommy Peoples and Mairéad Ní Mhaonaigh in recent decades.

The fiddle has ancient roots in Ireland, the first report of bowed instruments similar to the violin being in the Book of Leinster (ca. 1160). The modern violin was ubiquitous in Ireland by the early 1700s. However the first mention of the fiddle being in use in Donegal is from the blind harper Arthur O'Neill who in his 1760 memoirs described a wedding in Ardara as having "plenty of pipers and fiddlers". Donegal fiddlers participated in the development of the Irish music tradition in the 18th century during which jigs and slipjigs and later reels and hornpipes became the dominant musical forms. However, Donegal musicians, many of them being fishermen, also frequently travelled to Scotland, where they acquired tune types from the Scottish repertoire such as the Strathspey which was integrated into the Donegal tradition as "Highland" tunes. The Donegal tradition derives much of its unique character from the synthesis of Irish and Scottish stylistic features and repertoires. Aoidh notes however that while different types of art music were commonly played among the upper classes of Scottish society in the 18th century, the Donegal tradition drew exclusively from the popular types of Scottish music. Like some Scottish fiddlers (who, like Donegal fiddlers, tend to use a short bow and play in a straight-ahead fashion), some Donegal fiddlers worked at imitating the sound of the bagpipes. Workers from Donegal would bring their music to Scotland and also bring back Scottish tunes with them such music of J. Scott Skinner and Mackenzie Murdoch. Lilting, unaccompanied singing of wordless tunes, was also an important part of the Donegal musical tradition often performed by women in social settings. Describing the musical life of Arranmore Island in the late 19th century singer Róise Rua Nic Gríanna describes the most popular dances: "The Sets, the Lancers, the Maggie Pickie [i.e., Maggie Pickins] the Donkey, the Mazurka and the Barn dances". Among the travelling fiddlers of the late 19th century players such as John Mhosaí McGinley, Anthony Hilferty, the McConnells and the Dohertys are best known. As skill levels increased through apprenticeships several fiddle masters appeared such as the Cassidy's, Connie Haughey, Jimmy Lyons and Miock McShane of Teelin and Francie Dearg and Mickey Bán Byrne of Kilcar. These virtuosos played unaccompanied listening pieces in addition to the more common dance music.

The influences between Scotland and Donegal went both ways and were furthered by a wave of immigration from Donegal to Scotland in the 19th century (the regions share common names of dances), as can be heard in the volume of strathspeys, schottisches, marches, and Donegal's own strong piping tradition, has influenced and been influenced by music, and by the sounds, ornaments, and repertoire of the Píob Mhór, the traditional bagpipes of Ireland and Scotland. There are other differences between the Donegal style and the rest of Ireland. Instruments such as the tin whistle, flute, concertina and accordion were very rare in Donegal until modern times. Traditionally the píob mór and the fiddle were the only instruments used and the use of pipe or fiddle music was common in old wedding customs. Migrant workers carried their music to Scotland and also brought back a number of tunes of Scottish origin. The Donegal fiddlers may well have been the route by which Scottish tunes such as Lucy Campbell, Tarbolton Lodge (Tarbolton) and The Flagon (The Flogging Reel), that entered the Irish repertoire. These players prided themselves on their technical abilities, which included playing in higher positions (fairly uncommon among traditional Irish fiddlers), and sought out material which would demonstrate their skills.

As Irish music was consolidated and organised under the Comhaltas Ceoltóirí Éireann movement in the 1960s, both strengthened the interest in traditional music but sometimes conflicted with the Donegal tradition and its social conventions. The rigidly organised sessions of the Comhaltas reflected the traditions of Southern Ireland and Donegal fiddlers like John Doherty considered the National repertoire with its strong focus on reels to be less diverse than that of Donegal with its varied rhythms. Other old fiddlers dislike the ways comhaltas sessions were organised with a committee player, often not himself a musician, in charge. Sometimes Comhaltas representatives would even disparage the Donegal tradition, with its Scottish flavour, as being un-Irish, and prohibit them from playing local tunes with Scottish genealogies such as the "Highlands" at Comhaltas sessions. This sometimes cause antagonism between Donegal players and the main organisation of traditional music in ireland.

Outside of the Comhaltas movement however, Donegal fiddling stood strong with Paddy Glackin of Ceoltorí Laighean and the Bothy Band and later Tommy Peoples also with the Bothy Band and Mairead Ni Mhaonaigh with Altan, who all drew attention and prestige to the Donegal tradition within folk music circles throughout Ireland.

The Donegal style of fiddling is a label often applied to music from this area, though one also might plausibly identify several different, but related, styles within the county. To the extent to which there is one common style in the county, it is characterised by a rapid pace; a tendency to be more un-swung in the playing of the fast dance tune types (reel and jigs); short (non-slurred), aggressive bowing, sparse ornamentation, the use of bowed triplets more often than trills as ornaments, the use of double stops and droning; and the occurrence of "playing the octave", with one player playing the melody and the other playing the melody an octave lower. None of these characteristics are universal, and there is some disagreement as to the extent to which there is a common style at all. In general, however, the style is rather aggressive.

Another feature of Donegal fiddling that makes it distinctive among Irish musical traditions is the variety of rare tune types that are played. Highlands, a type of tune in 4/4 time with some similarities to Scottish strathspeys, which are also played in Donegal, are one of the most commonly played types of tune in the county. Other tune types common solely in the county include barndances, also called "Germans," and mazurkas.

There are a number of different strands to the history of fiddle playing in County Donegal. Perhaps the best-known and, in the last half of the twentieth century, the most influential has been that of the Doherty family. Hugh Doherty is the first known musician of this family. Born in 1790, he headed an unbroken tradition of fiddlers and pipers in the Doherty family until the death, in 1980, of perhaps the best-known Donegal fiddler, John Doherty. John, a travelling tinsmith, was known for his extremely precise and fast finger- and bow-work and vast repertoire, and is considered to be one of the greatest Irish fiddlers ever recorded. John's older brother, Mickey, was also recorded and, though Mickey was another of the great Irish fiddlers, his reputation has been overshadowed by John's.

There is no single Donegal style but several distinctive styles. These styles traditionally come from the geographical isolated regions of Donegal including Inishowen, eastern Donegal, The Rosses and Gweedore, Croaghs, Teelin, Kilcar, Glencolmcille, Ballyshannon and Bundoran. Even with improved communications and transport, these regions still have recognisably different ways of fiddle playing. Notable deceased players of the older Donegal styles include Neillidh ("Neilly") Boyle, Francie Byrne, Con Cassidy,Frank Cassidy, James Byrne (1946–2008), and P.V. O'Donnell (2011). Currently living Donegal fiddlers, include, Vincent Campbell, John Gallagher, Paddy Glackin, Danny O'Donnell, and Tommy Peoples.

Fiddle playing continues to be popular in Donegal. The three fiddlers of the Donegal "supergroup" Altan, Mairéad Ní Mhaonaigh, Paul O'Shaughnessy, and Ciarán Tourish, are generally admired within Donegal. An example of another fiddler-player from Donegal is Liz Doherty.
Another well regarded fiddle player hailing from Donegal is Aidan O'Donnell. TG4 Young Musician of the Year 2010 Aidan O'Donnell has been described as one of the finest young Irish musicians at present. He began his music making at the age of 12, and since then has performed with some of traditional music's finest artists, including Donal Lunny, Micheal Ó'Suilleabháin and the Chieftains. In 2007, he won the prestigious ‘Oireachtas na Geailge' fiddle title, and has been a regular tutor at the Irish World Academy of Music and Dance, at the University of Limerick for the past number of years.

The fiddle, and traditional music in general, remained popular in Donegal not only because of the international coverage of certain artists but because of local pride in the music. Traditional music "Seisiúns" are still common place both in pubs and in houses. The Donegal fiddle music has been influenced by recorded music, but this is claimed to have had a positive impact on the tradition. Modern Donegal fiddle music is often played in concerts and recorded on albums.



</doc>
<doc id="7975" url="https://en.wikipedia.org/wiki?curid=7975" title="Double-barreled shotgun">
Double-barreled shotgun

A double-barreled shotgun is a shotgun with two parallel barrels, allowing two shots to be fired in quick succession.

Modern double-barreled shotguns, often known as "doubles", are almost universally break open actions, with the barrels tilting up at the rear to expose the breech ends of the barrels for unloading and reloading. Since there is no reciprocating action needed to eject and reload the shells, doubles are more compact than repeating designs such as pump action or lever-action shotguns.

Double-barreled shotguns come in two basic configurations: the side-by-side shotgun (SxS) and the over/under shotgun ("over and under", O/U, etc.), indicating the arrangement of barrels. The original double-barreled guns were nearly all SxS designs, which was a more practical design of muzzle-loading firearms. Early cartridge shotguns also used the SxS action, because they kept the exposed hammers of the earlier muzzle-loading shotguns from which they evolved. When hammerless designs started to become common, the O/U design was introduced, and most modern sporting doubles are O/U designs.

One significant advantage that doubles have over single barrel repeating shotguns is the ability to provide access to more than one choke at a time. Some shotgun sports, such as skeet, use crossing targets presented in a narrow range of distance, and only require one level of choke. Others, like sporting clays, give the shooter targets at differing ranges, and targets that might approach or recede from the shooter, and so must be engaged at differing ranges. Having two barrels lets the shooter use a more open choke for near targets, and a tighter choke for distant targets, providing the optimal shot pattern for each distance.

Their disadvantage lies in the fact that the barrels of a double-barreled shotgun, whether "O/U" or "SxS", are not parallel, but slightly angled, so that shots from the barrels converge, usually at "40 yards out". For the "SxS" configuration, the shotstring continues on its path to the opposite side of the rib after the converging point; for example, the left barrel's discharge travels on the left of the rib till it hits dead center at 40 yards out, after that, the discharge continues on to the right. In the "O/U" configuration with a parallel rib, both barrels' discharges will keep to the dead center, but the discharge from the "under" barrel will shoot higher than the discharge from the "over" barrel after 40 yards. Thus, double-barreled shotguns are accurate only at practical shotgun ranges, though the range of their ammunition easily exceeds four to six times that distance.

"SxS" shotguns are often more expensive, and may take more practice to aim effectively than a "O/U". The off-center nature of the recoil in a "SxS" gun may make shooting the body-side barrel slightly more painful by comparison to an "O/U", single-shot, or pump/lever action shotgun. Gas-operated, and to a lesser extent recoil-operated, designs will recoil less than either. More "SxS" than "O/U" guns have traditional 'cast-off' stocks, where the end of the buttstock veers slightly to the right, allowing a right-handed user to point the gun more easily.

Double shotguns are also inherently more safe, as whether the shotgun is loaded or can be fired can be ascertained by anyone present if the action is broken open, for instance on a skeet, trap or hunting clays course when another shooter is firing; if the action is open, the gun cannot fire. Similarly, doubles are more easily examined to see if loaded than pump or semi-automatic shotguns, whose bolt must be opened and chamber closely examined or felt to make sure it is unloaded; with a double gun (or a break-action single gun), whether the gun is loaded, i.e., has cartridges in any chamber, is easily and immediately seen with a glance (and just as easily unloaded).

The early doubles used two triggers, one for each barrel. These were located front to back inside the trigger guard, the index finger being used to pull either trigger, as having two fingers inside the trigger guard can cause a very undesirable recoil induced double-discharge. Double trigger designs are typically set up for right-handed users. In double trigger designs, it is often possible to pull both triggers at once, firing both barrels simultaneously, though this is generally not recommended as it doubles the recoil, battering both shotgun and shooter, particularly if it was unanticipated or unintended. Discharging both barrels at the same time has long been a hunting trick employed by hunters using 8 gauge "elephant" shotguns, firing the two two-ounce slugs for sheer stopping power at close range.

Later models use a single trigger that alternately fires both barrels, called a "single selective trigger" or "SST". The SST does not allow firing both barrels at once, since the single trigger must be pulled twice in order to fire both barrels. The change from one barrel to the other may be done by a clockwork type system, where a cam alternates between barrels, or by an inertial system where the recoil of firing the first barrel toggles the trigger to the next barrel. A double-barreled shotgun with an inertial trigger works best with full power shotshells; shooting low recoil shotshells often will not reliably toggle the inertial trigger, causing an apparent failure to fire occasionally when attempting to depress the trigger a second time to fire the second barrel (this also can happen if the first shell fails to fire). Generally there is a method of selecting the order in which the barrels of an SST shotgun fire; commonly this is done through manipulation of the safety, pushing to one side to select top barrel first and the other side to select bottom barrel first. In the event that an inertial trigger does not toggle to the second barrel when firing low recoil shotshells, manually selecting the order to the second barrel will enable the second barrel to fire when the trigger is depressed again.

One of the advantages of the double, with double triggers or SST, is that a second shot can be taken almost immediately after the first, without removing the gun from the firing position on the shoulder and without any other action than a second trigger pull, utilizing different chokes for the two shots. (Assuming, of course, that full power shotshells are fired, at least for a double-barreled shotgun with an inertial type SST, as needed to toggle the inertial trigger.) This can be noticeably faster than a pump shotgun, which requires pumping to eject and reload for the second shot, and may be faster, or not slower, than a semi-automatic action. Note, however, in neither the pump or semi-automatic will the second shot be a different choke pattern from the first shot, whereas for a double, the two shots are usually with different chokes. Thus, depending on the nature of the hunt, the appropriate choke for the shot is always at hand. For example, while field hunting flushing birds, the first shot is usually closer than the second because the bird flies away from the shooter; so, the more open choke (and barrel) would be better for the first shot, and if a second shot is needed, as the bird is flying away, the more closed (and thus longer distance of an effective shot pattern) choke (and barrel) is then appropriate. Conversely, on a driven hunt, where the birds are driven towards the shooter, the closed (longer effective distance) choke (and barrel) should be fired first, saving the open (closer effective distance) choke (and barrel) for the now-closer incoming bird. None of this is possible with single-barrel shotguns, only with a double, whether SxS or O/U.

"Regulation" is a term used for multi-barreled firearms that indicates how close to the same point of aim the barrels will shoot. Regulation is very important, because a poorly regulated gun may hit consistently with one barrel, but miss consistently with the other, making the gun nearly useless for anything requiring two shots. However, the short ranges and spread of shot provide a significant overlap, so a small error in regulation in a double is often too small to be noticed. Generally the shotguns are regulated to hit the point of aim at a given distance, usually the maximum expected range since that is the range at which a full choke is used, and where precise regulation matters most.

The double-barreled shotgun is seen as a weapon of prestige and authority in rural parts of India, where it is known as "Dunali" (literally "two pipes"). It is especially common in Bihar, Purvanchal, Uttar Pradesh, Haryana and Punjab.



</doc>
<doc id="7976" url="https://en.wikipedia.org/wiki?curid=7976" title="Dessert">
Dessert

Dessert () is a confectionery course that concludes an evening meal. The course usually consists of sweet foods, and possibly a beverage such as dessert wine or liqueur, but may include coffee, cheeses, nuts, or other savory items. In some parts of the world, such as much of central and western Africa, and most parts of China, there is no tradition of a dessert course to conclude a meal.

The term "dessert" can apply to many confections, such as cakes, tarts, cookies, biscuits, gelatins, pastries, ice creams, pies, puddings, custards, and sweet soups. Fruit is also commonly found in dessert courses because of its naturally occurring sweetness. Some cultures sweeten foods that are more commonly savory to create desserts.

The word "dessert" originated from the French word "desservir," meaning "to clear the table." Its first known use was in 1600, in a health education manual entitled "Naturall and artificial Directions for Health", which was written by William Vaughan. In his "A History of Dessert" (2013), Michael Krondl explains it refers to the fact dessert was served after the table had been cleared of other dishes. The term dates from the 14th century but attained its current meaning around the beginning of the 20th century when "service à la française" (setting a variety of dishes on the table at the same time) was replaced with "service à la russe" (presenting a meal in courses.)"

The word "dessert" is most commonly used for this course in the United States, Canada, Australia, New Zealand and Ireland while "pudding" is more commonly used in the United Kingdom. Alternatives such as "sweets" or "afters" are also used in the United Kingdom and some other Commonwealth countries, including Hong Kong, and India.

Sweets were fed to the gods in ancient Mesopotamia and India and other ancient civilizations. Dried fruit and honey were probably the first sweeteners used in most of the world, but the spread of sugarcane around the world was essential to the development of dessert.

Sugarcane was grown and refined in India before 500 BCE and was crystallized, making it easy to transport, by 500 CE. Sugar and sugarcane were traded, making sugar available to Macedonia by 300 BCE and China by 600 CE. In South Asia, the Middle East and China, sugar has been a staple of cooking and desserts for over a thousand years. Sugarcane and sugar were little known and rare in Europe until the twelfth century or later, when the Crusades and then colonialization spread its use.

Herodotus mentions that, as opposed to the Greeks, the main Persian meal was simple, but they would eat many desserts afterwards.

Europeans began to manufacture sugar in the Middle Ages, and more sweet desserts became available. Even then sugar was so expensive usually only the wealthy could indulge on special occasions. The first apple pie recipe was published in 1381. The earliest documentation of the term "cupcake" was in "Seventy-five Receipts for Pastry, Cakes, and Sweetmeats" in 1828 in Eliza Leslie's "Receipts" cookbook.

The Industrial Revolution in Europe and later America caused desserts (and food in general) to be mass-produced, processed, preserved, canned, and packaged. Frozen foods, including desserts, became very popular starting in the 1920s when freezing emerged. These processed foods became a large part of diets in many industrialized nations. Many countries have desserts and foods distinctive to their nations or region.

Sweet desserts usually contain cane sugar, palm sugar, honey or some types of syrup such as molasses, maple syrup, treacle, or corn syrup. Other common ingredients in Western-style desserts are flour or other starches, Cooking fats such as butter or lard, dairy, eggs, salt, acidic ingredients such as lemon juice, and spices and other flavoring agents such as chocolate, peanut butter, fruits, and nuts. The proportions of these ingredients, along with the preparation methods, play a major part in the consistency, texture, and flavor of the end product.

Sugars contribute moisture and tenderness to baked goods. Flour or starch components serves as a protein and gives the dessert structure. Fats contribute moisture and can enable the development of flaky layers in pastries and pie crusts. The dairy products in baked goods keep the desserts moist. Many desserts also contain eggs, in order to form custard or to aid in the rising and thickening of a cake-like substance. Egg yolks specifically contribute to the richness of desserts. Egg whites can act as a leavening agent or provide structure. Further innovation in the healthy eating movement has led to more information being available about vegan and gluten-free substitutes for the standard ingredients, as well as replacements for refined sugar. Desserts can contain many spices and extracts to add a variety of flavors. Salt and acids are added to desserts to balance sweet flavors and create a contrast in flavors.

Some desserts are made with coffee,a coffee-flavoured version of a dessert can be made, for example an iced coffee soufflé or coffee biscuits. Alcohol can also be used as an ingredient, to make alcoholic desserts.

Dessert consist of variations of flavors, textures, and appearances. Desserts can be defined as a usually sweeter course that concludes a meal. This definition includes a range of courses ranging from fruits or dried nuts to multi-ingredient cakes and pies. Many cultures have different variations of dessert. In modern times the variations of desserts have usually been passed down or come from geographical regions. This is one cause for the variation of desserts. These are some major categories in which desserts can be placed.

Biscuits, (from the Old French word "bescuit" originally meaning "twice-baked" in Latin, also known as "cookies" in North America, are flattish bite-sized or larger short pastries generally intended to be eaten out of the hand. Biscuits can have a texture that is crispy, chewy, or soft. Examples include layered bars, crispy meringues, and soft chocolate chip cookies.

Cakes are sweet tender breads made with sugar and delicate flour. Cakes can vary from light, airy sponge cakes to dense cakes with less flour. Common flavourings include dried, candied or fresh fruit, nuts, cocoa or extracts. They may be filled with fruit preserves or dessert sauces (like pastry cream), iced with buttercream or other icings, and decorated with marzipan, piped borders, or candied fruit. Cake is often served as a celebratory dish on ceremonial occasions, for example weddings, anniversaries, and birthdays. Small-sized cakes have become popular, in the form of cupcakes and petits fours.

Chocolate is a typically sweet, usually brown, food preparation of "Theobroma cacao" seeds, roasted, ground, and often flavored. Pure, unsweetened chocolate contains primarily cocoa solids and cocoa butter in varying proportions. Much of the chocolate currently consumed is in the form of sweet chocolate, combining chocolate with sugar. Milk chocolate is sweet chocolate that additionally contains milk powder or condensed milk. White chocolate contains cocoa butter, sugar, and milk, but no cocoa solids. Dark chocolate is produced by adding fat and sugar to the cacao mixture, with no milk or much less than milk chocolate.
Candy, also called sweets or lollies, is a confection that features sugar as a principal ingredient. Many candies involve the crystallization of sugar which varies the texture of sugar crystals. Candies comprise many forms including caramel, marshmallows, and taffy.

These kinds of desserts usually include a thickened dairy base. Custards are cooked and thickened with eggs. Baked custards include crème brûlée and flan. Puddings are thickened with starches such as cornstarch or tapioca. Custards and puddings are often used as ingredients in other desserts, for instance as a filling for pastries or pies.

Many cuisines include a dessert made of deep-fried starch-based batter or dough. In many countries a doughnut is a flour-based batter that has been deep-fried. It is sometimes filled with custard or jelly. Fritters are fruit pieces in a thick batter that have been deep fried. Gulab jamun is an Indian dessert made of milk solids kneaded into a dough, deep-fried, and soaked in honey. Churros are a deep-fried and sugared dough that is eaten as dessert or a snack in many countries. Doughnuts are most famous for being a trademark favorite of fictional character Homer Simpson from the animated television series "The Simpsons".

Ice cream, gelato, sorbet and shaved-ice desserts fit into this category. Ice cream is a cream base that is churned as it is frozen to create a creamy consistency. Gelato uses a milk base and has less air whipped in than ice cream, making it denser. Sorbet is made from churned fruit and is not dairy based. Shaved-ice desserts are made by shaving a block of ice and adding flavored syrup or juice to the ice shavings.

Jellied desserts are made with a sweetened liquid thickened with gelatin or another thickening agent. They are traditional in many cultures. Grass jelly and annin tofu are Chinese jellied desserts. Yōkan is a Japanese jellied dessert. In English-speaking countries, many dessert recipes are based on gelatin with fruit or whipped cream added.

 Pastries are sweet baked pastry products. Pastries can either take the form of light and flaky bread with an airy texture, such as a croissant or unleavened dough with a high fat content and crispy texture, such as shortbread. Pastries are often flavored or filled with fruits, chocolate, nuts, and spices. Pastries are sometimes eaten with tea or coffee as a breakfast food.

Pies and cobblers are a crust with a filling. The crust can be either made from either a pastry or crumbs. Pie fillings range from fruits to puddings; cobbler fillings are generally fruit-based. Clafoutis are a batter with fruit-based filling poured over the top before baking.

Tong sui, literally translated as "sugar water" and also known as tim tong, is a collective term for any sweet, warm soup or custard served as a dessert at the end of a meal in Cantonese cuisine. "Tong sui" are a Cantonese specialty and are rarely found in other regional cuisines of China. Outside of Cantonese-speaking communities, soupy desserts generally are not recognized as a distinct category, and the term "tong sui" is not used.

Dessert wines are sweet wines typically served with dessert. There is no simple definition of a dessert wine. In the UK, a dessert wine is considered to be any sweet wine drunk with a meal, as opposed to the white fortified wines (fino and amontillado sherry) drunk before the meal, and the red fortified wines (port and madeira) drunk after it. Thus, most fortified wines are regarded as distinct from dessert wines, but some of the less strong fortified white wines, such as Pedro Ximénez sherry and Muscat de Beaumes-de-Venise, are regarded as honorary dessert wines. In the United States, by contrast, a dessert wine is legally defined as any wine over 14% alcohol by volume, which includes all fortified wines - and is taxed at higher rates as a result. Examples include Sauternes and Tokaji Aszú.

Throughout much of central and western Africa, there is no tradition of a dessert course following a meal. Fruit or fruit salad would be eaten instead, which may be spiced, or sweetened with a sauce. In some former colonies in the region, the colonial power has influenced desserts – for example, the Angolan "cocada amarela" (yellow coconut) resembles baked desserts in Portugal.

In Asia, desserts are often eaten between meals as snacks rather than as a concluding course. There is widespread use of rice flour in East Asian desserts, which often include local ingredients such as coconut milk, palm sugar, and tropical fruit. In India, where sugarcane has been grown and refined since before 500 BCE, desserts have been an important part of the diet for thousands of years; types of desserts include burfis, halvahs, jalebis, and laddus.

Dessert nowadays are made into drinks as well, such as Bubble Tea. It is originated in Taiwan, which locates in East Asia. Bubble tea is a kind of dessert made with flavor tea or milk with tapioca. It is well-known across the world.

In Ukraine and Russia, breakfast foods such as nalysnyky or blintz or oladi (pancakes), and syrniki are served with honey and jam as desserts.

European colonization of the Americas yielded the introduction of a number of ingredients and cooking styles. The various styles continued expanding well into the 19th and 20th centuries, proportional to the influx of immigrants.

Dulce de leche is a very common confection in Argentina. In Bolivia, sugarcane, honey and coconut are traditionally used in desserts. "Tawa tawa" is a Bolivian sweet fritter prepared using sugar cane, and "helado de canela" is a dessert that is similar to sherbet which is prepared with cane sugar and cinnamon. Coconut tarts, puddings cookies and candies are also consumed in Bolivia. Brazil has a variety of candies such as brigadeiros (chocolate fudge balls), cocada (a coconut sweet), beijinhos (coconut truffles and clove) and romeu e julieta (cheese with a guava jam known as goiabada). Peanuts are used to make paçoca, rapadura and pé-de-moleque. Local common fruits are turned in juices and used to make chocolates, popsicles and ice cream. In Chile, "kuchen" has been described as a "trademark dessert." Several desserts in Chile are prepared with "manjar", (caramelized milk), including "alfajor", "flan", "cuchufli" and "arroz con leche". Desserts consumed in Colombia include dulce de leche, waffle cookies, puddings, nougat, coconut with syrup and thickened milk with sugarcane syrup. Desserts in Ecuador tend to be simple, and desserts are a moderate part of the cuisine. Desserts consumed in Ecuador include tres leches cake, flan, candies and various sweets.

Desserts are typically eaten in Australia, and most daily meals "end with simple desserts," which can include various fruits. More complex desserts include cakes, pies and cookies, which are sometimes served during special occasions.

The market for desserts has grown over the last few decades, which was greatly increased by the commercialism of baking desserts and the rise of food productions. Desserts are present in most restaurants as the popularity has increased. Many commercial stores have been established as solely desserts stores. Ice cream parlors have been around since before 1800. Many businesses started advertising campaigns focusing solely on desserts. The tactics used to market desserts are very different depending on the audience for example desserts can be advertised with popular movie characters to target children. The rise of companies like Food Network has marketed many shows which feature dessert and their creation. Shows like these have displayed extreme desserts and made a game show atmosphere which made desserts a more competitive field.

Desserts are a standard staple in restaurant menus, with different degrees of variety. Pie and cheesecake were among the most popular dessert courses ordered in U.S. restaurants in 2012.

Dessert foods often contain relatively high amounts of sugar and fats and, as a result, higher calorie counts per gram than other foods. Fresh or cooked fruit with minimal added sugar or fat is an exception.




</doc>
<doc id="7978" url="https://en.wikipedia.org/wiki?curid=7978" title="Data Encryption Standard">
Data Encryption Standard

The Data Encryption Standard (DES ) is a symmetric-key algorithm for the encryption of electronic data. Although insecure, it was highly influential in the advancement of modern cryptography.

Developed in the early 1970s at IBM and based on an earlier design by Horst Feistel, the algorithm was submitted to the National Bureau of Standards (NBS) following the agency's invitation to propose a candidate for the protection of sensitive, unclassified electronic government data. In 1976, after consultation with the National Security Agency (NSA), the NBS eventually selected a slightly modified version (strengthened against differential cryptanalysis, but weakened against brute-force attacks), which was published as an official Federal Information Processing Standard (FIPS) for the United States in 1977.

The publication of an NSA-approved encryption standard simultaneously resulted in its quick international adoption and widespread academic scrutiny. Controversies arose out of classified design elements, a relatively short key length of the symmetric-key block cipher design, and the involvement of the NSA, nourishing suspicions about a backdoor. Today it is known that the S-boxes that had raised those suspicions were in fact designed by the NSA to actually remove a backdoor they secretly knew (differential cryptanalysis). However, the NSA also ensured that the key size was drastically reduced such that they could break it by brute force attack. The intense academic scrutiny the algorithm received over time led to the modern understanding of block ciphers and their cryptanalysis.

DES is insecure. This is mainly due to the 56-bit key size being too small. In January 1999, distributed.net and the Electronic Frontier Foundation collaborated to publicly break a DES key in 22 hours and 15 minutes (see chronology). There are also some analytical results which demonstrate theoretical weaknesses in the cipher, although they are infeasible to mount in practice. The algorithm is believed to be practically secure in the form of Triple DES, although there are theoretical attacks. This cipher has been superseded by the Advanced Encryption Standard (AES). Furthermore, DES has been withdrawn as a standard by the National Institute of Standards and Technology.

Some documentation makes a distinction between DES as a standard and as an algorithm, referring to the algorithm as the DEA (Data Encryption Algorithm).

The origins of DES go back to the early 1970s. In 1972, after concluding a study on the US government's computer security needs, the US standards body NBS (National Bureau of Standards)—now named NIST (National Institute of Standards and Technology)—identified a need for a government-wide standard for encrypting unclassified, sensitive information. Accordingly, on 15 May 1973, after consulting with the NSA, NBS solicited proposals for a cipher that would meet rigorous design criteria. None of the submissions, however, turned out to be suitable. A second request was issued on 27 August 1974. This time, IBM submitted a candidate which was deemed acceptable—a cipher developed during the period 1973–1974 based on an earlier algorithm, Horst Feistel's Lucifer cipher. The team at IBM involved in cipher design and analysis included Feistel, Walter Tuchman, Don Coppersmith, Alan Konheim, Carl Meyer, Mike Matyas, Roy Adler, Edna Grossman, Bill Notz, Lynn Smith, and Bryant Tuckerman.

On 17 March 1975, the proposed DES was published in the "Federal Register". Public comments were requested, and in the following year two open workshops were held to discuss the proposed standard. There was some criticism from various parties, including from public-key cryptography pioneers Martin Hellman and Whitfield Diffie, citing a shortened key length and the mysterious "S-boxes" as evidence of improper interference from the NSA. The suspicion was that the algorithm had been covertly weakened by the intelligence agency so that they—but no-one else—could easily read encrypted messages. Alan Konheim (one of the designers of DES) commented, "We sent the S-boxes off to Washington. They came back and were all different." The United States Senate Select Committee on Intelligence reviewed the NSA's actions to determine whether there had been any improper involvement. In the unclassified summary of their findings, published in 1978, the Committee wrote:

However, it also found that

Another member of the DES team, Walter Tuchman, stated "We developed the DES algorithm entirely within IBM using IBMers. The NSA did not dictate a single wire!"
In contrast, a declassified NSA book on cryptologic history states:

and
Some of the suspicions about hidden weaknesses in the S-boxes were allayed in 1990, with the independent discovery and open publication by Eli Biham and Adi Shamir of differential cryptanalysis, a general method for breaking block ciphers. The S-boxes of DES were much more resistant to the attack than if they had been chosen at random, strongly suggesting that IBM knew about the technique in the 1970s. This was indeed the case; in 1994, Don Coppersmith published some of the original design criteria for the S-boxes. According to Steven Levy, IBM Watson researchers discovered differential cryptanalytic attacks in 1974 and were asked by the NSA to keep the technique secret. Coppersmith explains IBM's secrecy decision by saying, "that was because [differential cryptanalysis] can be a very powerful tool, used against many schemes, and there was concern that such information in the public domain could adversely affect national security." Levy quotes Walter Tuchman: "[t]hey asked us to stamp all our documents confidential... We actually put a number on each one and locked them up in safes, because they were considered U.S. government classified. They said do it. So I did it". Bruce Schneier observed that "It took the academic community two decades to figure out that the NSA 'tweaks' actually improved the security of DES."

Despite the criticisms, DES was approved as a federal standard in November 1976, and published on 15 January 1977 as FIPS PUB 46, authorized for use on all unclassified data. It was subsequently reaffirmed as the standard in 1983, 1988 (revised as FIPS-46-1), 1993 (FIPS-46-2), and again in 1999 (FIPS-46-3), the latter prescribing "Triple DES" (see below). On 26 May 2002, DES was finally superseded by the Advanced Encryption Standard (AES), following a public competition. On 19 May 2005, FIPS 46-3 was officially withdrawn, but NIST has approved Triple DES through the year 2030 for sensitive government information.

The algorithm is also specified in ANSI X3.92 (Today X3 is known as INCITS and ANSI X3.92 as ANSI INCITS 92), NIST SP 800-67 and ISO/IEC 18033-3 (as a component of TDEA).

Another theoretical attack, linear cryptanalysis, was published in 1994, but it was the Electronic Frontier Foundation's DES cracker in 1998 that demonstrated that DES could be attacked very practically, and highlighted the need for a replacement algorithm. These and other methods of cryptanalysis are discussed in more detail later in this article.

The introduction of DES is considered to have been a catalyst for the academic study of cryptography, particularly of methods to crack block ciphers. According to a NIST retrospective about DES,

DES is the archetypal block cipher—an algorithm that takes a fixed-length string of plaintext bits and transforms it through a series of complicated operations into another ciphertext bitstring of the same length. In the case of DES, the block size is 64 bits. DES also uses a key to customize the transformation, so that decryption can supposedly only be performed by those who know the particular key used to encrypt. The key ostensibly consists of 64 bits; however, only 56 of these are actually used by the algorithm. Eight bits are used solely for checking parity, and are thereafter discarded. Hence the effective key length is 56 bits.

The key is nominally stored or transmitted as 8 bytes, each with odd parity. According to ANSI X3.92-1981 (Now, known as ANSI INCITS 92-1981), section 3.5:

Like other block ciphers, DES by itself is not a secure means of encryption, but must instead be used in a mode of operation. FIPS-81 specifies several modes for use with DES. Further comments on the usage of DES are contained in FIPS-74.

Decryption uses the same structure as encryption, but with the keys used in reverse order. (This has the advantage that the same hardware or software can be used in both directions.)

The algorithm's overall structure is shown in Figure 1: there are 16 identical stages of processing, termed "rounds". There is also an initial and final permutation, termed "IP" and "FP", which are inverses (IP "undoes" the action of FP, and vice versa). IP and FP have no cryptographic significance, but were included in order to facilitate loading blocks in and out of mid-1970s 8-bit based hardware.

Before the main rounds, the block is divided into two 32-bit halves and processed alternately; this criss-crossing is known as the Feistel scheme. The Feistel structure ensures that decryption and encryption are very similar processes—the only difference is that the subkeys are applied in the reverse order when decrypting. The rest of the algorithm is identical. This greatly simplifies implementation, particularly in hardware, as there is no need for separate encryption and decryption algorithms.

The ⊕ symbol denotes the exclusive-OR (XOR) operation. The "F-function" scrambles half a block together with some of the key. The output from the F-function is then combined with the other half of the block, and the halves are swapped before the next round. After the final round, the halves are swapped; this is a feature of the Feistel structure which makes encryption and decryption similar processes.

The F-function, depicted in Figure 2, operates on half a block (32 bits) at a time and consists of four stages:


The alternation of substitution from the S-boxes, and permutation of bits from the P-box and E-expansion provides so-called "confusion and diffusion" respectively, a concept identified by Claude Shannon in the 1940s as a necessary condition for a secure yet practical cipher.

Figure 3 illustrates the "key schedule" for encryption—the algorithm which generates the subkeys. Initially, 56 bits of the key are selected from the initial 64 by "Permuted Choice 1" ("PC-1")—the remaining eight bits are either discarded or used as parity check bits. The 56 bits are then divided into two 28-bit halves; each half is thereafter treated separately. In successive rounds, both halves are rotated left by one or two bits (specified for each round), and then 48 subkey bits are selected by "Permuted Choice 2" ("PC-2")—24 bits from the left half, and 24 from the right. The rotations (denoted by "«<" in the diagram) mean that a different set of bits is used in each subkey; each bit is used in approximately 14 out of the 16 subkeys.

The key schedule for decryption is similar—the subkeys are in reverse order compared to encryption. Apart from that change, the process is the same as for encryption. The same 28 bits are passed to all rotation boxes.

Although more information has been published on the cryptanalysis of DES than any other block cipher, the most practical attack to date is still a brute-force approach. Various minor cryptanalytic properties are known, and three theoretical attacks are possible which, while having a theoretical complexity less than a brute-force attack, require an unrealistic number of known or chosen plaintexts to carry out, and are not a concern in practice.

For any cipher, the most basic method of attack is brute force—trying every possible key in turn. The length of the key determines the number of possible keys, and hence the feasibility of this approach. For DES, questions were raised about the adequacy of its key size early on, even before it was adopted as a standard, and it was the small key size, rather than theoretical cryptanalysis, which dictated a need for a replacement algorithm. As a result of discussions involving external consultants including the NSA, the key size was reduced from 128 bits to 56 bits to fit on a single chip.

In academia, various proposals for a DES-cracking machine were advanced. In 1977, Diffie and Hellman proposed a machine costing an estimated US$20 million which could find a DES key in a single day. By 1993, Wiener had proposed a key-search machine costing US$1 million which would find a key within 7 hours. However, none of these early proposals were ever implemented—or, at least, no implementations were publicly acknowledged. The vulnerability of DES was practically demonstrated in the late 1990s. In 1997, RSA Security sponsored a series of contests, offering a $10,000 prize to the first team that broke a message encrypted with DES for the contest. That contest was won by the DESCHALL Project, led by Rocke Verser, Matt Curtin, and Justin Dolske, using idle cycles of thousands of computers across the Internet. The feasibility of cracking DES quickly was demonstrated in 1998 when a custom DES-cracker was built by the Electronic Frontier Foundation (EFF), a cyberspace civil rights group, at the cost of approximately US$250,000 (see EFF DES cracker). Their motivation was to show that DES was breakable in practice as well as in theory: ""There are many people who will not believe a truth until they can see it with their own eyes. Showing them a physical machine that can crack DES in a few days is the only way to convince some people that they really cannot trust their security to DES."" The machine brute-forced a key in a little more than 2 days' worth of searching.

The next confirmed DES cracker was the COPACOBANA machine built in 2006 by teams of the Universities of Bochum and Kiel, both in Germany. Unlike the EFF machine, COPACOBANA consists of commercially available, reconfigurable integrated circuits. 120 of these field-programmable gate arrays (FPGAs) of type XILINX Spartan-3 1000 run in parallel. They are grouped in 20 DIMM modules, each containing 6 FPGAs. The use of reconfigurable hardware makes the machine applicable to other code breaking tasks as well. One of the more interesting aspects of COPACOBANA is its cost factor. One machine can be built for approximately $10,000. The cost decrease by roughly a factor of 25 over the EFF machine is an example of the continuous improvement of digital hardware—see Moore's law. Adjusting for inflation over 8 years yields an even higher improvement of about 30x. Since 2007, SciEngines GmbH, a spin-off company of the two project partners of COPACOBANA has enhanced and developed successors of COPACOBANA. In 2008 their COPACOBANA RIVYERA reduced the time to break DES to less than one day, using 128 Spartan-3 5000's. SciEngines RIVYERA held the record in brute-force breaking DES, having utilized 128 Spartan-3 5000 FPGAs. Their 256 Spartan-6 LX150 model has further lowered this time.

In 2012, David Hutton and Moxie Marlinspike announced a system with 48 Xilinx Virtex-6 LX240T FPGAs, each FPGA containing 40 fully pipelined DES cores running at 400 MHz, for a total capacity of 768 gigakeys/sec. The system can exhaustively search the entire 56-bit DES key space in about 26 hours and this service is offered for a fee online.

There are three attacks known that can break the full 16 rounds of DES with less complexity than a brute-force search: differential cryptanalysis (DC), linear cryptanalysis (LC), and Davies' attack. However, the attacks are theoretical and are generally considered infeasible to mount in practice; these types of attack are sometimes termed certificational weaknesses.


There have also been attacks proposed against reduced-round versions of the cipher, that is, versions of DES with fewer than 16 rounds. Such analysis gives an insight into how many rounds are needed for safety, and how much of a "security margin" the full version retains.

Differential-linear cryptanalysis was proposed by Langford and Hellman in 1994, and combines differential and linear cryptanalysis into a single attack. An enhanced version of the attack can break 9-round DES with 2 chosen plaintexts and has a 2 time complexity (Biham and others, 2002).

DES exhibits the complementation property, namely that
where formula_2 is the bitwise complement of formula_3 formula_4 denotes encryption with key formula_5 formula_6 and formula_7 denote plaintext and ciphertext blocks respectively. The complementation property means that the work for a brute-force attack could be reduced by a factor of 2 (or a single bit) under a chosen-plaintext assumption. By definition, this property also applies to TDES cipher.

DES also has four so-called "weak keys". Encryption ("E") and decryption ("D") under a weak key have the same effect (see involution):
There are also six pairs of "semi-weak keys". Encryption with one of the pair of semiweak keys, formula_10, operates identically to decryption with the other, formula_11:
It is easy enough to avoid the weak and semiweak keys in an implementation, either by testing for them explicitly, or simply by choosing keys randomly; the odds of picking a weak or semiweak key by chance are negligible. The keys are not really any weaker than any other keys anyway, as they do not give an attack any advantage.

DES has also been proved not to be a group, or more precisely, the set formula_14 (for all possible keys formula_15) under functional composition is not a group, nor "close" to being a group. This was an open question for some time, and if it had been the case, it would have been possible to break DES, and multiple encryption modes such as Triple DES would not increase the security, because encryption under one key would be equivalent to decryption under another key.

Simplified DES (SDES) was designed for educational purposes only, to help students learn about modern cryptanalytic techniques.
SDES has similar properties and structure as DES, but has been simplified to make it much easier to perform encryption and decryption by hand with pencil and paper.
Some people feel that learning SDES gives insight into DES and other block ciphers, and insight into various cryptanalytic attacks against them.

Concerns about security and the relatively slow operation of DES in software motivated researchers to propose a variety of alternative block cipher designs, which started to appear in the late 1980s and early 1990s: examples include RC5, Blowfish, IDEA, NewDES, SAFER, CAST5 and FEAL. Most of these designs kept the 64-bit block size of DES, and could act as a "drop-in" replacement, although they typically used a 64-bit or 128-bit key. In the Soviet Union the GOST 28147-89 algorithm was introduced, with a 64-bit block size and a 256-bit key, which was also used in Russia later.

DES itself can be adapted and reused in a more secure scheme. Many former DES users now use Triple DES (TDES) which was described and analysed by one of DES's patentees (see FIPS Pub 46-3); it involves applying DES three times with two (2TDES) or three (3TDES) different keys. TDES is regarded as adequately secure, although it is quite slow. A less computationally expensive alternative is DES-X, which increases the key size by XORing extra key material before and after DES. GDES was a DES variant proposed as a way to speed up encryption, but it was shown to be susceptible to differential cryptanalysis.

On January 2, 1997, NIST announced that they wished to choose a successor to DES. In 2001, after an international competition, NIST selected a new cipher, the Advanced Encryption Standard (AES), as a replacement. The algorithm which was selected as the AES was submitted by its designers under the name Rijndael. Other finalists in the NIST AES competition included RC6, Serpent, MARS, and Twofish.




</doc>
<doc id="7983" url="https://en.wikipedia.org/wiki?curid=7983" title="Double-hulled tanker">
Double-hulled tanker

A double-hulled tanker refers to an oil tanker which has a double hull. They reduce the likelihood of leaks occurring than in single-hulled tankers, and their ability to prevent or reduce oil spills led to double hulls being standardized for oil tankers and other types of ships including by the International Convention for the Prevention of Pollution from Ships or MARPOL Convention. After the Exxon Valdez oil spill disaster in Alaska in 1989, the US Government required all new oil tankers built for use between US ports to be equipped with a full double hull.

A number of manufacturers have embraced oil tankers with a double hull because it strengthens the hull of ships, reducing the likelihood of oil disasters in low-impact collisions and groundings over single-hull ships. They reduce the likelihood of leaks occurring at low speed impacts in port areas when the ship is under pilotage. Research of impact damage of ships has revealed that double-hulled tankers are unlikely to perforate both hulls in a collision, preventing oil from seeping out. However, for smaller tankers, U shaped tanks might be susceptible to "free flooding" across the double bottom and up to the outside water level each side of the cargo tank. Salvors prefer to salvage doubled-hulled tankers because they permit the use of air pressure to vacuum out the flood water. In the 1960s, collision proof double hulls for nuclear ships were extensively investigated, due to escalating concerns over nuclear accidents.

The ability of double-hulled tankers to prevent or reduce oil spills led to double hulls being standardized for other types of ships including oil tankers by the International Convention for the Prevention of Pollution from Ships or MARPOL Convention. In 1992, MARPOL was amended, making it "mandatory for tankers of 5,000 dwt and more ordered after 6 July 1993 to be fitted with double hulls, or an alternative design approved by IMO". However, in the aftermath of the Erika incident of the coast off France in December 1999, members of IMO adopted a revised schedule for the phase-out of single-hull tankers, which came into effect on 1 September 2003, with further amendments validated on 5 April 2005.

After the Exxon Valdez oil spill disaster, when that ship grounded on Bligh Reef outside the port of Valdez, Alaska in 1989, the US Government required all new oil tankers built for use between US ports to be equipped with a full double hull. However, the damage to the Exxon Valdez penetrated sections of the hull (the slops oil tanks, or slop tanks) that were protected by a double bottom, or partial double hull.

Although double-hulled tankers reduce the likelihood of ships grazing rocks and creating holes in the hull, a double hull does not protect against major, high-energy collisions or groundings which cause the majority of oil pollution, despite this being the reason that the double hull was mandated by United States legislation. Double-hulled tankers, if poorly designed, constructed, maintained and operated can be as problematic, if not more problematic than their single-hulled counterparts. Double-hulled tankers have a more complex design and structure than their single-hulled counterparts, which means that they require more maintenance and care in operating, which if not subject to responsible monitoring and policing, may cause problems. Double hulls often result in the weight of the hull increasing by at least 20%, and because the steel weight of doubled-hulled tanks should not be greater than that of single-hulled ships, the individual hull walls are typically thinner and theoretically less resistant to wear. Double hulls by no means eliminate the possibility of the hulls breaking apart. Due to the air space between the hulls, there is also a potential problem with volatile gases seeping out through worn areas of the internal hull, increasing the risk of an explosion.

Although several international conventions against pollution are in place, as of 2003 there was still no formal body setting international mandatory standards, although the International Safety Guide for Oil Tankers and Terminals (ISGOTT) does provide guidelines giving advice on optimum use and safety, such as recommending that ballast tanks are not entered while loaded with cargo, and that weekly samples are made of the atmosphere inside for hydrocarbon gas. Due to the difficulties of maintenance, ship builders have been competitive in producing double-hulled ships which are easier to inspect, such as ballast and cargo tanks which are easily accessible and easier to spot corrosion in the hull. The Tanker Structure Cooperative Forum (TSCF) published the "Guide to Inspection and Maintenance of Double-Hull Tanker Structures" in 1995 giving advice based on experience of operating double-hulled tankers.



</doc>
<doc id="7984" url="https://en.wikipedia.org/wiki?curid=7984" title="Drink">
Drink

A drink or beverage is a liquid intended for human consumption.

In addition to their basic function of satisfying thirst, drinks play important roles in human culture. Common types of drinks include plain drinking water, milk, juices, coffee, tea, hot chocolate and soft drinks. In addition, alcoholic drinks such as wine, beer, and liquor, which contain the drug ethanol, have been part of human culture and development for more than 8,000 years.

Non-alcoholic drinks often signify drinks that would normally contain alcohol, such as beer and wine, but are made with less than .5 percent alcohol by volume. The category includes drinks that have undergone an alcohol removal process such as non-alcoholic beers and de-alcoholized wines.

When the human body becomes dehydrated it experiences the sensation of "thirst". This craving of fluids results in an instinctive need to drink. Thirst is regulated by the hypothalamus in response to subtle changes in the body's electrolyte levels, and also as a result of changes in the volume of blood circulating. The complete elimination of drinks, i.e. water, from the body will result in death faster than the removal of any other substance. Water and milk have been basic drinks throughout history. As water is essential for life, it has also been the carrier of many diseases.

As society developed, new techniques were discovered to create drinks from the plants that were available in different areas. The earliest archaeological evidence of wine production yet found has been at sites in Georgia ( BCE) and Iran ( BCE). Beer may have been known in Neolithic Europe as far back as 3000 BCE, and was mainly brewed on a domestic scale. The invention of beer (and bread) has been argued to be responsible for humanity's ability to develop technology and build civilization. Tea likely originated in Yunnan, China during the Shang Dynasty (1500 BCE–1046 BCE) as a medicinal drink.

Drinking has been a large part of socialising throughout the centuries. In Ancient Greece, a social gathering for the purpose of drinking was known as a symposium, where watered down wine would be drunk. The purpose of these gatherings could be anything from serious discussions to direct indulgence. In Ancient Rome, a similar concept of a "convivium" took place regularly.

Many early societies considered alcohol a gift from the gods, leading to the creation of gods such as Dionysus. Other religions forbid, discourage, or restrict the drinking of alcoholic drinks for various reasons. In some regions with a dominant religion the production, sale, and consumption of alcoholic drinks is forbidden to everybody, regardless of religion.

Toasting is a method of honouring a person or wishing good will by taking a drink. Another tradition is that of the loving cup, at weddings or other celebrations such as sports victories a group will share a drink in a large receptacle, shared by everyone until empty.

In East Africa and Yemen, coffee was used in native religious ceremonies. As these ceremonies conflicted with the beliefs of the Christian church, the Ethiopian Church banned the secular consumption of coffee until the reign of Emperor Menelik II. The drink was also banned in Ottoman Turkey during the 17th century for political reasons and was associated with rebellious political activities in Europe.

A drink is a form of liquid which has been prepared for human consumption. The preparation can include a number of different steps, some prior to transport, others immediately prior to consumption.

Water is the chief constituent in all drinks, and the primary ingredient in most. Water is purified prior to drinking. Methods for purification include filtration and the addition of chemicals, such as chlorination. The importance of purified water is highlighted by the World Health Organisation, who point out 94% of deaths from diarrhea – the third biggest cause of infectious death worldwide at 1.8 million annually – could be prevented by improving the quality of the victim's environment, particularly safe water.

Pasteurisation is the process of heating a liquid for a period of time at a specified temperature, then immediately cooling. The process reduces the growth of micro-organisms within the liquid, thereby increasing the time before spoilage. It is primarily used on milk, which prior to pasteurisation is commonly infected with pathogenic bacteria and therefore the more likely than any other part of the common diet in the developed world to cause illness.

The process of extracting juice from fruits and vegetables can take a number of forms. Simple crushing of most fruits will provide a significant amount of liquid, though a more intense pressure can be applied to get the maximum amount of juice from the fruit. Both crushing and pressing are processes used in the production of wine.

Infusion is the process of extracting flavours from plant material by allowing the material to remain suspended within water. This process is used in the production of teas, herbal teas and can be used to prepare coffee (when using a coffee press).

The name is derived from the word "percolate" which means "to cause (a solvent) to pass through a permeable substance especially for extracting a soluble constituent".
In the case of coffee-brewing the solvent is water, the permeable substance is the coffee grounds, and the soluble constituents are the chemical compounds that give coffee its color, taste, aroma, and stimulating properties.

Carbonation is the process of dissolving carbon dioxide into a liquid, such as water.

Fermentation is a metabolic process that converts sugar to ethanol. Fermentation has been used by humans for the production of drinks since the Neolithic age. In winemaking, grape juice is combined with yeast in an anaerobic environment to allow the fermentation. The amount of sugar in the wine and the length of time given for fermentation determine the alcohol level and the sweetness of the wine.

When brewing beer, there are four primary ingredients – water, grain, yeast and hops. The grain is encouraged to germinate by soaking and drying in heat, a process known as malting. It is then milled before soaking again to create the sugars needed for fermentation. This process is known as mashing. Hops are added for flavouring, then the yeast is added to the mixture (now called wort) to start the fermentation process.

Distillation is a method of separating mixtures based on differences in volatility of components in a boiling liquid mixture. It is one of the methods used in the purification of water. It is also a method of producing spirits from milder alcoholic drinks.

An alcoholic mixed drink that contains two or more ingredients is referred to as a cocktail. Cocktails were originally a mixture of spirits, sugar, water, and bitters. The term is now often used for almost any mixed drink that contains alcohol, including mixers, mixed shots, etc. A cocktail today usually contains one or more kinds of spirit and one or more mixers, such as soda or fruit juice. Additional ingredients may be sugar, honey, milk, cream, and various herbs.

A non-alcoholic drink is one that contains little or no alcohol. This category includes low-alcohol beer, non-alcoholic wine, and apple cider if they contain less than 0.5% alcohol by volume. The term "soft drink" specifies the absence of alcohol in contrast to "hard drink" and "drink". The term "drink" is theoretically neutral, but often is used in a way that suggests alcoholic content. Drinks such as soda pop, sparkling water, iced tea, lemonade, root beer, fruit punch, milk, hot chocolate, tea, coffee, milkshakes, and tap water and energy drinks are all soft drinks.

Water is the world's most consumed drink, however, 97% of water on Earth is non-drinkable salt water. Fresh water is found in rivers, lakes, wetlands, groundwater, and frozen glaciers. Less than 1% of the Earth's fresh water supplies are accessible through surface water and underground sources which are cost effective to retrieve.

In western cultures, water is often drunk cold. In the Chinese culture, it is typically drunk hot.

Regarded as one of the "original" drinks, milk is the primary source of nutrition for babies. In many cultures of the world, especially the Western world, humans continue to consume dairy milk beyond infancy, using the milk of other animals (especially cattle, goats and sheep) as a drink. Plant milk, a general term for any milk-like product that is derived from a plant source, also has a long history of consumption in various countries and cultures. The most popular varieties internationally are soy milk, almond milk, rice milk and coconut milk.

Tea, the second most consumed drink in the world, is produced from infusing dried leaves of the "camellia sinensis" shrub, in boiling water. There are many ways in which tea is prepared for consumption: lemon or milk and sugar are among the most common additives worldwide. Other additions include butter and salt in Bhutan, Nepal, and Tibet; bubble tea in Taiwan; fresh ginger in Indonesia, Malaysia and Singapore; mint in North Africa and Senegal; cardamom in Central Asia; rum to make Jagertee in Central Europe; and coffee to make yuanyang in Hong Kong. Tea is also served differently from country to country: in China and Japan tiny cups are used to serve tea; in Thailand and the United States tea is often served cold (as "iced tea") or with a lot of sweetener; Indians boil tea with milk and a blend of spices as masala chai; tea is brewed with a samovar in Iran, Kashmir, Russia and Turkey; and in the Australian Outback it is traditionally brewed in a billycan.
Tea leaves can be processed in different ways resulting in a drink which appears and tastes different. Chinese yellow and green tea are steamed, roasted and dried; Oolong tea is semi-fermented and appears green-black and black teas are fully fermented.

Around the world, people refer to other herbal infusions as "teas"; it is also argued that these were popular long before the "Camellia sinensis" shrub was used for tea making. Leaves, flowers, roots or bark can be used to make a herbal infusion and can be bought fresh, dried or powdered.

Coffee is a brewed drink prepared from the roasted seeds of several species of an evergreen shrub of the genus "Coffea". The two most common sources of coffee beans are the highly regarded "Coffea arabica", and the "robusta" form of the hardier "Coffea canephora". Coffee plants are cultivated in more than 70 countries. Once ripe, coffee "berries" are picked, processed, and dried to yield the seeds inside. The seeds are then roasted to varying degrees, depending on the desired flavor, before being ground and brewed to create coffee.

Coffee is slightly acidic (pH 5.0–5.1) and can have a stimulating effect on humans because of its caffeine content. It is one of the most popular drinks in the world. It can be prepared and presented in a variety of ways. The effect of coffee on human health has been a subject of many studies; however, results have varied in terms of coffee's relative benefit.

Coffee cultivation first took place in southern Arabia; the earliest credible evidence of coffee-drinking appears in the middle of the 15th century in the Sufi shrines of Yemen.

Carbonated drinks refer to drinks which have carbon dioxide dissolved into them. This can happen naturally through fermenting and in natural water spas or artificially by the dissolution of carbon dioxide under pressure. The first commercially available artificially carbonated drink is believed to have been produced by Thomas Henry in the late 1770s.
Cola, orange, various roots, ginger, and lemon/lime are commonly used to create non-alcoholic carbonated drinks; sugars and preservatives may be added later.

The most consumed carbonated soft drinks are produced by three major global brands: Coca-Cola, PepsiCo and the Dr Pepper Snapple Group.

Fruit juice is a natural product that contains few or no additives. Citrus products such as orange juice and tangerine juice are familiar breakfast drinks, while grapefruit juice, pineapple, apple, grape, lime, and lemon juice are also common. Coconut water is a highly nutritious and refreshing juice. Many kinds of berries are crushed; their juices are mixed with water and sometimes sweetened. Raspberry, blackberry and currants are popular juices drinks but the percentage of water also determines their nutritive value. Grape juice allowed to ferment produces wine.

Fruits are highly perishable so the ability to extract juices and store them was of significant value. Some fruits are highly acidic and mixing them with water and sugars or honey was often necessary to make them palatable. Early storage of fruit juices was labor-intensive, requiring the crushing of the fruits and the mixing of the resulting pure juices with sugars before bottling.

Vegetable juices are usually served warm or cold. Different types of vegetables can be used to make vegetable juice such as carrots, tomatoes, cucumbers, celery and many more. Some vegetable juices are mixed with some fruit juice to make the vegetable juice taste better. Many popular vegetable juices, particularly ones with high tomato content, are high in sodium, and therefore consumption of them for health must be carefully considered. Some vegetable juices provide the same health benefits as whole vegetables in terms of reducing risks of cardiovascular disease and cancer.

A drink is considered "alcoholic" if it contains ethanol, commonly known as alcohol (although in chemistry the definition of "alcohol" includes many other compounds). Beer has been a part of human culture for 8,000 years.

In many countries, imbibing alcoholic drinks in a local bar or pub is a cultural tradition.

Beer is an alcoholic drink produced by the saccharification of starch and fermentation of the resulting sugar. The starch and saccharification enzymes are often derived from malted cereal grains, most commonly malted barley and malted wheat. Most beer is also flavoured with hops, which add bitterness and act as a natural preservative, though other flavourings such as herbs or fruit may occasionally be included. The preparation of beer is called brewing. Beer is the world's most widely consumed alcoholic drink, and is the third-most popular drink overall, after water and tea. It is said to have been discovered by goddess Ninkasi around 5300 BCE, when she accidentally discovered yeast after leaving grain in jars that were later rained upon and left for several days. Women have been the chief creators of beer throughout history due to its association with domesticity and it, throughout much of history, being brewed in the home for family consumption. Only in recent history have men began to dabble in the field. It is thought by some to be the oldest fermented drink.

Some of humanity's earliest known writings refer to the production and distribution of beer: the Code of Hammurabi included laws regulating beer and beer parlours, and "The Hymn to Ninkasi", a prayer to the Mesopotamian goddess of beer, served as both a prayer and as a method of remembering the recipe for beer in a culture with few literate people. Today, the brewing industry is a global business, consisting of several dominant multinational companies and many thousands of smaller producers ranging from brewpubs to regional breweries.

Cider is a fermented alcoholic drink made from fruit juice, most commonly and traditionally apple juice, but also the juice of peaches, pears ("Perry" cider) or other fruit. Cider may be made from any variety of apple, but certain cultivars grown solely for use in cider are known as cider apples. The United Kingdom has the highest per capita consumption of cider, as well as the largest cider-producing companies in the world, , the U.K. produces 600 million litres of cider each year (130 million imperial gallons).

Wine is an alcoholic drink made from fermented grapes or other fruits. The natural chemical balance of grapes lets them ferment without the addition of sugars, acids, enzymes, water, or other nutrients. Yeast consumes the sugars in the grapes and converts them into alcohol and carbon dioxide. Different varieties of grapes and strains of yeasts produce different styles of wine. The well-known variations result from the very complex interactions between the biochemical development of the fruit, reactions involved in fermentation, terroir and subsequent appellation, along with human intervention in the overall process. The final product may contain tens of thousands of chemical compounds in amounts varying from a few percent to a few parts per billion.

Wines made from produce besides grapes are usually named after the product from which they are produced (for example, rice wine, pomegranate wine, apple wine and elderberry wine) and are generically called fruit wine. The term "wine" can also refer to starch-fermented or fortified drinks having higher alcohol content, such as barley wine, huangjiu, or sake.

Wine has a rich history dating back thousands of years, with the earliest production so far discovered having occurred  BC in Georgia. It had reached the Balkans by  BC and was consumed and celebrated in ancient Greece and Rome.

From its earliest appearance in written records, wine has also played an important role in religion. Red wine was closely associated with blood by the ancient Egyptians, who, according to Plutarch, avoided its free consumption as late as the 7th-century BC Saite dynasty, "thinking it to be the blood of those who had once battled against the gods". The Greek cult and mysteries of Dionysus, carried on by the Romans in their Bacchanalia, were the origins of western theater. Judaism incorporates it in the Kiddush and Christianity in its Eucharist, while alcohol consumption was forbidden in Islam.

Spirits are distilled beverages that contain no added sugar and have at least 20% alcohol by volume (ABV). Popular spirits include borovička, brandy, gin, rum, slivovitz, tequila, vodka, and whisky. Brandy is a spirit created by distilling wine, whilst vodka may be distilled from any starch- or sugar-rich plant matter; most vodka today is produced from grains such as sorghum, corn, rye or wheat.

Throughout history, people have come together in establishments to socialise whilst drinking. This includes cafés and coffeehouses, focus on providing hot drinks as well as light snacks. Many coffee houses in the Middle East, and in West Asian immigrant districts in the Western world, offer "shisha" ("nargile" in Turkish and Greek), flavored tobacco smoked through a hookah. Espresso bars are a type of coffeehouse that specialize in serving espresso and espresso-based drinks.

In China and Japan, the establishment would be a tea house, were people would socialise whilst drinking tea. Chinese scholars have used the teahouse for places of sharing ideas.

Alcoholic drinks are served in drinking establishments, which have different cultural connotations. For example, pubs are fundamental to the culture of Britain, Ireland, Australia, Canada, New England, Metro Detroit, South Africa and New Zealand. In many places, especially in villages, a pub can be the focal point of the community. The writings of Samuel Pepys describe the pub as the heart of England. Many pubs are controlled by breweries, so cask ale or keg beer may be a better value than wines and spirits.

In contrast, types of bars range from seedy bars or nightclubs, sometimes termed "dive bars", to elegant places of entertainment for the elite. Bars provide stools or chairs that are placed at tables or counters for their patrons. The term "bar" is derived from the specialized counter on which drinks are served. Some bars have entertainment on a stage, such as a live band, comedians, go-go dancers, or strippers. Patrons may sit or stand at the bar and be served by the bartender, or they may sit at tables and be served by cocktail servers.

Food and drink are often paired together to enhance the taste experience. This primarily happens with wine and a culture has grown up around the process. Weight, flavors and textures can either be contrasted or complemented. In recent years, food magazines began to suggest particular wines with recipes and restaurants would offer multi-course dinners matched with a specific wine for each course.

Different drinks have unique receptacles for their consumption. This is sometimes purely for presentations purposes, such as for cocktails. In other situations, the drinkware has practical application, such as coffee cups which are designed for insulation or brandy snifters which are designed to encourage evaporation but trap the aroma within the glass.

Many glasses include a stem, which allows the drinker to hold the glass without affecting the temperature of the drink. In champagne glasses, the bowl is designed to retain champagne's signature carbonation, by reducing the surface area at the opening of the bowl. Historically, champagne has been served in a champagne coupe, the shape of which allowed carbonation to dissipate even more rapidly than from a standard wine glass.

An important export commodity, coffee was the top agricultural export for twelve countries in 2004,
and it was the world's seventh-largest legal agricultural export by value in 2005. Green (unroasted) coffee is one of the most traded agricultural commodities in the world.

For the calendar year 2016, the United States beverage industry was valued at $24.1 billion with single-serve bottled water accounting for $900 million of the total. Carbonated beverages accounted for $81.6 billion in annual sales in the U.S.

Some drinks, such as wine, can be used as an alternative investment. This can be achieved by either purchasing and reselling individual bottles or cases of particular wines, or purchasing shares in an investment wine fund that pools investors' capital.




</doc>
<doc id="7985" url="https://en.wikipedia.org/wiki?curid=7985" title="Dill">
Dill

Dill ("Anethum graveolens") is an annual herb in the celery family Apiaceae. It is the only species in the genus "Anethum". Dill is widely grown in Eurasia where its leaves and seeds are used as a herb or spice for flavouring food.

Dill grows up to , with slender hollow stems and alternate, finely divided, softly delicate leaves long. The ultimate leaf divisions are broad, slightly broader than the similar leaves of fennel, which are threadlike, less than broad, but harder in texture. The flowers are white to yellow, in small umbels diameter. The seeds are long and thick, and straight to slightly curved with a longitudinally ridged surface.

The word "dill" and its close relatives are found in most of the Germanic languages; its ultimate origin is unknown. The generic name "Anethum" is the Latin form of the Greek ἄνῑσον / ἄνησον / ἄνηθον / ἄνητον, which meant both "dill" and "anise". The form "anīsum" came to be used for anise, "anēthum" for dill. The Latin word is the origin of dill's names in the Western Romance languages ("anet", "aneldo", etc.), and also of the obsolete English "anet". Most Slavic language names come from Proto-Slavic "*koprъ".

Fresh and dried dill leaves (sometimes called "dill weed" to distinguish it from dill seed) are widely used as herbs in Europe and central Asia.

Like caraway, the fernlike leaves of dill are aromatic and are used to flavor many foods such as gravlax (cured salmon) and other fish dishes, borscht and other soups, as well as pickles (where the dill flower is sometimes used). Dill is best when used fresh as it loses its flavor rapidly if dried; however, freeze-dried dill leaves retain their flavor relatively well for a few months.

Dill oil is extracted from the leaves, stems and seeds of the plant. The oil from the seeds is distilled and used in the manufacturing of soaps.

Dill is the eponymous ingredient in dill pickles.

In central and eastern Europe, Scandinavia, Baltic states, Russia, and Finland, dill is a popular culinary herb used in the kitchen along with chives or parsley. Fresh, finely cut dill leaves are used as topping in soups, especially the hot red borsht and the cold borsht mixed with curds, kefir, yoghurt, or sour cream, which is served during hot summer weather and is called okroshka. It is also popular in summer to drink fermented milk (curds, kefir, yoghurt, or buttermilk) mixed with dill (and sometimes other herbs).

In the same way, prepared dill is used as a topping for boiled potatoes covered with fresh butter – especially in summer when there are so-called "new", or young, potatoes. The dill leaves can be mixed with butter, making a dill butter, which can serve the same purpose. Dill leaves mixed with tvorog form one of the traditional cheese spreads used for sandwiches. Fresh dill leaves are used all year round as an ingredient in salads, "e.g.", one made of lettuce, fresh cucumbers and tomatoes, the way basil leaves are used in Italy and Greece.

Russian cuisine is noted for liberal use of dill. Its supposed antiflatulent activity caused some Russian cosmonauts to recommend it for manned spaceflight due to the confined quarters and closed air supply.

In Polish cuisine, fresh dill leaves mixed with sour cream are the basis for dressings. It is especially popular to use this kind of sauce with freshly cut cucumbers, which practically are wholly immersed in the sauce, making a salad called "mizeria". The dill leaves serve as a basis for cooking dill sauce, used hot for baked freshwater fish and for chicken or turkey breast, or used hot or cold for hard-boiled eggs. A dill-based soup (zupa koperkowa), served with potatoes and hard-boiled eggs, is also popular in Poland. Whole stems including roots and flower buds are traditionally used to prepare Polish-style pickled cucumbers (ogórki kiszone), especially the so-called low-salt cucumbers ("ogórki małosolne"). Whole stems of dill (often including the roots) are also cooked with potatoes, especially the potatoes of autumn and winter, so they resemble the flavor of the newer potatoes found in summer. Some kinds of fish, especially trout and salmon, are traditionally baked with the stems and leaves of dill.

In the Czech Republic, white dill sauce made of cream (or milk), butter, flour, vinegar and dill is called "koprová omáčka" (also "koprovka" or "kopračka") and is served either with boiled eggs and potatoes or with dumplings and boiled beef. Another Czech dish with dill is a soup called "kulajda" that contains mushrooms (traditionally wild ones).

In Germany, dill is popular as a seasoning for fish and many other dishes, chopped as a garnish on potatoes, and a flavoring in pickles.

In the UK, dill can be used in fish pie

In Romania dill ("mărar") is widely used as an ingredient for soups such as "borş" (pronounced "borsh"), pickles and other dishes, especially those based on peas, beans and cabbage. It is popular for dishes based on potatoes and mushrooms and can be found in many summer salads (especially cucumber salad, cabbage salad and lettuce salad). During springtime, it is used with spring onions in omelets. It often complements sauces based on sour cream or yogurt and is mixed with salted cheese and used as a filling. Another popular dish with dill as a main ingredient is dill sauce, which is served with eggs and fried sausages.

In Hungary, dill is very widely used. It is popular as a sauce or filling, and mixed with a type of cottage cheese. Dill is also used for pickling and in salads. The Hungarian name for dill is "kapor".

In Serbia, dill is known as "mirodjija" and is used as an addition to soups, potato and cucumber salads and French fries. It features in the Serbian proverb "бити мирођија у свакој чорби" /biti mirodjija u svakoj čorbi/ (to be a dill in every soup) which corresponds to the English proverb "to have a finger in every pie".

In Greece, dill is known as 'άνηθος' (anithos). In antiquity it was used as an add-in in wines, which they were called "anithites oinos" (wine with anithos-dill). In modern days, dill is used in salads, soups, sauces, and fish and vegetable dishes.

In Santa Maria, Azores, dill ("endro") is the most important ingredient of the traditional Holy Ghost soup ("sopa do Espírito Santo"). Dill is found practically everywhere in Santa Maria and is curiously rare in the other Azorean Islands.

In Sweden, dill is a common spice or herb. The top of fully grown dill is called "krondill" (English: Crown dill); this is used when cooking crayfish. The "krondill" is put into the water after the crayfish is boiled, but still in hot and salt water. Then the entire dish is stored in refrigerator for at least 24 hours before eating (with toasted bread and butter). "Krondill" is also used for cucumber pickles. Small cucumbers, sliced or not, are put into a solution of hot water, mild acetic vinegar (not made from wine and without colour), sugar and "krondill". After a month or two, the cucumber pickles are ready to eat, for instance, with pork, brown sauce and potatoes, as a "sweetener". The thinner part of dill and young plants may be used with boiled fresh potatoes (as the first potatoes for the year, which usually are small and have a very thin skin). It is used together with, or instead of other green herbs, like parsley, chives and basil, in salads. It is also often paired up with chives when used in food. Dill is often used to flavour fish and seafood in Sweden, for example gravlax and various herring pickles, among them the traditional sill i dill (literally "herring in dill"). In contrast to the various fish dishes flavoured with dill, there is also a traditional Swedish dish called dillkött, which is a meaty stew flavoured with dill. The dish commonly contains either pieces of veal or lamb that are boiled until tender and then served together with a vinegary dill sauce. Dill seeds may be used in breads or akvavit. A newer, non-traditional use of dill is paired up with chives as a flavouring of potato chips. This flavour of potato chips called "dillchips" is quite popular in Sweden.

In Iran, dill is known as "shevid" and is sometimes used with rice and called "shevid-polo". It is also used in Iranian "aash" recipes, and is also called "sheved" in Persian.

In India, dill is known as "Sholpa" in Bengali, "shepu" (शेपू) in Marathi and Konkani, "savaa" in Hindi or "soa" in Punjabi. In Telugu, it is called "Soa-kura" (for herb greens). It is also called "sabbasige soppu" (ಸಬ್ಬಸಿಗೆ ಸೊಪ್ಪು) in Kannada. In Tamil it is known as "sada kuppi"(சதகுப்பி). In Malayalam, it is ചതകുപ്പ ("chathakuppa") or ശതകുപ്പ ("sathakuppa"). In Sanskrit, this herb is called "shatapushpa". In Gujarati, it is known as "suva"(સૂવા). In India, dill is prepared in the manner of yellow "moong dal" as a main-course dish. It is considered to have very good antigas properties, so it is used as "mukhwas", or an after-meal digestive. It is also traditionally given to mothers immediately after childbirth. In the state of Uttar Pradesh in India, a smaller amount of fresh dill is cooked along with cut potatoes and fresh fenugreek leaves (Hindi आलू-मेथी-सोया).
In Manipur, dill, locally known as "pakhon", is an essential ingredient of "chagem pomba" – a traditional Manipuri dish made with fermented soybean and rice. In Sri Lanka dill is known in Sinhala as "maaduru".

In Laos and parts of northern Thailand, dill is known in English as Lao coriander (, ) and served as a side with salad yum or papaya salad. In the Lao language, it is called "phak see", and in Thai, it is known as "phak chee Lao". In Lao cuisine, Lao coriander is used extensively in traditional Lao dishes such as "mok pa" (steamed fish in banana leaf) and several coconut milk-based curries that contain fish or prawns.

In China dill is colloquially called "huíxiāng" (, perfums of Hui people), or more properly "shíluó" (). It is a common filling in baozi and xianbing and can be used vegetarian, with rice vermicelli, or combined with either meat or eggs. Vegetarian dill baozi are a common part of a Beijing breakfast. In baozi and xianbing, it is often interchangeable with non-bulbing fennel and the term can also refer to fennel, like caraway and coriander leaf share a name in Chinese as well. Dill is also stir fried as a potherb, often with egg, in the same manner as Chinese chives. It is commonly used in Taiwan as well. In Northern China, Beijing, Inner-Mongolia, Ningxia, Gansu, Xinjiang, dill seeds commonly called "zīrán" (), but also "kūmíng" (), "kūmíngzi" (), "shíluózi" (), "xiǎohuíxiāngzi" () are used with pepper for lamb meat. In the whole china, "yángchuàn" () or "yángròu chuàn" (), lamb brochette, a speciality from Uyghurs, uses cumin and pepper.

In Vietnam, the use of dill in cooking is regional; it is used mainly in northern Vietnamese cuisine.

In Arab countries, dill seed, called "ain jaradeh" (grasshopper's eye), is used as a spice in cold dishes such as "fattoush" and pickles. In Arab countries of the Persian Gulf, dill is called "shibint" and is used mostly in fish dishes. In Egypt, dillweed is commonly used to flavor cabbage dishes, including "mahshi koronb" (stuffed cabbage leaves).
In Israel, dill seed is used to spice in salads and also to flavor omelette alongside parsley, and is called "Shamir".

Successful cultivation requires warm to hot summers with high sunshine levels; even partial shade will reduce the yield substantially. It also prefers rich, well drained soil. The seeds are viable for three to ten years. The plants are somewhat monocarpic and quickly die after "bolting" (producing seeds). Hot temperatures can quicken bolting.

The seed is harvested by cutting the flower heads off the stalks when the seed is beginning to ripen. The seed heads are placed upside down in a paper bag and left in a warm, dry place for a week. The seeds then separate from the stems easily for storage in an airtight container.

These plants, like their fennel and parsley relatives, are often eaten by Black swallowtail caterpillars in areas where that species occurs. For this reason, they may be included in some butterfly gardens.

When used as a companion plant, dill attracts many beneficial insects as the umbrella flower heads go to seed. It makes a good companion plant for cucumbers and broccoli. It is a poor companion for carrots and tomatoes.






</doc>
<doc id="7988" url="https://en.wikipedia.org/wiki?curid=7988" title="Dual space">
Dual space

In mathematics, any vector space "V" has a corresponding dual vector space (or just dual space for short) consisting of all linear functionals on "V", together with the vector space structure of pointwise addition and scalar multiplication by constants.

The dual space as defined above is defined for all vector spaces, and to avoid ambiguity may also be called the "algebraic dual space". When defined for a topological vector space, there is a subspace of the dual space, corresponding to continuous linear functionals, called the "continuous dual space".

Dual vector spaces find application in many branches of mathematics that use vector spaces, such as in tensor analysis with finite-dimensional vector spaces. When applied to vector spaces of functions (which are typically infinite-dimensional), dual spaces are used to describe measures, distributions, and Hilbert spaces. Consequently, the dual space is an important concept in functional analysis.

Given any vector space "V" over a field "F", the (algebraic) dual space "V" (alternatively denoted by formula_1 or formula_2) is defined as the set of all linear maps (linear functionals). Since linear maps are vector space homomorphisms, the dual space is also sometimes denoted by Hom("V", "F"). The dual space "V" itself becomes a vector space over "F" when equipped with an addition and scalar multiplication satisfying:
for all "φ" and , , and . Elements of the algebraic dual space "V" are sometimes called covectors or one-forms.

The pairing of a functional "φ" in the dual space "V" and an element "x" of "V" is sometimes denoted by a bracket: 
or . This pairing defines a nondegenerate bilinear mapping called the natural pairing.

If "V" is finite-dimensional, then "V" has the same dimension as "V". Given a basis in "V", it is possible to construct a specific basis in "V", called the dual basis. This dual basis is a set of linear functionals on "V", defined by the relation
for any choice of coefficients . In particular, letting in turn each one of those coefficients be equal to one and the other coefficients zero, gives the system of equations
where formula_6 is the Kronecker delta symbol. For example, if "V" is R, and its basis chosen to be , then e and e are one-forms (functions that map a vector to a scalar) such that , , , and . (Note: The superscript here is the index, not an exponent).

In particular, if we interpret R as the space of columns of "n" real numbers, its dual space is typically written as the space of "rows" of "n" real numbers. Such a row acts on R as a linear functional by ordinary matrix multiplication. One way to see this is that a functional maps every "n"-vector "x" into a real number "y". Then, seeing this functional as a matrix "M", and "x", "y" as a matrix and a matrix (trivially, a real number) respectively, if we have , then, by dimension reasons, "M" must be a matrix, i.e., "M" must be a row vector.

If "V" consists of the space of geometrical vectors in the plane, then the level curves of an element of "V" form a family of parallel lines in "V", because the range is 1-dimensional, so that every point in the range is a multiple of any one nonzero element. So an element of "V" can be intuitively thought of as a particular family of parallel lines covering the plane. To compute the value of a functional on a given vector, one needs only to determine which of the lines the vector lies on. Or, informally, one "counts" how many lines the vector crosses. More generally, if "V" is a vector space of any dimension, then the level sets of a linear functional in "V" are parallel hyperplanes in "V", and the action of a linear functional on a vector can be visualized in terms of these hyperplanes.

If "V" is not finite-dimensional but has a basis e indexed by an infinite set "A", then the same construction as in the finite-dimensional case yields linearly independent elements e () of the dual space, but they will not form a basis.

Consider, for instance, the space R, whose elements are those sequences of real numbers that contain only finitely many non-zero entries, which has a basis indexed by the natural numbers N: for , e is the sequence consisting of all zeroes except in the "i"th position, which is "1". The dual space of R is (isomorphic to) R, the space of "all" sequences of real numbers: such a sequence ("a") is applied to an element ("x") of R to give the number ∑"ax", which is a finite sum because there are only finitely many nonzero "x". The dimension of R is countably infinite, whereas R does not have a countable basis.

This observation generalizes to any infinite-dimensional vector space "V" over any field "F": a choice of basis identifies "V" with the space ("F") of functions such that is nonzero for only finitely many , where such a function "f" is identified with the vector
in "V" (the sum is finite by the assumption on "f", and any may be written in this way by the definition of the basis).

The dual space of "V" may then be identified with the space "F" of "all" functions from "A" to "F": a linear functional "T" on "V" is uniquely determined by the values it takes on the basis of "V", and any function (with ) defines a linear functional "T" on "V" by
Again the sum is finite because "f" is nonzero for only finitely many "α".

Note that ("F") may be identified (essentially by definition) with the direct sum
of infinitely many copies of "F" (viewed as a 1-dimensional vector space over itself) indexed by "A", i.e., there are linear isomorphisms

On the other hand, "F" is (again by definition), the direct product of infinitely many copies of "F" indexed by "A", and so the identification
is a special case of a general result relating direct sums (of modules) to direct products.

Thus if the basis is infinite, then the algebraic dual space is "always" of larger dimension (as a cardinal number) than the original vector space. This is in contrast to the case of the continuous dual space, discussed below, which may be isomorphic to the original vector space even if the latter is infinite-dimensional.

If "V" is finite-dimensional, then "V" is isomorphic to "V". But there is in general no natural isomorphism between these two spaces. Any bilinear form on "V" gives a mapping of "V" into its dual space via

where the right hand side is defined as the functional on "V" taking each to . In other words, the bilinear form determines a linear mapping

defined by

If the bilinear form is nondegenerate, then this is an isomorphism onto a subspace of "V". If "V" is finite-dimensional, then this is an isomorphism onto all of "V". Conversely, any isomorphism formula_14 from "V" to a subspace of "V" (resp., all of "V" if "V" is finite dimensional) defines a unique nondegenerate bilinear form on "V" by

Thus there is a one-to-one correspondence between isomorphisms of "V" to subspaces of (resp., all of) "V" and nondegenerate bilinear forms on "V".

If the vector space "V" is over the complex field, then sometimes it is more natural to consider sesquilinear forms instead of bilinear forms. In that case, a given sesquilinear form determines an isomorphism of "V" with the complex conjugate of the dual space

The conjugate space "V" can be identified with the set of all additive complex-valued functionals such that

There is a natural homomorphism formula_18 from formula_19 into the double dual formula_20, defined byformula_21 for all formula_22. In other words, if formula_23 is the evaluation map defined by formula_24, then we define formula_25 as the map formula_26. This map formula_18 is always injective; it is an isomorphism if and only if formula_19 is finite-dimensional. Indeed, the isomorphism of a finite-dimensional vector space with its double dual is an archetypal example of a natural isomorphism. Note that infinite-dimensional Hilbert spaces are not a counterexample to this, as they are isomorphic to their continuous duals, not to their algebraic duals.

If is a linear map, then the "transpose" (or "dual") is defined by
for every . The resulting functional "f"("φ") in "V" is called the "pullback" of "φ" along "f".

The following identity holds for all and :
where the bracket [·,·] on the left is the natural pairing of "V" with its dual space, and that on the right is the natural pairing of "W" with its dual. This identity characterizes the transpose, and is formally similar to the definition of the adjoint.

The assignment produces an injective linear map between the space of linear operators from "V" to "W" and the space of linear operators from "W" to "V"; this homomorphism is an isomorphism if and only if "W" is finite-dimensional. If then the space of linear maps is actually an algebra under composition of maps, and the assignment is then an antihomomorphism of algebras, meaning that . In the language of category theory, taking the dual of vector spaces and the transpose of linear maps is therefore a contravariant functor from the category of vector spaces over "F" to itself. Note that one can identify ("f") with "f" using the natural injection into the double dual.

If the linear map "f" is represented by the matrix "A" with respect to two bases of "V" and "W", then "f" is represented by the transpose matrix "A" with respect to the dual bases of "W" and "V", hence the name. Alternatively, as "f" is represented by "A" acting on the left on column vectors, "f" is represented by the same matrix acting on the right on row vectors. These points of view are related by the canonical inner product on R, which identifies the space of column vectors with the dual space of row vectors.

Let "S" be a subset of "V". The annihilator of "S" in "V", denoted here "S", is the collection of linear functionals such that for all . That is, "S" consists of all linear functionals such that the restriction to "S" vanishes: .

The annihilator of a subset is itself a vector space. In particular, is all of "V" (vacuously), whereas is the zero subspace. Furthermore, the assignment of an annihilator to a subset of "V" reverses inclusions, so that if , then

Moreover, if "A" and "B" are two subsets of "V", then
and equality holds provided "V" is finite-dimensional. If "A" is any family of subsets of "V" indexed by "i" belonging to some index set "I", then
In particular if "A" and "B" are subspaces of "V", it follows that

If "V" is finite-dimensional, and "W" is a vector subspace, then
after identifying "W" with its image in the second dual space under the double duality isomorphism . Thus, in particular, forming the annihilator is a Galois connection on the lattice of subsets of a finite-dimensional vector space.

If "W" is a subspace of "V" then the quotient space "V"/"W" is a vector space in its own right, and so has a dual. By the first isomorphism theorem, a functional factors through "V"/"W" if and only if "W" is in the kernel of "f". There is thus an isomorphism
As a particular consequence, if "V" is a direct sum of two subspaces "A" and "B", then "V" is a direct sum of "A" and "B".

When dealing with topological vector spaces, one is typically only interested in the continuous linear functionals from the space into the base field formula_37 (or formula_38). This gives rise to the notion of the "continuous dual space" or "topological dual" which is a linear subspace of the algebraic dual space formula_39, denoted by formula_2. For any "finite-dimensional" normed vector space or topological vector space, such as Euclidean "n-"space, the continuous dual and the algebraic dual coincide. This is however false for any infinite-dimensional normed space, as shown by the example of discontinuous linear maps. Nevertheless, in the theory of topological vector spaces the terms "continuous dual space" and "topological dual space" are often replaced by "dual space", since there is no serious need to consider discontinuous maps in this field.

For a topological vector space formula_19 its "continuous dual space", or "topological dual space", or just "dual space" (in the sense of the theory of topological vector spaces) formula_2 is defined as the space of all continuous linear functionals formula_43.

There is a standard construction for introducing a topology on the continuous dual formula_2 of a topological vector space formula_19. Fix a collection formula_46 of bounded subsets of formula_19. Then one has the topology on formula_19 of uniform convergence on sets from formula_46, or what is the same thing, the topology generated by seminorms of the form 
where formula_51 is a continuous linear functional on formula_19, and formula_53 runs over the class formula_46.

This means that a net of functionals formula_55 tends to a functional formula_51 in formula_2 if and only if 
Usually (but not necessarily) the class formula_46 is supposed to satisfy the following conditions:

If these requirements are fulfilled then the corresponding topology on formula_2 is Hausdorff and the sets 
form its local base.

Here are the three most important special cases.



Each of these three choices of topology on formula_2 leads to a variant of reflexivity property for topological vector spaces.

Let 1 < "p" < ∞ be a real number and consider the Banach space "ℓ" of all sequences for which
is finite. Define the number "q" by . Then the continuous dual of "ℓ" is naturally identified with "ℓ": given an element , the corresponding element of is the sequence ("φ"(e)) where e denotes the sequence whose "n-"th term is 1 and all others are zero. Conversely, given an element , the corresponding continuous linear functional "φ" on is defined by for all (see Hölder's inequality).

In a similar manner, the continuous dual of is naturally identified with (the space of bounded sequences). Furthermore, the continuous duals of the Banach spaces "c" (consisting of all convergent sequences, with the supremum norm) and "c" (the sequences converging to zero) are both naturally identified with .

By the Riesz representation theorem, the continuous dual of a Hilbert space is again a Hilbert space which is anti-isomorphic to the original space. This gives rise to the bra–ket notation used by physicists in the mathematical formulation of quantum mechanics.

By the Riesz–Markov–Kakutani representation theorem, the continuous dual of certain spaces of continuous functions can be described using measures.

If is a continuous linear map between two topological vector spaces, then the (continuous) transpose is defined by the same formula as before:
The resulting functional is in. The assignment produces a linear map between the space of continuous linear maps from "V" to "W" and the space of linear maps from to . When "T" and "U" are composable continuous linear maps, then

When "V" and "W" are normed spaces, the norm of the transpose in is equal to that of "T" in. Several properties of transposition depend upon the Hahn–Banach theorem. For example, the bounded linear map "T" has dense range if and only if the transpose is injective.

When "T" is a compact linear map between two Banach spaces "V" and "W", then the transpose is compact. This can be proved using the Arzelà–Ascoli theorem.

When "V" is a Hilbert space, there is an antilinear isomorphism "i" from "V" onto its continuous dual. For every bounded linear map "T" on "V", the transpose and the adjoint operators are linked by

When "T" is a continuous linear map between two topological vector spaces "V" and "W", then the transpose is continuous when and are equipped with"compatible" topologies: for example when, for and , both duals have the strong topology of uniform convergence on bounded sets of "X", or both have the weak-∗ topology of pointwise convergence on "X". The transpose is continuous from to , or from to .

Assume that "W" is a closed linear subspace of a normed space "V", and consider the annihilator of "W" in,
Then, the dual of the quotient can be identified with "W", and the dual of "W" can be identified with the quotient . Indeed, let "P" denote the canonical surjection from "V" onto the quotient ; then, the transpose is an isometric isomorphism from into, with range equal to "W". If "j" denotes the injection map from "W" into "V", then the kernel of the transpose is the annihilator of "W":
and it follows from the Hahn–Banach theorem that induces an isometric isomorphism

If the dual of a normed space "V" is separable, then so is the space "V" itself. The converse is not true: for example the space is separable, but its dual is not.

The topology of "V" and the topology of real or complex numbers can be used to induce on "V′" a dual space topology.

In analogy with the case of the algebraic double dual, there is always a naturally defined continuous linear operator from a normed space "V" into its continuous double dual , defined by

As a consequence of the Hahn–Banach theorem, this map is in fact an isometry, meaning for all "x" in "V". Normed spaces for which the map Ψ is a bijection are called reflexive.

When "V" is a topological vector space, one can still define Ψ("x") by the same formula, for every , however several difficulties arise. First, when "V" is not locally convex, the continuous dual may be equal to {0} and the map Ψ trivial. However, if "V" is Hausdorff and locally convex, the map Ψ is injective from "V" to the algebraic dual of the continuous dual, again as a consequence of the Hahn–Banach theorem.

Second, even in the locally convex setting, several natural vector space topologies can be defined on the continuous dual , so that the continuous double dual is not uniquely defined as a set. Saying that Ψ maps from "V" to , or in other words, that Ψ("x") is continuous on for every , is a reasonable minimal requirement on the topology of , namely that the evaluation mappings
be continuous for the chosen topology on . Further, there is still a choice of a topology on , and continuity of Ψ depends upon this choice. As a consequence, defining reflexivity in this framework is more involved than in the normed case.




</doc>
<doc id="7989" url="https://en.wikipedia.org/wiki?curid=7989" title="Dianetics">
Dianetics

Dianetics (from Greek "dia", meaning "through", and "nous", meaning "mind") is a set of ideas and practices regarding the metaphysical relationship between the mind and body created by science fiction writer L. Ron Hubbard. Dianetics is practiced by followers of Scientology, the Nation of Islam (as of 2010), and independent Dianeticist groups.

Dianetics divides the mind into three parts: the conscious "analytical mind", the subconscious "reactive mind", and the somatic mind. The goal of Dianetics is to erase the content of the "reactive mind", which Scientologists believe interferes with a person's ethics, awareness, happiness, and sanity. The Dianetics procedure to achieve this erasure is called "auditing". In auditing, the Dianetic auditor asks a series of questions (or commands) and elicits answers to help a person locate and deal with painful experiences of the past, which Scientologists believe to be the content of the "reactive mind".

Practitioners of Dianetics believe that "the basic principle of existence is to survive" and that the basic personality of humans is sincere, intelligent, and good. The drive for goodness and survival is distorted and inhibited by aberrations "ranging from simple neuroses to different psychotic states to various kinds of sociopathic behavior patterns." Hubbard developed Dianetics, claiming that it could eradicate these aberrations.

When Hubbard formulated Dianetics, he described it as "a mix of Western technology and Oriental philosophy". He said that Dianetics "forms a bridge between" cybernetics and general semantics (a set of ideas about education originated by Alfred Korzybski, which received much attention in the science fiction world in the 1940s)—a claim denied by scholars of General Semantics, including S. I. Hayakawa, who expressed strong criticism of Dianetics as early as 1951. Hubbard claimed that Dianetics could increase intelligence, eliminate unwanted emotions and alleviate a wide range of illnesses he believed to be psychosomatic. Among the conditions purportedly treated were arthritis, allergies, asthma, some coronary difficulties, eye trouble, ulcers, migraine headaches, "sexual deviation" (which for Hubbard included homosexuality), and even death. Hubbard asserted that "memories of painful physical and emotional experiences accumulate in a specific region of the mind, causing illness and mental problems." He taught that "once these experiences have been purged through cathartic procedures he developed, a person can achieve superior health and intelligence." Hubbard also variously defined Dianetics as "a spiritual healing technology" and "an organized science of thought."

Dianetics predates Hubbard's classification of Scientology as an "applied religious philosophy". Early in 1951, he expanded his writings to include teachings related to the soul, or "thetan". Dianetics is practiced by several independent Dianetics-only groups not connected with Scientology, and also Free Zone or Independent Scientologists. The Church of Scientology has prosecuted a number of people in court for unauthorized publication of Scientology and Dianetics copyrighted material.

L. Ron Hubbard published Dianetics on May 9, 1950, as a "branch of self-help psychology". In Dianetics, Hubbard introduced the "phenomena known as 'engrams'" as the source of "all psychological pain, which in turn harmed mental and physical health." He also claimed that individuals could reach the state of "clear", or a state of "exquisite clarity and mental liberation, by exorcising their engrams to an 'auditor,' or listener acting as therapist."
While not accepted by the medical and scientific establishment, in the first two years of its publication, over 100,000 copies of the book were sold. Many enthusiasts emerged to form groups to study and practice Dianetics. The atmosphere from which Dianetics was written about in this period was one of "excited experimentation". Roy Wallis writes that Hubbard's work was regarded as an "initial exploration" for further development. Hubbard wrote an additional six books in 1951, drawing the attention of a significant fan base.

Hubbard always claimed that his ideas of Dianetics originated in the 1920s and 1930s. By his own account, LRH had been injured by the premature detonation of a primer mechanism on a small depth charge that had become stuck in the launch rack aboard the navy ship he was assigned to in 1941. His injuries were mainly flash burns to his eyes and so was despatched ashore and he spent a great deal of his recovery time in the Oak Knoll Naval Hospital's library, (despite claiming in his authorised biography that he was blinded). LRH encountered the work of Thompson, Korzybski, Jung, Freud, Perls and other psychoanalysts.

In his 1955 Phoenix Lectures Series, Hubbard himself, explains that he took the opportunity to enter an office where research papers on the US Naval Medical Research Division's work on PTSD were kept in a filing cabinet and he spent the lunch hour free to read the notes left lying on the desk of the Naval Medical Officer involved. Much of what he learned then, along with his recent mastery of hypnotherapy technique by mail order, was influential in his later development of ideas and concepts for Dianetics Therapy from 1947 onwards. All he needed was medical and scientific testing and approval from any source. However, his several attempts were blocked by several luminaries of the (AMA) American Medical Association in the years 1948–1958, such as Professors Duncan Cameron and Allan Whyte (White), who both were senior authorities within the AMA-funded Psychiatric Research Department, then conducting their own research into drug therapies and controversial psycho-surgical techniques on severely traumatised war veterans.

Hubbard claimed in his several public lectures during the 1950s to have "undertaken clinical research at several of the institutions" they, Cameron and Whyte, had directed. Historical AMA records show that LRH was never officially involved in any approved clinical trials or research into PTSD. It is thought that Hubbard simply privately visited patients and conducted unauthorised interviews with several war veterans suffering from Trauma, Psychosomatic illness and practiced some of the newly identified PTSD techniques being clinically tested by several AMA medical institutions after WW2. (from personal Interviews with Joseph A. Winter, in Peoria,1959).

In April 1950, Hubbard, and several others, (Marjorie Cameron, De Mille, Art Ceppos, AE Van Vogt, Joseph A. Winter, MD.), established the Hubbard Dianetic Research Foundation in Elizabeth, New Jersey to coordinate work related to the forthcoming publication of DMSMH by Random House in May 1950 in NYC. Through the marketing efforts of Hubbard's friend and mentor John W. Campbell Jnr., (editor of "Astounding Science Fiction" of Street and Smith fame), Hubbard's articles on Dianetics hit the newsstands in NYC and became an overnight sensation among the usual readers with almost 350,000 copies sold of the May 1950 issue. (See interviews with John Campbell in his published 1978 biography.)

Hubbard first introduced Dianetics to the public in the article "" published in the May 1950 issue of the magazine "Astounding Science Fiction". Hubbard wrote "" at that time, allegedly completing the 180,000-word book in six weeks. The introduction of the book was the subject of an Associated Press article on 29 March 1950, with the lead "Discovery of a sub-mind is claimed in a new book entitled "Dianetics"".

When "Dianetics" was published in 1950, Hubbard announced in the opening pages, "The first contribution of Dianetics is the discovery that the problems of thought and mental function can be resolved within the bounds of the finite universe, which is to say that all data needful to the solution of mental action and Man's endeavor can be measured, sensed and experienced as scientific truths independent of mysticism or metaphysics." This was in line with Hubbard's initial presentation of Dianetics as a science, almost four years before he founded Scientology.

Publication of "Dianetics: The Modern Science of Mental Health" brought in a flood of money, which Hubbard used to establish Dianetics foundations in six major American cities. Dianetics shared The New York Times best-seller list with other self-help writings, including Norman Vincent Peale's "The Art of Happiness" and Henry Overstreet's "The Mature Mind". Scholar Hugh B. Urban asserted that the initial success of Dianetics was reflective of Hubbard's "remarkable entrepreneurial skills." Posthumously, Publisher's Weekly awarded Hubbard a plaque to acknowledge Dianetics appearing on its bestseller list for one hundred weeks, consecutively.

Some of the initial strongest supporters of Dianetics in the 1950s were John W. Campbell, editor of Astounding Science Fiction and Joseph Augustus Winter, a writer and medical physician. Campbell published some of Hubbard's short stories and Winter hoped that his colleagues would likewise be attracted to Hubbard's Dianetics system.

In January 1951, the New Jersey Board of Medical Examiners instituted proceedings against the Hubbard Dianetic Research Foundation in Elizabeth for 'teaching medicine without a licence', which was quickly resolved when the courts were made aware that the HDRF deputy director Winter was registered as an MD in the state of Michigan and New York. . The Foundation closed its doors when Hubbard ditched the Foundation, causing the proceedings to be vacated, but its creditors began to demand settlement of its outstanding debts. Don Purcell, a millionaire Dianeticist from Wichita, Kansas, offered a brief respite from bankruptcy, but the Wichita Foundation's finances soon failed again in 1952 when Hubbard ran off to Phoenix with all his Dianetics materials to avoid the court bailiffs sent in by Don Purcell, who had paid a considerable amount of money to Hubbard for the copyrights to Dianetics in an effort to keep Hubbard from bankruptcy again.

In 1954, Hubbard defined Scientology as a religion focused on the spirit, differentiating it from Dianetics, and subsequently Dianetics Auditing Therapy, which he defined as a counseling based science that addressed the physical being. He stated, "Dianetics is a science which applies to man, a living organism; and Scientology is a religion." When Hubbard morphed Dianetics therapy into the religion of Scientology, Jesper Aagaard Petersen of Oxford University surmises that it could have been for the benefits from establishing it is a religion as much as it could have been from the result of Hubbard's "discovery of past life experiences and his exploration of the thetan." The reason being to avoid copyright infringement issues with use of the name Dianetics then held by Don Purcell. Purcell later donated the copyright ownership back (to Hubbard) after Winter and Van Vogt had independently negotiated charitable debt relief with the disenchanted oil millionaire Purcell.

With the temporary sale of assets resulting from the HDRF's bankruptcy, Hubbard no longer owned the rights to the name "Dianetics", but its philosophical framework still provided the seed for Scientology to grow. Scientologists refer to the book "Dianetics: The Modern Science of Mental Health" as "Book One." In 1952, Hubbard published a new set of teachings as "Scientology, a religious philosophy." Scientology did not replace Dianetics but extended it to cover new areas: Where the goal of Dianetics is to rid the individual of his reactive mind engrams, the stated goal of Scientology is to rehabilitate the individual's spiritual nature so that he may reach his full potential.

In 1963 and again in May 1969, Hubbard reorganized the material in Dianetics, the auditing commands, and original Volney Mathieson invented E-meter use, naming the package "Standard Dianetics." In a 1969 bulletin, "This bulletin combines HCOB 27 April 1969 'R-3-R Restated' with those parts of HCOB 24 June 1963 'Routine 3-R' used in the new Standard Dianetic Course and its application. This gives the complete steps of Routine 3-R Revised."

In 1978, Hubbard released "New Era Dianetics" (NED), a revised version supposed to produce better results in a shorter period of time. The course consists of 11 rundowns and requires a specifically trained auditor. It is similar to Standard Dianetics, but the person being audited is encouraged to find the decision or "postulate" he made during or as a result of the incident. ("Postulate" in Dianetics and Scientology has the meaning of "a conclusion, decision or resolution made by the individual himself; to conclude, decide or resolve a problem or to set a pattern for the future or to nullify a pattern of the past" in contrast to its conventional meanings.)

In the Church of Scientology, OTs study several levels of before reaching the highest level.

In the book, "", Hubbard describes techniques that he suggests can rid individuals of fears and psychosomatic illnesses. A basic idea in Dianetics is that the mind consists of two parts: the "analytical mind" and the "reactive mind." The "reactive mind", the mind which operates when a person is physically unconscious, acts as a record of shock, trauma, pain, and otherwise harmful memories. Experiences such as these, stored in the "reactive mind" are dubbed "engrams". Dianetics is proposed as a method to erase these engrams in the reactive mind to achieve a state of clear.

Hubbard described Dianetics as "an organized science of thought built on definite axioms: statements of natural laws on the order of those of the physical sciences". In April 1950, before the public release of Dianetics, he wrote: "To date, over two hundred patients have been treated; of those two hundred, two hundred cures have been obtained."

In Dianetics, the unconscious or reactive mind is described as a collection of "mental image pictures," which contain the recorded experience of past moments of unconsciousness, including all sensory perceptions and feelings involved, ranging from pre-natal experiences, infancy and childhood, to even the traumatic feelings associated with events from past lives and extraterrestrial cultures. The type of mental image picture created during a period of unconsciousness involves the exact recording of a painful experience. Hubbard called this phenomenon an engram, and defined it as "a complete recording of a moment of unconsciousness containing physical pain or painful emotion and all perceptions."

Hubbard said that in Dianetics, it was the analytical mind and not the reactive mind that was the most important because the analytical mind "computes decisions" even when these are dictated by the reactive mind. The damage and aberration caused by the reactive mind would not be possible without the analytic mind. Hubbard stated, "the analytical is so important to the intelligent being and the somatic mind so important to the athlete that Dianetics processing can be said to consist of deintensifying the reactive mind so that the analytical and somatic minds can be free to function properly."

Hubbard proposed that painful physical or emotional traumas caused "aberrations" (deviations from rational thinking) in the mind, which produced lasting adverse physical and emotional effects, similar to conversion disorders. When the analytical (conscious) mind shut down during these moments, events and perceptions of this period were stored as engrams in the unconscious or reactive mind. (In Hubbard's earliest publications on the subject, engrams were variously referred to as "Norns", "Impediments," and "comanomes" before "engram" was adapted from its existing usage at the suggestion of Joseph Winter.) Some commentators noted Dianetics's blend of science fiction and occult orientations at the time.

Hubbard claimed that these engrams are the cause of almost all psychological and physical problems. In addition to physical pain, engrams could include words or phrases spoken in the vicinity while the patient was unconscious. For instance, Winter cites the example of a patient with a persistent headache supposedly tracing the problem to a doctor saying, "Take him now," during the patient's birth. Hubbard similarly claimed that leukemia is traceable to "an engram containing the phrase 'It turns my blood to water.'" While it is sometimes claimed that the Church of Scientology no longer stands by Hubbard's claims that Dianetics can treat physical conditions, it still publishes them: "... when the knee injuries of the past are located and discharged, the arthritis ceases, no other injury takes its place and the person is finished with arthritis of the knee." "[The reactive mind] can give a man arthritis, bursitis, asthma, allergies, sinusitis, coronary trouble, high blood pressure ... And it is the only thing in the human being which can produce these effects ... Discharge the content of [the reactive mind] and the arthritis vanishes, myopia gets better, heart illness decreases, asthma disappears, stomachs function properly and the whole catalog of ills goes away and stays away."

Hubbard defined the third mind, or the somatic mind, as "that mind which, directed by the analytical or reactive mind, places solution into effect on the physical level." If an individual is not suffering from aberration or engrams are not restimulated, thus causing the person to relive pain, the analytical mind controls the somatic mind, in turn controlling blood flow, the heartbeat and endocrines. When a person is "aberrated," the reactive mind controls the somatic mind.

Some of the psychometric ideas in Dianetics, in particular the E-meter, can be traced to Carl Jung. Basic concepts, including conversion disorder, are derived from Sigmund Freud, whom Hubbard credited as an inspiration and source. Freud had speculated 40 years previously that traumas with similar content join together in "chains," embedded in the unconscious mind, to cause irrational responses in the individual. Such a chain would be relieved by inducing the patient to remember the earliest trauma, "with an accompanying expression of emotion."

According to Bent Corydon, Hubbard created the illusion that Dianetics was the first psychotherapy to address traumatic experiences in their own time, but others had done so as standard procedure.

One treatment method Hubbard drew from in developing Dianetics was abreaction therapy. Abreaction is a psychoanalytical term that means bringing to consciousness, and thus adequate expression, material that has been unconscious. "It includes not only the recollection of forgotten memories and experience, but also their reliving with appropriate emotional display and discharge of effect. This process is usually facilitated by the patient's gaining awareness of the causal relationship between the previously undischarged emotion and his symptoms."

According to Hubbard, before Dianetics psychotherapists had dealt with very light and superficial incidents (e.g. an incident that reminds the patient of a moment of loss), but with Dianetic therapy, the patient could actually erase moments of pain and unconsciousness. He emphasized: "The discovery of the engram is entirely the property of Dianetics. Methods of its erasure are also owned entirely by Dianetics..."

While 1950 style Dianetics was in some respects similar to older therapies, with the development of New Era Dianetics in 1978, the similarity vanished. New Era Dianetics uses an E-Meter and a rote procedure for running "chains" of related traumatic incidents.

Dianetics clarifies the understanding of psychosomatic illness in terms of "predisposition", "precipitation", and "prolongation".

With the use of Dianetics techniques, Hubbard claimed, the reactive mind could be processed and all stored engrams could be refiled as experience. The central technique was "auditing," a two-person question-and-answer therapy designed to isolate and dissipate engrams (or "mental masses"). An auditor addresses questions to a subject, observes and records the subject's responses, and returns repeatedly to experiences or areas under discussion that appear painful until the troubling experience has been identified and confronted. Through repeated applications of this method, the reactive mind could be "cleared" of its content having outlived its usefulness in the process of evolution; a person who has completed this process would be "Clear".

The benefits of going Clear, according to Hubbard, were dramatic. A Clear would have no compulsions, repressions, psychoses or neuroses, and would enjoy a near-perfect memory as well as a rise in IQ of as much as 50 points. He also claimed that "the atheist is activated by engrams as thoroughly as the zealot". He further claimed that widespread application of Dianetics would result in "A world without insanity, without criminals and without war."

One of the key ideas of Dianetics, according to Hubbard, is the fundamental existential command to survive. According to Hugh B. Urban, this would serve as the foundation of a big part of later Scientology.

According to the Scientology journal "The Auditor", the total number of "Clears" as of May 2006 stands at 50,311.

When Hubbard presented Dianetics, he did so in terms of "terra incognita," or to Scientologists the human mind. Hubbard wrote, “Dianetics is an adventure. It is an exploration of terra incognita, the human mind, the vast and hitherto unknown realm half an inch back of our foreheads." According to "Scientology in Popular Culture," Hubbard set out to colonize terra incognita, where in the “practice of empire was auditing, the new technology of empire was the E-meter. This exploration of the human mind “would become a defining feature of Scientology because it provided the portal through which he could conquer many enemy thetans.”

The procedure of Dianetics therapy (known as "auditing") is a two-person activity. One person, the "auditor", guides the other person, the "pre-clear". The pre-Clear's job is to look at the mind and talk to the auditor. The auditor acknowledges what the pre-Clear says and controls the process so the pre-Clear may put his full attention on his work.

The auditor and pre-Clear sit down in chairs facing each other. The process then follows in eleven distinct steps:


Auditing sessions are supposedly kept confidential. A few transcripts of auditing sessions with confidential information removed have been published as demonstration examples. Some extracts can be found in J.A. Winter's book "". Other, more comprehensive, transcripts of auditing sessions carried out by Hubbard himself can be found in volume 1 of the "Research & Discovery Series" (Bridge Publications, 1980). Examples of public group processing sessions can be found throughout the "Congresses" lecture series.

According to Hubbard, auditing enables the pre-Clear to "contact" and "release" engrams stored in the reactive mind, relieving him of the physical and mental aberrations connected with them. The pre-Clear is asked to inspect and familiarize himself with the exact details of his own experience; the auditor may not tell him anything about his case or evaluate any of the information the pre-Clear finds.

Hubbard's original book on Dianetics attracted highly critical reviews from science and medical writers and organizations. The American Psychological Association passed a resolution in 1950 calling "attention to the fact that these claims are not supported by empirical evidence of the sort required for the establishment of scientific generalizations." Subsequently, Dianetics has achieved no acceptance as a scientific theory, and scientists cite Dianetics as an example of a pseudoscience.

In August 1950, amidst the success of "", Hubbard held a demonstration in Los Angeles' Shrine Auditorium where he presented a young woman called Sonya Bianca (a pseudonym) to a large audience including many reporters and photographers as 'the world's first Clear." Despite Hubbard's claim that she had "full and perfect recall of every moment of her life", Bianca proved unable to answer questions from the audience testing her memory and analytical abilities, including the question of the color of Hubbard's tie. Hubbard explained Bianca's failure to display her promised powers of recall to the audience by saying that he had used the word "now" in calling her to the stage, and thus inadvertently froze her in "present time," which blocked her abilities. Later, in the late 1950s, Hubbard would claim that several people had reached the state of Clear by the time he presented Bianca as the world's first; these others, Hubbard said, he had successfully cleared in the late 1940s while working "incognito" in Hollywood posing as a swami. In 1966, Hubbard declared South African Scientologist John McMaster to be the first true Clear.

Few scientific investigations into the effectiveness of Dianetics have been published. Professor John A. Lee states in his 1970 evaluation of Dianetics:

The MEDLINE database records two independent scientific studies on Dianetics, both conducted in the 1950s under the auspices of New York University. Harvey Jay Fischer tested Dianetics therapy against three claims made by proponents and found it does not effect any significant changes in intellectual functioning, mathematical ability, or the degree of personality conflicts; Jack Fox tested Hubbard's thesis regarding recall of engrams, with the assistance of the Dianetic Research Foundation, and could not substantiate it.

Hubbard claimed, in an interview with "The New York Times" in November 1950, that "he had already submitted proof of claims made in the book to a number of scientists and associations." He added that the public as well as proper organizations were entitled to such proof and that he was ready and willing to give such proof in detail. In January 1951, the Hubbard Dianetic Research Foundation of Elizabeth, NJ published "Dianetic Processing: A Brief Survey of Research Projects and Preliminary Results", a booklet providing the results of psychometric tests conducted on 88 people undergoing Dianetics therapy. It presents case histories and a number of X-ray plates to support claims that Dianetics had cured "aberrations" including manic depression, asthma, arthritis, colitis and "overt homosexuality," and that after Dianetic processing, test subjects experienced significantly increased scores on a standardized IQ test. The report's subjects are not identified by name, but one of them is clearly Hubbard himself ("Case 1080A, R. L.").

The authors provide no qualifications, although they are described in Hubbard's book "Science of Survival" (where some results of the same study were reprinted) as psychotherapists. Critics of Dianetics are skeptical of this study, both because of the bias of the source and because the researchers appear to ascribe all physical benefits to Dianetics without considering possible outside factors; in other words, the report lacks any scientific controls. J.A. Winter, M.D., originally an associate of Hubbard and an early adopter of Dianetics, had by the end of 1950 cut his ties with Hubbard and written an account of his personal experiences with Dianetics. He described Hubbard as "absolutistic and authoritarian", and criticized the Hubbard Dianetic Research Foundation for failing to undertake "precise scientific research into the functioning of the mind". He also recommended that auditing be done by experts only and that it was dangerous for laymen to audit each other. Hubbard writes: "Again, Dianetics is not being released to a profession, for no profession could encompass it."

Commentators from a variety of backgrounds have described Dianetics as an example of pseudoscience. For example, philosophy professor Robert Carroll points to Dianetics' lack of empirical evidence:

W. Sumner Davis similarly comments that

The validity and practice of auditing have been questioned by a variety of non-Scientologist commentators. Commenting on the example cited by Winter, the science writer Martin Gardner asserts that "nothing could be clearer from the above dialogue than the fact that the dianetic explanation for the headache existed only in the mind of the therapist, and that it was with considerable difficulty that the patient was maneuvered into accepting it."

Other critics and medical experts have suggested that Dianetic auditing is a form of hypnosis, although the Church of Scientology has strongly denied that hypnosis forms any part of Dianetics. To the contrary, L. Ron Hubbard expressly warns not to use any hypnosis or hypnosis-like methods, because a person under hypnosis would be receptive to suggestions. This would decrease his self-determinism instead of increasing it, which is one of the prime goals of Dianetics. Winter [1950] comments that the leading nature of the questions asked of a pre-Clear "encourage fantasy", a common issue also encountered with hypnosis, which can be used to form false memories. The auditor is instructed not to make any assessment of a recalled memory's reality or accuracy, but instead to treat it as if it were objectively real. Professor Richard J. Ofshe, a leading expert on false memories, suggests that the feeling of well-being reported by pre-Clear at the end of an auditing session may be induced by post-hypnotic suggestion. Other researchers have identified quotations in Hubbard's work suggesting evidence that false memories were created in "Dianetics," specifically in the form of birth and pre-birth memories.

According to Hubbard, the majority of the people interested in the subject believed they could accomplish therapy alone. "It cannot be done" and he adds: "If a patient places himself in autohypnosis and regresses himself in an effort to reach illness or birth or prenatals, the only thing he will get is ill".





</doc>
<doc id="7990" url="https://en.wikipedia.org/wiki?curid=7990" title="Data warehouse">
Data warehouse

In computing, a data warehouse (DW or DWH), also known as an enterprise data warehouse (EDW), is a system used for reporting and data analysis, and is considered a core component of business intelligence. DWs are central repositories of integrated data from one or more disparate sources. They store current and historical data in one single place that are used for creating analytical reports for workers throughout the enterprise.

The data stored in the warehouse is uploaded from the operational systems (such as marketing or sales). The data may pass through an operational data store and may require data cleansing for additional operations to ensure data quality before it is used in the DW for reporting.

The typical Extract, transform, load (ETL)-based data warehouse uses staging, data integration, and access layers to house its key functions. The staging layer or staging database stores raw data extracted from each of the disparate source data systems. The integration layer integrates the disparate data sets by transforming the data from the staging layer often storing this transformed data in an operational data store (ODS) database. The integrated data are then moved to yet another database, often called the data warehouse database, where the data is arranged into hierarchical groups, often called dimensions, and into facts and aggregate facts. The combination of facts and dimensions is sometimes called a star schema. The access layer helps users retrieve data.

The main source of the data is cleansed, transformed, catalogued, and made available for use by managers and other business professionals for data mining, online analytical processing, market research and decision support. However, the means to retrieve and analyze data, to extract, transform, and load data, and to manage the data dictionary are also considered essential components of a data warehousing system. Many references to data warehousing use this broader context. Thus, an expanded definition for data warehousing includes business intelligence tools, tools to extract, transform, and load data into the repository, and tools to manage and retrieve metadata.

A data warehouse maintains a copy of information from the source transaction systems. This architectural complexity provides the opportunity to:

The environment for data warehouses and marts includes the following:


In regards to source systems listed above, R. Kelly Rainer states, "A common source for the data in data warehouses is the company's operational databases, which can be relational databases".

Regarding data integration, Rainer states, "It is necessary to extract data from source systems, transform them, and load them into a data mart or warehouse".

Rainer discusses storing data in an organization's data warehouse or data marts.

Metadata are data about data. "IT personnel need information about data sources; database, table, and column names; refresh schedules; and data usage measures".

Today, the most successful companies are those that can respond quickly and flexibly to market changes and opportunities. A key to this response is the effective and efficient use of data and information by analysts and managers. A "data warehouse" is a repository of historical data that are organized by subject to support decision makers in the organization. Once data are stored in a data mart or warehouse, they can be accessed.

A data mart is a simple form of a data warehouse that is focused on a single subject (or functional area), hence they draw data from a limited number of sources such as sales, finance or marketing. Data marts are often built and controlled by a single department within an organization. The sources could be internal operational systems, a central data warehouse, or external data. Denormalization is the norm for data modeling techniques in this system. Given that data marts generally cover only a subset of the data contained in a data warehouse, they are often easier and faster to implement.

Types of data marts include dependent, independent, and hybrid data marts.

Online analytical processing (OLAP) is characterized by a relatively low volume of transactions. Queries are often very complex and involve aggregations. For OLAP systems, response time is an effectiveness measure. OLAP applications are widely used by Data Mining techniques. OLAP databases store aggregated, historical data in multi-dimensional schemas (usually star schemas). OLAP systems typically have data latency of a few hours, as opposed to data marts, where latency is expected to be closer to one day. The OLAP approach is used to analyze multidimensional data from multiple sources and perspectives. The three basic operations in OLAP are : Roll-up (Consolidation), Drill-down and Slicing & Dicing.

Online transaction processing (OLTP) is characterized by a large number of short on-line transactions (INSERT, UPDATE, DELETE). OLTP systems emphasize very fast query processing and maintaining data integrity in multi-access environments. For OLTP systems, effectiveness is measured by the number of transactions per second. OLTP databases contain detailed and current data. The schema used to store transactional databases is the entity model (usually 3NF). Normalization is the norm for data modeling techniques in this system.

Predictive analytics is about finding and quantifying hidden patterns in the data using complex mathematical models that can be used to predict future outcomes. Predictive analysis is different from OLAP in that OLAP focuses on historical data analysis and is reactive in nature, while predictive analysis focuses on the future. These systems are also used for customer relationship management (CRM).

The concept of data warehousing dates back to the late 1980s when IBM researchers Barry Devlin and Paul Murphy developed the "business data warehouse". In essence, the data warehousing concept was intended to provide an architectural model for the flow of data from operational systems to decision support environments. The concept attempted to address the various problems associated with this flow, mainly the high costs associated with it. In the absence of a data warehousing architecture, an enormous amount of redundancy was required to support multiple decision support environments. In larger corporations, it was typical for multiple decision support environments to operate independently. Though each environment served different users, they often required much of the same stored data. The process of gathering, cleaning and integrating data from various sources, usually from long-term existing operational systems (usually referred to as legacy systems), was typically in part replicated for each environment. Moreover, the operational systems were frequently reexamined as new decision support requirements emerged. Often new requirements necessitated gathering, cleaning and integrating new data from "data marts" that was tailored for ready access by users.

Key developments in early years of data warehousing were:


A fact is a value or measurement, which represents a fact about the managed entity or system.

Facts, as reported by the reporting entity, are said to be at raw level. E.g. in a mobile telephone system, if a BTS (base transceiver station) received 1,000 requests for traffic channel allocation, it allocates for 820, and rejects the remaining, it would report three facts or measurements to a management system:

Facts at the raw level are further aggregated to higher levels in various dimensions to extract more service or business-relevant information from it. These are called aggregates or summaries or aggregated facts.

For instance, if there are three BTS in a city, then the facts above can be aggregated from the BTS to the city level in the network dimension. For example:


There are three or more leading approaches to storing data in a data warehouse — the most important approaches are the dimensional approach and the normalized approach.

The dimensional approach refers to Ralph Kimball's approach in which it is stated that the data warehouse should be modeled using a Dimensional Model/star schema. The normalized approach, also called the 3NF model (Third Normal Form) refers to Bill Inmon's approach in which it is stated that the data warehouse should be modeled using an E-R model/normalized model.

In a dimensional approach, transaction data are partitioned into "facts", which are generally numeric transaction data, and "dimensions", which are the reference information that gives context to the facts. For example, a sales transaction can be broken up into facts such as the number of products ordered and the total price paid for the products, and into dimensions such as order date, customer name, product number, order ship-to and bill-to locations, and salesperson responsible for receiving the order.

A key advantage of a dimensional approach is that the data warehouse is easier for the user to understand and to use. Also, the retrieval of data from the data warehouse tends to operate very quickly. Dimensional structures are easy to understand for business users, because the structure is divided into measurements/facts and context/dimensions. Facts are related to the organization's business processes and operational system whereas the dimensions surrounding them contain context about the measurement (Kimball, Ralph 2008). Another advantage offered by dimensional model is that it does not involve a relational database every time. Thus, this type of modeling technique is very useful for end-user queries in data warehouse.

The model of facts and dimensions can also be understood as data cube. Where the dimensions are the categorical coordinates in a multi-dimensional cube, while the fact is a value corresponding to the coordinates.

The main disadvantages of the dimensional approach are the following:

In the normalized approach, the data in the data warehouse are stored following, to a degree, database normalization rules. Tables are grouped together by "subject areas" that reflect general data categories (e.g., data on customers, products, finance, etc.). The normalized structure divides data into entities, which creates several tables in a relational database. When applied in large enterprises the result is dozens of tables that are linked together by a web of joins. Furthermore, each of the created entities is converted into separate physical tables when the database is implemented (Kimball, Ralph 2008).
The main advantage of this approach is that it is straightforward to add information into the database. Some disadvantages of this approach are that, because of the number of tables involved, it can be difficult for users to join data from different sources into meaningful information and to access the information without a precise understanding of the sources of data and of the data structure of the data warehouse.

Both normalized and dimensional models can be represented in entity-relationship diagrams as both contain joined relational tables. The difference between the two models is the degree of normalization (also known as Normal Forms). These approaches are not mutually exclusive, and there are other approaches. Dimensional approaches can involve normalizing data to a degree (Kimball, Ralph 2008).

In "Information-Driven Business", Robert Hillard proposes an approach to comparing the two approaches based on the information needs of the business problem. The technique shows that normalized models hold far more information than their dimensional equivalents (even when the same fields are used in both models) but this extra information comes at the cost of usability. The technique measures information quantity in terms of information entropy and usability in terms of the Small Worlds data transformation measure.

In the "bottom-up" approach, data marts are first created to provide reporting and analytical capabilities for specific business processes. These data marts can then be integrated to create a comprehensive data warehouse. The data warehouse bus architecture is primarily an implementation of "the bus", a collection of conformed dimensions and conformed facts, which are dimensions that are shared (in a specific way) between facts in two or more data marts.

The "top-down" approach is designed using a normalized enterprise data model. "Atomic" data, that is, data at the greatest level of detail, are stored in the data warehouse. Dimensional data marts containing data needed for specific business processes or specific departments are created from the data warehouse.

Data warehouses (DW) often resemble the hub and spokes architecture. Legacy systems feeding the warehouse often include customer relationship management and enterprise resource planning, generating large amounts of data. To consolidate these various data models, and facilitate the extract transform load process, data warehouses often make use of an operational data store, the information from which is parsed into the actual DW. To reduce data redundancy, larger systems often store the data in a normalized way. Data marts for specific reports can then be built on top of the data warehouse.

A hybrid DW database is kept on third normal form to eliminate data redundancy. A normal relational database, however, is not efficient for business intelligence reports where dimensional modelling is prevalent. Small data marts can shop for data from the consolidated warehouse and use the filtered, specific data for the fact tables and dimensions required. The DW provides a single source of information from which the data marts can read, providing a wide range of business information. The hybrid architecture allows a DW to be replaced with a master data management repository where operational, not static information could reside.

The data vault modeling components follow hub and spokes architecture. This modeling style is a hybrid design, consisting of the best practices from both third normal form and star schema. The data vault model is not a true third normal form, and breaks some of its rules, but it is a top-down architecture with a bottom up design. The data vault model is geared to be strictly a data warehouse. It is not geared to be end-user accessible, which when built, still requires the use of a data mart or star schema based release area for business purposes.

There are basic features that define the data in the data warehouse that include subject orientation, data integration, time-variant, nonvolatile data, and data granularity.

Unlike the operational systems, the data in the data warehouse revolves around subjects of the enterprise (database normalization). Subject orientation can be really useful for decision making.
Gathering the required objects is called subject oriented.

The data found within the data warehouse is integrated. Since it comes from several operational systems, all inconsistencies must be removed. Consistencies include naming conventions, measurement of variables, encoding structures, physical attributes of data, and so forth.

While operational systems reflect current values as they support day-to-day operations, data warehouse data represents data over a long time horizon (up to 10 years) which means it stores historical data. It is mainly meant for data mining and forecasting, If a user is searching for a buying pattern of a specific customer, the user needs to look at data on the current and past purchases.

The data in the data warehouse is read-only which means it cannot be updated, created, or deleted.

In the data warehouse, data is summarized at different levels.The user may start looking at the total sale units of a product in an entire region. Then the user looks at the states in that region. Finally, they may examine the individual stores in a certain state. Therefore, typically, the analysis starts at a higher level and moves down to lower levels of details.

The different methods used to construct/organize a data warehouse specified by an organization are numerous. The hardware utilized, software created and data resources specifically required for the correct functionality of a data warehouse are the main components of the data warehouse architecture. All data warehouses have multiple phases in which the requirements of the organization are modified and fine tuned.

Operational systems are optimized for preservation of data integrity and speed of recording of business transactions through use of database normalization and an entity-relationship model. Operational system designers generally follow Codd's 12 rules of database normalization to ensure data integrity. Fully normalized database designs (that is, those satisfying all Codd rules) often result in information from a business transaction being stored in dozens to hundreds of tables. Relational databases are efficient at managing the relationships between these tables. The databases have very fast insert/update performance because only a small amount of data in those tables is affected each time a transaction is processed. To improve performance, older data are usually periodically purged from operational systems.

Data warehouses are optimized for analytic access patterns. Analytic access patterns generally involve selecting specific fields and rarely if ever 'select *' as is more common in operational databases. Because of these differences in access patterns, operational databases (loosely, OLTP) benefit from the use of a row-oriented DBMS whereas analytics databases (loosely, OLAP) benefit from the use of a column-oriented DBMS. Unlike operational systems which maintain a snapshot of the business, data warehouses generally maintain an infinite history which is implemented through ETL processes that periodically migrate data from the operational systems over to the data warehouse.

These terms refer to the level of sophistication of a data warehouse:





</doc>
<doc id="7991" url="https://en.wikipedia.org/wiki?curid=7991" title="Disperser">
Disperser

A disperser is a one-sided extractor. Where an extractor requires that every event gets the same probability under the uniform distribution and the extracted distribution, only the latter is required for a disperser. So for a disperser, an event formula_1 we have:
formula_2

Definition (Disperser): "A" formula_3"-disperser is a function"

formula_4

"such that for every distribution" formula_5 "on" formula_6 "with" formula_7 "the support of the distribution" formula_8 "is of size at least" formula_9.

An ("N", "M", "D", "K", "e")-disperser is a bipartite graph with "N" vertices on the left side, each with degree "D", and "M" vertices on the right side, such that every subset of "K" vertices on the left side is connected to more than (1 − "e")"M" vertices on the right.

An extractor is a related type of graph that guarantees an even stronger property; every ("N", "M", "D", "K", "e")-extractor is also an ("N", "M", "D", "K", "e")-disperser.

A disperser is a high-speed mixing device used to disperse or dissolve pigments and other solids into a liquid.



</doc>
<doc id="7992" url="https://en.wikipedia.org/wiki?curid=7992" title="Devonian">
Devonian

The Devonian is a geologic period and system of the Paleozoic, spanning 60 million years from the end of the Silurian, million years ago (Mya), to the beginning of the Carboniferous, Mya. It is named after Devon, England, where rocks from this period were first studied.

The first significant adaptive radiation of life on dry land occurred during the Devonian. Free-sporing vascular plants began to spread across dry land, forming extensive forests which covered the continents. By the middle of the Devonian, several groups of plants had evolved leaves and true roots, and by the end of the period the first seed-bearing plants appeared. Various terrestrial arthropods also became well-established.

Fish reached substantial diversity during this time, leading the Devonian to often be dubbed the "Age of Fish". The first ray-finned and lobe-finned bony fish appeared, while the placoderms began dominating almost every known aquatic environment. The ancestors of all four-limbed vertebrates (tetrapods) began adapting to walking on land, as their strong pectoral and pelvic fins gradually evolved into legs. In the oceans, primitive sharks became more numerous than in the Silurian and Late Ordovician.

The first ammonites, species of molluscs, appeared. Trilobites, the mollusc-like brachiopods and the great coral reefs, were still common. The Late Devonian extinction which started about 375 million years ago severely affected marine life, killing off all placodermi, and all trilobites, save for a few species of the order Proetida.

The palaeogeography was dominated by the supercontinent of Gondwana to the south, the continent of Siberia to the north, and the early formation of the small continent of Euramerica in between.

The period is named after Devon, a county in southwestern England, where a controversial argument in the 1830s over the age and structure of the rocks found distributed throughout the county was eventually resolved by the definition of the Devonian period in the geological timescale. The Great Devonian Controversy was a long period of vigorous argument and counter-argument between the main protagonists of Roderick Murchison with Adam Sedgwick against Henry De la Beche supported by George Bellas Greenough. Murchison and Sedgwick won the debate and named the period they proposed as the Devonian System.

While the rock beds that define the start and end of the Devonian period are well identified, the exact dates are uncertain. According to the International Commission on Stratigraphy (Ogg, 2004), the Devonian extends from the end of the Silurian Mya, to the beginning of the Carboniferous Mya (in North America, the beginning of the Mississippian subperiod of the Carboniferous).

In nineteenth-century texts the Devonian has been called the "Old Red Age", after the red and brown terrestrial deposits known in the United Kingdom as the Old Red Sandstone in which early fossil discoveries were found. Another common term is "Age of the Fishes", referring to the evolution of several major groups of fish that took place during the period. Older literature on the Anglo-Welsh basin divides it into the Downtonian, Dittonian, Breconian and Farlovian stages, the latter three of which are placed in the Devonian.

The Devonian has also erroneously been characterised as a "greenhouse age", due to sampling bias: most of the early Devonian-age discoveries came from the strata of western Europe and eastern North America, which at the time straddled the Equator as part of the supercontinent of Euramerica where fossil signatures of widespread reefs indicate tropical climates that were warm and moderately humid but in fact the climate in the Devonian differed greatly during its epochs and between geographic regions. For example, during the Early Devonian, arid conditions were prevalent through much of the world including Siberia, Australia, North America, and China, but Africa and South America had a warm temperate climate. In the Late Devonian, by contrast, arid conditions were less prevalent across the world and temperate climates were more common.

The Devonian Period is formally broken into Early, Middle and Late subdivisions. The rocks corresponding to those epochs are referred to as belonging to the Lower, Middle and Upper parts of the Devonian System.

The Early Devonian lasted from and began with the Lochkovian stage, which lasted until the Pragian. It spanned from , and was followed by the Emsian, which lasted until the Middle Devonian began, . 
During this time, the first ammonoids appeared, descending from bactritoid nautiloids. Ammonoids during this time period were simple and differed little from their nautiloid counterparts. These ammonoids belong to the order Agoniatitida, which in later epochs evolved to new ammonoid orders, for example Goniatitida and Clymeniida. This class of cephalopod molluscs would dominate the marine fauna until the beginning of the Mesozoic era.

The Middle Devonian comprised two subdivisions: first the Eifelian, which then gave way to the Givetian .
During this time the jawless agnathan fishes began to decline in diversity in freshwater and marine environments partly due to drastic environmental changes and partly due to the increasing competition, predation and diversity of jawed fishes. The shallow, warm, oxygen-depleted waters of Devonian inland lakes, surrounded by primitive plants, provided the environment necessary for certain early fish to develop such essential characteristics as well developed lungs, and the ability to crawl out of the water and onto the land for short periods of time.

Finally, the Late Devonian started with the Frasnian, , during which the first forests took shape on land. The first tetrapods appeared in the fossil record in the ensuing Famennian subdivision, the beginning and end of which are marked with extinction events. This lasted until the end of the Devonian, .

The Devonian was a relatively warm period, and probably lacked any glaciers. The temperature gradient from the equator to the poles was not as large as it is today. The weather was also very arid, mostly along the equator where it was the driest. Reconstruction of tropical sea surface temperature from conodont apatite implies an average value of in the Early Devonian. levels dropped steeply throughout the Devonian period as the burial of the newly evolved forests drew carbon out of the atmosphere into sediments; this may be reflected by a Mid-Devonian cooling of around . The Late Devonian warmed to levels equivalent to the Early Devonian; while there is no corresponding increase in concentrations, continental weathering increases (as predicted by warmer temperatures); further, a range of evidence, such as plant distribution, points to a Late Devonian warming. The climate would have affected the dominant organisms in reefs; microbes would have been the main reef-forming organisms in warm periods, with corals and stromatoporoid sponges taking the dominant role in cooler times. The warming at the end of the Devonian may even have contributed to the extinction of the stromatoporoids.

The Devonian period was a time of great tectonic activity, as Euramerica and Gondwana drew closer together.

The continent Euramerica (or Laurussia) was created in the early Devonian by the collision of Laurentia and Baltica, which rotated into the natural dry zone along the Tropic of Capricorn, which is formed as much in Paleozoic times as nowadays by the convergence of two great air-masses, the Hadley cell and the Ferrel cell. In these near-deserts, the Old Red Sandstone sedimentary beds formed, made red by the oxidised iron (hematite) characteristic of drought conditions.

Near the equator, the plate of Euramerica and Gondwana were starting to meet, beginning the early stages of the assembling of Pangaea. This activity further raised the northern Appalachian Mountains and formed the Caledonian Mountains in Great Britain and Scandinavia.

The west coast of Devonian North America, by contrast, was a passive margin with deep silty embayments, river deltas and estuaries, found today in Idaho and Nevada; an approaching volcanic island arc reached the steep slope of the continental shelf in Late Devonian times and began to uplift deep water deposits, a collision that was the prelude to the mountain-building episode at the beginning of the Carboniferous called the Antler orogeny.

Sea levels were high worldwide, and much of the land lay under shallow seas, where tropical reef organisms lived. The deep, enormous Panthalassa (the "universal ocean") covered the rest of the planet. Other minor oceans were the Paleo-Tethys Ocean, Proto-Tethys Ocean, Rheic Ocean, and Ural Ocean (which was closed during the collision with Siberia and Baltica).

Sea levels in the Devonian were generally high. Marine faunas continued to be dominated by bryozoa, diverse and abundant brachiopods, the enigmatic hederellids, microconchids and corals. Lily-like crinoids (animals, their resemblance to flowers notwithstanding) were abundant, and trilobites were still fairly common. Among vertebrates, jawless armored fish (ostracoderms) declined in diversity, while the jawed fish (gnathostomes) simultaneously increased in both the sea and fresh water. Armored placoderms were numerous during the lower stages of the Devonian Period and became extinct in the Late Devonian, perhaps because of competition for food against the other fish species. Early cartilaginous (Chondrichthyes) and bony fishes (Osteichthyes) also become diverse and played a large role within the Devonian seas. The first abundant genus of shark, "Cladoselache", appeared in the oceans during the Devonian Period. The great diversity of fish around at the time has led to the Devonian being given the name "The Age of Fish" in popular culture.

The first ammonites also appeared during or slightly before the early Devonian Period around 400 Mya.

A now dry barrier reef, located in present-day Kimberley Basin of northwest Australia, once extended a thousand kilometres, fringing a Devonian continent. Reefs in general are built by various carbonate-secreting organisms that have the ability to erect wave-resistant frameworks close to sea level. The main contributors of the Devonian reefs were unlike modern reefs, which are constructed mainly by corals and calcareous algae. They were composed of calcareous algae and coral-like stromatoporoids, and tabulate and rugose corals, in that order of importance.

By the Devonian Period, life was well underway in its colonisation of the land. The moss forests and bacterial and algal mats of the Silurian were joined early in the period by primitive rooted plants that created the first stable soils and harbored arthropods like mites, scorpions, trigonotarbids and myriapods (although arthropods appeared on land much earlier than in the Early Devonian and the existence of fossils such as "Climactichnites" suggest that land arthropods may have appeared as early as the Cambrian). Also the first possible fossils of insects appeared around 416 Mya in the Early Devonian. Evidence for the earliest tetrapods takes the form of trace fossils in shallow lagoon environments within a marine carbonate platform/shelf during the Middle Devonian, although these traces have been questioned and an interpretation as fish feeding traces (Piscichnus) has been advanced.

Many Early Devonian plants did not have true roots or leaves like extant plants although vascular tissue is observed in many of those plants. Some of the early land plants such as "Drepanophycus" likely spread by vegetative growth and spores. The earliest land plants such as "Cooksonia" consisted of leafless, dichotomous axes and terminal sporangia and were generally very short-statured, and grew hardly more than a few centimetres tall. By far the largest land organism during this period was the enigmatic "Prototaxites", which was possibly the fruiting body of an enormous fungus, rolled liverwort mat, or another organism of uncertain affinities that stood more than 8 metres tall, and towered over the low, carpet-like vegetation. By the Middle Devonian, shrub-like forests of primitive plants existed: lycophytes, horsetails, ferns, and progymnosperms had evolved. Most of these plants had true roots and leaves, and many were quite tall. The earliest-known trees, from the genus "Wattieza", appeared in the Late Devonian around 385 Mya. In the Late Devonian, the tree-like ancestral Progymnosperm "Archaeopteris" which had conifer-like true wood and fern-like foliage and the cladoxylopsids grew. (See also: lignin.) These are the oldest-known trees of the world's first forests. By the end of the Devonian, the first seed-forming plants had appeared. This rapid appearance of so many plant groups and growth forms has been called the "Devonian Explosion".

The 'greening' of the continents acted as a carbon sink, and atmospheric concentrations of carbon dioxide may have dropped. This may have cooled the climate and led to a massive extinction event. See Late Devonian extinction.

Primitive arthropods co-evolved with this diversified terrestrial vegetation structure. The evolving co-dependence of insects and seed-plants that characterised a recognisably modern world had its genesis in the Late Devonian period. The development of soils and plant root systems probably led to changes in the speed and pattern of erosion and sediment deposition. The rapid evolution of a terrestrial ecosystem that contained copious animals opened the way for the first vertebrates to seek out a terrestrial living. By the end of the Devonian, arthropods were solidly established on the land.

A major extinction occurred at the beginning of the last phase of the Devonian period, the Famennian faunal stage (the Frasnian-Famennian boundary), about Mya, when all the fossil agnathan fishes, save for the psammosteid heterostraci, suddenly disappeared. A second strong pulse closed the Devonian period. The Late Devonian extinction was one of five major extinction events in the history of the Earth's biota, and was more drastic than the familiar extinction event that closed the Cretaceous.

The Devonian extinction crisis primarily affected the marine community, and selectively affected shallow warm-water organisms rather than cool-water organisms. The most important group to be affected by this extinction event were the reef-builders of the great Devonian reef systems.

Amongst the severely affected marine groups were the brachiopods, trilobites, ammonites, conodonts, and acritarchs, as well as jawless fish, and all placoderms. Land plants as well as freshwater species, such as our tetrapod ancestors, were relatively unaffected by the Late Devonian extinction event (there is a counterargument that the Devonian extinctions nearly wiped out the tetrapods).

The reasons for the Late Devonian extinctions are still unknown, and all explanations remain speculative. Canadian paleontologist Digby McLaren suggested in 1969 that the Devonian extinction events were caused by an asteroid impact. However, while there were Late Devonian collision events (see the Alamo bolide impact), little evidence supports the existence of a large enough Devonian crater.





</doc>
<doc id="7993" url="https://en.wikipedia.org/wiki?curid=7993" title="Dungeon Master (disambiguation)">
Dungeon Master (disambiguation)

A Dungeon Master is the organizer of a "Dungeons & Dragons" role-playing game.

Dungeon Master may also refer to:





</doc>
<doc id="7994" url="https://en.wikipedia.org/wiki?curid=7994" title="David Thompson (explorer)">
David Thompson (explorer)

David Thompson (30 April 1770 – 10 February 1857) was a British-Canadian fur trader, surveyor, and map-maker, known to some native peoples as "Koo-Koo-Sint" or "the Stargazer." Over Thompson's career, he travelled some across North America, mapping of North America along the way. For this historic feat, Thompson has been described as the "greatest land geographer who ever lived."

David Thompson was born in Westminster, Middlesex, to recent Welsh migrants David and Ann Thompson. When Thompson was two, his father died. Due to the financial hardship with his mother without resources, Thompson and his older brother were placed in the Grey Coat Hospital, a school for the disadvantaged of Westminster. Thompson graduated to the Grey Coat mathematical school, where he was introduced to basic navigation skills. He later built on these to make his career. In 1784, at the age of 14, he entered a seven-year apprenticeship with the Hudson's Bay Company. He set sail on 28 May of that year, and left England for North America.

Thompson arrived in Churchill (now in Manitoba) and was put to work as a secretary, copying the personal papers of the governor of Fort Churchill, Samuel Hearne. The next year he was transferred to nearby York Factory, and over the next few years spent time as a secretary at Cumberland House, Saskatchewan, and South Branch House before arriving at Manchester House in 1787. During those years he learned to keep accounts and other records, calculate values of furs (It was noted that he also had several expensive beaver pelts at that time even when a secretary's job would not pay terribly well), track supplies and other duties.

On 23 December 1788, Thompson seriously fractured his leg, forcing him to spend the next two winters at Cumberland House convalescing. It was during this time that he greatly refined and expanded his mathematical, astronomical, and surveying skills under the tutelage of Hudson's Bay Company surveyor Philip Turnor. It was also during this time that he lost sight in his right eye.

In 1790, with his apprenticeship nearing its end, Thompson requested a set of surveying tools in place of the typical parting gift of fine clothes offered by the company to those completing their indenture. He received both. He entered the employ of the Hudson's Bay Company as a fur trader. In 1792 he completed his first significant survey, mapping a route to Lake Athabasca (where today's Alberta/Saskatchewan border is located). In recognition of his map-making skills, the company promoted Thompson to surveyor in 1794. He continued working for the Hudson's Bay Company until 23 May 1797 when, frustrated with the Hudson's Bay Company's policies over promoting the use of alcohol with indigenous people in the fur trade, he left. He walked in the snow in order to enter the employ of the competition, the North West Company. There he continued to work as a fur trader and surveyor.

Thompson's decision to defect to the North West Company in 1797 without providing the customary one-year notice was not well received by his former employers. But the North West Company was more supportive of Thompson pursuing his interest in surveying and work on mapping the interior of what was to become Canada, as they judged it in the company's long-term interest. In 1797, Thompson was sent south by his employers to survey part of the Canada-US boundary along the water routes from Lake Superior to Lake of the Woods to satisfy unresolved questions of territory arising from the Jay Treaty between Great Britain and the United States after the American Revolutionary War.

By 1798 Thompson had completed a survey of from Grand Portage, through Lake Winnipeg, to the headwaters of the Assiniboine and Mississippi rivers, as well as two sides of Lake Superior. In 1798, the company sent him to Red Deer Lake (Lac La Biche in present-day Alberta) to establish a trading post. (The English translation of Lac la Biche: Red Deer Lake, was first recorded on the Mackenzie map of 1793.) Thompson spent the next few seasons trading based in Fort George (now in Alberta), and during this time led several expeditions into the Rocky Mountains.

In 1804, at the annual meeting of the North West Company in Kaministiquia, Thompson was made a full partner of the company. He spent the next few seasons based there managing the fur trading operations but still finding time to expand his surveys of the waterways around Lake Superior. At the 1806 company meeting, officers decided to send Thompson back out into the interior. Concern over the American-backed expedition of Lewis and Clark prompted the North West Company to charge Thompson with the task of finding a route to the Pacific to open up the lucrative trading territories of the Pacific Northwest.

After the general meeting in 1806, Thompson travelled to Rocky Mountain House and prepared for an expedition to follow the Columbia River to the Pacific. In June 1807 Thompson crossed the Rocky Mountains and spent the summer surveying the Columbia basin; he continued to survey the area over the next few seasons. Thompson mapped and established trading posts in Northwestern Montana, Idaho, Washington, and Western Canada. Trading posts he founded included Kootenae House, Kullyspell House and Saleesh House; the latter two were the first trading posts west of the Rockies in Idaho and Montana, respectively. These posts established by Thompson extended North West Company fur trading territory into the Columbia Basin drainage area. The maps he made of the Columbia River basin east of the Cascade Mountains were of such high quality and detail that they continued to be regarded as authoritative well into the mid-20th century. 

In early 1810, Thompson was returning eastward toward Montreal but, while en route at Rainy Lake, received orders to return to the Rocky Mountains and establish a route to the mouth of the Columbia. The North West Company was responding to the plans of American John Jacob Astor to send a ship around the Americas to establish a fur trading post of the Pacific Fur Company on the Pacific Coast. During his return, Thompson was delayed by an angry group of Peigan natives at Howse Pass. He was ultimately forced to seek a new route across the Rocky Mountains and found one through the Athabasca Pass.

David Thompson was the first European to navigate the full length of the Columbia River. During Thompson's 1811 voyage down the Columbia River, he camped at the junction with the Snake River on 9 July 1811. There he erected a pole and a notice claiming the country for Great Britain and stating the intention of the North West Company to build a trading post at the site. This notice was found later that year by Astor company workers looking to establish an inland fur post, contributing to their selection of a more northerly site at Fort Okanogan. The North West Company established its post of Fort Nez Percés near the Snake River confluence several years later. Continuing down the Columbia, Thompson passed the barrier of The Dalles with much less difficulty than that undergone by Lewis and Clark, as high water carried his boat over Celilo Falls and many of the rapids. On 14 July 1811, Thompson reached the partially constructed Fort Astoria at the mouth of the Columbia, arriving two months after the Pacific Fur Company's ship, the "Tonquin".

Before returning upriver and across the mountains, Thompson hired Naukane, a Native Hawaiian Takane labourer brought to Fort Astoria by the Pacific Fur Company's ship "Tonquin". Naukane, known as Coxe to Thompson, accompanied Thompson across the continent to Lake Superior before journeying on to England.

Thompson wintered at Saleesh House before beginning his final journey back to Montreal in 1812, where the North West Company was based.

In his published journals, Thompson recorded seeing large footprints near what is now Jasper, Alberta, in 1811. It has been suggested that these prints were similar to what has since been called the sasquatch. However, Thompson noted that these tracks showed "a small Nail at the end of each [toe]", and stated that these tracks "very much resembles a large Bear's Track".

In 1820, the English geologist, John Jeremiah Bigsby, attended a dinner party given by The Hon. William McGillivray at his home, Chateau St. Antoine, one of the early estates in Montreal's Golden Square Mile. He describes the party and some of the guests in his entertaining book "The Shoe and Canoe", giving an excellent description of David Thompson:

On 10 June 1799 at Île-à-la-Crosse, Thompson married Charlotte Small, a thirteen-year-old Métis daughter of Scottish fur trader Patrick Small and a Cree mother. Their marriage was formalised thirteen years later at the Scotch Presbyterian Church in Montreal on 30 October 1812. He and Charlotte had 13 children together; five of them were born before he left the fur trade. The family did not adjust easily to life in Eastern Canada; they lived in Montreal while he was traveling. Two of the children, John (aged 5) and Emma (aged 7), died of round worms, a common parasite. By the time of Thompson's death, the couple had been married 69 years, the longest marriage known in Canada pre-Confederation.

Upon his arrival back in Montreal, Thompson retired with a generous pension from the North West Company. He settled in nearby Terrebonne and worked on completing his great map, a summary of his lifetime of exploring and surveying the interior of North America. The map covered the wide area stretching from Lake Superior to the Pacific, and was given by Thompson to the North West Company. Thompson's 1814 map, his greatest achievement, was so accurate that 100 years later it was still the basis for many of the maps issued by the Canadian government. It now resides in the Archives of Ontario.

In 1815, Thompson moved his family to Williamstown, Upper Canada, and a few years later was employed to survey the newly established borders with the United States from Lake of the Woods to the Eastern Townships of Quebec, established by Treaty of Ghent after the War of 1812. In 1843 Thompson completed his atlas of the region from Hudson Bay to the Pacific Ocean.

Afterwards, Thompson returned to a life as a land owner, but soon financial misfortune would ruin him. By 1831 he was so deeply in debt he was forced to take up a position as a surveyor for the British American Land Company to provide for his family. His luck continued to worsen and he was forced to move in with his daughter and son-in-law in 1845. He began work on a manuscript chronicling his life exploring the continent, but this project was left unfinished when his sight failed him completely in 1851.

The land mass mapped by Thompson amounted to of wilderness (one-fifth of the continent). His contemporary, the great explorer Alexander Mackenzie, remarked that Thompson did more in ten months than he would have thought possible in two years.

Despite these significant achievements, Thompson died in Montreal in near obscurity on February 10, 1857, his accomplishments almost unrecognised. He never finished the book of his 28 years in the fur trade, based on his 77 field notebooks, before he died. In the 1890s geologist J.B. Tyrrell resurrected Thompson's notes and in 1916 published them as "David Thompson's Narrative", as part of the General Series of the Champlain Society. Further editions and re-examinations of Thompson's life and works were published in 1962 by Richard Glover, in 1971 by Victor Hopwood, and in 2015 by William Moreau.
Thompson's body was interred in Montreal's Mount Royal Cemetery in an unmarked grave. It was not until 1926 that efforts by J.B. Tyrell and the Canadian Historical Society resulted in the placing of a tombstone to mark his grave. The next year, Thompson was named a National Historic Person by the federal government, one of the earliest such designations. A federal plaque reflecting that status is located at Jasper National Park, Alberta. Meantime, Thompson's achievements are central reasons for other national historic designations:

In 1957, one hundred years after his death, Canada's post office department honoured him with his image on a postage stamp. The David Thompson Highway in Alberta was named in his honour, along with David Thompson High School situated on the side of the highway near Leslieville, Alberta. His prowess as a geographer is now well-recognized. He has been called "the greatest land geographer that the world has produced."

There is a monument dedicated to David Thompson (maintained by the state of North Dakota) near the former town site of the ghost town, Verendrye, North Dakota, located approximately north and west of Karlsruhe, North Dakota. Thompson Falls, Montana, and British Columbia's Thompson River are also named after the explorer.
The year 2007 marked the 150th year of Thompson's death and the 200th anniversary of his first crossing of the Rocky Mountains. Commemorative events and exhibits were planned across Canada and the United States from 2007 to 2011 as a celebration of his accomplishments.

In 2007, a commemorative plaque was placed on a wall at the Grey Coat Hospital, the school for the disadvantaged of Westminster David Thompson attended as a boy, by English author and TV presenter Ray Mears.

Thompson was the subject of a 1964 National Film Board of Canada short film "David Thompson: The Great Mapmaker ", as well as the BBC2 programme "Ray Mears' Northern Wilderness" (Episode 5), broadcast in November 2009.

He is referenced in the 1981 folk song "Northwest Passage" by Stan Rogers.

The national park service, Parks Canada, announced in 2018 that it had named its new research vessel , to be used for underwater archaeology, including sea floor mapping, and for marine science in the Pacific, Atlantic, Arctic Oceans, and the Great Lakes. It will be the main platform for research on the Wrecks of HMS "Erebus" and HMS "Terror" National Historic Site.







</doc>
<doc id="7995" url="https://en.wikipedia.org/wiki?curid=7995" title="Dioscoreales">
Dioscoreales

The Dioscoreales are an order of monocotyledonous flowering plants in modern classification systems, such as the Angiosperm Phylogeny Group and the Angiosperm Phylogeny Web. Within the monocots Dioscoreales are grouped in the lilioid monocots where they are in a sister group relationship with the Pandanales. Of necessity the Dioscoreales contain the family Dioscoreaceae which includes the yam ("Dioscorea") that is used as an important food source in many regions around the globe. Older systems tended to place all lilioid monocots with reticulate veined leaves (such as Smilacaceae and Stemonaceae together with Dioscoraceae) in Dioscoreales. As currently circumscribed by phylogenetic analysis using combined morphology and molecular methods, Dioscreales contains many reticulate veined vines in Dioscoraceae, it also includes the myco-heterotrophic Burmanniaceae and the autotrophic Nartheciaceae. The order consists of three families, 22 genera and about 850 species.

Dioscoreales are vines or herbaceous forest floor plants. They may be achlorophyllous or saprophytic. Synapomorphies include tuberous roots, glandular hairs, seed coat characteristics and the presence of calcium oxalate crystals. Other characteristics of the order include the presence of saponin steroids, annular vascular bundles that are found in both the stem and leaf. The leaves are often unsheathed at the base, have a distinctive petiole and reticulate veined lamina. Alternatively they may be small and scale-like with a sheathed base. The flowers are actinomorphic, and may be bisexual or dioecious, while the flowers or inflorescence bear glandular hairs. The perianth may be conspicuous or reduced and the style is short with well developed style branches. The tepals persist in the development of the fruit, which is a dry capsule or berry. In the seed, the endotegmen is tanniferous and the embryo short.

All of the species except the genera placed in Nartheciaceae express simultaneous microsporogenesis. Plants in Nartheciaceae show successive microsporogenesis which is one of the traits indicating that the family is sister to all the other members included in the order.

For the early history from Lindley (1853) onwards, see Caddick "et al." (2000) Table 1, Caddick et al. (2002a) Table 1 and Table 2 in Bouman (1995). The taxonomic classification of Dioscoreales has been complicated by the presence of a number of morphological features reminiscent of the dicotyledons, leading some authors to place the order as intermediate between the monocotyledons and the dicotyledons.
While Lindley did not use the term "Dioscoreales", he placed the family Dioscoraceae together with four other families in what he referred to as an Alliance (the equivalent of the modern Order) called Dictyogens. He reflected the uncertainty as to the place of this Alliance by placing it as a class of its own between Endogens (monocots) and Exogens (dicots) The botanical authority is given to von Martius (1835) by APG for his description of the Dioscoreae family or "Ordo", while other sources cite Hooker (Dioscoreales Hook.f.) for his use of the term "Dioscorales" in 1873 with a single family, Dioscoreae. However, in his more definitive work, the "Genera plantara" (1883), he simply placed Dioscoraceae in the Epigynae "Series".

Although Charles Darwin's Origin of Species (1859) preceded Bentham and Hooker's publication, the latter project was commenced much earlier and George Bentham was initially sceptical of Darwinism. The new phyletic approach changed the way that taxonomists considered plant classification, incorporating evolutionary information into their schemata, but this did little to further define the circumscription of Dioscoreaceae. The major works in the late nineteenth and early twentieth century employing this approach were in the German literature. Authors such as Eichler, Engler and Wettstein placed this family in the Liliiflorae, a major subdivision of monocotyledons. it remained to Hutchinson (1926) to resurrect the Dioscoreales to group Dioscoreaceae and related families together. Hutchinson's circumscription of Dioscoreales included three other families in addition to Dioscoreaceae, Stenomeridaceae, Trichopodaceae and Roxburghiaceae. Of these only Trichopodaceae was included in the Angiosperm Phylogeny Group (APG) classification (see below), but was subsumed into Dioscoraceae. Stenomeridaceae, as "Stenomeris" was also included in Dioscoreaceae as subfamily Stenomeridoideae, the remaining genera being grouped in subfamily Dioscoreoideae. Roxburghiaceae on the other hand was segregated in the sister order Pandanales as Stemonaceae. Most taxonomists in the twentieth century (the exception was the 1981 Cronquist system which placed most such plants in order Liliales, subclass Liliidae, class Liliopsida=monocotyledons, division Magnoliophyta=angiosperms) recognised Dioscoreales as a distinct order, but demonstrated wide variations in its composition.

Dahlgren, in the second version of his taxonomic classifiacation (1982) raised the Liliiflorae to a superorder and placed Dioscoreales as an order within it. In his system, Dioscoreales contained only three families, Dioscoreaceae, Stemonaceae ("i.e." Hutchinson's Roxburghiaceae) and Trilliaceae. The latter two families had been treated as a separate order (Stemonales, or Roxburghiales) by other authors, such as Huber (1969). The APG would later assign these to Pandanales and Liliales respectively. Dahlgren's construction of Dioscoreaceae included the Stenomeridaceae and Trichopodaceae, doubting these were distinct, and Croomiaceae in Stemonaceae. Furthermore, he expressed doubts about the order's homogeneity, especially Trilliaceae. The Dioscoreales at that time were marginally distinguishable from the Asparagales. In his examination of Huber's Stemonales, he found that the two constituent families had as close an affinity to Dioscoreaceae as to each other, and hence included them. He also considered closely related families and their relationship to Dioscoreales, such as the monogeneric Taccaceae, then in its own order, Taccales. Similar considerations were discussed with respect to two Asparagales families, Smilacaceae and Petermanniaceae.

In Dahlgren's third and final version (1985) that broader circumscription of Dioscoreales was created within the superorder Lilianae, subclass Liliidae (monocotyledons), class Magnoliopsida (angiosperms) and comprised the seven families Dioscoreaceae, Petermanniaceae, Smilacaceae, Stemonaceae, Taccaceae, Trichopodaceae and Trilliaceae. Thismiaceae has either been treated as a separate family closely related to Burmanniaceae or as a tribe (Thismieae) within a more broadly defined Burmanniaceae, forming a separate Burmanniales order in the Dahlgren system. The related Nartheciaceae were treated as tribe Narthecieae within the Melanthiaceae in a third order, the Melanthiales, by Dahlgren. Dahlgren considered the Dioscoreales to most strongly resemble the ancestral monocotyledons, and hence sharing "dicotyledonous" characteristics, making it the most central monocotyledon order. Of these seven families, Bouman considered Dioscoreaceae, Trichopodaceae, Stemonaceae and Taccaceae to represent the "core" families of the order. However, that study also indicated both a clear delineation of the order from other orders particularly Asparagales, and a lack of homogeneity within the order.

The increasing availability of molecular phylogenetics methods in addition to mophological characteristics in the 1990s led to major reconsiderations of the relationships within the monocotyledons. In that large multi-institutional examination of the seed plants using the plastid gene "rbc"L the authors used Dahlgren's system as their basis, but followed Thorne (1992) in altering the suffixes of the superorders from ""-iflorae"" to ""-anae"". This demonstrated that the Lilianae comprised three lineages corresponding to Dahlgren's
Dioscoreales, Liliales, and Asparagales orders.

Under the Angiosperm Phylogeny Group system of 1998, which took Dahlgren's system as a basis, the order was placed in the monocot clade and comprised the five families Burmanniaceae, Dioscoreaceae, Taccaceae, Thismiaceae and Trichopodaceae.

In APG II (2003), a number of changes were made to Dioscoreales, as a result of an extensive study by Caddick and colleagues (2002), using an analysis of three genes, "rbc"L, "atp"B and 18S rDNA, in addition to morphology. These studies resulted in a re-examination of the relationships between most of the genera within the order. Thismiaceae was shown to be a sister group to Burmanniaceae, and so was included in it. The monotypic families Taccaceae and Trichopodaceae were included in Dioscoreaceae, while Nartheciaceae could also be grouped within Dioscoreales. APG III (2009) did not change this, so the order now comprises three families Burmanniaceae, Dioscoreaceae and Nartheciaceae.

Although further research on the deeper relationships within Dioscoreales continues, the APG IV (2016) authors felt it was still premature to propose a restructuring of the order. Specifically these issues involve conflicting information as to the relationship between "Thismia" and Burmanniaceae, and hence whether Thismiaceae should be subsumed in the latter, or reinstated.

Molecular phylogenetics in Dioscoreales poses special problems due to the absence of plastid genes in mycoheterotrophs. Dioscoreales is monophyletic and is placed as a sister order to Pandanales, as shown in Cladogram I.

The data for the evolution of the order is collected from molecular analyses since there are no such fossils found. It is estimated that Dioscoreales and its sister clade Pandanales split up around 121 millions of years ago during Early Cretaceous when the stem group was formed. Then it took 3 to 6 millions of years for the crown group to differentiate in Mid Cretaceous.

The three families of Dioscreales constitutes about 22 genera and about 849 species making it one of the smaller monocot orders. Of these, the largest group is "Dioscorea" (yams) with about 450 species. By contrast the second largest genus is "Burmannia" with about 60 species, and most have only one or two.

Some authors, preferring the original APG (1998)families, continue to treat Thismiaceae separately from Burmanniaceae and Taccaceae from Dioscoreaceae. But in the 2015 study of Hertwerk and colleagies, seven genera representing all three families were examined with an eight gene dataset. Dioscoreales was monophyletic and three subclades were represented corresponding to the APG families. Dioscoreaceae and Burmanniaceae were in a sister group relationship.

Named after the type genus "Dioscorea", which in turn was named by Linnaeus in 1753 to honour the Greek physician and botanist Dioscorides.

Species from this order are distributed across all of the continents except Antarctica. They are mainly tropical or subtropical representatives but however there are members of Dioscoreaceae and Nartheciaceae families found in cooler regions of Europe and North America. Order Dioscoreales contains plants that are able to form an underground organ for reservation of nutritions as many other monocots. An exception is the family Burmanniaceae which is entirely myco-heterotrophic and contains species that lack photosynthetic abilities.

The three families included in order Dioscoreales also represent three different ecological groups of plants. Dioscoreaceae contains mainly vines ("Dioscorea") and other crawling species ("Epipetrum"). Nartheciaceae on the other hand is a family composed of herbeceous plants with a rather lily-like appearance ("Aletris") while Burmanniaceae is entirely myco-heterotrophic group.

Many members of Dioscoreaceae produce tuberous starchy roots (yams) which form staple foods in tropical regions. They have also been the source of steroids for the pharmaceutical industry, including the production of oral contraceptives.







</doc>
<doc id="8000" url="https://en.wikipedia.org/wiki?curid=8000" title="Default">
Default

Default may refer to:





</doc>
<doc id="8002" url="https://en.wikipedia.org/wiki?curid=8002" title="Deposition">
Deposition

Deposition may refer to:






</doc>
<doc id="8005" url="https://en.wikipedia.org/wiki?curid=8005" title="Dentistry">
Dentistry

Dentistry is a branch of medicine that consists of the study, diagnosis, prevention, and treatment of diseases, disorders, and conditions of the oral cavity, commonly in the dentition but also the oral mucosa, and of adjacent and related structures and tissues, particularly in the maxillofacial (jaw and facial) area. Although primarily associated with teeth among the general public, the field of dentistry or dental medicine is not limited to teeth but includes other aspects of the craniofacial complex including the temporomandibular joint and other supporting, muscular, lymphatic, nervous, vascular, and anatomical structures. 

Dentistry is often also understood to subsume the now largely defunct medical specialty of stomatology (the study of the mouth and its disorders and diseases) for which reason the two terms are used interchangeably in certain regions.

Dental treatments are carried out by a dental team, which often consists of a dentist and dental auxiliaries (dental assistants, dental hygienists, dental technicians, as well as dental therapists). Most dentists either work in private practices (primary care), dental hospitals or (secondary care) institutions (prisons, armed forces bases, etc.).

The history of dentistry is almost as ancient as the history of humanity and civilization with the earliest evidence dating from 7000 BC. Remains from the early Harappan periods of the Indus Valley Civilization ( BC) show evidence of teeth having been drilled dating back 9,000 years. It is thought that dental surgery was the first specialization from medicine.

The term dentistry comes from "dentist", which comes from French "dentiste", which comes from the French and Latin words for tooth. The term for the associated scientific study of teeth is odontology (from Ancient Greek ὀδούς (odoús, "tooth")) – the study of the structure, development, and abnormalities of the teeth.

Dentistry usually encompasses practices related to the oral cavity. According to the World Health Organization, oral diseases are major public health problems due to their high incidence and prevalence across the globe, with the disadvantaged affected more than other socio-economic groups.

The majority of dental treatments are carried out to prevent or treat the two most common oral diseases which are dental caries (tooth decay) and periodontal disease (gum disease or pyorrhea). Common treatments involve the restoration of teeth, extraction or surgical removal of teeth, scaling and root planing and endodontic root canal treatment.

All dentists in the United States undergo at least three years of undergraduate studies, but nearly all complete a bachelor's degree. This schooling is followed by four years of dental school to qualify as a "Doctor of Dental Surgery" (DDS) or "Doctor of Dental Medicine" (DMD). Dentists need to complete additional qualifications or continuing education to carry out more complex treatments such as sedation, oral and maxillofacial surgery, and dental implants.

By nature of their general training they can carry out the majority of dental treatments such as restorative (fillings, crowns, bridges), prosthetic (dentures), endodontic (root canal) therapy, periodontal (gum) therapy, and extraction of teeth, as well as performing examinations, radiographs (x-rays), and diagnosis. Dentists can also prescribe medications such as antibiotics, sedatives, and any other drugs used in patient management.

Dentists also encourage prevention of oral diseases through proper hygiene and regular, twice yearly, checkups for professional cleaning and evaluation. Oral infections and inflammations may affect overall health and conditions in the oral cavity may be indicative of systemic diseases, such as osteoporosis, diabetes, celiac disease or cancer. Many studies have also shown that gum disease is associated with an increased risk of diabetes, heart disease, and preterm birth. The concept that oral health can affect systemic health and disease is referred to as "oral-systemic health".

Dr. John M. Harris started the world's first dental school in Bainbridge, Ohio, and helped to establish dentistry as a health profession. It opened on 21 February 1828, and today is a dental museum. The first dental college, Baltimore College of Dental Surgery, opened in Baltimore, Maryland, US in 1840. The second in the United States was the Philadelphia College of Dental Surgery, established in 1852. In 1907, Temple University accepted a bid to incorporate the school.

Studies show that dentists that graduated from different countries, or even from different dental schools in one country, may make different clinical decisions for the same clinical condition. For example, dentists that graduated from Israeli dental schools may recommend the removal of asymptomatic impacted third molar (wisdom teeth) more often than dentists that graduated from Latin American or Eastern European dental schools.

In the United Kingdom, the 1878 British Dentists Act and 1879 Dentists Register limited the title of "dentist" and "dental surgeon" to qualified and registered practitioners. However, others could legally describe themselves as "dental experts" or "dental consultants". The practice of dentistry in the United Kingdom became fully regulated with the 1921 Dentists Act, which required the registration of anyone practising dentistry. The British Dental Association, formed in 1880 with Sir John Tomes as president, played a major role in prosecuting dentists practising illegally. Dentists in the United Kingdom are now regulated by the General Dental Council.

In Korea, Taiwan, Japan, Finland, Sweden, Brazil, Chile, the United States, and Canada, a dentist is a healthcare professional qualified to practice dentistry after graduating with a degree of either Doctor of Dental Surgery (DDS) or Doctor of Dental Medicine (DMD). This is equivalent to the Bachelor of Dental Surgery/Baccalaureus Dentalis Chirurgiae (BDS, BDent, BChD, BDSc) that is awarded in the UK and British Commonwealth countries. In most western countries, to become a qualified dentist one must usually complete at least four years of postgraduate study; within the European Union the education has to be at least five years. Dentists usually complete between five and eight years of post-secondary education before practising. Though not mandatory, many dentists choose to complete an internship or residency focusing on specific aspects of dental care after they have received their dental degree.

Some dentists undertake further training after their initial degree in order to specialize. Exactly which subjects are recognized by dental registration bodies varies according to location. Examples include:


Tooth decay was low in pre-agricultural societies, the growth in farming society about 10,000 years ago correlated with an increase in tooth decay (cavities). An infected tooth from Italy partially cleaned with flint tools, between 13,820 and 14,160 years old, represents the oldest known dentistry, although a 2017 study suggests that 130,000 years ago the Neanderthals already used rudimentary dentistry tools. The Indus Valley Civilization (IVC) has yielded evidence of dentistry being practised as far back as 7000 BC. An IVC site in Mehrgarh indicates that this form of dentistry involved curing tooth related disorders with bow drills operated, perhaps, by skilled bead crafters. The reconstruction of this ancient form of dentistry showed that the methods used were reliable and effective. The earliest dental filling, made of beeswax, was discovered in Slovenia and dates from 6500 years ago. Dentistry was practiced in prehistoric Malta, as evidenced by a skull which had an abscess lanced from the root of a tooth dating back to around 2500 BC.

An ancient Sumerian text describes a "tooth worm" as the cause of dental caries. Evidence of this belief has also been found in ancient India, Egypt, Japan, and China. The legend of the worm is also found in the writings of Homer, and as late as the 14th century AD the surgeon Guy de Chauliac still promoted the belief that worms cause tooth decay.

Recipes for the treatment of toothache, infections and loose teeth are spread throughout the Ebers Papyrus, Kahun Papyri, Brugsch Papyrus, and Hearst papyrus of Ancient Egypt. The Edwin Smith Papyrus, written in the 17th century BC but which may reflect previous manuscripts from as early as 3000 BC, discusses the treatment of dislocated or fractured jaws. In the 18th century BC, the Code of Hammurabi referenced dental extraction twice as it related to punishment. Examination of the remains of some ancient Egyptians and Greco-Romans reveals early attempts at dental prosthetics. However, it is possible the prosthetics were prepared after death for aesthetic reasons.

Ancient Greek scholars Hippocrates and Aristotle wrote about dentistry, including the eruption pattern of teeth, treating decayed teeth and gum disease, extracting teeth with forceps, and using wires to stabilize loose teeth and fractured jaws. Some say the first use of dental appliances or bridges comes from the Etruscans from as early as 700 BC. In ancient Egypt, Hesy-Ra is the first named "dentist" (greatest of the teeth). The Egyptians bound replacement teeth together with gold wire. Roman medical writer Cornelius Celsus wrote extensively of oral diseases as well as dental treatments such as narcotic-containing emollients and astringents. The earliest dental amalgams were first documented in a Tang Dynasty medical text written by the Chinese physician Su Kung in 659, and appeared in Germany in 1528.

During the Islamic Golden Age Dentistry was discussed in several famous books of medicine such as The Canon in medicine written by Avicenna and Al-Tasreef by Al-Zahrawi who is considered the greatest surgeon of the Middle ages, Avicenna said that jaw fracture should be reduced according to the occlusal guidance of the teeth; this principle is still valid in modern times. while Al-Zahrawi made a lot of surgical tools that resemble the modern tools.

Historically, dental extractions have been used to treat a variety of illnesses. During the Middle Ages and throughout the 19th century, dentistry was not a profession in itself, and often dental procedures were performed by barbers or general physicians. Barbers usually limited their practice to extracting teeth which alleviated pain and associated chronic tooth infection. Instruments used for dental extractions date back several centuries. In the 14th century, Guy de Chauliac most probably invented the dental pelican (resembling a pelican's beak) which was used to perform dental extractions up until the late 18th century. The pelican was replaced by the dental key which, in turn, was replaced by modern forceps in the 19th century.

The first book focused solely on dentistry was the "Artzney Buchlein" in 1530, and the first dental textbook written in English was called "Operator for the Teeth" by Charles Allen in 1685.

In the United Kingdom there was no formal qualification for the providers of dental treatment until 1859 and it was only in 1921 that the practice of dentistry was limited to those who were professionally qualified. The Royal Commission on the National Health Service in 1979 reported that there were then more than twice as many registered dentists per 10,000 population in the UK than there were in 1921.

It was between 1650 and 1800 that the science of modern dentistry developed. The English physician Thomas Browne in his "A Letter to a Friend" ( pub. 1690) made an early dental observation with characteristic humour:

The French surgeon Pierre Fauchard became known as the "father of modern dentistry". Despite the limitations of the primitive surgical instruments during the late 17th and early 18th century, Fauchard was a highly skilled surgeon who made remarkable improvisations of dental instruments, often adapting tools from watchmakers, jewelers and even barbers, that he thought could be used in dentistry. He introduced dental fillings as treatment for dental cavities. He asserted that sugar derivate acids like tartaric acid were responsible for dental decay, and also suggested that tumors surrounding the teeth and in the gums could appear in the later stages of tooth decay.

Fauchard was the pioneer of dental prosthesis, and he discovered many methods to replace lost teeth. He suggested that substitutes could be made from carved blocks of ivory or bone. He also introduced dental braces, although they were initially made of gold, he discovered that the teeth position could be corrected as the teeth would follow the pattern of the wires. Waxed linen or silk threads were usually employed to fasten the braces. His contributions to the world of dental science consist primarily of his 1728 publication Le chirurgien dentiste or The Surgeon Dentist. The French text included "basic oral anatomy and function, dental construction, and various operative and restorative techniques, and effectively separated dentistry from the wider category of surgery".

After Fauchard, the study of dentistry rapidly expanded. Two important books, "Natural History of Human Teeth" (1771) and "Practical Treatise on the Diseases of the Teeth" (1778), were published by British surgeon John Hunter. In 1763 he entered into a period of collaboration with the London-based dentist James Spence. He began to theorise about the possibility of tooth transplants from one person to another. He realised that the chances of an (initially, at least) successful tooth transplant would be improved if the donor tooth was as fresh as possible and was matched for size with the recipient. These principles are still used in the transplantation of internal organs. Hunter conducted a series of pioneering operations, in which he attempted a tooth transplant. Although the donated teeth never properly bonded with the recipients' gums, one of Hunter's patients stated that he had three which lasted for six years, a remarkable achievement for the period.

Major advances were made in the 19th century, and dentistry evolved from a trade to a profession. The profession came under government regulation by the end of the 19th century. In the UK the Dentist Act was passed in 1878 and the British Dental Association formed in 1879. In the same year, Francis Brodie Imlach was the first ever dentist to be elected President of the Royal College of Surgeons (Edinburgh), raising dentistry onto a par with clinical surgery for the first time.

Long term occupational noise exposure can contribute to permanent hearing loss, which is referred to as noise-induced hearing loss (NIHL) and tinnitus. Noise exposure can cause excessive stimulation of the hearing mechanism, which damages the delicate structures of the inner ear. NIHL can occur when an individual is exposed to sound levels above 90 dBA according to the Occupational Safety and Health Administration (OSHA). Regulations state that the permissible noise exposure levels for individuals is 90 dBA. For the National Institute for Occupational Safety and Health (NIOSH), exposure limits are set to 85 dBA. Exposures below 85 dBA are not considered to be hazardous. Time limits are placed on how long an individual can stay in an environment above 85 dBA before it causes hearing loss. OSHA places that limitation at 8 hours for 85 dBA. The exposure time becomes shorter as the dBA level increases.

Within the field of dentistry, a variety of cleaning tools are used including piezoelectric and sonic scalers, and ultrasonic scalers and cleaners. While a majority of the tools do not exceed 75 dBA, prolonged exposure over many years can lead to hearing loss or complaints of tinnitus. Few dentists have reported using personal hearing protective devices, which could offset any potential hearing loss or tinnitus.


</doc>
<doc id="8007" url="https://en.wikipedia.org/wiki?curid=8007" title="Diameter">
Diameter

In geometry, a diameter of a circle is any straight line segment that passes through the center of the circle and whose endpoints lie on the circle. It can also be defined as the longest chord of the circle. Both definitions are also valid for the diameter of a sphere.

In more modern usage, the length of a diameter is also called the diameter. In this sense one speaks of "the" diameter rather than "a" diameter (which refers to the line itself), because all diameters of a circle or sphere have the same length, this being twice the radius r.

For a convex shape in the plane, the diameter is defined to be the largest distance that can be formed between two opposite parallel lines tangent to its boundary, and the "width" is often defined to be the smallest such distance. Both quantities can be calculated efficiently using rotating calipers. For a curve of constant width such as the Reuleaux triangle, the width and diameter are the same because all such pairs of parallel tangent lines have the same distance.

For an ellipse, the standard terminology is different. A diameter of an ellipse is any chord passing through the center of the ellipse. For example, conjugate diameters have the property that a tangent line to the ellipse at the endpoint of one of them is parallel to the other one. The longest diameter is called the major axis.

The word "diameter" is derived from Greek διάμετρος ("diametros"), "diameter of a circle", from διά ("dia"), "across, through" and μέτρον ("metron"), "measure". It is often abbreviated DIA, dia, d, or ⌀.

The definitions given above are only valid for circles, spheres and convex shapes. However, they are special cases of a more general definition that is valid for any kind of "n"-dimensional convex or non-convex object, such as a hypercube or a set of scattered points. The diameter of a subset of a metric space is the least upper bound of the set of all distances between pairs of points in the subset. So, if "A" is the subset, the diameter is
If the distance function d is viewed here as having codomain R (the set of all real numbers), this implies that the diameter of the empty set (the case ) equals −∞ (negative infinity). Some authors prefer to treat the empty set as a special case, assigning it a diameter equal to 0, which corresponds to taking the codomain of d to be the set of nonnegative reals.

For any solid object or set of scattered points in n-dimensional Euclidean space, the diameter of the object or set is the same as the diameter of its convex hull. In medical parlance concerning a lesion or in geology concerning a rock, the diameter of an object is the supremum of the set of all distances between pairs of points in the object.

In differential geometry, the diameter is an important global Riemannian invariant.

In plane geometry, a diameter of a conic section is typically defined as any chord which passes through the conic's centre; such diameters are not necessarily of uniform length, except in the case of the circle, which has eccentricity "e" = 0.

The symbol or variable for diameter, ⌀, is similar in size and design to ø, the Latin small letter o with stroke. In Unicode it is defined as . On an Apple Macintosh, the diameter symbol can be entered via the character palette (this is opened by pressing in most applications), where it can be found in the Technical Symbols category.

The character will sometimes not display correctly, however, since many fonts do not include it. In many situations the letter ø (the Latin small letter o with stroke) is an acceptable substitute, which in Unicode is . It can be obtained in UNIX-like operating systems using a Compose key by pressing, in sequence, and on a Macintosh by pressing (the letter o, not the number 0).

In Microsoft Word the diameter symbol can be acquired by typing 2300 and then pressing Alt+X.

In LaTeX the diameter symbol can be obtained with the command codice_1 from the wasysym package.

The diameter symbol ⌀ is distinct from the empty set symbol ∅, from an (italic) uppercase phi "Φ", and from the Nordic vowel Ø. See also slashed zero.

In German, the diameter symbol (German "") is also used as an average symbol ("Durchschnittszeichen").


</doc>
<doc id="8008" url="https://en.wikipedia.org/wiki?curid=8008" title="Direct examination">
Direct examination

The direct examination or examination-in-chief is one stage in the process of adducing evidence from witnesses in a court of law. Direct examination is the questioning of a witness by the party who called him or her, in a trial. Direct examination is usually performed to elicit evidence in support of facts which will satisfy a required element of a party's claim or defense.

In direct examination, one is generally prohibited from asking leading questions. This prevents a lawyer from feeding answers to a favorable witness. An exception to this rule occurs if one side has called a witness, but it is either understood, or becomes clear, that the witness is hostile to the questioner's side of the controversy. The lawyer may then ask the court to declare the person he or she has called to the stand a hostile witness. If the court does so, the lawyer may thereafter ask witness leading questions during direct examination.

The techniques of direct examination are taught in courses on trial advocacy. Each direct examination is integrated with the overall case strategy through either a theme and theory or, with more advanced strategies, a line of effort.



</doc>
<doc id="8011" url="https://en.wikipedia.org/wiki?curid=8011" title="Alcohol intoxication">
Alcohol intoxication

Alcohol intoxication, also known as drunkenness or alcohol poisoning, is negative behavior and physical effects due to the recent drinking of ethanol (alcohol). Symptoms at lower doses may include mild sedation and poor coordination. At higher doses, there may be slurred speech, trouble walking, and vomiting. Extreme doses may result in a decreased effort to breathe (respiratory depression), coma, or death. Complications may include seizures, aspiration pneumonia, injuries including suicide, and low blood sugar.
Alcohol intoxication typically begins after two or more alcoholic drinks. Risk factors include a social situation where heavy drinking is common and a person having an impulsive personality. Diagnosis is usually based on the history of events and physical examination. Verification of events by the people a person was with may be useful. Legally, alcohol intoxication is often defined as a blood alcohol concentration (BAC) of greater than 5.4-17.4 mmol/L (25–80 mg/dL or 0.025-0.080%). This can be measured by blood or breath testing. Alcohol is then broken down at a rate of about 3.3 mmol/L (15 mg/dL) per hour.
Management of alcohol intoxication involves supportive care. Typically this includes putting the person in the recovery position, keeping them warm, and making sure they are breathing sufficiently. Gastric lavage and activated charcoal have not been found to be useful. Repeated assessments may be required to rule out other potential causes of a persons symptoms.
Alcohol intoxication is very common, especially in the Western world. Most people who drink alcohol have at some time been intoxicated. In the United States acute intoxication directly results in about 2,200 deaths per year, and indirectly more than 30,000 deaths per year. Acute intoxication has been documented throughout history and alcohol remains one of the world's most widespread recreational drugs. Some religions consider alcohol intoxication to be a sin.

Alcohol intoxication is the negative health effects due to the recent drinking of ethanol (alcohol). When severe it may become a medical emergency. Some effects of alcohol intoxication, such as euphoria and lowered social inhibition, are central to alcohol's desirability.

The signs and symptoms of acute alcohol poisoning include:


Alcohol is metabolized by a normal liver at the rate of about 8 grams of pure ethanol per hour. 8 grams or is one British standard unit. An "abnormal" liver with conditions such as hepatitis, cirrhosis, gall bladder disease, and cancer is likely to result in a slower rate of metabolism.
Ethanol is metabolised to acetaldehyde by alcohol dehydrogenase (ADH), which is found in many tissues, including the gastric mucosa. Acetaldehyde is metabolised to acetate by acetaldehyde dehydrogenase (ALDH), which is found predominantly in liver mitochondria. Acetate is used by the muscle cells to produce acetyl-CoA using the enzyme acetyl-CoA synthetase, and the acetyl-CoA is then used in the citric acid cycle.

Ethanol's acute effects are due largely to its nature as a central nervous system depressant, and are dependent on blood alcohol concentrations:


As drinking increases, people become sleepy, or fall into a stupor. After a very high level of consumption, the respiratory system becomes depressed and the person will stop breathing. Comatose patients may aspirate their vomit (resulting in vomitus in the lungs, which may cause "drowning" and later pneumonia if survived). CNS depression and impaired motor co-ordination along with poor judgment increases the likelihood of accidental injury occurring. It is estimated that about one-third of alcohol-related deaths are due to accidents and another 14% are from intentional injury.

In addition to respiratory failure and accidents caused by effects on the central nervous system, alcohol causes significant metabolic derangements. Hypoglycaemia occurs due to ethanol's inhibition of gluconeogenesis, especially in children, and may cause lactic acidosis, ketoacidosis, and acute renal failure. Metabolic acidosis is compounded by respiratory failure. Patients may also present with hypothermia.

In the past, alcohol was believed to be a non-specific pharmacological agent affecting many neurotransmitter systems in the brain. However, molecular pharmacology studies have shown that alcohol has only a few primary targets. In some systems, these effects are facilitatory and in others inhibitory.

Among the neurotransmitter systems with enhanced functions are: GABA, 5-HT receptor agonism (responsible for GABAergic (GABA receptor PAM), glycinergic, and cholinergic effects), nicotinic acetylcholine receptors.

Among those that are inhibited are: NMDA, dihydropyridine-sensitive L-type Ca2+ channels and G-protein-activated inwardly rectifying K+ channels.

The result of these direct effects is a wave of further indirect effects involving a variety of other neurotransmitter and neuropeptide systems, leading finally to the behavioural or symptomatic effects of alcohol intoxication.

Many of the effects of activating GABA receptors have the same effects as that of ethanol consumption. Some of these effects include anxiolytic, anticonvulsant, sedative, and hypnotic effects, cognitive impairment, and motor incoordination. This correlation between activating GABA receptors and the effects of ethanol consumption has led to the study of ethanol and its effects on GABA receptors. It has been shown that ethanol does in fact exhibit positive allosteric binding properties to GABA receptors. However, binding is only limited to pentamers containing the δ-subunit rather than the γ-subunit. GABA receptors containing the δ-subunit have been shown to be located exterior to the synapse and are involved with tonic inhibition rather than its γ-subunit counterpart, which is involved in phasic inhibition. The δ-subunit has been shown to be able to form the allosteric binding site which makes GABA receptors containing the δ-subunit more sensitive to ethanol concentrations, even to moderate social ethanol consumption levels (30mM). While it has been shown by Santhakumar et al. that GABA receptors containing the δ-subunit are sensitive to ethanol modulation, depending on subunit combinations receptors, could be more or less sensitive to ethanol. It has been shown that GABA receptors that contain both δ and β3-subunits display increased sensitivity to ethanol. One such receptor that exhibits ethanol insensitivity is α3-β6-δ GABA. It has also been shown that subunit combination is not the only thing that contributes to ethanol sensitivity. Location of GABA receptors within the synapse may also contribute to ethanol sensitivity.

Definitive diagnosis relies on a blood test for alcohol, usually performed as part of a toxicology screen. Law enforcement officers in the United States and other countries often use breathalyzer units and field sobriety tests as more convenient and rapid alternatives to blood tests. There are also various models of breathalyzer units that are available for consumer use. Because these may have varying reliability and may produce different results than the tests used for law-enforcement purposes, the results from such devices should be conservatively interpreted.

Many informal intoxication tests exist, which, in general, are unreliable and not recommended as deterrents to excessive intoxication or as indicators of the safety of activities such as motor vehicle driving, heavy equipment operation, machine tool use, etc.

For determining whether someone is intoxicated by alcohol by some means other than a blood-alcohol test, it is necessary to rule out other conditions such as hypoglycemia, stroke, usage of other intoxicants, mental health issues, and so on. It is best if his / her behavior has been observed while the subject is sober to establish a baseline. Several well-known criteria can be used to establish a probable diagnosis. For a physician in the acute-treatment setting, acute alcohol intoxication can mimic other acute neurological disorders, or is frequently combined with other recreational drugs that complicate diagnosis and treatment.

Acute alcohol poisoning is a medical emergency due to the risk of death from respiratory depression or aspiration of vomit if vomiting occurs while the person is unresponsive. Emergency treatment strives to stabilize and maintain an open airway and sufficient breathing, while waiting for the alcohol to metabolize. This can be done by removal of any vomitus or, if the person is unconscious or has impaired gag reflex, intubation of the trachea.

Other measures may include
Additional medication may be indicated for treatment of nausea, tremor, and anxiety.

A normal liver detoxifies the blood of alcohol over a period of time that depends on the initial level and the patient's overall physical condition. An abnormal liver will take longer but still succeeds, provided the alcohol does not cause liver failure.

People having drunk heavily for several days or weeks may have withdrawal symptoms after the acute intoxication has subsided.

A person consuming a dangerous amount of alcohol persistently can develop memory blackouts and idiosyncratic intoxication or pathological drunkenness symptoms.

Long-term persistent consumption of excessive amounts of alcohol can cause liver damage and have other deleterious health effects.

Alcohol intoxication is a risk factor in some cases of catastrophic injury, in particular for unsupervised recreational activity. A study in the province of Ontario based on epidemiological data from 1986, 1989, 1992, and 1995 states that 79.2% of the 2,154 catastrophic injuries recorded for the study were preventable, of which 346 involved alcohol consumption. The activities most commonly associated with alcohol-related catastrophic injury were snowmobiling (124), fishing (41), diving (40), boating (31) and canoeing (7), swimming (31), riding an all-terrain vehicle (24), and cycling (23). These events are often associated with unsupervised young males, often inexperienced in the activity, and many result in drowning. Alcohol use is also associated with unsafe sex.

Laws on drunkenness vary. In the United States, it is a criminal offense for a person to be drunk while driving a motorized vehicle, except in Wisconsin, where it is only a fine for the first offense. It is also a criminal offense to fly an aircraft or (in some American states) to assemble or operate an amusement park ride while drunk. Similar laws also exist in the United Kingdom and most other countries.

In some countries, it is also an offense to serve alcohol to an already-intoxicated person, and, often, alcohol can be sold only by persons qualified to serve responsibly through alcohol server training.

The (BAC) for legal operation of a vehicle is typically measured as a percentage of a unit volume of blood. This percentage ranges from 0.00% in Romania and the United Arab Emirates; to 0.05% in Australia, South Africa, Germany, Scotland and New Zealand (but 0.00% for under 20 year olds); to 0.08% in England and Wales, the United States and Canada.

The United States Federal Aviation Administration prohibits crew members from performing their duties with a BAC greater than 0.04% within eight hours of consuming an alcoholic beverage, or while under the influence of alcohol.

In the United States, the United Kingdom, and Australia, public intoxication is a crime (also known as "being drunk and disorderly" or "being drunk and incapable").

In some countries, there are special facilities, sometimes known as "drunk tanks", for the temporary detention of persons found to be drunk.

Some religious groups permit the consumption of alcohol. Some permit consumption but prohibit intoxication, while others prohibit alcohol consumption altogether. Many Christian denominations such as Catholic, Orthodox, and Lutheran use wine as a part of the Eucharist and permit the drinking of alcohol but consider it sinful to become intoxicated.

In the Bible, the book of Proverbs contains several chapters dealing with the bad effects of drunkenness and warning to stay away from intoxicating beverages. The book of Leviticus tells of Nadab and Abihu, Aaron the Priest's eldest sons, who were killed for serving in the temple after drinking wine, presumably while intoxicated. The book continues to discuss monasticism where drinking wine is prohibited. The story of Samson in the Book of Judges tells of a monk from the tribe of Dan who is prohibited from cutting his hair and drinking wine. Romans 13:13–14, 1 Corinthians 6:9–11, Galatians 5:19–21, and Ephesians 5:18 are among a number of other Bible passages that speak against drunkenness. While Proverbs 31:4, warns against kings and rulers drinking wine and strong drink, Proverbs 31:6–7 promotes giving strong drink to the perishing and wine to those whose lives are bitter, to forget their poverty and troubles.

Some Protestant Christian denominations prohibit the drinking of alcohol based upon Biblical passages that condemn drunkenness, but others allow moderate use of alcohol. In some Christian groups, a small amount of wine is part of the rite of communion.

In the Church of Jesus Christ of Latter-day Saints, alcohol consumption is forbidden, and teetotalism has become a distinguishing feature of its members. Jehovah's Witnesses allow moderate alcohol consumption among its members.

In the Qur'an, there is a prohibition on the consumption of grape-based alcoholic beverages, and intoxication is considered as an abomination in the Hadith. Islamic schools of law (Madh'hab) have interpreted this as a strict prohibition of the consumption of all types of alcohol and declared it to be haraam ("forbidden"), although other uses may be permitted.

In Buddhism, in general, the consumption of intoxicants is discouraged for both monastics and lay followers. Many followers of Buddhism observe a code of conduct known as the Five Precepts, of which the fifth precept is an undertaking to refrain from the consumption of intoxicating substances (except for medical reasons). In the Bodhisattva Vows of the "Brahma Net Sutra", observed by some monastic communities and some lay followers, distribution of intoxicants is likewise discouraged as well as consumption.

In the branch of Hinduism known as Gaudiya Vaishnavism, one of the four regulative principles forbids the taking of intoxicants, including alcohol.

In Judaism, in accordance with the biblical stance against drinking, wine drinking was not permitted for priests and monks The biblical command to sanctify the Sabbath day and other holidays has been interpreted as having three ceremonial meals which include drinking of wine, the Kiddush. The Jewish marriage ceremony ends with the bride and groom drinking a shared cup of wine after reciting seven blessings, and according to western "Ashkenazi" traditions, after a fast day. But it has been customary and in many cases even mandated to drink moderately so as to stay sober, and only after the prayers are over.

During the Seder night on Passover (Pesach) there is an obligation to drink 4 ceremonial cups of wine, while reciting the Haggadah. It has been assumed as the source for the wine drinking ritual at the communion in some Christian groups. During Purim there is an obligation to become intoxicated, although, as with many other decrees, in many communities this has been avoided, by allowing sleep during the day to replace it.

In the 1920s due to the new beverages law, a rabbi from the Reform Judaism movement proposed using grape-juice for the ritual instead of wine. Although refuted at first, the practice became widely accepted by orthodox Jews as well.

At the Cave of the Patriarchs in Hebron—the Ibrahimi Mosque as it is called by the Muslims, the Jewish wine drinking rituals during weddings, the Sabbath day and holidays, are a cause for tension with the Muslims who unwillingly share the site under Israeli authority.

In the movie "Animals are Beautiful People", an entire section was dedicated to showing many different animals including monkeys, elephants, hogs, giraffes, and ostriches, eating over-ripe marula tree fruit causing them to sway and lose their footing in a manner similar to human drunkenness.

In elephant warfare, practiced by the Greeks during the Maccabean revolt and by Hannibal during the Punic wars, it has been recorded that the elephants would be given wine before the attack, and only then would they charge forward after being agitated by their driver.

It is a regular practice to give small amounts of beer to race horses in Ireland. Ruminant farm animals have natural fermentation occurring in their stomach, and adding alcoholic beverages in small amounts to their drink will generally do them no harm, and will not cause them to become drunk.




</doc>
<doc id="8013" url="https://en.wikipedia.org/wiki?curid=8013" title="Data compression">
Data compression

In signal processing, data compression, source coding, or bit-rate reduction involves encoding information using fewer bits than the original representation. Compression can be either lossy or lossless. Lossless compression reduces bits by identifying and eliminating statistical redundancy. No information is lost in lossless compression. Lossy compression reduces bits by removing unnecessary or less important information.

The process of reducing the size of a data file is often referred to as data compression. In the context of data transmission, it is called source coding; encoding done at the source of the data before it is stored or transmitted. Source coding should not be confused with channel coding, for error detection and correction or line coding, the means for mapping data onto a signal.

Compression is useful because it reduces resources required to store and transmit data. Computational resources are consumed in the compression process and, usually, in the reversal of the process (decompression). Data compression is subject to a space–time complexity trade-off. For instance, a compression scheme for video may require expensive hardware for the video to be decompressed fast enough to be viewed as it is being decompressed, and the option to decompress the video in full before watching it may be inconvenient or require additional storage. The design of data compression schemes involves trade-offs among various factors, including the degree of compression, the amount of distortion introduced (when using lossy data compression), and the computational resources required to compress and decompress the data.

Lossless data compression algorithms usually exploit statistical redundancy to represent data without losing any information, so that the process is reversible. Lossless compression is possible because most real-world data exhibits statistical redundancy. For example, an image may have areas of color that do not change over several pixels; instead of coding "red pixel, red pixel, ..." the data may be encoded as "279 red pixels". This is a basic example of run-length encoding; there are many schemes to reduce file size by eliminating redundancy.

The Lempel–Ziv (LZ) compression methods are among the most popular algorithms for lossless storage. DEFLATE is a variation on LZ optimized for decompression speed and compression ratio, but compression can be slow. In the mid-1980s, following work by Terry Welch, the Lempel–Ziv–Welch (LZW) algorithm rapidly became the method of choice for most general-purpose compression systems. LZW is used in GIF images, programs such as PKZIP, and hardware devices such as modems. LZ methods use a table-based compression model where table entries are substituted for repeated strings of data. For most LZ methods, this table is generated dynamically from earlier data in the input. The table itself is often Huffman encoded.

The strongest modern lossless compressors use probabilistic models, such as prediction by partial matching. The Burrows–Wheeler transform can also be viewed as an indirect form of statistical modelling.

The class of grammar-based codes are gaining popularity because they can compress "highly repetitive" input extremely effectively, for instance, a biological data collection of the same or closely related species, a huge versioned document collection, internet archival, etc. The basic task of grammar-based codes is constructing a context-free grammar deriving a single string. Sequitur and Re-Pair are practical grammar compression algorithms for which software is publicly available.

In a further refinement of the direct use of probabilistic modelling, statistical estimates can be coupled to an algorithm called arithmetic coding. Arithmetic coding is a more modern coding technique that uses the mathematical calculations of a finite-state machine to produce a string of encoded bits from a series of input data symbols. It can achieve superior compression to other techniques such as the better-known Huffman algorithm. It uses an internal memory state to avoid the need to perform a one-to-one mapping of individual input symbols to distinct representations that use an integer number of bits, and it clears out the internal memory only after encoding the entire string of data symbols. Arithmetic coding applies especially well to adaptive data compression tasks where the statistics vary and are context-dependent, as it can be easily coupled with an adaptive model of the probability distribution of the input data. An early example of the use of arithmetic coding was its use as an optional (but not widely used) feature of the JPEG image coding standard. It has since been applied in various other designs including H.263, H.264/MPEG-4 AVC and HEVC for video coding.

Lossy data compression is the converse of lossless data compression. In the late 1980s, digital images became more common, and standards for compressing them emerged. In the early 1990s, lossy compression methods began to be widely used. In these schemes, some loss of information is acceptable. Dropping nonessential detail from the data source can save storage space. Lossy data compression schemes are designed by research on how people perceive the data in question. For example, the human eye is more sensitive to subtle variations in luminance than it is to the variations in color. JPEG image compression works in part by rounding off nonessential bits of information. There is a corresponding trade-off between preserving information and reducing size. A number of popular compression formats exploit these perceptual differences, including those used in music files, images, and video.

Lossy image compression can be used in digital cameras, to increase storage capacities with minimal degradation of picture quality. Similarly, DVDs use the lossy MPEG-2 video coding format for video compression.

In lossy audio compression, methods of psychoacoustics are used to remove non-audible (or less audible) components of the audio signal. Compression of human speech is often performed with even more specialized techniques; speech coding, or voice coding, is sometimes distinguished as a separate discipline from "audio compression". Different audio and speech compression standards are listed under audio coding formats. "Voice compression" is used in internet telephony, for example, audio compression is used for CD ripping and is decoded by the audio players.

The theoretical background of compression is provided by information theory (which is closely related to algorithmic information theory) for lossless compression and rate–distortion theory for lossy compression. These areas of study were essentially created by Claude Shannon, who published fundamental papers on the topic in the late 1940s and early 1950s. Coding theory is also related to this. The idea of data compression is also deeply connected with statistical inference.

There is a close connection between machine learning and compression: a system that predicts the posterior probabilities of a sequence given its entire history can be used for optimal data compression (by using arithmetic coding on the output distribution) while an optimal compressor can be used for prediction (by finding the symbol that compresses best, given the previous history). This equivalence has been used as a justification for using data compression as a benchmark for "general intelligence."

However a new, alternative view can show compression algorithms implicitly map strings into implicit feature space vectors, and compression-based similarity measures compute similarity within these feature spaces. For each compressor C(.) we define an associated vector space ℵ, such that C(.) maps an input string x, corresponds to the vector norm ||~x||. An exhaustive examination of the feature spaces underlying all compression algorithms is precluded by space; instead, feature vectors chooses to examine three representative lossless compression methods, LZW, LZ77, and PPM.

Data compression can be viewed as a special case of data differencing: Data differencing consists of producing a "difference" given a "source" and a "target," with patching producing a "target" given a "source" and a "difference," while data compression consists of producing a compressed file given a target, and decompression consists of producing a target given only a compressed file. Thus, one can consider data compression as data differencing with empty source data, the compressed file corresponding to a "difference from nothing." This is the same as considering absolute entropy (corresponding to data compression) as a special case of relative entropy (corresponding to data differencing) with no initial data.

When one wishes to emphasize the connection, one may use the term "differential compression" to refer to data differencing.

Audio data compression, not to be confused with dynamic range compression, has the potential to reduce the transmission bandwidth and storage requirements of audio data. Audio compression algorithms are implemented in software as audio codecs. Lossy audio compression algorithms provide higher compression at the cost of fidelity and are used in numerous audio applications. These algorithms almost all rely on psychoacoustics to eliminate or reduce fidelity of less audible sounds, thereby reducing the space required to store or transmit them.

In both lossy and lossless compression, information redundancy is reduced, using methods such as coding, pattern recognition, and linear prediction to reduce the amount of information used to represent the uncompressed data.

The acceptable trade-off between loss of audio quality and transmission or storage size depends upon the application. For example, one 640 MB compact disc (CD) holds approximately one hour of uncompressed high fidelity music, less than 2 hours of music compressed losslessly, or 7 hours of music compressed in the MP3 format at a medium bit rate. A digital sound recorder can typically store around 200 hours of clearly intelligible speech in 640 MB.

Lossless audio compression produces a representation of digital data that decompress to an exact digital duplicate of the original audio stream, unlike playback from lossy compression techniques such as Vorbis and MP3. Compression ratios are around 50–60 % of original size, which is similar to those for generic lossless data compression. Lossless compression is unable to attain high compression ratios due to the complexity of waveforms and the rapid changes in sound forms. Codecs like FLAC, Shorten, and TTA use linear prediction to estimate the spectrum of the signal. Many of these algorithms use convolution with the filter [-1 1] to slightly whiten or flatten the spectrum, thereby allowing traditional lossless compression to work more efficiently. The process is reversed upon decompression.

When audio files are to be processed, either by further compression or for editing, it is desirable to work from an unchanged original (uncompressed or losslessly compressed). Processing of a lossily compressed file for some purpose usually produces a final result inferior to the creation of the same compressed file from an uncompressed original. In addition to sound editing or mixing, lossless audio compression is often used for archival storage, or as master copies.

A number of lossless audio compression formats exist. Shorten was an early lossless format. Newer ones include Free Lossless Audio Codec (FLAC), Apple's Apple Lossless (ALAC), MPEG-4 ALS, Microsoft's Windows Media Audio 9 Lossless (WMA Lossless), Monkey's Audio, TTA, and WavPack. See list of lossless codecs for a complete listing.

Some audio formats feature a combination of a lossy format and a lossless correction; this allows stripping the correction to easily obtain a lossy file. Such formats include MPEG-4 SLS (Scalable to Lossless), WavPack, and OptimFROG DualStream.

Other formats are associated with a distinct system, such as:

Lossy audio compression is used in a wide range of applications. In addition to the direct applications (MP3 players or computers), digitally compressed audio streams are used in most video DVDs, digital television, streaming media on the internet, satellite and cable radio, and increasingly in terrestrial radio broadcasts. Lossy compression typically achieves far greater compression than lossless compression (data of 5 percent to 20 percent of the original stream, rather than 50 percent to 60 percent), by discarding less-critical data.

The innovation of lossy audio compression was to use psychoacoustics to recognize that not all data in an audio stream can be perceived by the human auditory system. Most lossy compression reduces perceptual redundancy by first identifying perceptually irrelevant sounds, that is, sounds that are very hard to hear. Typical examples include high frequencies or sounds that occur at the same time as louder sounds. Those sounds are coded with decreased accuracy or not at all.

Due to the nature of lossy algorithms, audio quality suffers when a file is decompressed and recompressed (digital generation loss). This makes lossy compression unsuitable for storing the intermediate results in professional audio engineering applications, such as sound editing and multitrack recording. However, they are very popular with end users (particularly MP3) as a megabyte can store about a minute's worth of music at adequate quality.

To determine what information in an audio signal is perceptually irrelevant, most lossy compression algorithms use transforms such as the modified discrete cosine transform (MDCT) to convert time domain sampled waveforms into a transform domain. Once transformed, typically into the frequency domain, component frequencies can be allocated bits according to how audible they are. Audibility of spectral components calculated using the absolute threshold of hearing and the principles of simultaneous masking—the phenomenon wherein a signal is masked by another signal separated by frequency—and, in some cases, temporal masking—where a signal is masked by another signal separated by time. Equal-loudness contours may also be used to weight the perceptual importance of components. Models of the human ear-brain combination incorporating such effects are often called psychoacoustic models.

Other types of lossy compressors, such as the linear predictive coding (LPC) used with speech, are source-based coders. These coders use a model of the sound's generator (such as the human vocal tract with LPC) to whiten the audio signal (i.e., flatten its spectrum) before quantization. LPC may be thought of as a basic perceptual coding technique: reconstruction of an audio signal using a linear predictor shapes the coder's quantization noise into the spectrum of the target signal, partially masking it.

Lossy formats are often used for the distribution of streaming audio or interactive applications (such as the coding of speech for digital transmission in cell phone networks). In such applications, the data must be decompressed as the data flows, rather than after the entire data stream has been transmitted. Not all audio codecs can be used for streaming applications, and for such applications a codec designed to stream data effectively will usually be chosen.

Latency results from the methods used to encode and decode the data. Some codecs will analyze a longer segment of the data to optimize efficiency, and then code it in a manner that requires a larger segment of data at one time to decode. (Often codecs create segments called a "frame" to create discrete data segments for encoding and decoding.) The inherent latency of the coding algorithm can be critical; for example, when there is a two-way transmission of data, such as with a telephone conversation, significant delays may seriously degrade the perceived quality.

In contrast to the speed of compression, which is proportional to the number of operations required by the algorithm, here latency refers to the number of samples that must be analysed before a block of audio is processed. In the minimum case, latency is zero samples (e.g., if the coder/decoder simply reduces the number of bits used to quantize the signal). Time domain algorithms such as LPC also often have low latencies, hence their popularity in speech coding for telephony. In algorithms such as MP3, however, a large number of samples have to be analyzed to implement a psychoacoustic model in the frequency domain, and latency is on the order of 23 ms (46 ms for two-way communication)).

Speech encoding is an important category of audio data compression. The perceptual models used to estimate what a human ear can hear are generally somewhat different from those used for music. The range of frequencies needed to convey the sounds of a human voice are normally far narrower than that needed for music, and the sound is normally less complex. As a result, speech can be encoded at high quality using a relatively low bit rate.

If the data to be compressed is analog (such as a voltage that varies with time), quantization is employed to digitize it into numbers (normally integers). This is referred to as analog-to-digital (A/D) conversion. If the integers generated by quantization are 8 bits each, then the entire range of the analog signal is divided into 256 intervals and all the signal values within an interval are quantized to the same number. If 16-bit integers are generated, then the range of the analog signal is divided into 65,536 intervals.

This relation illustrates the compromise between high resolution (a large number of analog intervals) and high compression (small integers generated). This application of quantization is used by several speech compression methods. This is accomplished, in general, by some combination of two approaches:

Perhaps the earliest algorithms used in speech encoding (and audio data compression in general) were the A-law algorithm and the µ-law algorithm.

A literature compendium for a large variety of audio coding systems was published in the IEEE Journal on Selected Areas in Communications (JSAC), February 1988. While there were some papers from before that time, this collection documented an entire variety of finished, working audio coders, nearly all of them using perceptual (i.e. masking) techniques and some kind of frequency analysis and back-end noiseless coding. Several of these papers remarked on the difficulty of obtaining good, clean digital audio for research purposes. Most, if not all, of the authors in the JSAC edition were also active in the MPEG-1 Audio committee.

The world's first commercial broadcast automation audio compression system was developed by Oscar Bonello, an engineering professor at the University of Buenos Aires. In 1983, using the psychoacoustic principle of the masking of critical bands first published in 1967, he started developing a practical application based on the recently developed IBM PC computer, and the broadcast automation system was launched in 1987 under the name Audicom. Twenty years later, almost all the radio stations in the world were using similar technology manufactured by a number of companies.

Video compression is a practical implementation of source coding in information theory. In practice, most video codecs are used alongside audio compression techniques to store the separate but complementary data streams as one combined package using so-called "container formats".

Uncompressed video requires a very high data rate. Although lossless video compression codecs perform at a compression factor of 5 to 12, a typical MPEG-4 lossy compression video has a compression factor between 20 and 200.

Video data may be represented as a series of still image frames. Such data usually contains abundant amounts of spatial and temporal redundancy. Video compression algorithms attempt to reduce redundancy and store information more compactly. 

Most video compression formats and codecs exploit both spatial and temporal redundancy (e.g. through difference coding with motion compensation). Similarities can be encoded by only storing differences between e.g. temporally adjacent frames (inter-frame coding) or spatially adjacent pixels (intra-frame coding).
Inter-frame compression (a temporal delta encoding) is one of the most powerful compression techniques. It (re)uses data from one or more earlier or later frames in a sequence to describe the current frame. Intra-frame coding, on the other hand, uses only data from within the current frame, effectively being still-image compression. And the intra-frame coding always uses lossy compression algorithms. 

A class of specialized formats used in camcorders and video editing use less complex compression schemes that restrict their prediction techniques to intra-frame prediction.

Usually video compression additionally employs lossy compression techniques like quantization that reduce aspects of the source data that are (more or less) irrelevant to the human visual perception by exploiting perceptual features of human vision. For example, small differences in color are more difficult to perceive than are changes in brightness. Compression algorithms can average a color across these similar areas to reduce space, in a manner similar to those used in JPEG image compression. As in all lossy compression, there is a trade-off between video quality, cost of processing the compression and decompression, and system requirements. Highly compressed video may present visible or distracting artifacts.

Other methods than the prevalent DCT-based transform formats, such as fractal compression, matching pursuit and the use of a discrete wavelet transform (DWT), have been the subject of some research, but are typically not used in practical products (except for the use of wavelet coding as still-image coders without motion compensation). Interest in fractal compression seems to be waning, due to recent theoretical analysis showing a comparative lack of effectiveness of such methods.

Inter-frame coding works by comparing each frame in the video with the previous one. Individual frames of a video sequence are compared from one frame to the next, and the video compression codec sends only the differences to the reference frame. If the frame contains areas where nothing has moved, the system can simply issue a short command that copies that part of the previous frame into the next one. If sections of the frame move in a simple manner, the compressor can emit a (slightly longer) command that tells the decompressor to shift, rotate, lighten, or darken the copy. This longer command still remains much shorter than intraframe compression. Usually the encoder will also transmit a residue signal which describes the remaining more subtle differences to the reference imagery. Using entropy coding, these residue signals have a more compact representation than the full signal. In areas of video with more motion, the compression must encode more data to keep up with the larger number of pixels that are changing. Commonly during explosions, flames, flocks of animals, and in some panning shots, the high-frequency detail leads to quality decreases or to increases in the variable bitrate.

Today, nearly all commonly used video compression methods (e.g., those in standards approved by the ITU-T or ISO) share the same basic architecture that dates back to H.261 which was standardized in 1988 by the ITU-T. They mostly rely on the DCT, applied to rectangular blocks of neighboring pixels, and temporal prediction using motion vectors, as well as nowadays also an in-loop filtering step.

In the prediction stage, various deduplication and difference-coding techniques are applied that help decorrelate data and describe new data based on already transmitted data.

Then rectangular blocks of (residue) pixel data are transformed to the frequency domain to ease targeting irrelevant information in quantization and for some spatial redundancy reduction. The discrete cosine transform (DCT) that is widely used in this regard was introduced by N. Ahmed, T. Natarajan and K. R. Rao in 1974.

In the main lossy processing stage that data gets quantized in order to reduce information that is irrelevant to human visual perception.

In the last stage statistical redundancy gets largely eliminated by an entropy coder which often applies some form of arithmetic coding.

In an additional in-loop filtering stage various filters can be applied to the reconstructed image signal. By computing these filters also inside the encoding loop they can help compression because they can be applied to reference material before it gets used in the prediction process and they can be guided using the original signal. The most popular example are deblocking filters that blur out blocking artefacts from quantization discontinuities at transform block boundaries.

All basic algorithms of today's dominant video codec architecture have been invented before 1979.
In 1950, the Bell Labs filed the patent on DPCM which soon was applied to video coding. Entropy coding started in the 1940s with the introduction of Shannon–Fano coding on which the widely used Huffman coding is based that was developed in 1950; the more modern context-adaptive binary arithmetic coding (CABAC) was published in the early 1990s. Transform coding (using the Hadamard transform) was introduced in 1969, the popular discrete cosine transform (DCT) appeared in 1974 in scientific literature.
The ITU-T's standard H.261 from 1988 introduced the prevalent basic architecture of video compression technology.

Genetics compression algorithms are the latest generation of lossless algorithms that compress data (typically sequences of nucleotides) using both conventional compression algorithms and genetic algorithms adapted to the specific datatype. In 2012, a team of scientists from Johns Hopkins University published a genetic compression algorithm that does not use a reference genome for compression. HAPZIPPER was tailored for HapMap data and achieves over 20-fold compression (95% reduction in file size), providing 2- to 4-fold better compression and in much faster time than the leading general-purpose compression utilities. For this, Chanda, Elhaik, and Bader introduced MAF based encoding (MAFE), which reduces the heterogeneity of the dataset by sorting SNPs by their minor allele frequency, thus homogenizing the dataset. Other algorithms in 2009 and 2013 (DNAZip and GenomeZip) have compression ratios of up to 1200-fold—allowing 6 billion basepair diploid human genomes to be stored in 2.5 megabytes (relative to a reference genome or averaged over many genomes).. For a benchmark in genetics/genomics data compressors, see 

It is estimated that the total amount of data that is stored on the world's storage devices could be further compressed with existing compression algorithms by a remaining average factor of 4.5:1. It is estimated that the combined technological capacity of the world to store information provides 1,300 exabytes of hardware digits in 2007, but when the corresponding content is optimally compressed, this only represents 295 exabytes of Shannon information.



</doc>
<doc id="8022" url="https://en.wikipedia.org/wiki?curid=8022" title="History of the Democratic Republic of the Congo">
History of the Democratic Republic of the Congo

The region that is now the Democratic Republic of the Congo was first settled about 80,000 years ago. The Kingdom of Kongo remained present in the region between the 14th and the early 19th centuries. Belgian colonization began when King Leopold II founded the Congo Free State, a corporate state run solely by King Leopold. Reports of widespread murder and torture in the rubber plantations led the Belgian government to seize the Congo from Leopold II and establish the Belgian Congo. Under Belgian rule numerous Christian organizations attempted to Westernize the Congolese people.

After an uprising by the Congolese people, Belgium surrendered to the independence of the Congo in 1960. However, the Congo remained unstable because tribal leaders had more power than the central government. Prime Minister Patrice Lumumba tried to restore order with the aid of the Soviet Union as part of the Cold War, causing the United States to support a coup led by Colonel Joseph Mobutu in 1965. Mobutu quickly seized complete power of the Congo and renamed the country Zaire. He sought to Africanize the country, changing his own name to Mobutu Sese Seko, and demanded that African citizens change their Western names to traditional African names. Mobutu sought to repress any opposition to his rule, in which he successfully did throughout the 1980s. However, with his regime weakened in the early 1990s, Mobutu was forced to agree to a power-sharing government with the opposition party. Mobutu remained the head of state and promised elections within the next two years that never took place.

In the First Congo War, Rwanda invaded Zaire; Mobutu lost power during this process. Laurent-Desire Kabila took power and renamed the country the Democratic Republic of the Congo. After a disappointing rule under Kabila, the Second Congo War broke out, resulting in a regional war in which different African nations took part. Kabila was assassinated by his bodyguard in 2001, and his son, Joseph, succeeded him and was later elected president by the Congolese government in 2006. Kabila quickly sought peace. Foreign soldiers remained in the Congo for a few years and a power-sharing government between Kabila and the opposition party was set up. Kabila later resumed complete control over the Congo and was re-elected in a disputed election in 2011. Today, the Congo remains dangerously unstable.

The area now known as the Democratic Republic of the Congo was populated as early as 80,000 years ago, as shown by the 1988 discovery of the Semliki harpoon at Katanda, one of the oldest barbed harpoons ever found, which is believed to have been used to catch giant river catfish. During its recorded history, the area has also been known as "Congo", "Congo Free State", "Belgian Congo", and "Zaire". 

The Kingdom of Kongo existed from the 14th to the early 19th century. Until the arrival of the Portuguese it was the dominant force in the region along with the Kingdom of Luba, the Kingdom of Lunda, the Mongo people and the Anziku Kingdom.

The Congo Free State was a corporate state privately controlled by Leopold II of Belgium through the "Association internationale africaine", a non-governmental organization. Leopold was the sole shareholder and chairman. The state included the entire area of the present Democratic Republic of the Congo. Under Leopold II, the Congo Free State became one of the most infamous international scandals of the turn of the twentieth century. The report of the British Consul Roger Casement led to the arrest and punishment of white officials who had been responsible for cold-blooded killings during a rubber-collecting expedition in 1900, including a Belgian national who caused the shooting of at least 122 Congolese natives. Estimates of the total death toll vary considerably. The first census was only done in 1924, so it is even more difficult to quantify the population loss of the period. Roger Casement's famous 1904 report estimated ten million people. According to Casement's report, indiscriminate "war", starvation, reduction of births and tropical diseases caused the country's depopulation. European and U.S. press agencies exposed the conditions in the Congo Free State to the public in 1900. By 1908 public and diplomatic pressure had led Leopold II to annex the Congo as the Belgian Congo colony.

On 15 November 1908 King Léopold II of Belgium formally relinquished personal control of the Congo Free State. The renamed Belgian Congo was put under the direct administration of the Belgian government and its Ministry of Colonies.

Belgian rule in the Congo was based around the "colonial trinity" ("trinité coloniale") of state, missionary and private company interests. The privileging of Belgian commercial interests meant that large amounts of capital flowed into the Congo and that individual regions became specialised. The interests of the government and private enterprise became closely tied; the state helped companies break strikes and remove other barriers imposed by the indigenous population. The country was split into nesting, hierarchically organised administrative subdivisions, and run uniformly according to a set "native policy" ("politique indigène")—in contrast to the British and the French, who generally favoured the system of indirect rule whereby traditional leaders were retained in positions of authority under colonial oversight. There was also a high degree of racial segregation. Large numbers of white immigrants who moved to the Congo after the end of World War II came from across the social spectrum, but were nonetheless always treated as superior to blacks.

During the 1940s and 1950s, the Congo experienced an unprecedented level of urbanisation and the colonial administration began various development programmes aimed at making the territory into a "model colony". Notable advances were made in treating diseases such as African trypanosomiasis. One of the results of these measures was the development of a new middle class of Europeanised African "évolués" in the cities. By the 1950s the Congo had a wage labour force twice as large as that in any other African colony. The Congo's rich natural resources, including uranium—much of the uranium used by the U.S. nuclear programme during World War II was Congolese—led to substantial interest in the region from both the Soviet Union and the United States as the Cold War developed.

Following riots in Leopoldville 4–7 January 1959, and in Stanleyville on 31 October 1959, the Belgians realised they could not maintain control of such a vast country in the face of rising demands for independence. Belgian and Congolese political leaders held a Round Table Conference in Brussels beginning on 18 January 1960. At the end of the conference on 27 January 1960 it was announced that elections would be held in the Congo on 22 May 1960, and full independence granted on 30 June 1960. The Congo was indeed granted its independence on 30 June 1960, and adopted the name "Republic of the Congo" (République du Congo). The French colony of Middle Congo (Moyen Congo) also chose the name Republic of Congo upon its independence, so the two countries are more commonly known as Congo-Léopoldville and Congo-Brazzaville, after their capital cities. President Mobutu changed the country's official name to Zaire in 1971. That name comes from Portuguese, adapted from the Kongo word nzere or nzadi ("river that swallows all rivers")

In 1960, the country was very unstable—regional tribal leaders held far more power than the central government—and with the departure of the Belgian administrators, almost no skilled bureaucrats remained in the country. The first Congolese graduated university only in 1956, and very few in the new nation had any idea how to manage a country of such size.

Parliamentary elections in 1960 produced the nationalist Patrice Lumumba as prime minister and pro-Western Joseph Kasavubu as president of the newly independent Republic of The Congo.

On 5 July 1960, a military mutiny by Congolese soldiers against their European officers broke out in the capital and rampant looting began. On 11 July 1960 the richest province of the country, Katanga, seceded under Moise Tshombe. The United Nations sent 20,000 peacekeepers to protect Europeans in the country and try to restore order. Western paramilitaries and mercenaries, often hired by mining companies to protect their interests, also began to pour into the country. In this period Congo's second richest province, Kasai, also announced its independence on 8 August 1960.

Prime Minister Lumumba turned to the USSR for assistance. Nikita Khrushchev agreed to help, offering advanced weaponry and technical advisors. The United States viewed the Soviet presence as an attempt to take advantage of the situation and gain a proxy state in sub-Saharan Africa. UN forces were ordered to block any shipments of arms into the country. The United States also looked for a way to replace Lumumba as leader. President Kasavubu had clashed with Prime Minister Lumumba and advocated an alliance with the West rather than the Soviets. The U.S. sent weapons and CIA personnel to aid forces allied with Kasavubu and combat the Soviet presence. On 14 September 1960, with U.S. and CIA support, Colonel Joseph Mobutu overthrew the government and arrested Lumumba.

On 17 January 1961 Mobutu sent Lumumba to Élisabethville (now Lubumbashi), capital of Katanga. In full view of the press he was beaten and forced to eat copies of his own speeches. For three weeks afterward, he was not seen or heard from. Then Katangan radio announced implausibly that he had escaped and been killed by villagers. It was soon clear that in fact he had been tortured and killed along with two others shortly after his arrival. In 2001, a Belgian inquiry established that he had been shot by Katangan gendarmes in the presence of Belgian officers, under Katangan command. Lumumba was beaten, placed in front of a firing squad with two allies, cut up, buried, dug up and what remained was dissolved in acid.

In Stanleyville, those loyal to the deposed Lumumba set up a rival government under Antoine Gizenga which lasted from 31 March 1961 until it was reintegrated on 5 August 1961. After some reverses, UN and Congolese government forces succeeded in recapturing the breakaway provinces of South Kasai on 30 December 1961, and Katanga on 15 January 1963.

A new crisis erupted in the Simba Rebellion of 1964-1965 which saw half the country taken by the rebels. European mercenaries, US, and Belgian troops were called in by the Congolese government to defeat the rebellion.

Unrest and rebellion plagued the government until November 1965, when Lieutenant General Mobutu, by then commander in chief of the national army, seized control of the country and declared himself president for five years. Mobutu quickly consolidated his power and was elected unopposed as president in 1970. Embarking on a campaign of cultural awareness, Mobutu renamed the country the Republic of Zaire in 1971 and required citizens to adopt African names and drop their French-language ones. Relative peace and stability prevailed until 1977 and 1978 when Katangan rebels, based in Angola, launched a series of invasions (Shaba I and II) into the Shaba (Katanga) region. These rebels were driven out with the aid of Belgian paratroopers.

Zaire remained a one-party state in the 1980s. Although Mobutu successfully maintained control during this period, opposition parties, most notably the Union pour la Démocratie et le Progrès Social (UDPS), were active. Mobutu's attempts to quell these groups drew significant international criticism.

As the Cold War came to a close, internal and external pressures on Mobutu increased. In late 1989 and early 1990, Mobutu was weakened by a series of domestic protests, by heightened international criticism of his regime's human rights practices, by a faltering economy, and by government corruption, most notably his own massive embezzlement of government funds for personal use.

In April 1990, Mobutu declared the Third Republic, agreeing to a limited multi-party system with elections and a constitution. As details of the reforms were delayed, soldiers in September 1991 began looting Kinshasa to protest their unpaid wages. Two thousand French and Belgian troops, some of whom were flown in on U.S. Air Force planes, arrived to evacuate the 20,000 endangered foreign nationals in Kinshasa.

In 1992, after previous similar attempts, the long-promised Sovereign National Conference was staged, encompassing over 2,000 representatives from various political parties. The conference gave itself a legislative mandate and elected Archbishop Laurent Monsengwo Pasinya as its chairman, along with Étienne Tshisekedi wa Mulumba, leader of the UDPS, as prime minister. By the end of the year Mobutu had created a rival government with its own prime minister. The ensuing stalemate produced a compromise merger of the two governments into the High Council of Republic-Parliament of Transition (HCR-PT) in 1994, with Mobutu as head of state and Kengo Wa Dondo as prime minister. Although presidential and legislative elections were scheduled repeatedly over the next two years, they never took place.

By 1996, tensions from the war and genocide in neighboring Rwanda had spilled over into Zaire. Rwandan Hutu militia forces (Interahamwe) who had fled Rwanda following the ascension of a Tutsi-led government had been using Hutu refugee camps in eastern Zaire as bases for incursions into Rwanda. In October 1996
Rwandan forces attacked refugee camps in the Rusizi River plain near the intersection of the Congolese, Rwandan and Burundi borders meet, scattering refugees. They took Uvira, then Bukavu, Goma and Mugunga.

Hutu militia forces soon allied with the Zairian armed forces (FAZ) to launch a campaign against Congolese ethnic Tutsis in eastern Zaire. In turn, these Tutsis formed a militia to defend themselves against attacks. When the Zairian government began to escalate the massacres in November 1996, Tutsi militias erupted in rebellion against Mobutu.

The Tutsi militia was soon joined by various opposition groups and supported by several countries, including Rwanda and Uganda. This coalition, led by Laurent-Desire Kabila, became known as the Alliance des Forces Démocratiques pour la Libération du Congo-Zaïre (AFDL). The AFDL, now seeking the broader goal of ousting Mobutu, made significant military gains in early 1997. Various Zairean politicians who had unsuccessfully opposed the dictatorship of Mobutu for many years now saw an opportunity for them in the invasion of Zaire by two of the region's strongest military forces. Following failed peace talks between Mobutu and Kabila in May 1997, Mobutu left the country, and Kabila marched unopposed to Kinshasa on 20 May. Kabila named himself president, consolidated power around himself and the AFDL, and reverted the name of the country to the Democratic Republic of Congo.

Kabila demonstrated little ability to manage the problems of his country, and lost his allies. To counterbalance the power and influence of Rwanda in DRC, Ugandan troops created another rebel movement called the Movement for the Liberation of Congo (MLC), led by the Congolese warlord Jean-Pierre Bemba. They attacked in August 1998, backed by Rwandan and Ugandan troops. Soon afterwards, Angola, Namibia, and Zimbabwe became involved militarily in the Congo, with Angola and Zimbabwe supporting the government. While the six African governments involved in the war signed a ceasefire accord in Lusaka in July 1999, the Congolese rebels did not and the ceasefire broke down within months. 

Kabila was assassinated in 2001 by a bodyguard called Rashidi Kasereka, 18, who was then shot dead, according to Justice Minister Mwenze Kongolo. Another account of the assassination says that the real killer escaped.

Kabila was succeeded by his son, Joseph. Upon taking office, Kabila called for multilateral peace talks to end the war. Kabila partly succeeded when a further peace deal was brokered between him, Uganda, and Rwanda leading to the apparent withdrawal of foreign troops.

Currently, the Ugandans and the MLC still hold a wide section of the north of the country; Rwandan forces and its front, the Rassemblement Congolais pour la Démocratie (RCD) control a large section of the east; and government forces or their allies hold the west and south of the country. There were reports that the conflict is being prolonged as a cover for extensive looting of the substantial natural resources in the country, including diamonds, copper, zinc, and coltan. The conflict was reignited in January 2002 by ethnic clashes in the northeast and both Uganda and Rwanda then halted their withdrawal and sent in more troops. Talks between Kabila and the rebel leaders, held in Sun City, lasted a full six weeks, beginning in April 2002. In June, they signed a peace accord under which Kabila would share power with former rebels. By June 2003, all foreign armies except those of Rwanda had pulled out of Congo. 

Few people in the Congo have been unaffected by the conflict. A survey conducted in 2009 by the ICRC and Ipsos shows that three quarters (76%) of the people interviewed have been affected in some way–either personally or due to the wider consequences of armed conflict.

The response of the international community has been incommensurate with the scale of the disaster resulting from the war in the Congo. Its support for political and diplomatic efforts to end the war has been relatively consistent, but it has taken no effective steps to abide by repeated pledges to demand accountability for the war crimes and crimes against humanity that were routinely committed in Congo. 
The United Nations Security Council and the U.N. Secretary-General have frequently denounced human rights abuses and the humanitarian disaster that the war unleashed on the local population, but have shown little will to tackle the responsibility of occupying powers for the atrocities taking place in areas under their control, areas where the worst violence in the country took place. In particular Rwanda and Uganda have escaped any significant sanction for their role.

DR Congo had a transitional government in July 2003 until the election was over. A constitution was approved by voters and on 30 July 2006 the Congo held its first multi-party elections since independence in 1960. Joseph Kabila took 45% of the votes and his opponent Jean-Pierre Bemba 20%. That was the origin of a fight between the two parties from 20–22 August 2006 in the streets of the capital, Kinshasa. Sixteen people died before policemen and MONUC took control of the city. A new election was held on 29 October 2006, which Kabila won with 70% of the vote. Bemba has decried election "irregularities." On 6 December 2006 Joseph Kabila was sworn in as President.

In December 2011, Joseph Kabila was re-elected for a second term as president. After the results were announced on 9 December, there was violent unrest in Kinshasa and Mbuji-Mayi, where official tallies showed that a strong majority had voted for the opposition candidate Etienne Tshisekedi. Official observers from the Carter Center reported that returns from almost 2,000 polling stations in areas where support for Tshisekedi was strong had been lost and not included in the official results. They described the election as lacking credibility. On 20 December, Kabila was sworn in for a second term, promising to invest in infrastructure and public services. However, Tshisekedi maintained that the result of the election was illegitimate and said that he intended also to "swear himself in" as president.

On 19 January 2015 protests led by students at the University of Kinshasa broke out. The protests began following the announcement of a proposed law that would allow Kabila to remain in power until a national census can be conducted (elections had been planned for 2016). By Wednesday 21 January clashes between police and protesters had claimed at least 42 lives (although the government claimed only 15 people had been killed).

Similarly, in September 2016, violent protests were met with brutal force by the police and Republican Guard soldiers. Opposition groups claim 80 dead, including the Students' Union leader. From Monday 19 September Kinshasa residents, as well as residents elsewhere in Congo, where mostly confined to their homes. Police arrested anyone remotely connected to the opposition as well as innocent onlookers. Government propaganda, on television, and actions of covert government groups in the streets, act against opposition as well as foreigners. The president's mandate was due to end on 19 December 2016, but no plans have been made to elect a replacement at that time. More demonstrations are planned to mark the passing of this date. Opposition groups claim that the outcome of late elections would be civil war.

Maman Sidikou, the Secretary-General's Special Representative for DR Congo and head of MONUSCO, said that a tipping point into uncontrolable violence could come about very quickly if the political situation were not normalised.

The inability of the state and the world's largest United Nations peacekeeping force to provide security throughout the vast country has led to the emergence of up to 70 armed groups around 2016, perhaps the largest number in the world. By 2018, the number of armed groups had increased to about 120. 

Armed groups are often accused of being proxies or being supported by regional governments interested in Eastern Congo’s vast mineral wealth. Some argue that much of the lack of security by the national army is strategic on the part of the government, who let the army profit from illegal logging and mining operations in return for loyalty. Different rebel groups often target civilians by ethnicity as they cannot tell who is a rebel or who they are providing support to and militias often become oriented around ethnicity.

Laurent Nkunda with other soldiers from RCD-Goma who were integrated into the army defected and called themselves the National Congress for the Defence of the People (CNDP). Starting in 2004, CNDP, believed to be backed by Rwanda as a way to tackle the Hutu group Democratic Forces for the Liberation of Rwanda (FDLR), rebelled against the government, claiming to protect the Banyamulenge (Congolese Tutsis). In 2009, after a deal between the DRC and Rwanda, Rwandan troops entered the DRC and arrested Nkunda and were allowed to pursue FDLR militants. The CNDP signed a peace treaty with the government where its soldiers would be integrated into the national army.

In April 2012, the leader of the CNDP, Bosco Ntaganda and troops loyal to him mutinied, claiming a violation of the peace treaty and formed a rebel group, the March 23 Movement (M23), which was believed to be backed by Rwanda. On 20 November 2012, M23 took control of Goma, a provincial capital with a population of one million people. The UN authorized the Force Intervention Brigade (FIB), which was the first UN peacekeeping force with a mandate to neutralize opposition rather than a defensive mandate, and the FIB quickly defeated M23. The FIB was then to fight the FDLR but were hampered by the efforts of the Congolese government, who some believe tolerate the FDLR as a counterweight to Rwandan interests. Since 2017, fighters from M23, most of whom had fled into Uganda and Rwanda (both were believed to have supported them), started crossing back into DRC with the rising crisis over Kabila's extension of his term limit. DRC claimed of clashes with M23.

The Allied Democratic Forces has been waging an insurgency in the Democratic Republic of the Congo and is blamed for the Beni massacre in 2016. While the Congolese army maintains that the ADF is an Islamist insurgency, most observers feel that they are only a criminal group interested in gold mining and logging.

In June 2017, the group, mostly based in South Kivu, called the National People’s Coalition for the Sovereignty of Congo (CNPSC) led by William Yakutumba was formed and became the strongest rebel group in the east, even briefly capturing a few strategic towns. The rebel group is one of three alliances of various Mai-Mai militias and has been referred to as the Alliance of Article 64, a reference to Article 64 of the constitution, which says the people have an obligation to fight the efforts of those who seek to take power by force, in reference to President Kabila. Bembe warlord Yakutumba's Mai-Mai Yakutumba is the largest component of the CNPSC and has had friction with the Congolese Tutsis who often make up commanders in army units.

In 2012, the Congolese army in its attempt to crush the Rwandan backed and Tutsi-dominated CNDP and M23 rebels, empowered and used Hutu groups such as the FDLR and a Hutu dominated Mai-Mai group called Nyatura as proxies in its fight. The Nyatura and FDLR even arbitrarily executed up to 264 mostly Tembo civilians in 2012. In 2015, the army then launched an offensive against the FDLR militia. The FDLR are accused of killing at least 14 Nande people in January 2016 and of killing 10 Nandes and burning houses in July 2016 while an FDLR allied group Maï Maï Nyatura are also accused of killing Nandes. The Nande-dominate UPDI militia, a Nande militia called Mai-Mai Mazembe and a militia dominated by Nyanga people, the "Nduma Defence of Congo" (NDC), also called Maï-Maï Sheka and led by Gédéon Kyungu Mutanga, are accused of attacking Hutus. In North Kivu, in 2017, an alliance of Mai-Mai groups called the National Movement of Revolutionaries (MNR) began attacks in June 2017 includes Nande Mai-Mai leaders from groups such as Corps du Christ and Mai-Mai Mazembe. Another alliance of Mai-Mai groups is CMC which brings together Hutu militia Nyatura and are active along the border between North Kivu and South Kivu.

In Northern Katanga Province starting in 2013, the Pygmy Batwa people, whom the Luba people often exploit and allegedly enslave, rose up into militias, such as the "Perci" militia, and attacked Luba villages. A Luba militia known as "Elements" or "Elema" attacked back, notably killing at least 30 people in the "Vumilia 1" displaced people camp in April 2015. Since the start of the conflict, hundreds have been killed and tens of thousands have been displaced from their homes. The weapons used in the conflict are often arrows and axes, rather than guns.

Elema also began fighting the government mainly with machetes, bows and arrows in Congo’s Haut Katanga and Tanganyika provinces. The government forces fought alongside a tribe known as the Abatembo and targeting civilians of the Luba and the Tabwa tribes who were believed to be sympathetic to the Elema.

In the Kasaï-Central province, starting in 2016, the largely Luba Kamwina Nsapu militia led by Kamwina Nsapu attacked state institutions. The leader was killed by authorities in August 2016 and the militia reportedly took revenge by attacking civilians. By June 2017, more than 3,300 people had been killed and 20 villages have been completely destroyed, half of them by government troops. The militia has expanded to the neighboring Kasai-Oriental area, Kasaï and Lomami.

A traditional chief critical of Kabila was killed by security forces, precipitating conflict
that has killed more than 3,000 people since.

The UN discovered dozens of mass graves. Rebels and government forces are accused of human rights abuses, as well as a state-linked militia called Bana Mura, which shares a name with the hill in the east where presidential guards train.

The Ituri conflict involved fighting between the agriculturalist Lendu and pastoralist Hema ethnic groups in the Ituri region of the north-eastern DRC. While "Ituri conflict" often refers to the major fighting from 1999 to 2003, fighting has existed before and continues since that time. In 2018, with the deterioration in security over Kabila's extending his stay in power, more than 100 people were killed, hundreds of homes burnt and 200,000 people were forced to flee.

In October 2009 a new conflict started in Dongo, Sud-Ubangi District where clashes had broken out over access to fishing ponds.





</doc>
<doc id="8023" url="https://en.wikipedia.org/wiki?curid=8023" title="Geography of the Democratic Republic of the Congo">
Geography of the Democratic Republic of the Congo

The Democratic Republic of the Congo is by the Congo River Basin, which covers an area of almost . The country's only outlet to the Atlantic Ocean is a narrow strip of land on the north bank of the Congo River.

The vast, low-lying central area is a plateau-shaped basin sloping toward the west, covered by tropical rainforest and criss-crossed by rivers, a large area of this has been categorized by the World Wildlife Fund as the Central Congolian lowland forests ecoregion. The forest center is surrounded by mountainous terraces in the west, plateaus merging into savannahs in the south and southwest. 

Dense grasslands extend beyond the Congo River in the north. High mountains of the Ruwenzori Range (some above ) are found on the eastern borders with Rwanda and Uganda (see Albertine Rift montane forests for a description of this area).

The Democratic Republic of the Congo lies on the Equator, with one-third of the country to the north and two-thirds to the south. The climate is hot and humid in the river basin and cool and dry in the southern highlands, with a cold, alpine climate in the Rwenzori Mountains. 

South of the Equator, the rainy season lasts from October to May and north of the Equator, from April to November. Along the Equator, rainfall is fairly regular throughout the year. During the wet season, thunderstorms often are violent but seldom last more than a few hours. The average rainfall for the entire country is about .

Location of Congo:
Central Africa, northeast of Angola

Geographic coordinates: 

Map references:
Africa

Area:
"total:"
2,344,858 km
"land:"
2,267,048 km
"water:"
77,810 km

Area - comparative:
slightly less than one-fourth the size of the US

Land boundaries:
"total:"
10,481 km
"border countries:"
Angola 2,646 km, Burundi 236 km, Central African Republic 1,747 km, Republic of the Congo 1,229 km, Rwanda 221 km, South Sudan 714 km, Tanzania 479 km, Uganda 877 km, Zambia 2,332 km

Coastline:

Maritime claims:
"territorial sea:"

"exclusive economic zone:"
boundaries with neighbors

Climate:
tropical; hot and humid in equatorial river basin; cooler and drier in southern highlands; cooler-cold and wetter in eastern highlands and the Ruwenzori Range; north of Equator - wet season April to October, dry season December to February; south of Equator - wet season November to March, dry season April to October
Terrain:
vast central plateau covered by tropical rainforest, surrounded by mountains in the west, plains and savanna in the south/southwest, and grasslands in the north. The high mountains of the Ruwenzori Range on the eastern borders.

Elevation extremes:
"lowest point:"
Atlantic Ocean 0 m
"highest point:"
Pic Marguerite on Mont Ngaliema (Mount Stanley) 5,110 m

Natural resources:
cobalt, copper, niobium, petroleum, industrial and gem diamonds, gold, silver, zinc, manganese, tin, uranium, coal, hydropower, timber

Land use:
"arable land:"
3.09% 
"permanent crops:"
0.36%
96.55 (2012 est.)

Irrigated land:
105 km (2003)

Total renewable water resources:
1,283 km (2011)

Freshwater withdrawal (domestic/industrial/agricultural):
"total:"
0.68 km/yr (68%/21%/11%)
"per capita:"
11.25 m/yr (2005)

Natural hazards:
periodic droughts in south; Congo River floods (seasonal); in the east, in the Albertine Rift, there are active volcanoes

Environment - current issues:
Poaching threatens wildlife populations (for example, the painted hunting dog, "Lycaon pictus", is now considered extirpated from the Congo due to human overpopulation and poaching); water pollution; deforestation (chiefly due to land conversion to agriculture by indigenous farmers); refugees responsible for significant deforestation, soil erosion, and wildlife poaching; mining of minerals (coltan — a mineral used in creating capacitors, diamonds, and gold) causing environmental damage

Environment - international agreements:
"party to:"
Biodiversity, Climate Change, Desertification, Endangered Species, Hazardous Wastes, Law of the Sea, Marine Dumping, Nuclear Test Ban, Ozone Layer Protection, Tropical Timber 83, Tropical Timber 94, Wetlands
"signed, but not ratified:"
Environmental Modification

Geography:
D.R. Congo is one of 6 African states that straddles the Equator; it's the largest African state that has the Equator passing through it. Very narrow strip of land that controls the lower Congo River and is the only outlet to South Atlantic Ocean; dense tropical rainforest in central river basin and eastern highlands.

This is a list of the extreme points of the Democratic Republic of the Congo, the points that are farther north, south, east or west than any other location.





</doc>
<doc id="8024" url="https://en.wikipedia.org/wiki?curid=8024" title="Demographics of the Democratic Republic of the Congo">
Demographics of the Democratic Republic of the Congo

This article is about the demographic features of the population of the Democratic Republic of the Congo, including ethnicity, education level, health of the populace, economic status, religious affiliations and other aspects of the population.

As many as 250 ethnic groups have been distinguished and named. The most numerous people are the Luba, Mongo, and Bakongo.

Although 700 local languages and dialects are spoken, the linguistic variety is bridged both by the use of French and the intermediary languages Kongo, Luba-Kasai, Swahili, and Lingala.

According to the total population was in , compared to only 12,184,000 in 1950. The proportion of children below the age of 15 in 2010 was 46.3%, 51.1% was between 15 and 65 years of age, while 2.7% was 65 years or older

Structure of the population (DHS 2013-2014) (Males 45 548, Females 49 134 = 94 682) :

Registration of vital events in the Democratic Republic of the Congo is incomplete. The Population Departement of the United Nations prepared the following estimates.

Total Fertility Rate (TFR) (Wanted Fertility Rate) and Crude Birth Rate (CBR):

Fertility data as of 2013-2014 (DHS Program):

More than 250 ethnic groups have been identified and named of which the majority are Bantu. The four largest groups - Mongo, Luba, Kongo (all Bantu), and the Mangbetu-Azande make up about 45% of the population. The country has also 60,000 White Congolese most of Belgian ancestry who remained after independence.
Bantu peoples (80%):

Central Sudanic/Ubangian :

Nilotic peoples :

Pygmy peoples :
More than 600,000 pygmies (around 1% of the total population) are believed to live in the DR Congo's huge forests, where they survive by hunting wild animals and gathering fruits.

The four major languages in the DRC are French (official), Lingala (a lingua franca trade language), Kingwana (a dialect of Swahili), Kikongo, and Tshiluba. There are over 200 ethnic languages.

French is generally the medium of instruction in schools. English is taught as a compulsory foreign language in Secondary and High School around the country. It is a required subject in the Faculty of Economics at major universities around the country and there are numerous language schools in the country that teach it. In the town of Beni, for instance, there is a Bilingual University that offer courses in both French and English. President Kabila himself is fluent in both English and French, as was his father.

A survey conducted by the Demographic and Health Surveys program in 2013-2014 indicated that Christians constituted 93.7% of the population (Catholics 29.7%, Protestants 26.8%, and other Christians 37.2%). An indigenous religion, Kimbanguism, has the adherence of 2.8%, while Muslims make up 1.2%.

Another estimate found Christianity was followed by 95.8% of the population, according to the Pew Research Center in 2010.

The CIA The World Factbook states: Roman Catholic 50%, Protestant 20%, Kimbanguist 10%, Islam 10%, Other (includes Syncretic Sects and Indigenous beliefs) 10%.

Joshua Project figures: Roman Catholic 43.9%, Protestant 24.8%, Other Christian 23.7%, Muslim 1.6%, Non-religious 0.6%, Hindu 0.1% other syncretic sects and indigenous beliefs 5.3%.

The following demographic statistics are from the CIA World Factbook, unless otherwise indicated.

-0.54 migrant(s)/1,000 population
"note": fighting between the Congolese Government and Uganda- and Rwanda-backed Congolese rebels spawned a regional war in DRC in August 1998, which left 2.33 million Congolese internally displaced and caused 412,000 Congolese refugees to flee to surrounding countries (2011 est.)

Given the situation in the country and the condition of state structures, it is extremely difficult to obtain reliable data however evidence suggests that DRC continues to be a destination country for immigrants in spite of recent declines. Immigration is seen to be very diverse in nature, with refugees and asylum-seekers - products of the numerous and violent conflicts in the Great Lakes Region - constituting an important subset of the population in the country.

Additionally, the country’s large mine operations attract migrant workers from Africa and beyond and there is considerable migration for commercial activities from other African countries and the rest of the world, but these movements are not well studied. Transit migration towards South Africa and Europe also plays a role. Immigration in the DRC has decreased steadily over the past two decades, most likely as a result of the armed violence that the country has experienced.

According to the International Organization for Migration, the number of immigrants in the DRC has declined from just over 1 million in 1960, to 754,000 in 1990, to 480,000 in 2005, to an estimated 445,000 in 2010. Valid figures are not available on migrant workers in particular, partly due to the predominance of the informal economy in the DRC. Data are also lacking on irregular immigrants, however given neighbouring country ethnic links to nationals of the DRC, irregular migration is assumed to be a significant phenomenon in the country.

Figures on the number of Congolese nationals abroad vary greatly depending on the source, from 3 to 6 million. This discrepancy is due to a lack of official, reliable data. Emigrants from the DRC are above all long-term emigrants, the majority of which live within Africa and to a lesser extent in Europe; 79.7% and 15.3% respectively, according to estimates on 2000 data. Most Congolese emigrants however, remain in Africa, with new destination countries including South Africa and various points en route to Europe.

In addition to being a host country, the DRC has also produced a considerable number of refugees and asylum-seekers located in the region and beyond. These numbers peaked in 2004 when, according to UNHCR, there were more than 460,000 refugees from the DRC; in 2008, Congolese refugees numbered 367,995 in total, 68% of which were living in other African countries.

The table below shows DRC born people who have emigrated abroad in selected Western countries (although it excludes their descendants).
These are only estimates and do not account for Congolese migrants residing illegally in these and other countries. 





</doc>
<doc id="8025" url="https://en.wikipedia.org/wiki?curid=8025" title="Economy of the Democratic Republic of the Congo">
Economy of the Democratic Republic of the Congo

Sparsely populated in relation to its area, the Democratic Republic of the Congo is home to a vast potential of natural resources and mineral wealth. Its untapped deposits of raw minerals are estimated to be worth in excess of US$24 trillion. Despite this, the economy has declined drastically since the mid-1980s.

At the time of its independence in 1960, the Democratic Republic of the Congo was the second most industrialized country in Africa after South Africa. It boasted a thriving mining sector and its agriculture sector was relatively productive. Since then, corruption, war and political instability have been a severe detriment to further growth, today leaving DRC with a GDP per capita among the world's lowest.

Despite this the DRC is quickly modernizing having tied with Malaysia for the largest positive change in HDI development in 2016. And projects which include strengthening the health system for maternal and child health, expansion of electricity access, water supply reconstructions, and urban and social rehabilitation programs.

The two recent conflicts (the First and Second Congo Wars), which began in 1996, have dramatically reduced national output and government revenue, have increased external debt, and have resulted in deaths of more than five million people from war, and associated famine and disease. Malnutrition affects approximately two thirds of the country's population.

Agriculture is the mainstay of the economy, accounting for 57.9% of GDP in 1997. In 1996, agriculture employed 66% of the work force.

Rich in minerals, the Democratic Republic of the Congo has a difficult history of predatory mineral extraction, which has been at the heart of many struggles within the country for many decades, but particularly in the 1990s. The economy of the third largest country in Africa relies heavily on mining. However, much economic activity occurs in the informal sector and is not reflected in GDP data.

In 2006 Transparency International ranked the Democratic Republic of the Congo 156 out of 163 countries in the Corruption Perception Index, tying Bangladesh, Chad, and Sudan with a 2.0 rating. President Joseph Kabila established the Commission of Repression of Economic Crimes upon his ascension to power in 2001.

The conflicts in the DRC were over water, minerals, and other resources. Political agendas have worsened the economy, as in times of crisis, the elite benefit while the general populace suffers. This is worsened as a result of corrupt national and international corporations. The corporations instigate and allow the fighting for resources because they benefit from it. A large proportion of fatalities in the country are attributed to a lack of basic services, which is a reflection of the treatment of the citizens of the DRC. The influx of refugees since the war in 1998 only serves to worsen the issue of poverty. Money of the taxpayers in the DRC is often misappropriated by the corrupt leaders of the country, who often use the money to benefit themselves instead of the citizens of the DRC. The DRC is consistently rated the lowest on the UN Human Development Index.

Forced labor was important for the rural sector. The corporations that dominated the economy were mostly owned by Belgium, but British capital also played an important role. Independence caused the Congo to become the most industrialized country in Sub-Saharan Africa, after South Africa. The 1950s were a period of rising income and expectations. Congo was said to have the best public health system in Africa, but there was also a huge wealth disparity. Belgian companies favored workers in certain areas more and exported them to work in different areas, restricting opportunities for others. Favored groups also received better education and were able to secure jobs for people in the same ethnic group which increased tensions. In 1960 there were only 16 university graduates out of a population of 20 million. Belgium still had economic power and independence gave little opportunity for improvement. Common refrains included “no elite, no trouble” and “before independence = after independence”. When the Belgians left, most of the government officials and educated residents left with them. Before independence there were just 3 out of 5000 government jobs held by Congolese people. The resulting loss of institutional knowledge and human capital crippled the government.

After the Congo crisis, Mobutu arose as the country's sole ruler and stabilized the country politically. Economically, however, the situation continued to decline, and by 1979, the purchasing power was only 4% of that from 1960. Starting in 1976 the IMF provided stabilizing loans to the dictatorship. Much of the money was embezzled by Mobutu and his circle. This was not a secret as the 1982 report by IMF’s envoy Erwin Blumenthal documented. He stated, it is “alarmingly clear that the corruptive system in Zaire with all its wicked and ugly manifestations, its mismanagement and fraud will destroy all endeavors of international institutions, of friendly governments, and of the commercial banks towards recovery and rehabilitation of Zaire’s economy". Blumenthal indicated that there was “no chance” that creditors would ever recover their loans. Yet the IMF and the World Bank continued to lend money that was either embezzled, stolen, or "wasted on elephant projects". “Structural adjustment programmes” implemented as a condition of IMF loans cut support for health care, education, and infrastructure.

International Bank for Reconstruction and Development (IBRD) Trust Fund for the Congo.
Poor infrastructure, an uncertain legal framework, corruption, and lack of openness in government economic policy and financial operations remain a brake on investment and growth. A number of International Monetary Fund (IMF) and World Bank missions have met with the new government to help it develop a coherent economic plan but associated reforms are on hold.

Faced with continued currency depreciation, the government resorted to more drastic measures and in January 1999 banned the widespread use of U.S. dollars for all domestic commercial transactions, a position it later adjusted. The government has been unable to provide foreign exchange for economic transactions, while it has resorted to printing money to finance its expenditure. Growth was negative in 2000 because of the difficulty of meeting the conditions of international donors, continued low prices of key exports, and post-coup instability.
Although depreciated, congolese francs have been stable for few years (Ndonda, 2014)

Conditions improved in late 2002 with the withdrawal of a large portion of the invading foreign troops. A number of IMF and World Bank missions have met with the government to help it develop a coherent economic plan, and President Kabila has begun implementing reforms.

The DRC is embarking on the establishment of special economic zones (SEZ) to encourage the revival of its industry. The first SEZ was planned to come into being in 2012 in N'Sele, a commune of Kinshasa, and will focus on agro-industries. The Congolese authorities also planned to open another zone dedicated to mining (Katanga) and a third dedicated to cement (in the Bas-Congo). There are three phases to the program that each have their own objectives. Phase I was the precursor to the actual investment in the Special Economic Zone where policymakers agreed to the framework, the framework was studied for its establishment, and to predict the potential market demand for the land. Stage one of Phase II involved submitting laws for the Special Economic Zone, finding good sites for businesses, and currently there is an effort to help the government attract foreign investment. Stage two of Phase II hasn’t been started yet and it involves assisting the government in creating framework for the country, creating an overall plan for the site, figuring out what the environmental impact of the project will be, and guessing how much it will cost and what the return can be made on the investment. Phase III involves the World Bank creating a transaction phase that will keep everything competitive. The program is looking for options to hand over the program to the World Bank which could be very beneficial for the western part of the country.

Ongoing conflicts dramatically reduced government revenue and increased external debt. As Reyntjens wrote, “Entrepreneurs of insecurity are engaged in extractive activities that would be impossible in a stable state environment. The criminalization context in which these activities occur offers avenues for considerable factional and personal enrichment through the trafficking of arms, illegal drugs, toxic products, mineral resources and dirty money.” Ethnic rivalries were made worse because of economic interests and looting and coltan smuggling took place. Illegal monopolies formed in the country where they used forced labor for children to mine or work as soldiers. National parks were overrun with people looking to exploit minerals and resources. Increased poverty and hunger from the war and that increased the hunting of rare wildlife. Education was denied when the country was under foreign control and very few people make money off the minerals in the country. The national resources are not the root cause for the continued fighting in the region, however, the competition has become an incentive to keep fighting.[1] The DRC’s level of economic freedom is one of the lowest in the world, putting it in the repressed category. The armed militias fight with the government in the eastern section of the country over the mining sector or the corruption of the government, and weak policies lead to the instability of the economy. Human rights abuses also ruin economic activity; the DRC has a 7% unemployment rate, but still has one of the lowest GDP’s per capita in the world. A major problem for people trying to start their own companies is that the minimum amount of capital needed to launch the company is 5 times the average annual income, and prices are regulated by the government, which almost forces people to have to work for the larger, more corrupt businesses; otherwise, they won’t have work. It is hard for the DRC to encourage foreign trade because of the regulatory barriers.

International Bank for Reconstruction and Development (IBRD) Trust Fund for the Congo.
Poor infrastructure, an uncertain legal framework, corruption, and lack of openness in government economic policy and financial operations remain a brake on investment and growth. A number of International Monetary Fund (IMF) and World Bank missions have met with the new government to help it develop a coherent economic plan but associated reforms are on hold.

Faced with continued currency depreciation, the government resorted to more drastic measures and in January 1999 banned the widespread use of U.S. dollars for all domestic commercial transactions, a position it later adjusted. The government has been unable to provide foreign exchange for economic transactions, while it has resorted to printing money to finance its expenditure. Growth was negative in 2000 because of the difficulty of meeting the conditions of international donors, continued low prices of key exports, and post-coup instability. 125 companies in 2003 contributed to the conflict in DRC showing the corruption.

With the help of the International Development Association the DRC has worked toward the reestablishment of social services. This is done by giving 15 million people access to basic health services and giving bed nets to prevent malaria from spreading to people. With the Emergency Demobilization and Reintegration Program more than 107,000 adults and 34,000 child soldiers stood down their militarized posture. The travel time from Lubumbashi to Kasomeno in Katanga went down from seven days to two hours because of the improved roads which led to the decrease of prices of main goods by 60%. With the help of the IFC, KfW, and the EU the DRC improved its businesses by reducing the time it took to create a business by 51%, reducing the time it took to get construction permits by 54%, and reducing the number of taxes from 118 to 30. Improvements in health have been noticeable specifically that deliveries attended by trained staff jumped from 47 to 80%. In education 14 million textbooks were provided to children, completion rates of school have increased, and higher education was made available to students that chose to pursue it.

The Democratic Republic of Congo ranks 183 on the low end of the ease of doing business scale as ranked by the World Bank. This measures the difficulties of starting a business, enforcing contracts, paying taxes, resolving insolvency, protecting investors, trading across borders, getting credit, getting electricity, dealing with construction permits and registering property (World Bank 2014:8). Noticeable changes in the Ease of doing business rank from 2015 to 2016 are an 83 rank increase in the ability to start businesses from 172 to 89. Another noticeable change was the rank of dealing with construction permits which increased from 157 to 131.

The IMF plans on giving the DRC a $1 billion loan after its two-year suspension after it failed to give details about a mining deal from one of its state owned mines and an Israeli billionaire, Dan Gertler. The loan may be necessary for the country because there will be elections in December 2016 for the next president and the cost of funding this would range around $1.1 billion. The biggest problem with the vote is getting a country of 68 million people the size of Western Europe to polling stations with less than 1,860 miles of paved roads.

Agriculture is the mainstay of the economy, accounting for 57.9% of the GDP in 1997. Main cash crops include coffee, palm oil, rubber, cotton, sugar, tea, and cocoa. Food crops include cassava, plantains, maize, groundnuts, and rice. In 1996, agriculture employed 66% of the work force.

The Democratic Republic of Congo also possesses 50 percent of Africa’s forests and a river system that could provide hydro-electric power to the entire continent, according to a United Nations report on the country’s strategic significance and its potential role as an economic power in central Africa. Fish are the single most important source of animal protein in the DRC. Total production of marine, river, and lake fisheries in 2003 was estimated at 222,965 tons, all but 5,000 tons from inland waters. PEMARZA, a state agency, carries on marine fishing.

Forests cover 60 percent of the total land area. There are vast timber resources, and commercial development of the country’s 61 million hectares (150 million acres) of exploitable wooded area is only beginning. The Mayumbe area of Bas-Congo was once the major center of timber exploitation, but forests in this area were nearly
depleted. The more extensive forest regions of the central cuvette and of the Ubangi River valley have increasingly been tapped.

Roundwood removals were estimated at 72,170,000 m in 2003, about 95 percent for fuel. Some 14 species are presently being harvested. Exports of forest products in 2003 totalled $25.7 million. Foreign capital is necessary in order for forestry to expand, and the government recognizes that changes in tax structure and export procedures will be needed to facilitate economic growth.

Rich in minerals, the DRC has a difficult history of predatory mineral extraction, which has been at the heart of many struggles within the country for many decades, but particularly in the 1990s. Although the economy of the Democratic Republic of the Congo, the second largest country in Africa who has historically relied heavily on mining, is no longer reflected in the GDP data as the mining industry has suffered from long-term "uncertain legal framework, corruption, and a lack of transparency in government policy." The informal sector .

In her book entitled "The Real Economy of Zaire", MacGaffey described a second, often illegal economy, "system D," which is outside the official economy (MacGaffey 1991:27). and therefore is not reflected in the GDP.

exploitation of mineral substances as MIBA EMAXON and De Beers 
The economy of the second largest country in Africa relies heavily on mining. The Congo is the world's largest producer of cobalt ore, and a major producer of copper and industrial diamonds. The Congo has 70% of the world’s coltan, and more than 30% of the world’s diamond reserves., mostly in the form of small, industrial diamonds. The coltan is a major source of tantalum, which is used in the fabrication of electronic components in computers and mobile phones. In 2002, tin was discovered in the east of the country, but, to date, mining has been on a small scale.

Katanga Mining Limited, a London-based company, owns the Luilu Metallurgical Plant, which has a capacity of 175,000 tonnes of copper and 8,000 tonnes of cobalt per year, making it the largest cobalt refinery in the world. After a major rehabilitation program, the company restarted copper production in December 2007 and cobalt production in May 2008.

Much economic activity occurs in the informal sector and is not reflected in GDP data.

Ground transport in the Democratic Republic of Congo has always been difficult. The terrain and climate of the Congo Basin present serious barriers to road and rail construction, and the distances are enormous across this vast country. Furthermore, chronic economic mismanagement and internal conflict has led to serious under-investment over many years.

On the other hand, the Democratic Republic of Congo has thousands of kilometres of navigable waterways, and traditionally water transport has been the dominant means of moving around approximately two-thirds of the country.




</doc>
<doc id="8026" url="https://en.wikipedia.org/wiki?curid=8026" title="Politics of the Democratic Republic of the Congo">
Politics of the Democratic Republic of the Congo

Politics of the Democratic Republic of Congo take place in a framework of a republic in transition from a civil war to a semi-presidential republic.

On 18 and 19 December 2005, a successful nationwide referendum was carried out on a draft constitution, which set the stage for elections in 2006. The voting process, though technically difficult due to the lack of infrastructure, was facilitated and organized by the Congolese Independent Electoral Commission with support from the UN mission to the Congo (MONUC). Early UN reports indicate that the voting was for the most part peaceful, but spurred violence in many parts of the war-torn east and the Kasais.

In 2006, many Congolese complained that the constitution was a rather ambiguous document and were unaware of its contents. This is due in part to the high rates of illiteracy in the country. However, interim President Kabila urged Congolese to vote 'Yes', saying the constitution is the country's best hope for peace in the future. 25 million Congolese turned out for the two-day balloting. According to results released in January 2006, the constitution was approved by 84% of voters. The new constitution also aims to decentralize authority, dividing the vast nation into 25 semi-autonomous provinces, drawn along ethnic and cultural lines.

The country's first democratic elections in four decades were held on 30 July 2006.

From the day of the arguably ill-prepared independence of the Democratic Republic of the Congo, the tensions between the powerful leaders of the political elite, such as Joseph Kasa Vubu, Patrice Lumumba, Moise Tshombe, Joseph Mobutu and others, jeopardize the political stability of the new state. From Tshombe's secession of the Katanga, to the assassination of Lumumba, to the two coups d'état of Mobutu, the country has known periods of true nationwide peace, but virtually no period of genuine democratic rule.

The Regime of Marshall Mobutu Sese Seko lasted 32 years (1965–1997), during which all but the first seven years the country was named Zaire. The dictatorial regime operated as a one-party-state, which saw most of the powers concentrated between President Mobutu, who was simultaneously the head of the state-party (Popular Movement of the Revolution), and a series of essentially rubber-stamping institutions.

One particularity of the Regime was the claim to be thriving for an "authentic" system, different from Western, or Soviet influences. This lasted roughly between the establishment of Zaire in 1971, and the official beginning of the transition towards democracy, on 24 April 1990. This was true at the regular people's level as everywhere else. People were ordered by law to drop their Western Christian names; the titles Mr. and Mrs. were abandoned for the male and female versions of the French word for "citizen"; Men were forbidden to wear suits, and women to wear pants. At the institutional level, many of the institutions also changed denominations, but the end result was a system that borrowed from both systems:


Every corporation, whether financial or union, as well as every division of the administration, were set up as branches of the party, the CEOs, Union leaders, and division directors being sworn-in as section presidents of the party. Every aspect of life was regulated to some degree by the party, and the will of its founding-president, Mobutu Sese Seko.

Most of the petty aspects of the regime disappeared after 1990, and the beginning of the democratic transition. The latter was intended to be fairly short-lived, but Mobutu's power plays dragged it in length, to ultimately 1997, when the forces-led by Laurent Kabila eventually toppled the regime, after a 9-month-long successful military campaign.

The government of former president Mobutu Sese Seko was toppled by a rebellion led by Laurent Kabila in May 1997, with the support of Rwanda and Uganda. They were later to turn against Kabila and backed a rebellion against him in August 1998. Troops from Zimbabwe, Angola, Namibia, Chad, and Sudan intervened to support the Kinshasa regime. A cease-fire was signed on 10 July 1999 by the DROC, Zimbabwe, Angola, Uganda, Namibia, Rwanda, and Congolese armed rebel groups, but fighting continued.

Under Laurent Kabila's regime, all executive, legislative, and military powers were first vested in the President, Laurent-Désiré Kabila. The judiciary was independent, with the president having the power to dismiss or appoint. The president was first head of a 26-member cabinet dominated by the Alliance of Democratic Forces for the Liberation of Congo (ADFL). Towards the end of the 90s, Laurent Kabila created and appointed a Transitional Parliament, with a seat in the buildings of the former Katanga Parliament, in the southern town of Lubumbashi, in a move to unite the country, and to legitimate his regime. Kabila was assassinated on 16 January 2001 and his son Joseph Kabila was named head of state ten days later.

The younger Kabila continued with his father's Transitional Parliament, but overhauled his entire cabinet, replacing it with a group of technocrats, with the stated aim of putting the country back on the track of development, and coming to a decisive end of the Second Congo War. In October 2002, the new president was successful in getting occupying Rwandan forces to withdraw from eastern Congo; two months later, an agreement was signed by all remaining warring parties to end the fighting and set up a Transition Government, the make-up of which would allow representation for all negotiating parties. Two founding documents emerged from this: The , and the Global and Inclusive Agreement, both of which describe and determine the make-up and organization of the Congolese institutions, until planned elections in July 2006, at which time the provisions of the new constitution, democratically approved by referendum in December 2005, will take full effect and that is how it happened.

Under the Global and All-Inclusive Agreement, signed on 17 December 2002, in Pretoria, there was to be one President and four Vice-Presidents, one from the government, one from the Rally for Congolese Democracy, one from the MLC, and one from civil society. The position of Vice-President expired after the 2006 elections.

After being for three years (2003–06) in the interregnum between two constitutions, the Democratic Republic of the Congo is now under the regime of the Constitution of the Third Republic. The constitution, adopted by referendum in 2005, and promulgated by President Joseph Kabila in February 2006, establishes a decentralized semi-presidential republic, with a separation of powers between the three branches of government - executive, legislative and judiciary, and a distribution of prerogatives between the central government and the provinces.

As of 8 August 2017 there are 54 political parties legally operating in the Congo.

Since the July 2006 elections, the country is led by a semi-presidential, strongly-decentralized state. The executive at the central level, is divided between the President, and a Prime Minister appointed by him/her from the party having the majority of seats in Parliament. Should there be no clear majority, the President can appoint a "government former" that will then have the task to win the confidence of the National Assembly. The President appoints the government members (ministers) at the proposal of the Prime Minister. In coordination, the President and the government have the charge of the executive. The Prime minister and the government are responsible to the lower-house of Parliament, the National Assembly.

At the province level, the Provincial legislature (Provincial Assembly) elects a governor, and the governor, with his government of up to 10 ministers, is in charge of the provincial executive. Some domains of government power are of the exclusive provision of the Province, and some are held concurrently with the Central government. This is not a Federal state however, simply a decentralized one, as the majority of the domains of power are still vested in the Central government. The governor is responsible to the Provincial Assembly.

The semi-presidential system has been described by some as "conflictogenic" and "dictatogenic", as it ensures frictions, and a reduction of pace in government life, should the President and the Prime Minister be from different sides of the political arena. This was seen several times in France, a country that shares the semi-presidential model. It was also, arguably, in the first steps of the Congo into independence, the underlying cause of the crisis between Prime Minister Patrice Lumumba and President Joseph Kasa Vubu, who ultimately dismissed each other, in 1960.

In January 2015 the 2015 Congolese protests broke out in the country's capital following the release of a draft law that would extend the presidential term limits and allow Joseph Kabila to run again for office.

The Inter-Congolese dialogue, that set-up the transitional institutions, created a bicameral parliament, with a National Assembly and Senate, made up of appointed representatives of the parties to the dialogue. These parties included the preceding government, the rebel groups that were fighting against the government, with heavy Rwandan and Ugandan support, the internal opposition parties, and the Civil Society. At the beginning of the transition, and up until recently, the National Assembly is headed by the MLC with Speaker Hon. Olivier Kamitatu, while the Senate is headed by a representative of the Civil Society, namely the head of the Church of Christ in Congo, Mgr. Pierre Marini Bodho. Hon. Kamitatu has since left both the MLC and the Parliament to create his own party, and ally with current President Joseph Kabila. Since then, the position of Speaker is held by Hon. Thomas Luhaka, of the MLC.

Aside from the regular legislative duties, the Senate had the charge to draft a new constitution for the country. That constitution was adopted by referendum in December 2005, and decreed into law on 18 February 2006.

The Parliament of the third republic is also bicameral, with a National Assembly and a Senate. Members of the National Assembly, the lower - but the most powerful - house, are elected by direct suffrage. Senators are elected by the legislatures of the 26 provinces.

The Congolese Judicial Branch Consists of a Supreme Court, which handles federal crimes.

10 provinces (provinces, singular - province) and one city* (ville): Bandundu, Bas-Congo, Équateur, Kasai-Occidental, Kasai-Oriental, Katanga, Kinshasa*, Maniema, North Kivu, Orientale.

Each province is divided into districts.

25 provinces (provinces, singular - province) and city* (ville): Bas-Uele | Équateur | Haut-Lomami | Haut-Katanga | Haut-Uele | Ituri | Kasaï | Kasaï oriental | Kongo central | Kwango | Kwilu | Lomami | Lualaba | Lulua | Mai-Ndombe | Maniema | Mongala | North Kivu | Nord-Ubangi | Sankuru | South Kivu | Sud-Ubangi | Tanganyika | Tshopo | Tshuapa | Kinshasa*

ACCT, ACP, AfDB, AU, CEEAC, CEPGL, ECA, FAO, G-19, G-24, G-77, IAEA, IBRD, ICAO, ICC, ICRM, IDA, IFAD, IFC, IFRCS, IHO, ILO, IMF, UN, UNCTAD, UNESCO, UNHCR, UNIDO, UPU, WCO WFTU, WHO, WIPO, WMO, WToO, WTrO


</doc>
<doc id="8027" url="https://en.wikipedia.org/wiki?curid=8027" title="Telecommunications in the Democratic Republic of the Congo">
Telecommunications in the Democratic Republic of the Congo

Telecommunications in the Democratic Republic of the Congo include radio, television, fixed and mobile telephones, and the Internet.


Radio is the dominant medium; a handful of stations, including state-run Radio-Télévision Nationale Congolaise (RTNC), broadcast across the country. The United Nations Mission (MONUSCO) and a Swiss-based NGO, Fondation Hirondelle, operate one of country's leading stations, Radio Okapi. The network employs mostly-Congolese staff and aims to bridge political divisions. Radio France Internationale (RFI), which is widely available on FM, is the most popular news station. The BBC broadcasts on FM in Kinshasa (92.7), Lubumbashi (92.0), Kisangani (92.0), Goma (93.3) and Bukavu (102.2).







</doc>
<doc id="8028" url="https://en.wikipedia.org/wiki?curid=8028" title="Transport in the Democratic Republic of the Congo">
Transport in the Democratic Republic of the Congo

Ground transport in the Democratic Republic of Congo (DRC) has always been difficult. The terrain and climate of the Congo Basin present serious barriers to road and rail construction, and the distances are enormous across this vast country. Furthermore, chronic economic mismanagement and internal conflict has led to serious under-investment over many years.

On the other hand, the DRC has thousands of kilometres of navigable waterways, and traditionally water transport has been the dominant means of moving around approximately two-thirds of the country.

As an illustration of transport difficulties in the DRC, even before wars damaged the infrastructure, the so-called "national" route, used to get supplies to Bukavu from the seaport of Matadi, consisted of the following:
In other words, goods had to be loaded and unloaded eight times and the total journey would take many months.

Many of the routes listed below are in poor condition and may be operating at only a fraction of their original capacity (if at all), despite recent attempts to make improvements. Up to 2006 the United Nations Joint Logistics Centre (UNJLC) had an operation in Congo to support humanitarian relief agencies working there, and its bulletins and maps about the transport situation are archived on ReliefWeb.

The First and Second Congo Wars saw great destruction of transport infrastructure from which the country has not yet recovered. Many vehicles were destroyed or commandeered by militias, especially in the north and east of the country, and the fuel supply system was also badly affected. Consequently, outside of Kinshasa, Matadi and Lubumbashi, private and commercial road transport is almost non-existent and traffic is scarce even where roads are in good condition. The few vehicles in use outside these cities are run by the United Nations, aid agencies, the DRC government, and a few larger companies such as those in the mining and energy sectors. High-resolution satellite photos on the Internet show large cities such as Bukavu, Butembo and Kikwit virtually devoid of traffic, compared to similar photos of towns in neighbouring countries.

Air transport is the only effective means of moving between many places within the country. The Congolese government, the United Nations, aid organisations and large companies use air rather than ground transport to move personnel and freight. The UN operates a large fleet of aircraft and helicopters, and compared to other African countries the DRC has a large number of small domestic airlines and air charter companies. The transport (and smuggling) of minerals with a high value for weight is also carried out by air, and in the east, some stretches of paved road isolated by destroyed bridges or impassable sections have been turned into airstrips.

For the ordinary citizen though, especially in rural areas, often the only options are to cycle, walk or go by dugout canoe.

Some parts of the DRC are more accessible from neighbouring countries than from Kinshasa. For example, Bukavu itself and Goma and other north-eastern towns are linked by paved road from the DRC border to the Kenyan port of Mombasa, and most goods for these cities have been brought via this route in recent years. Similarly, Lubumbashi and the rest of Katanga Province is linked to Zambia, through which the paved highway and rail networks of Southern Africa can be accessed. Such links through neighbouring countries are generally more important for the east and south-east of the country, and are more heavily used, than surface links to the capital.

In 2007 China agreed to lend the DRC US$5bn for two major transport infrastructure projects to link mineral-rich Katanga, specifically Lubumbashi, by rail to an ocean port (Matadi) and by road to the Kisangani river port, and to improve its links to the transport network of Southern Africa in Zambia. The two projects would also link the major parts of the country not served by water transport, and the main centres of the economy. Loan repayments will be from concessions for raw materials which China desperately needs: copper, cobalt, gold and nickel, as well as by toll revenues from the road and railway. In the face of reluctance by the international business community to invest in DRC, this represents a revitalisation of DRC's infrastructure much needed by its government.

The China Railway Seventh Group Co. Ltd will be in charge of the contract, under signed by the China Railway Engineering Corporation, with construction to be started from June 2008.

The Democratic Republic of the Congo has fewer all-weather paved highways than any country of its population and size in Africa — a total of 2250 km, of which only 1226 km is in good condition (see below). To put this in perspective, the road distance across the country in any direction is more than 2500 km (e.g. Matadi to Lubumbushi, 2700 km by road). The figure of 2250 km converts to 35 km of paved road per 1,000,000 of population. Comparative figures for Zambia and Botswana are 721 km and 3427 km respectively.

The road network is theoretically divided into four categories (national roads, priority regional roads, secondary regional roads and local roads), however, the United Nations Joint Logistics Centre (UNJLC) reports that this classification is of little practical use because some roads simply do not exist. For example, National Road 9 is not operational and cannot be detected by remote sensing methods.

The two principal highways are:

The total road network in 2005, according to the UNJLC, consisted of:


The UNJLC also points out that the pre-Second Congo War network no longer exists, and is dependent upon 20,000 bridges and 325 ferries, most of which are in need of repair or replacement. In contrast, a Democratic Republic of the Congo government document shows that, also in 2005, the network of main highways in good condition was as follows:

The 2000 Michelin "Motoring and Tourist Map 955 of Southern and Central Africa", which categorizes roads as "surfaced", "improved" (generally unsurfaced but with gravel added and graded), "partially improved" and "earth roads" and "tracks" shows that there were 2694 km of paved highway in 2000. These figures indicate that, compared to the more recent figures above, there has been a deterioration this decade, rather than improvement.

Three routes in the Trans-African Highway network pass through DR Congo:

The DRC has more navigable rivers and moves more passengers and goods by boat and ferry than any other country in Africa. Kinshasa, with 7 km of river frontage occupied by wharfs and jetties, is the largest inland waterways port on the continent. However, much of the infrastructure — vessels and port handling facilities — has, like the railways, suffered from poor maintenance and internal conflict.

The total length of waterways is estimated at 15,000 km including the Congo River, its tributaries, and unconnected lakes.

The 1000-kilometre Kinshasa-Kisangani route on the Congo River is the longest and best-known. It is operated by river tugs pushing several barges lashed together, and for the hundreds of passengers and traders these function like small floating towns. Rather than mooring at riverside communities along the route, traders come out by canoe and small boat alongside the river barges and transfer goods on the move.

Most waterway routes do not operate to regular schedules. It is common for an operator to moor a barge at a riverside town and collect freight and passengers over a period of weeks before hiring a river tug to tow or push the barge to its destination.


The middle Congo River and its tributaries from the east are the principal domestic waterways in the DRC. The two principal river routes are:
See the diagrammatic transport map above for other river waterways.

The most-used domestic lake waterways are:

Most large Congo river ferry boats were destroyed during the civil war. Only smaller boats are running and they are irregular.





petroleum products 390 km

1 petroleum tanker

Due to the lack of roads, operating railroads and ferry transportation many people traveling around the country fly on aircraft. As of 2016 the country does not have an international passenger airline and relies on foreign-based airlines for international connections. Congo Airways provides domestic flights and are based at Kinshasa's N'djili Airport which serves as the country's main international airport. Lubumbashi International Airport in the country's south-east is also serviced by several international airlines. 

<br>"total:"
24
<br>"over 3,047 m:"
4
<br>"2,438 to 3,047 m:"
2
<br>"1,524 to 2,437 m:"
16
<br>"914 to 1,523 m:"
2 (2002 est.)

<br>"total:"
205
<br>"1,524 to 2,437 m:"
19
<br>"914 to 1,523 m:"
95
<br>"under 914 m:"
91 (2002 est.)

All air carriers certified by the Democratic Republic of the Congo have been banned from operating at airports in the European Community by the European Commission because of inadequate safety standards.





The Democratic Republic of the Congo has a rocketry program called Troposphere.




</doc>
<doc id="8029" url="https://en.wikipedia.org/wiki?curid=8029" title="Armed Forces of the Democratic Republic of the Congo">
Armed Forces of the Democratic Republic of the Congo

The Armed Forces of the Democratic Republic of the Congo ( (FARDC)) is the state organisation responsible for defending the Democratic Republic of the Congo. The FARDC was rebuilt patchily as part of the peace process which followed the end of the Second Congo War in July 2003.

The majority of FARDC members are land forces, but it also has a small air force and an even smaller navy. In 2010–11 the three services may have numbered between 144,000 and 159,000 personnel. In addition, there is a presidential force called the Republican Guard, but it and the Congolese National Police (PNC) are not part of the Armed Forces.

The government in the capital city Kinshasa, the United Nations, the European Union, and bilateral partners which include Angola, South Africa, and Belgium are attempting to create a viable force with the ability to provide the Democratic Republic of Congo with stability and security. However, this process is being hampered by corruption, inadequate donor coordination, and competition between donors. The various military units now grouped under the FARDC banner are some of the most unstable in Africa after years of war and underfunding.

To assist the new government, since February 2000 the United Nations has had the United Nations Mission in the Democratic Republic of Congo (now called MONUSCO), which currently has a strength of over 16,000 peacekeepers in the country. Its principal tasks are to provide security in key areas, such as the South Kivu and North Kivu in the east, and to assist the government in reconstruction. Foreign rebel groups are also in the Congo, as they have been for most of the last half-century. The most important is the Democratic Forces for the Liberation of Rwanda (FDLR), against which Laurent Nkunda's troops were fighting, but other smaller groups such as the anti-Ugandan Lord's Resistance Army are also present.

The legal standing of the FARDC was laid down in the Transitional Constitution, articles 118 and 188. This was then superseded by provisions in the 2006 Constitution, articles 187 to 192. Law 04/023 of 12 November 2004 establishes the General Organisation of Defence and the Armed Forces. In mid-2010, the Congolese Parliament was debating a new defence law, provisionally designated Organic Law 130.

The first organised Congolese troops, known as the , were created in 1888 when King Leopold II of Belgium, who held the Congo Free State as his private property, ordered his Secretary of the Interior to create military and police forces for the state. In 1908, under international pressure, Leopold ceded administration of the colony to the government of Belgium as the Belgian Congo. It remained under the command of a Belgian officer corps through to the independence of the colony in 1960. The "Force Publique" saw combat in Cameroun, and successfully invaded and conquered areas of German East Africa, notably present day Rwanda, during World War I. Elements of the "Force Publique" were also used to form Belgian colonial units that fought in the East African Campaign during World War II.

At independence on 30 June 1960, the army suffered from a dramatic deficit of trained leaders, particularly in the officer corps. This was because the "Force Publique" had always only been officered by Belgian or other expatriate whites. The Belgian Government made no effort to train Congolese commissioned officers until the very end of the colonial period, and in 1958, only 23 African cadets had been admitted even to the military secondary school. The highest rank available to Congolese was adjutant, which only four soldiers achieved before independence. Though 14 Congolese cadets were enrolled in the Royal Military Academy in Brussels in May, they were not scheduled to graduate as second lieutenants until 1963. Ill-advised actions by Belgian officers led to an enlisted ranks' rebellion on 5 July 1960, which helped spark the Congo Crisis. Lieutenant General Émile Janssens, the "Force Publique" commander, wrote during a meeting of soldiers that 'Before independence=After Independence', pouring cold water on the soldiers' desires for an immediate raise in their status.

Vanderstraeten says that on the morning of 8 July 1960, following a night during which all control had been lost over the soldiers, numerous ministers arrived at Camp Leopold with the aim of calming the situation. Both Prime Minister Patrice Lumumba and President Joseph Kasa-Vubu eventually arrived, and the soldiers listened to Kasa-Vubu "religiously." After his speech, Kasa-Vubu and the ministers present retired into the camp canteen to hear a delegation from the soldiers. Vanderstraeten says that, according to Joseph Ileo, their demands ("revendications") included the following:
The "laborious" discussions which then followed were later retrospectively given the label of an "extraordinary ministerial council." Gérald-Libois writes that '..the special meeting of the council of ministers took steps for the immediate Africanisation of the officer corps and named Victor Lundula, who was born in Kasai and was burgomaster of Jadotville, as Commander-in-Chief of the ANC; Colonel Joseph-Désiré Mobutu as chief of staff; and the Belgian, Colonel Henniquiau, as chief advisor to the ANC.' Thus General Janssens was dismissed. Both Lundula and Mobutu were former sergeants of the "Force Publique". It appears that Maurice Mpolo, Minister of Youth and Sports, was given the defence portfolio.

On 8–9 July 1960, the soldiers were invited to appoint black officers, and 'command of the army passed securely into the hands of former sergeants,' as the soldiers in general chose the most-educated and highest-ranked Congolese army soldiers as their new officers. Most of the Belgian officers were retained as advisors to the new Congolese hierarchy, and calm returned to the two main garrisons at Leopoldville and Thysville. The "Force Publique" was renamed the "Armée nationale congolaise" (ANC), or Congolese National Armed Forces. However, in Katanga Belgian officers resisted the Africanisation of the army.

On 9 July 1960, there was a "Force Publique" mutiny at Camp Massart at Elizabethville; five or seven Europeans were killed. The army revolt and resulting rumours caused severe panic across the country, and Belgium despatched troops and the naval Task Group 218.2 to protect its citizens. Belgian troops intervened in Elisabethville and Luluabourg (10 July), Matadi (11 July), Leopoldville (13 July) and elsewhere. There were immediate suspicions that Belgium planned to re-seize the country while doing so. Large numbers of Belgian colonists fled the country. At the same time, on 11 July, Moise Tshombe declared the independence of Katanga Province in the south-east, closely backed by remaining Belgian administrators and soldiers.
On 14 July 1960, in response to requests by Prime Minister Lumumba, the UN Security Council adopted United Nations Security Council Resolution 143. This called upon Belgium to remove its troops and for the UN to provide 'military assistance' to the Congolese forces to allow them 'to meet fully their tasks'. Lumumba demanded that Belgium remove its troops immediately, threatening to seek help from the Soviet Union if they did not leave within two days. The UN reacted quickly and established the United Nations Operation in the Congo (ONUC). The first UN troops arrived the next day but there was instant disagreement between Lumumba and the UN over the new force's mandate. Because the Congolese army had been in disarray since the mutiny, Lumumba wanted to use the UN troops to subdue Katanga by force. Lumumba became extremely frustrated with the UN's unwillingness to use force against Tshombe and his secession. He cancelling a scheduled meeting with Secretary General Hammarskjöld on August 14 and wrote a series of angry letters instead. To Hammarskjöld, the secession of Katanga was an internal Congolese matter and the UN was forbidden to intervene by Article 2 of the United Nations Charter. Disagreements over what the UN force could and could not do continued throughout its deployment.

By 20 July 1960, 3,500 troops for ONUC had arrived in the Congo. The first contingent of Belgian forces had left Leopoldville on 16 July upon the arrival of the United Nations troops. Following assurances that contingents of the Force would arrive in sufficient numbers, the Belgian authorities agreed to withdraw all their forces from the Leopoldville area by 23 July. The last Belgian troops left the country by 23 July, as United Nations forces continued to deploy throughout the Congo. The build of ONUC continued, its strength increasing to over 8,000 by 25 July and to over 11,000 by 31 July 1960. A basic agreement between the United Nations and the Congolese Government on the operation of the Force was agreed by 27 July. On 9 August, Albert Kalonji proclaimed the independence of South Kasai.
During the crucial period of July–August 1960, Mobutu built up "his" national army by channeling foreign aid to units loyal to him, by exiling unreliable units to remote areas, and by absorbing or dispersing rival armies. He tied individual officers to him by controlling their promotion and the flow of money for payrolls. Researchers working from the 1990s have concluded that money was directly funnelled to the army by the U.S. Central Intelligence Agency, the UN, and Belgium. Despite this, by September 1960, following the four-way division of the country, there were four separate armed forces: Mobotu's ANC itself, numbering about 12,000, the South Kasai Constabulary loyal to Albert Kalonji (3,000 or less), the Katanga Gendarmerie which were part of Moise Tshombe's regime (totalling about 10,000), and the Stanleyville dissident ANC loyal to Antoine Gizenga (numbering about 8,000).

In August 1960, due to rejection of requests to the UN for aid to suppress the South Kasai and Katanga revolts, Lumumba's government decided to request Soviet help. de Witte writes that 'Leopoldville asked the Soviet Union for planes, lorries, arms, and equipment. ... Shortly afterwards, on 22 or 23 August, about 1,000 soldiers left for Kasai.' de Witte goes on to write that on 26–27 August, the ANC seized Bakwanga, Albert Kalonji's capital in South Kasai, without serious resistance. "In the next two days it temporarily put an end to the secession of Kasai."

The Library of Congress Country Study for the Congo says at this point that:
"[On 5 September 1960] Kasavubu also appointed Mobutu as head of the ANC. Joseph Ileo was chosen as the new prime minister and began trying to form a new government. Lumumba and his cabinet responded by accusing Kasa-Vubu of high treason and voted to dismiss him. Parliament refused to confirm the dismissal of either Lumumba or Kasavubu and sought to bring about a reconciliation between them. After a week's deadlock, Mobutu announced on September 14 that he was assuming power until 31 December 1960, in order to "neutralize" both Kasavubu and Lumumba."

In early January 1961, ANC units loyal to Lumumba invaded northern Katanga to support a revolt of Baluba tribesmen against Tshombe's secessionist regime. On 23 January 1961, Kasa-Vubu promoted Mobutu to major-general; De Witte argues that this was a political move, "aimed to strengthen the army, the president's sole support, and Mobutu's position within the army."

United Nations Security Council Resolution 161 of 21 February 1961, called for the withdrawal of Belgian officers from command positions in the ANC, and the training of new Congolese officers with UN help. ONUC made a number of attempts to retrain the ANC from August 1960 to June 1963, often been set back by political changes. By March 1963 however, after the visit of Colonel Michael Greene of the United States Army, and the resulting "Greene Plan", the pattern of bilaterally agreed military assistance to various Congolese military components, instead of a single unified effort, was already taking shape.
In early 1964, a new crisis broke out as Congolese rebels calling themselves "Simba" (Swahili for "Lion") rebelled against the government. They were led by Pierre Mulele, Gaston Soumialot and Christophe Gbenye who were former members of Gizenga's Parti Solidaire Africain (PSA). The rebellion affected Kivu and Eastern (Orientale) provinces. By August they had captured Stanleyville and set up a rebel government there. As the rebel movement spread, discipline became more difficult to maintain, and acts of violence and terror increased. Thousands of Congolese were executed, including government officials, political leaders of opposition parties, provincial and local police, school teachers, and others believed to have been Westernised. Many of the executions were carried out with extreme cruelty, in front of a monument to Lumumba in Stanleyville. Tshombe decided to use foreign mercenaries as well as the ANC to suppress the rebellion. Mike Hoare was employed to create the English-speaking 5 Commando ANC at Kamina, with the assistance of a Belgian officer, Colonel Frederic Vanderwalle, while 6 Commando ANC was French-speaking and originally under the command of a Belgian Army colonel, Lamouline. By August 1964, the mercenaries, with the assistance of other ANC troops, were making headway against the Simba rebellion. Fearing defeat, the rebels started taking hostages of the local white population in areas under their control. These hostages were rescued in Belgian airdrops (Dragon Rouge and Dragon Noir) over Stanleyville and Paulis airlift sed by U.S. aircraft. The operation coincided with the arrival of mercenary units (seemingly including the hurriedly formed 5th Mechanised Brigade) at Stanleyville which was quickly captured. It took until the end of the year to completely put down the remaining areas of rebellion.

After five years of turbulence, in 1965 Mobutu used his position as ANC Chief of Staff to seize power in the Congo. Although Mobutu succeeded in taking power, his position was soon threatened by the Kisangani Mutinies, also known as the Stanleyville Mutinies or Mercenaries' Mutinies, which were eventually suppressed.

As a general rule, since that time, the armed forces have not intervened in politics as a body, rather being tossed and turned as ambitious men have shaken the country. In reality, the larger problem has been the misuse and sometimes abuse of the military and police by political and ethnic leaders.

On 16 May 1968 a parachute brigade of two regiments (each of three battalions) was formed which eventually was to grow in size to a full division.

The country was renamed Zaire in 1971 and the army was consequently designated the (FAZ). In 1971 the army's force consisted of the 1st Groupement at Kananga, with one guard battalion, two infantry battalions, and a gendarmerie battalion attached, and the 2nd Groupement (Kinshasa), the 3rd Groupement (Kisangani), the 4th Groupement (Lubumbashi), the 5th Groupement (Bukavu), the 6th Groupement (Mbandaka), and the 7th Groupement (Boma). Each was about the size of a brigade, and commanded by 'aging generals who have had no military training, and often not much positive experience, since they were NCOs in the Belgian Force Publique.' By the late 1970s the number of groupements reached nine, one per administrative region. The parachute division (Division des Troupes Aéroportées Renforcées de Choc, DITRAC) operated semi-independently from the rest of the army.

In July 1972 a number of the aging generals commanding the "groupements" were retired. Général d'armée Louis Bobozo, and Generaux de Corps d'Armée Nyamaseko Mata Bokongo, Nzoigba Yeu Ngoli, Muke Massaku, Ingila Grima, Itambo Kambala Wa Mukina, Tshinyama Mpemba, and General de Division Yossa Yi Ayira, the last having been commander of the Kamina base, were all retired on 25 July 1972. Taking over as military commander-in-chief, now titled Captain General, was newly promoted General de Division Bumba Moaso, former commander of the parachute division.

A large number of countries supported the FAZ in the early 1970s. Three hundred Belgian personnel were serving as staff officers and advisors throughout the Ministry of Defence, Italians were supporting the Air Force, Americans were assisting with transport and communications, Israelis with airborne forces training, and there were British advisors with the engineers. In 1972 the state-sponsored political organization, the Mouvement Populaire de la Révolution (MPR), resolved at a party congress to form activist cells in each military unit. The decision caused consternation among the officer corps, as the army had been apolitical (and even anti-political) since before independence.

On 11 June 1975 several military officers were arrested in what became known as the "coup monté et manqué." Amongst those arrested were Générals Daniel Katsuva wa Katsuvira, Land Forces Chief of Staff, Utshudi Wembolenga, Commandant of the 2nd Military Region at Kalemie; Fallu Sumbu, Military Attaché of Zaïre in Washington, Colonel Mudiayi wa Mudiayi, the military attaché of Zaïre in Paris, the military attache in Brussels, a paracommando battalion commander, and several others. The regime alleged these officers and others (including Mobutu's civil "secrétaire particulier") had plotted the assassination of Mobutu, high treason, and disclosure of military secrets, among other offences. The alleged coup was investigated by a revolutionary commission headed by Boyenge Mosambay Singa, at that time head of the Gendarmerie. Writing in 1988, Michael Schatzberg said the full details of the coup had yet to emerge.

In late 1975, Mobutu, in a bid to install a pro-Kinshasa government in Angola and thwart the Marxist Popular Movement for the Liberation of Angola (MPLA)'s drive for power, deployed FAZ armoured cars, paratroopers, and three infantry battalions to Angola in support of the National Liberation Front of Angola (FNLA).
On 10 November 1975, an anti-Communist force made up of 1,500 FNLA fighters, 100 Portuguese Angolan soldiers, and two FAZ battalions passed near the city of Quifangondo, only north of Luanda, at dawn on 10 November. The force, supported by South African aircraft and three 140 mm artillery pieces, marched in a single line along the Bengo River to face an 800-strong Cuban force across the river. Thus the Battle of Quifangondo began. The Cubans and MPLA fighters bombarded the FNLA with mortar and 122 mm rockets, destroying most of the FNLA's armoured cars and six Jeeps carrying antitank rockets in the first hour of fighting.

Mobutu's support for the FNLA policy backfired when the MPLA won in Angola. The MPLA, then, acting ostensibly at least as the Front for Congolese National Liberation, occupied Zaire's southeastern Katanga Province, then known as Shaba, in March 1977, facing little resistance from the FAZ. This invasion is sometimes known as Shaba I. Mobutu had to request assistance, which was provided by Morocco in the form of regular troops who routed the MPLA and their Cuban advisors out of Katanga. Also important were Egyptian pilots who flew Zaire's Mirage 5 combat aircraft. The humiliation of this episode led to civil unrest in Zaire in early 1978, which the FAZ had to put down.
The poor performance of Zaire's military during Shaba I gave evidence of chronic weaknesses. One problem was that some of the Zairian soldiers in the area had not received pay for extended periods. Senior officers often kept the money intended for the soldiers, typifying a generally disreputable and inept senior leadership in the FAZ. As a result, many soldiers simply deserted rather than fight. Others stayed with their units but were ineffective. During the months following the Shaba invasion, Mobutu sought solutions to the military problems that had contributed to the army's dismal performance. He implemented sweeping reforms of the command structure, including wholesale firings of high-ranking officers. He merged the military general staff with his own presidential staff and appointed himself chief of staff again, in addition to the positions of minister of defence and supreme commander that he already held. He also redeployed his forces throughout the country instead of keeping them close to Kinshasa, as had previously been the case. The Kamanyola Division, at the time considered the army's best formation, and considered the president's own, was assigned permanently to Shaba. In addition to these changes, the army's strength was reduced by 25 percent. Also, Zaire's allies provided a large influx of military equipment, and Belgian, French, and American advisers assisted in rebuilding and retraining the force.

Despite these improvements, a second invasion by the former Katangan gendarmerie, known as Shaba II in May–June 1978, was only dispersed with the despatch of the French 2e régiment étranger de parachutistes and a battalion of the Belgian Paracommando Regiment. Kamanyola Division units collapsed almost immediately. French units fought the Battle of Kolwezi to recapture the town from the FLNC. The U.S. provided logistical assistance.

In July 1975, according to the IISS Military Balance, the FAZ was made up of 14 infantry battalions, seven "Guard" battalions, and seven other infantry battalions variously designated as "parachute" (or possibly "commando"; probably the units of the new parachute brigade originally formed in 1968). There were also an armoured car regiment and a mechanised infantry battalion. Organisationally, the army was made up of seven brigade groups and one parachute division. In addition to these units, a tank battalion was reported to have formed by 1979.

In January 1979 "General de Division" Mosambaye Singa Boyenge was named as both military region commander and Region Commissioner for Shaba.

In 1984, a militarised police force, the Civil Guard, was formed. It was eventually commanded by Général d'armée Kpama Baramoto Kata.

Thomas Turner wrote in the late 1990s that "[m]ajor acts of violence, such as the killings that followed the "Kasongo uprising" in Bandundu Region in 1978, the killings of diamond miners in Kasai-Oriental Region in 1979, and, more recently, the massacre of students in Lubumbashi in 1990, continued to intimidate the population."

The authors of the Library of Congress Country Study on Zaire commented in 1992–93 that: "The maintenance status of equipment in the inventory has traditionally varied, depending on a unit's priority and the presence or absence of foreign advisers and technicians. A considerable portion of military equipment is not operational, primarily as a result of shortages of spare parts, poor maintenance, and theft. For example, the tanks of the 1st Armoured Brigade often have a nonoperational rate approaching 70 to 80 percent. After a visit by a Chinese technical team in 1985, most of the tanks operated, but such an improved status generally has not lasted long beyond the departure of the visiting team. Several factors complicate maintenance in Zairian units. Maintenance personnel often lack the training necessary to maintain modern military equipment. Moreover, the wide variety of military equipment and the staggering array of spare parts necessary to maintain it not only clog the logistic network but also are expensive.

The most important factor that negatively affects maintenance is the low and irregular pay that soldiers receive, resulting in the theft and sale of spare parts and even basic equipment to supplement their meager salaries. When not stealing spare parts and equipment, maintenance personnel often spend the better part of their duty day looking for other ways to profit. American maintenance teams working in Zaire found that providing a free lunch to the work force was a good, sometimes the only, technique to motivate personnel to work at least half of the duty day.

The army's logistics corps [was tasked].. to provide logistic support and conduct direct, indirect, and depot-level maintenance for the FAZ. But because of Zaire's lack of emphasis on maintenance and logistics, a lack of funding, and inadequate training, the corps is understaffed, underequipped, and generally unable to accomplish its mission. It is organised into three battalions assigned to Mbandaka, Kisangani, and Kamina, but only the battalion at Kamina is adequately staffed; the others are little more than skeleton" units.

The poor state of discipline of the Congolese forces became apparent again in 1990. Foreign military assistance to Zaire ceased following the end of the Cold War and Mobutu deliberately allowed the military's condition to deteriorate so that it did not threaten his hold on power. Protesting low wages and lack of pay, paratroopers began looting Kinshasa in September 1991 and were only stopped after intervention by French ('Operation Baumier') and Belgian ('Operation Blue Beam') forces.

In 1993, according to the Library of Congress Country Studies, the 25,000-member FAZ ground forces consisted of one infantry division (with three infantry brigades); one airborne brigade (with three parachute battalions and one support battalion); one special forces (commando/counterinsurgency) brigade; the Special Presidential Division; one independent armoured brigade; and two independent infantry brigades (each with three infantry battalions, one support battalion). These units were deployed throughout the country, with the main concentrations in Shaba Region (approximately half the force). The Kamanyola Division, consisting of three infantry brigades operated generally in western Shaba Region; the 21st Infantry Brigade was located in Lubumbashi; the 13th Infantry Brigade was deployed throughout eastern Shaba; and at least one battalion of the 31st Airborne Brigade stayed at Kamina. The other main concentration of forces was in and around Kinshasa: the 31st Airborne Brigade was deployed at N'djili Airport on the outskirts of the capital; the Special Presidential Division (DSP) resided adjacent to the presidential compound; and the 1st Armoured Brigade was at Mbanza-Ngungu (in Bas-Congo, approximately southwest of Kinshasa). Finally the 41st Commando Brigade was at Kisangani.

This superficially impressive list of units overstates the actual capability of the armed forces at the time. Apart from privileged formations such as the Presidential Division and the 31st Airborne Brigade, most units were poorly trained, divided and so badly paid that they regularly resorted to looting. What operational abilities the armed forces had were gradually destroyed by politicisation of the forces, tribalisation, and division of the forces, included purges of suspectedly disloyal groups, intended to allow Mobutu to divide and rule. All this occurred against the background of increasing deterioration of state structures under the kleptocratic Mobutu regime.

Much of the origins of the recent conflict in what is now the Democratic Republic of the Congo stems from the turmoil following the Rwandan Genocide of 1994, which then led to the Great Lakes refugee crisis. Within the largest refugee camps, beginning in Goma in Nord-Kivu, were Rwandan Hutu fighters, who were eventually organised into the Rassemblement Démocratique pour le Rwanda, who launched repeated attacks into Rwanda. Rwanda eventually backed Laurent-Désiré Kabila and his quickly organised Alliance of Democratic Forces for the Liberation of Congo in invading Zaire, aiming to stop the attacks on Rwanda in the process of toppling Mobutu's government. When the militias rebelled, backed by Rwanda, the FAZ, weakened as is noted above, proved incapable of mastering the situation and preventing the overthrow of Mobutu in 1997.

When Kabila took power in 1997, the country was renamed the Democratic Republic of the Congo and so the name of the national army changed once again, to the "Forces armées congolaises" (FAC). Tanzania sent six hundred military advisors to train Kabila's new army in May 1997. (Prunier says that the instructors were still at the Kitona base when the Second Congo War broke out, and had to be quickly returned to Tanzania. Prunier said "South African aircraft carried out the evacuation after a personal conversation between President Mkapa and not-yet-president Thabo Mbeki. Command over the armed forces in the first few months of Kabila's rule was vague. Gérard Prunier writes that "there was no minister of defence, no known chief of staff, and no ranks; all officers were Cuban-style 'commanders' called 'Ignace', 'Bosco', Jonathan', or 'James', who occupied connecting suites at the Intercontinental Hotel and had presidential list cell-phone numbers. None spoke French or Lingala, but all spoke Kinyarwanda, Swahili, and, quite often, English." On being asked by Belgian journalist Colette Braeckman what was the actual army command structure apart from himself, Kabila answered 'We are not going to expose ourselves and risk being destroyed by showing ourselves openly... . We are careful so that the true masters of the army are not known. It is strategic. Please, let us drop the matter.' Kabila's new "Forces armées congolaises" were riven with internal tensions. The new FAC had Banyamulenge fighters from South Kivu, "kadogo" child soldiers from various eastern tribes, such as Thierry Nindaga, Safari Rwekoze, etc... [the mostly] Lunda Katangese Tigers of the former FNLC, and former FAZ personnel. Mixing these disparate and formerly warring elements together led to mutuny. On 23 February 1998, a mostly Banyamulenge unit mutiniued at Bukavu after its officers tried to disperse the soldiers into different units spread all around the Congo. By mid-1998, formations on the outbreak of the Second Congo War included the Tanzanian-supported 50th Brigade, headquartered at Camp Kokolo in Kinshasa, and the 10th Brigade — one of the best and largest units in the army — stationed in Goma, as well as the 12th Brigade in Bukavu. The declaration of the 10th Brigade's commander, former DSP officer Jean-Pierre Ondekane, on 2 August 1998 that he no longer recognised Kabila as the state's president was one of the factors in the beginning of the Second Congo War.

The FAC performed poorly throughout the Second Congo War and "demonstrated little skill or recognisable military doctrine". At the outbreak of the war in 1998 the Army was ineffective and the DRC Government was forced to rely on assistance from Angola, Chad, Namibia and Zimbabwe. As well as providing expeditionary forces, these countries unsuccessfully attempted to retrain the DRC Army. North Korea and Tanzania also provided assistance with training. During the first year of the war the Allied forces defeated the Rwandan force which had landed in Bas-Congo and the rebel forces south-west of Kinshasa and eventually halted the rebel and Rwandan offensive in the east of the DRC. These successes contributed to the Lusaka Ceasefire Agreement which was signed in July 1999. Following the Lusaka Agreement, in mid-August 1999 President Kabila issued a decree dividing the country into eight military regions. The first military region, Congolese state television reported, would consist of the two Kivu provinces, Orientale Province would form the second region, and Maniema and Kasai-Oriental provinces the third. Katanga and Équateur would fall under the fourth and fifth regions, respectively, while Kasai-Occidental and Bandundu would form the sixth region. Kinshasa and Bas-Congo would form the seventh and eighth regions, respectively. In November 1999 the Government attempted to form a 20,000-strong paramilitary force designated the People's Defence Forces. This force was intended to support the FAC and national police but never became effective.

The Lusaka Ceasefire Agreement was not successful in ending the war, and fighting resumed in September 1999. The FAC's performance continued to be poor and both the major offensives the Government launched in 2000 ended in costly defeats. President Kabila's mismanagement was an important factor behind the FAC's poor performance, with soldiers frequently going unpaid and unfed while the Government purchased advanced weaponry which could not be operated or maintained. The defeats in 2000 are believed to have been the cause of President Kabila's assassination in January 2001. Following the assassination, Joseph Kabila assumed the presidency and was eventually successful in negotiating an end to the war in 2002–2003.

The December 2002 Global and All-Inclusive Agreement devoted Chapter VII to the armed forces. It stipulated that the armed forces chief of staff, and the chiefs of the army, air force, and navy were not to come from the same warring faction. The new "national, restructured and integrated" army would be made up from Kabila's government forces (the FAC), the RCD, and the MLC. Also stipulated in VII(b) was that the RCD-N, RCD-ML, and the Mai-Mai would become part of the new armed forces. An intermediate mechanism for physical identification of the soldiers, and their origin, date of enrolment, and unit was also called for (VII(c)). It also provided for the creation of a Conseil Superieur de la Defense (Superior Defence Council) which would declare states of siege or war and give advice on security sector reform, disarmament/demobilization, and national defence policy.

A decision on which factions were to name chiefs of staff and military regional commanders was announced on 19 August 2003 as the first move in military reform, superimposed on top of the various groups of fighters, government and former rebels. Kabila was able to name the armed forces chief of staff, Lieutenant General Liwanga Mata, who previously served as navy chief of staff under Laurent Kabila. Kabila was able to name the air force commander (John Numbi), the RCD-Goma received the Land Force commander's position (Sylvain Buki) and the MLC the navy (Dieudonne Amuli Bahigwa). Three military regional commanders were nominated by the former Kinshasa government, two commanders each by the RCD-Goma and the MLC, and one region commander each by the RCD-K/ML and RCD-N. However these appointments were announced for Kabila's "Forces armées congolaises" (FAC), not the later FARDC. Another report however says that the military region commanders were only nominated in January 2004, and that the troop deployment on the ground did not change substantially until the year afterward.
On 24 January 2004, a decree created the "Structure Militaire d'Intégration" (SMI, Military Integration Structure). Together with the SMI, CONADER also was designated to manage the combined "tronc commun" DDR element and military reform programme. The first post-Sun City military law appears to have been passed on 12 November 2004, which formally created the new national Forces Armées de la République Démocratique du Congo (FARDC). Included in this law was article 45, which recognised the incorporation of a number of armed groups into the FARDC, including the former government army Forces Armées Congolaises (FAC), ex-FAZ personnel also known as former President Mobutu's 'les tigres', the RCD-Goma, RCD-ML, RCD-N, MLC, the Mai-Mai, as well as other government-determined military and paramilitary groups.

Turner writes that the two most prominent opponents of military integration ("brassage") were Colonel Jules Mutebusi, a Munyamulenge from South Kivu, and Laurent Nkunda, a Rwandaphone Tutsi who Turner says was allegedly from Rutshuru in North Kivu. In May–June 2004 Mutebusi led a revolt against his superiors from Kinshasa in South Kivu. Nkunda began his long series of revolts against central authority by helping Mutebusi in May–June 2004. In November 2004 a Rwandan government force entered North Kivu to attack the FDLR, and, it seems, reinforced and resupplied RCD-Goma (ANC) at the same time. Kabila despatched 10,000 government troops to the east in response, launching an attack that was called "Operation Bima". In the midst of this tension, Nkunda's men launched attacks in North Kivu in December 2004.

There was another major personnel reshuffle on 12 June 2007. FARDC chief General Kisempia Sungilanga Lombe was replaced with General Dieudonne Kayembe Mbandankulu. General Gabriel Amisi Kumba retained his post as Land Forces commander. John Numbi, a trusted member of Kabila's inner circle, was shifted from air force commander to Police Inspector General. U.S. diplomats reported that the former Naval Forces Commander Maj. General Amuli Bahigua (ex-MLC) became the FARDC's Chief of Operations; former FARDC Intelligence Chief General Didier Etumba (ex-FAC) was promoted to Vice Admiral and appointed Commander of Naval Forces; Maj. General Rigobert Massamba (ex-FAC), a former commander of the Kitona air base, was appointed as Air Forces Commander; and Brig. General Jean-Claude Kifwa, commander of the Republican Guard, was appointed as a regional military commander.

Due to significant delays in the DDR and integration process, of the eighteen brigades, only seventeen have been declared operational, over two and a half years after the initial target date. Responding to the situation, the Congolese Minister of Defence presented a new defence reform master plan to the international community in February 2008. Essentially the three force tiers all had their readiness dates pushed back: the first, territorial forces, to 2008–12, the mobile force to 2008–10, and the main defence force to 2015.

Much of the east of the country remains insecure, however. In the far northeast this is due primarily to the Ituri conflict. In the area around Lake Kivu, primarily in North Kivu, fighting continues among the Democratic Forces for the Liberation of Rwanda and between the government FARDC and Laurent Nkunda's troops, with all groups greatly exacerbating the issues of internal refugees in the area of Goma, the consequent food shortages, and loss of infrastructure from the years of conflict. In 2009, several United Nations officials stated that the army is a major problem, largely due to corruption that results in food and pay meant for soldiers being diverted and a military structure top-heavy with colonels, many of whom are former warlords. In a 2009 report itemizing FARDC abuses, Human Rights Watch urged the UN to stop supporting government offensives against eastern rebels until the abuses ceased.

In 2010, thirty FARDC officers were given scholarships to study in Russian military academies. This is part of a greater effort by Russia to help improve the FARDC. A new military attaché and other advisers from Russia visited the DRC.

On 22 November 2012, Gabriel Amisi Kumba was suspended from his position in the Forces Terrestres by president Joseph Kabila due to an inquiry into his alleged role in the sale of arms to various rebel groups in the eastern part of the country, which may have implicated the rebel group M23. In December 2012 it was reported that members of Army units in the north east of the country are often not paid due to corruption, and these units rarely made against villages by the Lord's Resistance Army.

The FARDC deployed 850 soldiers and 150 PNC police officers as part of an international force in the Central African Republic, which the DRC borders to the north. The country had been in a state of civil war since 2012, when the president was ousted by rebel groups. The DRC was urged by French president François Hollande to keep its troops in CAR.

In July 2014, the Congolese army carried out a joint operation with UN troops in the Masisi and Walikale territories of the North Kivu province. In the process, they liberated over 20 villages and a mine from the control of two rebel groups, the Mai Mai Cheka and the Alliance for the Sovereign and Patriotic Congo.

In October 2017 the UN published a report announcing that the FARDC no longer employed child soldiers but was still listed under militaries that committed sexual violations against children.

Troops operating with MONUSCO in North Kiuv were attacked by likely rebels from the Allied Democratic Forces on December 8, 2017. After a protracted firefight the troops suffered 5 dead along with 14 dead among the UN force.

The President, Major General Joseph Kabila is the Commander-in-Chief of the Armed Forces. The Minister of Defence, formally Ministers of Defence, Disarmament, and Veterans (Ancien Combattants), with the French acronym MDNDAC, is Alexandre Luba Ntambo.

The Colonel Tshatshi Military Camp in the Kinshasa suburb of Ngaliema hosts the defence department and the Chiefs of Staff central command headquarters of the FARDC. Jane's data from 2002 appears inaccurate; there is at least one ammunition plant in Katanga.

Below the Chief of Staff, the current organisation of the FARDC is not fully clear. There is known to be a Military Intelligence branch – Service du Renseignement militaire (SRM), the former DEMIAP. The FARDC is known to be broken up into the Land Forces ("Forces Terrestres"), Navy and Air Force. The Land Forces are distributed around ten military regions, up from the previous eight, following the ten provinces of the country. There is also a training command, the Groupement des Écoles Supérieurs Militaires (GESM) or Group of Higher Military Schools, which, in January 2010, was under the command of Major General Marcellin Lukama. The Navy and Air Forces are composed of various "groupments" (see below). There is also a central logistics base.

It should be made clear also that Joseph Kabila does not trust the military; the Republican Guard is the only component he trusts. Major General John Numbi, former Air Force chief, now inspector general of police, ran a parallel chain of command in the east to direct the 2009 Eastern Congo offensive, Operation Umoja Wetu; the regular chain of command was by-passed. Previously Numbi negotiated the agreement to carry out the "mixage" process with Laurent Nkunda. Commenting on a proposed vote of no confidence in the Minister of Defence in September 2012, Baoudin Amba Wetshi of "lecongolais.cd" described Ntolo as a "scapegoat". Wetshi said that all key military and security questions were handled in total secrecy by the President and other civil and military personalities trusted by him, such as John Numbi, Gabriel Amisi Kumba ('Tango Four'), Delphin Kahimbi, and others such as Kalev Mutond and Pierre Lumbi Okongo.

The available information on armed forces' Chiefs of Staff is incomplete and sometimes contradictory. In addition to armed forces chiefs of staff, in 1966 Lieutenant Colonel Ferdinand Malila was listed as Army Chief of Staff.

Virtually all officers have now changed positions, but this list gives an outline of the structure in January 2005. Despite the planned subdivision of the country into more numerous provinces, the actual splitting of the former provinces has not taken place.

In September 2014, President Kabila reshuffled the command structure and in addition to military regions created three new 'defense zones' which would be subordinated directly to the general staff. The defense zones essentially created a new layer between the general staff and the provincial commanders. The military regions themselves were reorganized and do not correspond with the ones that existed prior to the reshuffle. New commanders of branches were also appointed: A Congolese military analyst based in Brussels, Jean-Jacques Wondo, provided an outline of the updated command structure of the FARDC following the shake up of the high command:


Regional commanders:

The land forces are made up of about 14 integrated brigades of fighters from all the former warring factions who have gone through a "brassage" integration process (see next paragraph) and a not-publicly known number of non-integrated brigades that remain solely made up of single factions (the Congolese Rally for Democracy (RCD)'s "Armée national congolaise", the ex-government former Congolese Armed Forces (FAC), the ex-RCD KML, the ex-Movement for the Liberation of Congo, the armed groups of the Ituri conflict (the Mouvement des Révolutionnaires Congolais (MRC), Forces de Résistance Patriotique d'Ituri (FRPI), and the Front Nationaliste Intégrationniste (FNI)), and the Mai-Mai).

It appears that about the same time that Presidential Decree 03/042 of 18 December 2003 established the National Commission for Demobilisation and Reinsertion (CONADER), "..all ex-combatants were officially declared as FARDC soldiers and the then FARDC brigades [were to] rest deployed until the order to leave for "brassage.""
The reform plan adopted in 2005 envisaged the formation of eighteen integrated brigades through the "brassage" process as its first of three stages. The process consists firstly of regroupment, where fighters are disarmed. Then they are sent to orientation centres, run by CONADER, where fighters take the choice of either returning to civilian society or remaining in the armed forces. Combatants who choose demobilisation receive an initial cash payment of US $110. Those who choose to stay within the FARDC are then transferred to one of six integration centres for a 45-day training course, which aims to build integrated formations out of factional fighters previously heavily divided along ethnic, political and regional lines. The centres are spread out around the country at Kitona, Kamina, Kisangani, Rumangabo and Nyaleke (within the Virunga National Park) in Nord-Kivu, and Luberizi (on the border with Burundi) in South Kivu. The process has suffered severe difficulties due to construction delays, administration errors, and the amount of travel former combatants have to do, as the three stages' centres are widely separated. Following the first 18 integrated brigades, the second goal is the formation of a ready reaction force of two to three brigades, and finally, by 2010 when MONUC is anticipated to have withdrawn, the creation of a Main Defence Force of three divisions.

In February 2008, then Defence Minister Chikez Diemu described the reform plan at the time as:
"The short term, 2008–2010, will see the setting in place of a Rapid Reaction Force; the medium term, 2008–2015, with a Covering Force; and finally the long term, 2015–2020, with a Principal Defence Force." Diemu added that the reform plan rests on a programme of synergy based on the four pillars of dissuasion, production, reconstruction and excellence. "The Rapid Reaction Force is expected to focus on dissuasion, through a Rapid Reaction Force of 12 battalions, capable of aiding MONUC to secure the east of the country and to realise constitutional missions."
Amid the other difficulties in building new armed forces for the DRC, in early 2007 the integration and training process was distorted as the DRC government under Kabila attempted to use it to gain more control over the dissident general Laurent Nkunda. A hastily negotiated verbal agreement in Rwanda saw three government FAC brigades integrated with Nkunda's former ANC 81st and 83rd Brigades in what was called "mixage". "Mixage" brought multiple factions into composite brigades, but without the 45-day retraining provided by "brassage", and it seems that actually, the process was limited to exchanging battalions between the FAC and Nkunda brigades in North Kivu, without further integration. Due to Nkunda's troops having greater cohesion, Nkunda effectively gained control of all five brigades, which was not the intention of the DRC central government. However, after Nkunda used the "mixage" brigades to fight the FDLR, strains arose between the FARDC and Nkunda-loyalist troops within the brigades and they fell apart in the last days of August 2007. The International Crisis Group says that "by 30 August [2007] Nkunda's troops had left the mixed brigades and controlled a large part of the Masisi and Rutshuru territories" (of North Kivu).

Both formally integrated brigades and the non-integrated units continue to conduct arbitrary arrests, rapes, robbery, and other crimes and these human rights violations are "regularly" committed by both officers and members of the rank and file. Members of the Army also often strike deals to gain access to resources with the militias they are meant to be fighting.

The various brigades and other formations and units number at least 100,000 troops. The status of these brigades has been described as "pretty chaotic." A 2007 disarmament and repatriation study said "army units that have not yet gone through the process of brassage are usually much smaller than what they ought to be. Some non-integrated brigades have only 500 men (and are thus nothing more than a small battalion) whereas some battalions may not even have the size of a normal company (over a 100 men)."

A number of outside donor countries are also carrying out separate training programmes for various parts of the Forces du Terrestres (Land Forces). The People's Republic of China has trained Congolese troops at Kamina in Katanga from at least 2004 to 2009, and the Belgian government is training at least one "rapid reaction" battalion. When Kabila visited U.S. President George W. Bush in Washington D.C., he also asked the U.S. Government to train a battalion, and as a result, a private contractor, Protection Strategies Incorporated, started training a FARDC battalion at Camp Base, Kisangani, in February 2010. The company was supervised by United States Special Operations Command Africa. The various international training programmes are not well integrated.

Attempting to list the equipment available to the DRC's land forces is difficult; most figures are unreliable estimates based on known items delivered in the past. The figures below are from the IISS Military Balance 2014. Much of the Army's equipment is non-operational due to insufficient maintenance—in 2002 only 20 percent of the Army's armoured vehicles were estimated as being serviceable.

In addition to these 2014 figures, in March 2010, it was reported that the DRC's land forces had ordered USD $80 million worth of military equipment from Ukraine which included 20 T-72 main battle tanks, 100 trucks and various small arms. Tanks have been used in the Kivus in the 2005–09 period.

In February 2014, Ukraine revealed that it had achieved the first export order for the T-64 tank to the DRC Land Forces for 50 T-64BV-1s.

In June 2015 it was reported that Georgia had sold 12 of its Didgori-2 to the DRC for $4 million. The vehicles were specifically designed for reconnaissance and special operations. Two of the vehicles are a recently developed conversion to serve for medical field evacuation.

The United Nations confirmed in 2011, both from sources in the Congolese military and from officials of the Commission nationale de contrôle des armes légères et de petit calibre et de réduction de la violence armée, that the ammunition plant called Afridex in Likasi, Katanga Province, manufactures ammunition for small arms and light weapons.

In addition to the other land forces, President Joseph Kabila also has a Republican Guard presidential force ("Garde Républicaine" or GR), formerly known as the Special Presidential Security Group (GSSP). FARDC military officials state that the Garde Républicaine is not the responsibility of FARDC, but of the Head of State. Apart from Article 140 of the Law on the Army and Defence, no legal stipulation on the DRC's Armed Forces makes provision for the GR as a distinct unit within the national army. In February 2005 President Joseph Kabila passed a decree which appointed the GR's commanding officer and "repealed any previous provisions contrary" to that decree. The GR, more than 10,000 strong (the ICG said 10,000 to 15,000 in January 2007), has better working conditions and is paid regularly, but still commits rapes and robberies in the vicinity of its bases.

In an effort to extend his personal control across the country, Joseph Kabila has deployed the GR at key airports, ostensibly in preparation for an impending presidential visit. there were Guards deployed in the central prison of Kinshasa, N'djili Airport, Bukavu, Kisangani, Kindu, Lubumbashi, Matadi, and Moanda, where they appear to answer to no local commander and have caused trouble with MONUC troops there.

The GR is also supposed to undergo the integration process, but in January 2007, only one battalion had been announced as having been integrated. Formed at a brassage centre in the Kinshasa suburb of Kibomango, the battalion included 800 men, half from the former GSSP and half from the MLC and RCD Goma.

Up until June 2016, the GR comprised three brigades, the 10th Brigade at Camp Tshatshi and the 11th at Camp Kimbembe, both in Kinshasa, and the 13th Brigade at Camp Simi Simi in Kisangani. It was reorganised on the basis of eight fighting regiments, the 14th Security and Honor Regiment, an artillery regiment, and a command brigade/regiment from that time.

There are currently large numbers of United Nations troops stationed in the DRC. The United Nations Organization Stabilization Mission in the Democratic Republic of the Congo (MONUSCO), on had a strength of over 19,000 peacekeepers (including 16,998 military personnel) and has a mission of assisting Congolese authorities maintain security. The UN and foreign military aid missions, the most prominent being EUSEC RD Congo, are attempting to assist the Congolese in rebuilding the armed forces, with major efforts being made in trying to assure regular payment of salaries to armed forces personnel and also in military justice. Retired Canadian Lieutenant General Marc Caron also served for a time as Security Sector Reform advisor to the head of MONUC.

Groups of anti-Rwandan government rebels like the FDLR, and other foreign fighters remain inside the DRC. The FDLR which is the greatest concern, was some 6,000 strong, in July 2007. By late 2010 the FDLR's strength however was estimated at 2,500. The other groups are smaller: the Ugandan Lord's Resistance Army, the Ugandan rebel group the Allied Democratic Forces in the remote area of Mt Rwenzori, and the Burundian Parti pour la Libération du Peuple Hutu—Forces Nationales de Liberation (PALIPEHUTU-FNL).

Finally there is a government paramilitary force, created in 1997 under President Laurent Kabila. The National Service is tasked with providing the army with food and with training the youth in a range of reconstruction and developmental activities. There is not much further information available, and no internet-accessible source details the relationship of the National Service to other armed forces bodies; it is not listed in the constitution. President Kabila, in one of the few comments available, says National Service will provide a gainful activity for street children. Obligatory civil service administered through the armed forces was also proposed under the Mobutu regime during the "radicalisation" programme of December 1974 – January 1975; the FAZ was opposed to the measure and the plan "took several months to die."

All military aircraft in the DRC are operated by the Air Force. Jane's World Air Forces states that the Air Force has an estimated strength of 1,800 personnel and is organised into two Air Groups. These Groups command five wings and nine squadrons, of which not all are operational. 1 Air Group is located at Kinshasa and consists of Liaison Wing, Training Wing and Logistical Wing and has a strength of five squadrons. 2 Tactical Air Group is located at Kaminia and consists of Pursuit and Attack Wing and Tactical Transport Wing and has a strength of four squadrons. Foreign private military companies have reportedly been contracted to provide the DRC's aerial reconnaissance capability using small propeller aircraft fitted with sophisticated equipment. Jane's states that National Air Force of Angola fighter aircraft would be made available to defend Kinshasa if it came under attack.

Like the other services, the Congolese Air Force is not capable of carrying out its responsibilities. Few of the Air Force's aircraft are currently flyable or capable of being restored to service and it is unclear whether the Air Force is capable of maintaining even unsophisticated aircraft. Moreover, Jane's states that the Air Force's Ecole de Pilotage is 'in near total disarray' though Belgium has offered to restart the Air Force's pilot training program.

Before the downfall of Mobutu, a small navy operated on the Congo river. One of its installations was at the village of N'dangi near the presidential residence in Gbadolite. The port at N'dangi was the base for several patrol boats, helicopters and the presidential yacht. The 2002 edition of "Jane's Sentinel" described the Navy as being "in a state of near total disarray" and stated that it did not conduct any training or have operating procedures. The Navy shares the same discipline problems as the other services. It was initially placed under command of the MLC when the transition began,so the current situation is uncertain.

The 2007 edition of "Jane's Fighting Ships" states that the Navy is organised into four commands, based at Matadi, near the coast; the capital Kinshasa, further up the Congo river; Kalemie, on Lake Tanganyika; and Goma, on Lake Kivu. The International Institute for Strategic Studies, in its 2007 edition of the "Military Balance", confirms the bases listed in "Jane's" and adds a fifth base at Boma, a coastal city near Matadi. Various sources also refer to numbered Naval Regions. Operations of the 1st Naval Region have been reported in Kalemie, the 4th near the northern city of Mbandaka, and the 5th at Goma.

The IISS lists the Navy at 1,000 personnel and a total of eight patrol craft, of which only one is operational, a Shanghai II Type 062 class gunboat designated "102". There are five other 062s as well as two Swiftships which are not currently operational, though some may be restored to service in the future. According to "Jane's", the Navy also operates barges and small craft armed with machine guns.

As of 2012, the Navy on paper consisted of about 6,700 personnel and up to 23 patrol craft. In reality there was probably around 1,000 service members, and only 8 of the boats were 50 ft in length or larger, the sole operational vessel being a Shanghai II Type 062 class gunboat. The service maintains bases in Kinshasa, Boma, Matadi, Boma, and on Lake Tanganyika.





</doc>
<doc id="8032" url="https://en.wikipedia.org/wiki?curid=8032" title="Geography of Denmark">
Geography of Denmark

Denmark is a Nordic country located in Northern Europe. It consists of the Jutland peninsula and several islands in the Baltic sea, referred to as the Danish Archipelago. Denmark is located southwest of Sweden and due south of Norway and is bordered by the German state (and former possession) Schleswig-Holstein to the south, on Denmark's only land border, 68 kilometres (42 miles) long.

Denmark borders both the Baltic and North Seas along its tidal shoreline. Denmark's general coastline is much shorter, at , as it would not include most of the 1,419 offshore islands (each defined as exceeding 100 square meters in area) and the 180 km long Limfjorden, which separates Denmark's second largest island, North Jutlandic Island, 4,686 km in size, from the rest of Jutland. No location in Denmark is further from the coast than . The size of the land area of Denmark cannot be stated exactly since the ocean constantly erodes and adds material to the coastline, and because of human land reclamation projects (to counter erosion). On the southwest coast of Jutland, the tide is between , and the tideline moves outward and inward on a stretch.

A circle enclosing the same area as Denmark would be 742 km (461 miles) long. Denmark has 443 named islands (1,419 islands above 100 m²), of which 72 are inhabited (, Statistics Denmark). The largest islands are Zealand "(Sjælland)" and Funen "(Fyn)". The island of Bornholm is located east of the rest of the country, in the Baltic Sea. Many of the larger islands are connected by bridges; the Øresund Bridge connects Zealand with Sweden; the Great Belt Bridge connects Funen with Zealand; and the Little Belt Bridge connects Jutland with Funen. Ferries or small aircraft connect to the smaller islands. Main cities are the capital Copenhagen on Zealand; Århus, Aalborg and Esbjerg in Jutland; and Odense on Funen.

Denmark experiences a temperate climate. This means that the winters are mild and windy and the summers are cool. The local terrain is generally flat with a few gently rolling plains. The territory of Denmark includes the island of Bornholm in the Baltic Sea and the rest of metropolitan Denmark, but excludes the Faroe Islands and Greenland. Its position gives Denmark complete control of the Danish Straits (Skagerrak and Kattegat) linking the Baltic and North Seas. The country's natural resources include petroleum, natural gas, fish, salt, limestone, chalk, stone, gravel and sand.

Irrigated land: 4,354 km² (2007)

Total renewable water resources: 6 km (2011)

Freshwater withdrawal (domestic/industrial/agricultural):
<br>"total:" 0.66 km/yr (58%/5%/36%)
<br> "per capita:" 118.4 m/yr (2009)

 Denmark has a population of 5,543,453. About a quarter of Danes live in the capital Copenhagen.




</doc>
<doc id="8033" url="https://en.wikipedia.org/wiki?curid=8033" title="Demographics of Denmark">
Demographics of Denmark

This article is about the demographic features of the population of Denmark, including population density, ethnicity, education level, health of the populace, economic status, religious affiliations and other aspects of the population.

Since 1980, the number of Danes has remained constant at around 5 million in Denmark and nearly all the population growth from 5.1 up to the 2018 total of 5.8 million was due to immigration.

According to 2017 figures from Statistics Denmark, 86.9% of Denmark’s population of over 5,760,694 was of Danish descent, defined as having at least one parent who was born in Denmark and has Danish citizenship. The remaining 13.1% were of a foreign background, defined as immigrants or descendants of recent immigrants. With the same definition, the most common countries of origin were Poland, Turkey, Germany, Iraq, Romania, Syria, Somalia, Iran, Afghanistan, and Yugoslavia and its successor states.
More than 752 618 individuals (13.1%) are migrants and their descendants (146 798 second generation migrants born in Denmark).

Of these 752 618 immigrants and their descendants:

Non-Scandinavian ethnic minorities include:


Ethnic minorities in Denmark include a handful of groups:


Data according to Statistics Denmark, which collects the official statistics for Denmark.




The Church of Denmark () is state-supported and, according to statistics from January 2006, accounts for about 80% of Denmark's religious affiliation. Denmark has had religious freedom guaranteed since 1849 by the Constitution, and numerous other religions are officially recognised, including several Christian denominations, Muslim, Jewish, Buddhist, Hindu and other congregations as well as Forn Siðr, a revival of Scandinavian pagan tradition. The Department of Ecclesiastical Affairs recognises roughly a hundred religious congregations for tax and legal purposes such as conducting wedding ceremonies.

Islam is the second largest religion in Denmark.

For historical reasons, there is a formal distinction between 'approved' ("godkendte") and 'recognised' ("anerkendte") congregations of faith. The latter include 11 traditional denominations, such as Roman Catholics, the Reformed Church, the Mosaic Congregation, Methodists and Baptists, some of whose privileges in the country date hundreds of years back. These have the additional rights of having priests appointed by royal resolution and to christen/name children with legal effect.

Denmark's population from 1769 to 2007.

The following demographic statistics are from the CIA World Factbook, unless otherwise indicated.

Population:

Age structure:

Median age:

Population growth rate:

Net migration rate:

Urbanisation:

Sex ratio:

Infant mortality rate:

Life expectancy at birth:

Total fertility rate:

HIV/AIDS - adult prevalence rate:

HIV/AIDS - people living with HIV/AIDS:

HIV/AIDS - deaths:

Nationality:

Ethnic groups:

Religions:

Languages:

Literacy:

School life expectancy (primary to tertiary education):

Education expenditures:




</doc>
