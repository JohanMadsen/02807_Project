<doc id="12855" url="https://en.wikipedia.org/wiki?curid=12855" title="George Bernard Shaw">
George Bernard Shaw

George Bernard Shaw (26 July 1856 – 2 November 1950), known at his insistence simply as Bernard Shaw, was an Irish playwright, critic, polemicist, and political activist. His influence on Western theatre, culture and politics extended from the 1880s to his death and beyond. He wrote more than sixty plays, including major works such as "Man and Superman" (1902), "Pygmalion" (1912)" and Saint Joan" (1923). With a range incorporating both contemporary satire and historical allegory, Shaw became the leading dramatist of his generation, and in 1925 was awarded the Nobel Prize in Literature.

Born in Dublin, Shaw moved to London in 1876, where he struggled to establish himself as a writer and novelist, and embarked on a rigorous process of self-education. By the mid-1880s he had become a respected theatre and music critic. Following a political awakening, he joined the gradualist Fabian Society and became its most prominent pamphleteer. Shaw had been writing plays for years before his first public success, "Arms and the Man" in 1894. Influenced by Henrik Ibsen, he sought to introduce a new realism into English-language drama, using his plays as vehicles to disseminate his political, social and religious ideas. By the early twentieth century his reputation as a dramatist was secured with a series of critical and popular successes that included "Major Barbara", "The Doctor's Dilemma" and "Caesar and Cleopatra".

Shaw's expressed views were often contentious; he promoted eugenics and alphabet reform, and opposed vaccination and organised religion. He courted unpopularity by denouncing both sides in the First World War as equally culpable, and although not a republican, castigated British policy on Ireland in the postwar period. These stances had no lasting effect on his standing or productivity as a dramatist; the inter-war years saw a series of often ambitious plays, which achieved varying degrees of popular success. In 1938 he provided the screenplay for a filmed version of "Pygmalion" for which he received an Academy Award. His appetite for politics and controversy remained undiminished; by the late 1920s he had largely renounced Fabian Society gradualism and often wrote and spoke favourably of dictatorships of the right and left—he expressed admiration for both Mussolini and Stalin. In the final decade of his life he made fewer public statements, but continued to write prolifically until shortly before his death, aged ninety-four, having refused all state honours, including the Order of Merit in 1946.

Since Shaw's death scholarly and critical opinion has varied about his works, but he has regularly been rated as second only to Shakespeare among British dramatists; analysts recognise his extensive influence on generations of English-language playwrights. The word "Shavian" has entered the language as encapsulating Shaw's ideas and his means of expressing them. 

Shaw was born at 3 Upper Synge Street in Portobello, a lower-middle-class part of Dublin. He was the youngest child and only son of George Carr Shaw (1814–1885) and Lucinda Elizabeth (Bessie) Shaw ("née" Gurly; 1830–1913). His elder siblings were Lucinda (Lucy) Frances (1853–1920) and Elinor Agnes (1855–1876). The Shaw family was of English descent and belonged to the dominant Protestant Ascendancy in Ireland; George Carr Shaw, an ineffectual alcoholic, was among the family's less successful members. His relatives secured him a sinecure in the civil service, from which he was pensioned off in the early 1850s; thereafter he worked irregularly as a corn merchant. In 1852 he married Bessie Gurly; in the view of Shaw's biographer Michael Holroyd she married to escape a tyrannical great-aunt. If, as Holroyd and others surmise, George's motives were mercenary, then he was disappointed, as Bessie brought him little of her family's money. She came to despise her ineffectual and often drunken husband, with whom she shared what their son later described as a life of "shabby-genteel poverty".

By the time of Shaw's birth, his mother had become close to George John Lee, a flamboyant figure well known in Dublin's musical circles. Shaw retained a lifelong obsession that Lee might have been his biological father; there is no consensus among Shavian scholars on the likelihood of this. The young Shaw suffered no harshness from his mother, but he later recalled that her indifference and lack of affection hurt him deeply. He found solace in the music that abounded in the house. Lee was a conductor and teacher of singing; Bessie had a fine mezzo-soprano voice and was much influenced by Lee's unorthodox method of vocal production. The Shaws' house was often filled with music, with frequent gatherings of singers and players.

In 1862, Lee and the Shaws agreed to share a house, No. 1 Hatch Street, in an affluent part of Dublin, and a country cottage on Dalkey Hill, overlooking Killiney Bay. Shaw, a sensitive boy, found the less salubrious parts of Dublin shocking and distressing, and was happier at the cottage. Lee's students often gave him books, which the young Shaw read avidly; thus, as well as gaining a thorough musical knowledge of choral and operatic works, he became familiar with a wide spectrum of literature.

Between 1865 and 1871, Shaw attended four schools, all of which he hated. His experiences as a schoolboy left him disillusioned with formal education: "Schools and schoolmasters", he later wrote, were "prisons and turnkeys in which children are kept to prevent them disturbing and chaperoning their parents." In October 1871 he left school to become a junior clerk in a Dublin firm of land agents, where he worked hard, and quickly rose to become head cashier. During this period, Shaw was known as "George Shaw"; after 1876, he dropped the "George" and styled himself "Bernard Shaw".

In June 1873, Lee left Dublin for London and never returned. A fortnight later, Bessie followed him; the two girls joined her. Shaw's explanation of why his mother followed Lee was that without the latter's financial contribution the joint household had to be broken up. Left in Dublin with his father, Shaw compensated for the absence of music in the house by teaching himself to play the piano.

Early in 1876 Shaw learned from his mother that Agnes was dying of tuberculosis. He resigned from the land agents, and in March travelled to England to join his mother and Lucy at Agnes's funeral. He never again lived in Ireland, and did not visit it for twenty-nine years.
Initially, Shaw refused to seek clerical employment in London. His mother allowed him to live free of charge in her house in South Kensington, but he nevertheless needed an income. He had abandoned a teenage ambition to become a painter, and had no thought yet of writing for a living, but Lee found a little work for him, ghost-writing a musical column printed under Lee's name in a satirical weekly, "The Hornet". Lee's relations with Bessie deteriorated after their move to London. Shaw maintained contact with Lee, who found him work as a rehearsal pianist and occasional singer.

Eventually Shaw was driven to applying for office jobs. In the interim he secured a reader's pass for the British Museum Reading Room (the forerunner of the British Library) and spent most weekdays there, reading and writing. His first attempt at drama, begun in 1878, was a blank-verse satirical piece on a religious theme. It was abandoned unfinished, as was his first try at a novel. His first completed novel, "Immaturity" (1879), was too grim to appeal to publishers and did not appear until the 1930s. He was employed briefly by the newly formed Edison Telephone Company in 1879–80, and as in Dublin achieved rapid promotion. Nonetheless, when the Edison firm merged with the rival Bell Telephone Company, Shaw chose not to seek a place in the new organisation. Thereafter he pursued a full-time career as an author.

For the next four years Shaw made a negligible income from writing, and was subsidised by his mother. In 1881, for the sake of economy, and increasingly as a matter of principle, he became a vegetarian. He grew a beard to hide a facial scar left by smallpox. In rapid succession he wrote two more novels: "The Irrational Knot" (1880) and "Love Among the Artists" (1881), but neither found a publisher; each was serialised a few years later in the socialist magazine "Our Corner".

In 1880 Shaw began attending meetings of the Zetetical Society, whose objective was to "search for truth in all matters affecting the interests of the human race". Here he met Sidney Webb, a junior civil servant who, like Shaw, was busy educating himself. Despite difference of style and temperament, the two quickly recognised qualities in each other and developed a lifelong friendship. Shaw later reflected: "You knew everything that I didn't know and I knew everything you didn't know ... We had everything to learn from one another and brains enough to do it".

Shaw's next attempt at drama was a one-act playlet in French, "Un Petit Drame", written in 1884 but not published in his lifetime. In the same year the critic William Archer suggested a collaboration, with a plot by Archer and dialogue by Shaw. The project foundered, but Shaw returned to the draft as the basis of "Widowers' Houses" in 1892, and the connection with Archer proved of immense value to Shaw's career.

On 5 September 1882 Shaw attended a meeting at the Memorial Hall, Farringdon, addressed by the political economist Henry George. Shaw then read George's book "Progress and Poverty", which awakened his interest in economics. He began attending meetings of the Social Democratic Federation (SDF), where he discovered the writings of Karl Marx, and thereafter spent much of 1883 reading "Das Kapital". He was not impressed by the SDF's founder, H. M. Hyndman, whom he found autocratic, ill-tempered and lacking leadership qualities. Shaw doubted the ability of the SDF to harness the working classes into an effective radical movement and did not join it—he preferred, he said, to work with his intellectual equals.

After reading a tract, "Why Are The Many Poor?", issued by the recently formed Fabian Society, Shaw went to the society's next advertised meeting, on 16 May 1884. He became a member in September, and before the year's end had provided the society with its first manifesto, published as Fabian Tract No. 2. He joined the society's executive committee in January 1885, and later that year recruited Webb and also Annie Besant, a fine orator.

From 1885 to 1889 Shaw attended the fortnightly meetings of the British Economic Association; it was, Holroyd observes, "the closest Shaw had ever come to university education." This experience changed his political ideas; he moved away from Marxism and became an apostle of gradualism. When in 1886–87 the Fabians debated whether to embrace anarchism, as advocated by Charlotte Wilson, Besant and others, Shaw joined the majority in rejecting this approach. After a rally in Trafalgar Square addressed by Besant was violently broken up by the authorities on 13 November 1887 ("Bloody Sunday"), Shaw became convinced of the folly of attempting to challenge police power. Thereafter he largely accepted the principle of "permeation" as advocated by Webb: the notion whereby socialism could best be achieved by infiltration of people and ideas into existing political parties.

Throughout the 1880s the Fabian Society remained small, its message of moderation frequently unheard among more strident voices. Its profile was raised in 1889 with the publication of "Fabian Essays in Socialism", edited by Shaw who also provided two of the essays. The second of these, "Transition", details the case for gradualism and permeation, asserting that "the necessity for cautious and gradual change must be obvious to everyone". In 1890 Shaw produced Tract No. 13, "What Socialism Is", a revision of an earlier tract in which Charlotte Wilson had defined socialism in anarchistic terms. In Shaw's new version, readers were assured that "socialism can be brought about in a perfectly constitutional manner by democratic institutions".

The mid-1880s marked a turning point in Shaw's life, both personally and professionally: he lost his virginity, had two novels published, and began a career as a critic. He had been celibate until his twenty-ninth birthday, when his shyness was overcome by Jane (Jenny) Patterson, a widow some years his senior. Their affair continued, not always smoothly, for eight years. Shaw's sex life has caused much speculation and debate among his biographers, but there is a consensus that the relationship with Patterson was one of his few non-platonic romantic liaisons.

The published novels, neither commercially successful, were his two final efforts in this genre: "Cashel Byron's Profession" written in 1882–83, and "An Unsocial Socialist", begun and finished in 1883. The latter was published as a serial in "ToDay" magazine in 1884, although it did not appear in book form until 1887. "Cashel Byron" appeared in magazine and book form in 1886.

In 1884 and 1885, through the influence of Archer, Shaw was engaged to write book and music criticism for London papers. When Archer resigned as art critic of "The World" in 1886 he secured the succession for Shaw. The two figures in the contemporary art world whose views Shaw most admired were William Morris and John Ruskin, and he sought to follow their precepts in his criticisms. Their emphasis on morality appealed to Shaw, who rejected the idea of art for art's sake, and insisted that all great art must be didactic.

Of Shaw's various reviewing activities in the 1880s and 1890s it was as a music critic that he was best known. After serving as deputy in 1888, he became musical critic of "The Star" in February 1889, writing under the pen-name Corno di Bassetto. In May 1890 he moved back to "The World", where he wrote a weekly column as "G.B.S." for more than four years. In the 2016 version of the "Grove Dictionary of Music and Musicians", Robert Anderson writes, "Shaw's collected writings on music stand alone in their mastery of English and compulsive readability." Shaw ceased to be a salaried music critic in August 1894, but published occasional articles on the subject throughout his career, his last in 1950.

From 1895 to 1898, Shaw was the theatre critic for "The Saturday Review", edited by his friend Frank Harris. As at "The World", he used the by-line "G.B.S." He campaigned against the artificial conventions and hypocrisies of the Victorian theatre and called for plays of real ideas and true characters. By this time he had embarked in earnest on a career as a playwright: "I had rashly taken up the case; and rather than let it collapse I manufactured the evidence".

After using the plot of the aborted 1884 collaboration with Archer to complete "Widowers' Houses" (it was staged twice in London, in December 1892), Shaw continued writing plays. At first he made slow progress; "The Philanderer", written in 1893 but not published until 1898, had to wait until 1905 for a stage production. Similarly, "Mrs Warren's Profession" (1893) was written five years before publication and nine years before reaching the stage.
Shaw's first box-office success was "Arms and the Man" (1894), a mock-Ruritanian comedy satirising conventions of love, military honour and class. The press found the play overlong, and accused Shaw of mediocrity, sneering at heroism and patriotism, heartless cleverness, and copying W.S.Gilbert's style. The public took a different view, and the management of the theatre staged extra matinée performances to meet the demand. The play ran from April to July, toured the provinces and was staged in New York. Among the cast of the London production was Florence Farr, with whom Shaw had a romantic relationship between 1890 and 1894, much resented by Jenny Patterson.

The success of "Arms and the Man" was not immediately replicated. "Candida", which presented a young woman making a conventional romantic choice for unconventional reasons, received a single performance in South Shields in 1895; in 1897 a playlet about Napoleon called "The Man of Destiny" had a single staging at Croydon. In the 1890s Shaw's plays were better known in print than on the West End stage; his biggest success of the decade was in New York in 1897, when Richard Mansfield's production of the historical melodrama "The Devil's Disciple" earned the author more than £2,000 in royalties.

In January 1893, as a Fabian delegate, Shaw attended the Bradford conference which led to the foundation of the Independent Labour Party. He was sceptical about the new party, and scorned the likelihood that it could switch the allegiance of the working class from sport to politics. He persuaded the conference to adopt resolutions abolishing indirect taxation, and taxing unearned income "to extinction". Back in London, Shaw produced what Margaret Cole, in her Fabian history, terms a "grand philippic" against the minority Liberal administration that had taken power in 1892. "To Your Tents, O Israel" excoriated the government for ignoring social issues and concentrating solely on Irish Home Rule, a matter Shaw declared of no relevance to socialism. In 1894 the Fabian Society received a substantial bequest from a sympathiser, Henry Hunt Hutchinson—Holroyd mentions £10,000. Webb, who chaired the board of trustees appointed to supervise the legacy, proposed to use most of it to found a school of economics and politics. Shaw demurred; he thought such a venture was contrary to the specified purpose of the legacy. He was eventually persuaded to support the proposal, and the London School of Economics and Political Science (LSE) opened in the summer of 1895.

By the later 1890s Shaw's political activities lessened as he concentrated on making his name as a dramatist. In 1897 he was persuaded to fill an uncontested vacancy for a "vestryman" (parish councillor) in London's St Pancras district. At least initially, Shaw took to his municipal responsibilities seriously; when London government was reformed in 1899 and the St Pancras vestry became the Metropolitan Borough of St Pancras, he was elected to the newly formed borough council.

In 1898, as a result of overwork, Shaw's health broke down. He was nursed by Charlotte Payne-Townshend, a rich Anglo-Irish woman whom he had met through the Webbs. The previous year she had proposed that she and Shaw should marry. He had declined, but when she insisted on nursing him in a house in the country, Shaw, concerned that this might cause scandal, agreed to their marriage. The ceremony took place on 1 June 1898, in the register office in Covent Garden. The bride and bridegroom were both aged forty-one. In the view of the biographer and critic St John Ervine, "their life together was entirely felicitous". There were no children of the marriage, which it is generally believed was never consummated; whether this was wholly at Charlotte's wish, as Shaw liked to suggest, is less widely credited. In the early weeks of the marriage Shaw was much occupied writing his Marxist analysis of Wagner's "Ring" cycle, published as "The Perfect Wagnerite" late in 1898. In 1906 the Shaws found a country home in Ayot St Lawrence, Hertfordshire; they renamed the house "Shaw's Corner", and lived there for the rest of their lives. They retained a London flat in the Adelphi and later at Whitehall Court.

During the first decade of the twentieth century, Shaw secured a firm reputation as a playwright. In 1904 J. E. Vedrenne and Harley Granville-Barker established a company at the Royal Court Theatre in Sloane Square, Chelsea to present modern drama. Over the next five years they staged fourteen of Shaw's plays. The first, "John Bull's Other Island", a comedy about an Englishman in Ireland, attracted leading politicians and was seen by Edward VII, who laughed so much that he broke his chair. The play was withheld from Dublin's Abbey Theatre, for fear of the affront it might provoke, although it was shown at the city's Royal Theatre in November 1907. Shaw later wrote that William Butler Yeats, who had requested the play, "got rather more than he bargained for... It was uncongenial to the whole spirit of the neo-Gaelic movement, which is bent on creating a new Ireland after its own ideal, whereas my play is a very uncompromising presentment of the real old Ireland." Nonetheless, Shaw and Yeats were close friends; Yeats and Lady Gregory tried unsuccessfully to persuade Shaw to take up the vacant co-directorship of the Abbey Theatre after J. M. Synge's death in 1909. Shaw admired other figures in the Irish Literary Revival, including George Russell and James Joyce, and was a close friend of Seán O'Casey, who was inspired to become a playwright after reading "John Bull's Other Island".

"Man and Superman", completed in 1902, was a success both at the Royal Court in 1905 and in Robert Loraine's New York production in the same year. Among the other Shaw works presented by Vedrenne and Granville-Barker were "Major Barbara" (1905), depicting the contrasting morality of arms manufacturers and the Salvation Army; "The Doctor's Dilemma" (1906), a mostly serious piece about professional ethics; and "Caesar and Cleopatra", Shaw's counterblast to Shakespeare's "Antony and Cleopatra", seen in New York in 1906 and in London the following year.

Now prosperous and established, Shaw experimented with unorthodox theatrical forms described by his biographer Stanley Weintraub as "discussion drama" and "serious farce". These plays included "Getting Married" (premiered 1908), "The Shewing-Up of Blanco Posnet" (1909), "Misalliance" (1910), and "Fanny's First Play" (1911). "Blanco Posnet" was banned on religious grounds by the Lord Chamberlain (the official theatre censor in England), and was produced instead in Dublin; it filled the Abbey Theatre to capacity. "Fanny's First Play", a comedy about suffragettes, had the longest initial run of any Shaw play—622 performances.

"Androcles and the Lion" (1912), a less heretical study of true and false religious attitudes than "Blanco Posnet", ran for eight weeks in September and October 1913. It was followed by one of Shaw's most successful plays, "Pygmalion", written in 1912 and staged in Vienna the following year, and in Berlin shortly afterwards. Shaw commented, "It is the custom of the English press when a play of mine is produced, to inform the world that it is not a play—that it is dull, blasphemous, unpopular, and financially unsuccessful. ... Hence arose an urgent demand on the part of the managers of Vienna and Berlin that I should have my plays performed by them first." The British production opened in April 1914, starring Sir Herbert Tree and Mrs Patrick Campbell as, respectively, a professor of phonetics and a cockney flower-girl. There had earlier been a romantic liaison between Shaw and Campbell that caused Charlotte Shaw considerable concern, but by the time of the London premiere it had ended. The play attracted capacity audiences until July, when Tree insisted on going on holiday, and the production closed. His co-star then toured with the piece in the US.

In 1899, when the Boer War began, Shaw wished the Fabians to take a neutral stance on what he deemed, like Home Rule, to be a "non-Socialist" issue. Others, including the future Labour prime minister Ramsay MacDonald, wanted unequivocal opposition, and resigned from the society when it followed Shaw. In the Fabians' war manifesto, "Fabianism and the Empire" (1900), Shaw declared that "until the Federation of the World becomes an accomplished fact we must accept the most responsible Imperial federations available as a substitute for it".

As the new century began, Shaw became increasingly disillusioned by the limited impact of the Fabians on national politics. Thus, although a nominated Fabian delegate, he did not attend the London conference at the Memorial Hall, Farringdon Street in February 1900, that created the Labour Representation Committee—precursor of the modern Labour Party. By 1903, when his term as borough councillor expired, he had lost his earlier enthusiasm, writing: "After six years of Borough Councilling I am convinced that the borough councils should be abolished". Nevertheless, in 1904 he stood in the London County Council elections. After an eccentric campaign, which Holroyd characterises as "[making] absolutely certain of not getting in", he was duly defeated. It was Shaw's final foray into electoral politics. Nationally, the 1906 general election produced a huge Liberal majority and an intake of 29 Labour members. Shaw viewed this outcome with scepticism; he had a low opinion of the new prime minister, Sir Henry Campbell-Bannerman, and saw the Labour members as inconsequential: "I apologise to the Universe for my connection with such a body".

In the years after the 1906 election, Shaw felt that the Fabians needed fresh leadership, and saw this in the form of his fellow-writer H. G. Wells, who had joined the society in February 1903. Wells's ideas for reform—particularly his proposals for closer cooperation with the Independent Labour Party—placed him at odds with the society's "Old Gang", led by Shaw. According to Cole, Wells "had minimal capacity for putting [his ideas] across in public meetings against Shaw's trained and practised virtuosity". In Shaw's view, "the Old Gang did not extinguish Mr Wells, he annihilated himself". Wells resigned from the society in September 1908; Shaw remained a member, but left the executive in April 1911. He later wondered whether the Old Gang should have given way to Wells some years earlier: "God only knows whether the Society had not better have done it". Although less active—he blamed his advancing years—Shaw remained a Fabian.

In 1912 Shaw invested £1,000 for a one-fifth share in the Webbs' new publishing venture, a socialist weekly magazine called "The New Statesman", which appeared in April 1913. He became a founding director, publicist, and in due course a contributor, mostly anonymously. He was soon at odds with the magazine's editor, Clifford Sharp, who by 1916 was rejecting his contributions—"the only paper in the world that refuses to print anything by me", according to Shaw.

After the First World War began in August 1914, Shaw produced his tract "Common Sense About the War", which argued that the warring nations were equally culpable. Such a view was anathema in an atmosphere of fervent patriotism, and offended many of Shaw's friends; Ervine records that "[h]is appearance at any public function caused the instant departure of many of those present."

Despite his errant reputation, Shaw's propagandist skills were recognised by the British authorities, and early in 1917 he was invited by Field Marshal Haig to visit the Western Front battlefields. Shaw's 10,000-word report, which emphasised the human aspects of the soldier's life, was well received, and he became less of a lone voice. In April 1917 he joined the national consensus in welcoming America's entry into the war: "a first class moral asset to the common cause against junkerism".

Three short plays by Shaw were premiered during the war. "The Inca of Perusalem", written in 1915, encountered problems with the censor for burlesquing not only the enemy but the British military command; it was performed in 1916 at the Birmingham Repertory Theatre. "O'Flaherty V.C.", satirising the government's attitude to Irish recruits, was banned in the UK and was presented at a Royal Flying Corps base in Belgium in 1917. "Augustus Does His Bit", a genial farce, was granted a licence; it opened at the Royal Court in January 1917.

Shaw had long supported the principle of Irish Home Rule within the British Empire (which he thought should become the British Commonwealth).
In April 1916 he wrote scathingly in "The New York Times" about militant Irish nationalism: "In point of learning nothing and forgetting nothing these fellow-patriots of mine leave the Bourbons nowhere." Total independence, he asserted, was impractical; alliance with a bigger power (preferably England) was essential. The Dublin Easter Rising later that month took him by surprise. After its suppression by British forces, he expressed horror at the summary execution of the rebel leaders, but continued to believe in some form of Anglo-Irish union. In "How to Settle the Irish Question" (1917), he envisaged a federal arrangement, with national and imperial parliaments. Holroyd records that by this time the separatist party Sinn Féin was in the ascendency, and Shaw's and other moderate schemes were forgotten.

In the postwar period, Shaw despaired of the British government's coercive policies towards Ireland, and joined his fellow-writers Hilaire Belloc and G. K. Chesterton in publicly condemning these actions. The Anglo-Irish Treaty of December 1921 led to the partition of Ireland between north and south, a provision that dismayed Shaw. In 1922 civil war broke out in the south between its pro-treaty and anti-treaty factions, the former of whom had established the Irish Free State. Shaw visited Dublin in August, and met Michael Collins, then head of the Free State's Provisional Government. Shaw was much impressed by Collins, and was saddened when, three days later, the Irish leader was ambushed and killed by anti-treaty forces. In a letter to Collins's sister, Shaw wrote: "I met Michael for the first and last time on Saturday last, and am very glad I did. I rejoice in his memory, and will not be so disloyal to it as to snivel over his valiant death". Shaw remained a British subject all his life, but took dual British-Irish nationality in 1934.

Shaw's first major work to appear after the war was "Heartbreak House", written in 1916–17 and performed in 1920. It was produced on Broadway in November, and was coolly received; according to "The Times": "Mr Shaw on this occasion has more than usual to say and takes twice as long as usual to say it". After the London premiere in October 1921 "The Times" concurred with the American critics: "As usual with Mr Shaw, the play is about an hour too long", although containing "much entertainment and some profitable reflection". Ervine in "The Observer" thought the play brilliant but ponderously acted, except for Edith Evans as Lady Utterword.

Shaw's largest-scale theatrical work was "Back to Methuselah", written in 1918–20 and staged in 1922. Weintraub describes it as "Shaw's attempt to fend off 'the bottomless pit of an utterly discouraging pessimism'". This cycle of five interrelated plays depicts evolution, and the effects of longevity, from the Garden of Eden to the year 31,920 AD. Critics found the five plays strikingly uneven in quality and invention. The original run was brief, and the work has been revived infrequently. Shaw felt he had exhausted his remaining creative powers in the huge span of this "Metabiological Pentateuch". He was now sixty-seven, and expected to write no more plays.

This mood was short-lived. In 1920 Joan of Arc was proclaimed a saint by Pope Benedict XV; Shaw had long found Joan an interesting historical character, and his view of her veered between "half-witted genius" and someone of "exceptional sanity". He had considered writing a play about her in 1913, and the canonisation prompted him to return to the subject. He wrote "Saint Joan" in the middle months of 1923, and the play was premiered on Broadway in December. It was enthusiastically received there, and at its London premiere the following March. In Weintraub's phrase, "even the Nobel prize committee could no longer ignore Shaw after Saint Joan". The citation for the literature prize for 1925 praised his work as "... marked by both idealism and humanity, its stimulating satire often being infused with a singular poetic beauty". He accepted the award, but rejected the monetary prize that went with it, on the grounds that "My readers and my audiences provide me with more than sufficient money for my needs".

After "Saint Joan", it was five years before Shaw wrote a play. From 1924, he spent four years writing what he described as his "magnum opus", a political treatise entitled "The Intelligent Woman's Guide to Socialism and Capitalism". The book was published in 1928 and sold well. At the end of the decade Shaw produced his final Fabian tract, a commentary on the League of Nations. He described the League as "a school for the new international statesmanship as against the old Foreign Office diplomacy", but thought that it had not yet become the "Federation of the World".

Shaw returned to the theatre with what he called "a political extravaganza", "The Apple Cart", written in late 1928. It was, in Ervine's view, unexpectedly popular, taking a conservative, monarchist, anti-democratic line that appealed to contemporary audiences. The premiere was in Warsaw in June 1928, and the first British production was two months later, at Sir Barry Jackson's inaugural Malvern Festival. The other eminent creative artist most closely associated with the festival was Sir Edward Elgar, with whom Shaw enjoyed a deep friendship and mutual regard. He described "The Apple Cart" to Elgar as "a scandalous Aristophanic burlesque of democratic politics, with a brief but shocking sex interlude".

During the 1920s Shaw began to lose faith in the idea that society could be changed through Fabian gradualism, and became increasingly fascinated with dictatorial methods. In 1922 he had welcomed Mussolini's accession to power in Italy, observing that amid the "indiscipline and muddle and Parliamentary deadlock", Mussolini was "the right kind of tyrant". Shaw was prepared to tolerate certain dictatorial excesses; Weintraub in his ODNB biographical sketch comments that Shaw's "flirtation with authoritarian inter-war regimes" took a long time to fade, and Beatrice Webb thought he was "obsessed" about Mussolini.

Shaw's enthusiasm for the Soviet Union dated to the early 1920s when he had hailed Lenin as "the one really interesting statesman in Europe". Having turned down several chances to visit, in 1931 he joined a party led by Nancy Astor. The carefully managed trip culminated in a lengthy meeting with Stalin, whom Shaw later described as "a Georgian gentleman" with no malice in him. At a dinner given in his honour, Shaw told the gathering: "I have seen all the 'terrors' and I was terribly pleased by them". In March 1933 Shaw was a co-signatory to a letter in "The Manchester Guardian" protesting at the continuing misrepresentation of Soviet achievements: "No lie is too fantastic, no slander is too stale ... for employment by the more reckless elements of the British press."

Shaw's admiration for Mussolini and Stalin demonstrated his growing belief that dictatorship was the only viable political arrangement. When the Nazi Party came to power in Germany in January 1933, Shaw described Hitler as "a very remarkable man, a very able man", and professed himself proud to be the only writer in England who was "scrupulously polite and just to Hitler". His principal admiration was for Stalin, whose regime he championed uncritically throughout the decade. Shaw saw the 1939 Molotov–Ribbentrop Pact as a triumph for Stalin who, he said, now had Hitler under his thumb.

Shaw's first play of the decade was "Too True to be Good", written in 1931 and premiered in Boston in February 1932. The reception was unenthusiastic. Brooks Atkinson of "The New York Times" commenting that Shaw had "yielded to the impulse to write without having a subject", judged the play a "rambling and indifferently tedious conversation". The correspondent of "The New York Herald Tribune" said that most of the play was "discourse, unbelievably long lectures" and that although the audience enjoyed the play it was bewildered by it.
During the decade Shaw travelled widely and frequently. Most of his journeys were with Charlotte; she enjoyed voyages on ocean liners, and he found peace to write during the long spells at sea. Shaw met an enthusiastic welcome in South Africa in 1932, despite his strong remarks about the racial divisions of the country. In December 1932 the couple embarked on a round-the-world cruise. In March 1933 they arrived at San Francisco, to begin Shaw's first visit to the US. He had earlier refused to go to "that awful country, that uncivilized place", "unfit to govern itself... illiberal, superstitious, crude, violent, anarchic and arbitrary". He visited Hollywood, with which he was unimpressed, and New York, where he lectured to a capacity audience in the Metropolitan Opera House. Harried by the intrusive attentions of the press, Shaw was glad when his ship sailed from New York harbour. New Zealand, which he and Charlotte visited the following year, struck him as "the best country I've been in"; he urged its people to be more confident and loosen their dependence on trade with Britain. He used the weeks at sea to complete two plays—"The Simpleton of the Unexpected Isles" and "The Six of Calais"—and begin work on a third, "The Millionairess".

Despite his contempt for Hollywood and its aesthetic values, Shaw was enthusiastic about cinema, and in the middle of the decade wrote screenplays for prospective film versions of "Pygmalion" and "Saint Joan". The latter was never made, but Shaw entrusted the rights to the former to the unknown Gabriel Pascal, who produced it at Pinewood Studios in 1938. Shaw was determined that Hollywood should have nothing to do with the film, but was powerless to prevent it from winning one Academy Award ("Oscar"); he described his award for "best-written screenplay" as an insult, coming from such a source. He became the first person to have been awarded both a Nobel Prize and an Oscar. In a 1993 study of the Oscars, Anthony Holden observes that "Pygmalion" was soon spoken of as having "lifted movie-making from illiteracy to literacy".

Shaw's final plays of the 1930s were "Cymbeline Refinished" (1936), "Geneva" (1936) and "In Good King Charles's Golden Days" (1939). The first, a fantasy reworking of Shakespeare, made little impression, but the second, a satire on European dictators, attracted more notice, much of it unfavourable. In particular, Shaw's parody of Hitler as "Herr Battler" was considered mild, almost sympathetic. The third play, an historical conversation piece first seen at Malvern, ran briefly in London in May 1940. James Agate commented that the play contained nothing to which even the most conservative audiences could take exception, and though it was long and lacking in dramatic action only "witless and idle" theatregoers would object. After their first runs none of the three plays were seen again in the West End during Shaw's lifetime.

Towards the end of the decade, both Shaws began to suffer ill health. Charlotte was increasingly incapacitated by Paget's disease of bone, and he developed pernicious anaemia. His treatment, involving injections of concentrated animal liver, was successful, but this breach of his vegetarian creed distressed him and brought down condemnation from militant vegetarians.

Although Shaw's works since "The Apple Cart" had been received without great enthusiasm, his earlier plays were revived in the West End throughout the Second World War, starring such actors as Edith Evans, John Gielgud, Deborah Kerr and Robert Donat. In 1944 nine Shaw plays were staged in London, including "Arms and the Man" with Ralph Richardson, Laurence Olivier, Sybil Thorndike and Margaret Leighton in the leading roles. Two touring companies took his plays all round Britain. The revival in his popularity did not tempt Shaw to write a new play, and he concentrated on prolific journalism. A second Shaw film produced by Pascal, "Major Barbara" (1941), was less successful both artistically and commercially than "Pygmalion", partly because of Pascal's insistence on directing, to which he was unsuited.

Following the outbreak of war on 3 September 1939 and the rapid conquest of Poland, Shaw was accused of defeatism when, in a "New Statesman" article, he declared the war over and demanded a peace conference. Nevertheless, when he became convinced that a negotiated peace was impossible, he publicly urged the neutral United States to join the fight. The London blitz of 1940–41 led the Shaws, both in their mid-eighties, to live full-time at Ayot St Lawrence. Even there they were not immune from enemy air raids, and stayed on occasion with Nancy Astor at her country house, Cliveden. In 1943, the worst of the London bombing over, the Shaws moved back to Whitehall Court, where medical help for Charlotte was more easily arranged. Her condition deteriorated, and she died in September.

Shaw's final political treatise, "Everybody's Political What's What", was published in 1944. Holroyd describes this as "a rambling narrative ... that repeats ideas he had given better elsewhere and then repeats itself". The book sold well—85,000 copies by the end of the year. After Hitler's suicide in May 1945, Shaw approved of the formal condolences offered by the Irish Taoiseach, Éamon de Valera, at the German embassy in Dublin. Shaw disapproved of the postwar trials of the defeated German leaders, as an act of self-righteousness: "We are all potential criminals".

Pascal was given a third opportunity to film Shaw's work with "Caesar and Cleopatra" (1945). It cost three times its original budget and was rated "the biggest financial failure in the history of British cinema". The film was poorly received by British critics, although American reviews were friendlier. Shaw thought its lavishness nullified the drama, and he considered the film "a poor imitation of Cecil B. de Mille".
In 1946, the year of Shaw's ninetieth birthday, he accepted the freedom of Dublin and became the first honorary freeman of the borough of St Pancras, London. In the same year the government asked Shaw informally whether he would accept the Order of Merit. He declined, believing that an author's merit could only be determined by the posthumous verdict of history. 1946 saw the publication, as "The Crime of Imprisonment", of the preface Shaw had written 20 years previously to a study of prison conditions. It was widely praised; a reviewer in the "American Journal of Public Health" considered it essential reading for any student of the American criminal justice system.

Shaw continued to write into his nineties. His last plays were "Buoyant Billions" (1947), his final full-length work; "Farfetched Fables" (1948) a set of six short plays revisiting several of his earlier themes such as evolution; a comic play for puppets, "Shakes versus Shav" (1949), a ten-minute piece in which Shakespeare and Shaw trade insults; and "Why She Would Not" (1950), which Shaw described as "a little comedy", written in one week shortly before his ninety-fourth birthday.

During his later years, Shaw enjoyed tending the gardens at Shaw's Corner. He died at the age of ninety-four of renal failure precipitated by injuries incurred when falling while pruning a tree. He was cremated at Golders Green Crematorium on 6 November 1950. His ashes, mixed with those of Charlotte, were scattered along footpaths and around the statue of Saint Joan in their garden.

Shaw published a collected edition of his plays in 1934, comprising forty-two works. He wrote a further twelve in the remaining sixteen years of his life, mostly one-act pieces. Including eight earlier plays that he chose to omit from his published works, the total is sixty-two.

Shaw's first three full-length plays dealt with social issues. He later grouped them as "Plays Unpleasant". "Widower's Houses" (1892) concerns the landlords of slum properties, and introduces the first of Shaw's New Women—a recurring feature of later plays. "The Philanderer" (1893) develops the theme of the New Woman, draws on Ibsen, and has elements of Shaw's personal relationships, the character of Julia being based on Jenny Patterson. In a 2003 study Judith Evans describes "Mrs Warren's Profession" (1893) as "undoubtedly the most challenging" of the three Plays Unpleasant, taking Mrs Warren's profession—prostitute and, later, brothel-owner—as a metaphor for a prostituted society.

Shaw followed the first trilogy with a second, published as "Plays Pleasant". "Arms and the Man" (1894) conceals beneath a mock-Ruritanian comic romance a Fabian parable contrasting impractical idealism with pragmatic socialism. The central theme of "Candida" (1894) is a woman's choice between two men; the play contrasts the outlook and aspirations of a Christian Socialist and a poetic idealist. The third of the Pleasant group, "You Never Can Tell" (1896), portrays social mobility, and the gap between generations, particularly in how they approach social relations in general and mating in particular.

The "Three Plays for Puritans"—comprising "The Devil's Disciple" (1896), "Caesar and Cleopatra" (1898) and "Captain Brassbound's Conversion" (1899)—all centre on questions of empire and imperialism, a major topic of political discourse in the 1890s. The three are set, respectively, in 1770s America, Ancient Egypt, and 1890s Morocco. "The Gadfly", an adaptation of the popular novel by Ethel Voynich, was unfinished and unperformed. "The Man of Destiny" (1895) is a short curtain raiser about Napoleon.

Shaw's major plays of the first decade of the twentieth century address individual social, political or ethical issues. "Man and Superman" (1902) stands apart from the others in both its subject and its treatment, giving Shaw's interpretation of creative evolution in a combination of drama and associated printed text. "The Admirable Bashville" (1901), a blank verse dramatisation of Shaw's novel "Cashel Byron's Profession", focuses on the imperial relationship between Britain and Africa. "John Bull's Other Island" (1904), comically depicting the prevailing relationship between Britain and Ireland, was popular at the time but fell out of the general repertoire in later years. "Major Barbara" (1905) presents ethical questions in an unconventional way, confounding expectations that in the depiction of an armaments manufacturer on the one hand and the Salvation Army on the other the moral high ground must invariably be held by the latter. "The Doctor's Dilemma" (1906), a play about medical ethics and moral choices in allocating scarce treatment, was described by Shaw as a tragedy. With a reputation for presenting characters who did not resemble real flesh and blood, he was challenged by Archer to present an on-stage death, and here did so, with a deathbed scene for the anti-hero.

"Getting Married" (1908) and "Misalliance" (1909)—the latter seen by Judith Evans as a companion piece to the former—are both in what Shaw called his "disquisitionary" vein, with the emphasis on discussion of ideas rather than on dramatic events or vivid characterisation. Shaw wrote seven short plays during the decade; they are all comedies, ranging from the deliberately absurd "Passion, Poison, and Petrifaction" (1905) to the satirical "Press Cuttings" (1909).

In the decade from 1910 to the aftermath of the First World War Shaw wrote four full-length plays, the third and fourth of which are among his most frequently staged works. "Fanny's First Play" (1911) continues his earlier examinations of middle-class British society from a Fabian viewpoint, with additional touches of melodrama and an epilogue in which theatre critics discuss the play. "Androcles and the Lion" (1912), which Shaw began writing as a play for children, became a study of the nature of religion and how to put Christian precepts into practice. "Pygmalion" (1912) is a Shavian study of language and speech and their importance in society and in personal relationships. To correct the impression left by the original performers that the play portrayed a romantic relationship between the two main characters Shaw rewrote the ending to make it clear that the heroine will marry another, minor character. Shaw's only full-length play from the war years is "Heartbreak House" (1917), which in his words depicts "cultured, leisured Europe before the war" drifting towards disaster. Shaw named Shakespeare ("King Lear") and Chekhov ("The Cherry Orchard") as important influences on the piece, and critics have found elements drawing on Congreve ("The Way of the World") and Ibsen ("The Master Builder").

The short plays range from genial historical drama in "The Dark Lady of the Sonnets" and "Great Catherine" (1910 and 1913) to a study of polygamy in "Overruled"; three satirical works about the war ("The Inca of Perusalem", "O'Flaherty V.C." and "Augustus Does His Bit", 1915–16); a piece that Shaw called "utter nonsense" ("The Music Cure", 1914) and a brief sketch about a "Bolshevik empress" ("Annajanska", 1917).

"Saint Joan" (1923) drew widespread praise both for Shaw and for Sybil Thorndike, for whom he wrote the title role and who created the part in Britain. In the view of the commentator Nicholas Grene, Shaw's Joan, a "no-nonsense mystic, Protestant and nationalist before her time" is among the 20th century's classic leading female roles. "The Apple Cart" (1929) was Shaw's last popular success. He gave both that play and its successor, "Too True to Be Good" (1931), the subtitle "A political extravaganza", although the two works differ greatly in their themes; the first presents the politics of a nation (with a brief royal love-scene as an interlude) and the second, in Judith Evans's words, "is concerned with the social mores of the individual, and is nebulous." Shaw's plays of the 1930s were written in the shadow of worsening national and international political events. Once again, with "On the Rocks" (1933) and "The Simpleton of the Unexpected Isles" (1934), a political comedy with a clear plot was followed by an introspective drama. The first play portrays a British prime minister considering, but finally rejecting, the establishment of a dictatorship; the second is concerned with polygamy and eugenics and ends with the Day of Judgement.

"The Millionairess" (1934) is a farcical depiction of the commercial and social affairs of a successful businesswoman. "Geneva" (1936) lampoons the feebleness of the League of Nations compared with the dictators of Europe. "In Good King Charles's Golden Days" (1939), described by Weintraub as a warm, discursive high comedy, also depicts authoritarianism, but less satirically than "Geneva". As in earlier decades, the shorter plays were generally comedies, some historical and others addressing various political and social preoccupations of the author. Ervine writes of Shaw's later work that although it was still "astonishingly vigorous and vivacious" it showed unmistakable signs of his age. "The best of his work in this period, however, was full of wisdom and the beauty of mind often displayed by old men who keep their wits about them."

Shaw's collected musical criticism, published in three volumes, runs to more than 2,700 pages. It covers the British musical scene from 1876 to 1950, but the core of the collection dates from his six years as music critic of "The Star" and "The World" in the late 1880s and early 1890s. In his view music criticism should be interesting to everyone rather than just the musical élite, and he wrote for the non-specialist, avoiding technical jargon—"Mesopotamian words like 'the dominant of D major'". He was fiercely partisan in his columns, promoting the music of Wagner and decrying that of Brahms and those British composers such as Stanford and Parry whom he saw as Brahmsian. He campaigned against the prevailing fashion for performances of Handel oratorios with huge amateur choirs and inflated orchestration, calling for "a chorus of twenty capable artists". He railed against opera productions unrealistically staged or sung in languages the audience did not speak.

In Shaw's view, the London theatres of the 1890s presented too many revivals of old plays and not enough new work. He campaigned against "melodrama, sentimentality, stereotypes and worn-out conventions". As a music critic he had frequently been able to concentrate on analysing new works, but in the theatre he was often obliged to fall back on discussing how various performers tackled well-known plays. In a study of Shaw's work as a theatre critic, E. J. West writes that Shaw "ceaselessly compared and contrasted artists in interpretation and in technique". Shaw contributed more than 150 articles as theatre critic for "The Saturday Review", in which he assessed more than 212 productions. He championed Ibsen's plays when many theatregoers regarded them as outrageous, and his 1891 book "Quintessence of Ibsenism" remained a classic throughout the twentieth century. Of contemporary dramatists writing for the West End stage he rated Oscar Wilde above the rest: "... our only thorough playwright. He plays with everything: with wit, with philosophy, with drama, with actors and audience, with the whole theatre". Shaw's collected criticisms were published as "Our Theatres in the Nineties" in 1932.

Shaw maintained a provocative and frequently self-contradictory attitude to Shakespeare (whose name he insisted on spelling "Shakespear"). Many found him difficult to take seriously on the subject; Duff Cooper observed that by attacking Shakespeare, "it is Shaw who appears a ridiculous pigmy shaking his fist at a mountain." Shaw was, nevertheless, a knowledgeable Shakespearian, and in an article in which he wrote, "With the single exception of Homer, there is no eminent writer, not even Sir Walter Scott, whom I can despise so entirely as I despise Shakespear when I measure my mind against his," he also said, "But I am bound to add that I pity the man who cannot enjoy Shakespear. He has outlasted thousands of abler thinkers, and will outlast a thousand more". Shaw had two regular targets for his more extreme comments about Shakespeare: undiscriminating "Bardolaters", and actors and directors who presented insensitively cut texts in over-elaborate productions. He was continually drawn back to Shakespeare, and wrote three plays with Shakespearean themes: "The Dark Lady of the Sonnets", "Cymbeline Refinished" and "Shakes versus Shav". In a 2001 analysis of Shaw's Shakespearian criticisms, Robert Pierce concludes that Shaw, who was no academic, saw Shakespeare's plays—like all theatre—from an author's practical point of view: "Shaw helps us to get away from the Romantics' picture of Shakespeare as a titanic genius, one whose art cannot be analyzed or connected with the mundane considerations of theatrical conditions and profit and loss, or with a specific staging and cast of actors."

Shaw's political and social commentaries were published variously in Fabian tracts, in essays, in two full-length books, in innumerable newspaper and journal articles and in prefaces to his plays. The majority of Shaw's Fabian tracts were published anonymously, representing the voice of the society rather than of Shaw, although the society's secretary Edward Pease later confirmed Shaw's authorship. According to Holroyd, the business of the early Fabians, mainly under the influence of Shaw, was to "alter history by rewriting it". Shaw's talent as a pamphleteer was put to immediate use in the production of the society's manifesto—after which, says Holroyd, he was never again so succinct.
After the turn of the twentieth century, Shaw increasingly propagated his ideas through the medium of his plays. An early critic, writing in 1904, observed that Shaw's dramas provided "a pleasant means" of proselytising his socialism, adding that "Mr Shaw's views are to be sought especially in the prefaces to his plays". After loosening his ties with the Fabian movement in 1911, Shaw's writings were more personal and often provocative; his response to the furore following the issue of "Common Sense About the War" in 1914, was to prepare a sequel, "More Common Sense About the War". In this, he denounced the pacifist line espoused by Ramsay MacDonald and other socialist leaders, and proclaimed his readiness to shoot all pacifists rather than cede them power and influence. On the advice of Beatrice Webb, this pamphlet remained unpublished.

"The Intelligent Woman's Guide", Shaw's main political treatise of the 1920s, attracted both admiration and criticism. MacDonald considered it the world's most important book since the Bible; Harold Laski thought its arguments outdated and lacking in concern for individual freedoms. Shaw's increasing flirtation with dictatorial methods is evident in many of his subsequent pronouncements. A "New York Times" report dated 10 December 1933 quoted a recent Fabian Society lecture in which Shaw had praised Hitler, Mussolini and Stalin: "[T]hey are trying to get something done, [and] are adopting methods by which it is possible to get something done". As late as the Second World War, in "Everybody's Political What's What", Shaw blamed the Allies' "abuse" of their 1918 victory for the rise of Hitler, and hoped that, after defeat, the Führer would escape retribution "to enjoy a comfortable retirement in Ireland or some other neutral country". These sentiments, according to the Irish philosopher-poet Thomas Duddy, "rendered much of the Shavian outlook passé and contemptible".

"Creative evolution", Shaw's version of the new science of eugenics, became an increasing theme in his political writing after 1900. He introduced his theories in "The Revolutionist's Handbook" (1903), an appendix to "Man and Superman", and developed them further during the 1920s in "Back to Methuselah". A 1946 "Life" magazine article observed that Shaw had "always tended to look at people more as a biologist than as an artist". By 1933, in the preface to "On the Rocks", he was writing that "if we desire a certain type of civilization and culture we must exterminate the sort of people who do not fit into it"; critical opinion is divided on whether this was intended as irony. In an article in the American magazine "Liberty" in September 1938, Shaw included the statement: "There are many people in the world who ought to be liquidated". Many commentators assumed that such comments were intended as a joke, although in the worst possible taste. Otherwise, "Life" magazine concluded, "this silliness can be classed with his more innocent bad guesses".

Shaw's fiction-writing was largely confined to the five unsuccessful novels written in the period 1879–1885. "Immaturity" (1879) is a semi-autobiographical portrayal of mid-Victorian England, Shaw's "own "David Copperfield"" according to Weintraub. "The Irrational Knot" (1880) is a critique of conventional marriage, in which Weintraub finds the characterisations lifeless, "hardly more than animated theories". Shaw was pleased with his third novel, "Love Among the Artists" (1881), feeling that it marked a turning point in his development as a thinker, although he had no more success with it than with its predecessors. "Cashel Byron's Profession" (1882) is, says Weintraub, an indictment of society which anticipates Shaw's first full-length play, "Mrs Warren's Profession". Shaw later explained that he had intended "An Unsocial Socialist" as the first section of a monumental depiction of the downfall of capitalism. Gareth Griffith, in a study of Shaw's political thought, sees the novel as an interesting record of conditions, both in society at large and in the nascent socialist movement of the 1880s.

Shaw's only subsequent fiction of any substance was his 1932 novella "The Adventures of the Black Girl in Her Search for God", written during a visit to South Africa in 1932. The eponymous girl, intelligent, inquisitive, and converted to Christianity by insubstantial missionary teaching, sets out to find God, on a journey that after many adventures and encounters, leads her to a secular conclusion. The story, on publication, offended some Christians and was banned in Ireland by the Board of Censors.

Shaw was a prolific correspondent throughout his life. His letters, edited by Dan H. Laurence, were published between 1965 and 1988. Shaw once estimated his letters would occupy twenty volumes; Laurence commented that, unedited, they would fill many more. Shaw wrote more than a quarter of a million letters, of which about ten per cent have survived; 2,653 letters are printed in Laurence's four volumes. Among Shaw's many regular correspondents were his childhood friend Edward McNulty; his theatrical colleagues (and "amitiés amoureuses") Mrs Patrick Campbell and Ellen Terry; writers including Lord Alfred Douglas, H. G. Wells and G. K. Chesterton; the boxer Gene Tunney; the nun Laurentia McLachlan; and the art expert Sydney Cockerell. In 2007 a 316-page volume consisting entirely of Shaw's letters to "The Times" was published.

Shaw's diaries for 1885–1897, edited by Weintraub, were published in two volumes, with a total of 1,241 pages, in 1986. Reviewing them, the Shaw scholar Fred Crawford wrote: "Although the primary interest for Shavians is the material that supplements what we already know about Shaw's life and work, the diaries are also valuable as a historical and sociological document of English life at the end of the Victorian age." After 1897, pressure of other writing led Shaw to give up keeping a diary.

Through his journalism, pamphlets and occasional longer works, Shaw wrote on many subjects. His range of interest and enquiry included vivisection, vegetarianism, religion, language, cinema and photography, on all of which he wrote and spoke copiously. Collections of his writings on these and other subjects were published, mainly after his death, together with volumes of "wit and wisdom" and general journalism.

Despite the many books written about him (Holroyd counts 80 by 1939) Shaw's autobiographical output, apart from his diaries, was relatively slight. He gave interviews to newspapers—"GBS Confesses", to "The Daily Mail" in 1904 is an example—and provided sketches to would-be biographers whose work was rejected by Shaw and never published. In 1939 Shaw drew on these materials to produce "Shaw Gives Himself Away", a miscellany which, a year before his death, he revised and republished as "Sixteen Self Sketches" (there were seventeen). He made it clear to his publishers that this slim book was in no sense a full autobiography.

In his lifetime Shaw professed many beliefs, often contradictory. This inconsistency was partly an intentional provocation—the Spanish scholar-statesman Salvador de Madariaga describes Shaw as "a pole of negative electricity set in a people of positive electricity". In one area at least Shaw was constant: in his lifelong refusal to follow normal English forms of spelling and punctuation. He favoured archaic spellings such as "shew" for "show"; he dropped the "u" in words like "honour" and "favour"; and wherever possible he rejected the apostrophe in contractions such as "won't" or "that's". In his will, Shaw ordered that, after some specified legacies, his remaining assets were to form a trust to pay for fundamental reform of the English alphabet into a phonetic version of forty letters. Though Shaw's intentions were clear, his drafting was flawed, and the courts initially ruled the intended trust void. A later out-of-court agreement provided a sum of £8,300 for spelling reform; the bulk of his fortune went to the residuary legatees—the British Museum, the Royal Academy of Dramatic Art and the National Gallery of Ireland. Most of the £8,300 went on a special phonetic edition of "Androcles and the Lion" in the Shavian alphabet, published in 1962 to a largely indifferent reception.

Shaw's views on religion and Christianity were less consistent. Having in his youth proclaimed himself an atheist, in middle age he explained this as a reaction against the Old Testament image of a vengeful Jehovah. By the early twentieth century, he termed himself a "mystic", although Gary Sloan, in an essay on Shaw's beliefs, disputes his credentials as such. In 1913 Shaw declared that he was not religious "in the sectarian sense", aligning himself with Jesus as "a person of no religion". In the preface (1915) to "Androcles and the Lion", Shaw asks "Why not give Christianity a chance?" contending that Britain's social order resulted from the continuing choice of Barabbas over Christ. In a broadcast just before the Second World War, Shaw invoked the Sermon on the Mount, "a very moving exhortation, and it gives you one first-rate tip, which is to do good to those who despitefully use you and persecute you". In his will, Shaw stated that his "religious convictions and scientific views cannot at present be more specifically defined than as those of a believer in creative revolution". He requested that no one should imply that he accepted the beliefs of any specific religious organisation, and that no memorial to him should "take the form of a cross or any other instrument of torture or symbol of blood sacrifice".

Shaw espoused racial equality, and inter-marriage between people of different races. Despite his expressed wish to be fair to Hitler, he called anti-Semitism "the hatred of the lazy, ignorant fat-headed Gentile for the pertinacious Jew who, schooled by adversity to use his brains to the utmost, outdoes him in business". In "The Jewish Chronicle" he wrote in 1932, "In every country you can find rabid people who have a phobia against Jews, Jesuits, Armenians, Negroes, Freemasons, Irishmen, or simply foreigners as such. Political parties are not above exploiting these fears and jealousies."

In 1903 Shaw joined in a controversy about vaccination against smallpox. He called vaccination "a peculiarly filthy piece of witchcraft"; in his view immunisation campaigns were a cheap and inadequate substitute for a decent programme of housing for the poor, which would, he declared, be the means of eradicating smallpox and other infectious diseases. Less contentiously, Shaw was keenly interested in transport; Laurence observed in 1992 a need for a published study of Shaw's interest in "bicycling, motorbikes, automobiles, and planes, climaxing in his joining the Interplanetary Society in his nineties". Shaw published articles on travel, took photographs of his journeys, and submitted notes to the Royal Automobile Club.

Shaw strove throughout his adult life to be referred to as "Bernard Shaw" rather than "George Bernard Shaw", but confused matters by continuing to use his full initials—G.B.S.—as a by-line, and often signed himself "G.Bernard Shaw". He left instructions in his will that his executor (the Public Trustee) was to license publication of his works only under the name Bernard Shaw. Shaw scholars including Ervine, Judith Evans, Holroyd, Laurence and Weintraub, and many publishers have respected Shaw's preference, although the Cambridge University Press was among the exceptions with its 1988 "Cambridge Companion to George Bernard Shaw".

Shaw did not found a school of dramatists as such, but Crawford asserts that today "we recognise [him] as second only to Shakespeare in the British theatrical tradition ... the proponent of the theater of ideas" who struck a death-blow to 19th-century melodrama. According to Laurence, Shaw pioneered "intelligent" theatre, in which the audience was required to think, thereby paving the way for the new breeds of twentieth-century playwrights from Galsworthy to Pinter.

Crawford lists numerous playwrights whose work owes something to that of Shaw. Among those active in Shaw's lifetime he includes Noël Coward, who based his early comedy "The Young Idea" on "You Never Can Tell" and continued to draw on the older man's works in later plays. T. S. Eliot, by no means an admirer of Shaw, admitted that the epilogue of "Murder in the Cathedral", in which Becket's slayers explain their actions to the audience, might have been influenced by "Saint Joan". The critic Eric Bentley comments that Eliot's later play "The Confidential Clerk" "had all the earmarks of Shavianism ... without the merits of the real Bernard Shaw". Among more recent British dramatists, Crawford marks Tom Stoppard as "the most Shavian of contemporary playwrights"; Shaw's "serious farce" is continued in the works of Stoppard's contemporaries Alan Ayckbourn, Henry Livings and Peter Nichols.
Shaw's influence crossed the Atlantic at an early stage. Bernard Dukore notes that he was successful as a dramatist in America ten years before achieving comparable success in Britain. Among many American writers professing a direct debt to Shaw, Eugene O'Neill became an admirer at the age of seventeen, after reading "The Quintessence of Ibsenism". Other Shaw-influenced American playwrights mentioned by Dukore are Elmer Rice, for whom Shaw "opened doors, turned on lights, and expanded horizons"; William Saroyan, who empathised with Shaw as "the embattled individualist against the philistines"; and S. N. Behrman, who was inspired to write for the theatre after attending a performance of "Caesar and Cleopatra": "I thought it would be agreeable to write plays like that".

Assessing Shaw's reputation in a 1976 critical study, T. F. Evans described Shaw as unchallenged in his lifetime and since as the leading English-language dramatist of the (twentieth) century, and as a master of prose style. The following year, in a contrary assessment, the playwright John Osborne castigated "The Guardian's" theatre critic Michael Billington for referring to Shaw as "the greatest British dramatist since Shakespeare". Osborne responded that Shaw "is the most fraudulent, inept writer of Victorian melodramas ever to gull a timid critic or fool a dull public". Despite this hostility, Crawford sees the influence of Shaw in some of Osborne's plays, and concludes that though the latter's work is neither imitative nor derivative, these affinities are sufficient to classify Osborne as an inheritor of Shaw.

In a 1983 study, R. J. Kaufmann suggests that Shaw was a key forerunner—"godfather, if not actually finicky paterfamilias"—of the Theatre of the Absurd. Two further aspects of Shaw's theatrical legacy are noted by Crawford: his opposition to stage censorship, which was finally ended in 1968, and his efforts which extended over many years to establish a National Theatre. Shaw's short 1910 play "The Dark Lady of the Sonnets", in which Shakespeare pleads with Queen Elizabeth I for the endowment of a state theatre, was part of this campaign.

Writing in "The New Statesman" in 2012 Daniel Janes commented that Shaw's reputation had declined by the time of his 150th anniversary in 2006 but had recovered considerably. In Janes's view, the many current revivals of Shaw's major works showed the playwright's "almost unlimited relevance to our times". In the same year, Mark Lawson wrote in "The Guardian" that Shaw's moral concerns engaged present-day audiences, and made him—like his model, Ibsen—one of the most popular playwrights in contemporary British theatre.

The Shaw Festival in Niagara-on-the-Lake, Ontario, Canada is the second largest repertory theatre company in North America. It produces plays by or written during the lifetime of Shaw as well as some contemporary works. The Gingold Theatrical Group, founded in 2006, presents works by Shaw and others in New York City that feature the humanitarian ideals that his work promoted. It became the first theatre group to present all of Shaw's stage work through its monthly concert series "Project Shaw".

In the 1940s the author Harold Nicolson advised the National Trust not to accept the bequest of Shaw's Corner, predicting that Shaw would be totally forgotten within fifty years. In the event, Shaw's broad cultural legacy, embodied in the widely used term "Shavian", has endured and is nurtured by Shaw Societies in various parts of the world. The original society was founded in London in 1941 and survives; it organises meetings and events, and publishes a regular bulletin "The Shavian". The Shaw Society of America began in June 1950; it foundered in the 1970s but its journal, adopted by Penn State University Press, continued to be published as "Shaw: The Annual of Bernard Shaw Studies" until 2004. A second American organisation, founded in 1951 as "The Bernard Shaw Society", remains active as of 2016. More recent societies have been established in Japan and India.

Besides his collected music criticism, Shaw has left a varied musical legacy, not all of it of his choosing. Despite his dislike of having his work adapted for the musical theatre ("my plays set themselves to a verbal music of their own") two of his plays were turned into musical comedies: "Arms and the Man" was the basis of "The Chocolate Soldier" in 1908, with music by Oscar Straus, and "Pygmalion" was adapted in 1956 as "My Fair Lady" with book and lyrics by Alan Jay Lerner and music by Frederick Loewe. Although he had a high regard for Elgar, Shaw turned down the composer's request for an opera libretto, but played a major part in persuading the BBC to commission Elgar's Third Symphony, and was the dedicatee of "The Severn Suite" (1930).

The substance of Shaw's political legacy is uncertain. In 1921 Shaw's erstwhile collaborator William Archer, in a letter to the playwright, wrote: "I doubt if there is any case of a man so widely read, heard, seen, and known as yourself, who has produced so little effect on his generation." Margaret Cole, who considered Shaw the greatest writer of his age, professed never to have understood him. She thought he worked "immensely hard" at politics, but essentially, she surmises, it was for fun—"the fun of a brilliant artist". After Shaw's death, Pearson wrote: "No one since the time of Tom Paine has had so definite an influence on the social and political life of his time and country as Bernard Shaw."

In its obituary tribute to Shaw, "The Times Literary Supplement" concluded:






</doc>
<doc id="12858" url="https://en.wikipedia.org/wiki?curid=12858" title="Galvanization">
Galvanization

Galvanization or galvanizing is the process of applying a protective zinc coating to steel or iron, to prevent rusting. The most common method is hot-dip galvanizing, in which the parts are submerged in a bath of molten zinc.

Galvanizing protects the underlying iron or steel in the following main ways:

Named via French from the name of Italian scientist Luigi Galvani, the earliest use of the term was, in early 19th-century scientific research and medical practice, stimulation of a muscle by the application of an electric current. It continues to be used metaphorically of any stimulus which results in activity by a person or group of people.

The earliest known example of galvanized iron was encountered by Europeans on 17th-century Indian armor in the Royal Armouries Museum collection. The armored application is the origin of the metaphorical use of the verb "galvanize", as in to reinforce.
In modern usage, the term "galvanizing" has largely come to be associated with zinc coatings, to the exclusion of other metals. Galvanic paint, a precursor to hot-dip galvanizing, was patented by Stanislas Sorel, of Paris, in December 1837.

Hot-dip galvanizing deposits a thick, robust layer of zinc iron alloys on the surface of a steel item. In the case of automobile bodies, where additional decorative coatings of paint will be applied, a thinner form of galvanizing is applied by electrogalvanizing. The hot-dip process generally does not reduce strength on a measurable scale, with the exception of high-strength steels (>1100 MPa) where hydrogen embrittlement can become a problem. This deficiency is a consideration affecting the manufacture of wire rope and other highly-stressed products. 

The protection provided by hot-dip galvanizing is insufficient for products that will be constantly exposed to corrosive materials such as acids, including acid rain in outdoor uses. For these applications, more expensive stainless steel is preferred. Some nails made today are galvanized. Nonetheless, electroplating is used on its own for many outdoor applications because it is cheaper than hot-dip zinc coating and looks good when new. Another reason not to use hot-dip zinc coating is that for bolts and nuts of size M10 (US 3/8") or smaller, the thick hot-dipped coating fills in too much of the threads, which reduces strength (because the dimension of the steel prior to coating must be reduced for the fasteners to fit together). This means that for cars, bicycles, and many other light mechanical products, the practical alternative to electroplating bolts and nuts is not hot-dip zinc coating, but making the fasteners from stainless steel or (stronger but much more expensive) titanium.
The size of crystallites (grains) in galvanized coatings is a visible and aesthetic feature, known as "spangle". By varying the number of particles added for heterogeneous nucleation and the rate of cooling in a hot-dip process, the spangle can be adjusted from an apparently uniform surface (crystallites too small to see with the naked eye) to grains several centimetres wide. Visible crystallites are rare in other engineering materials, even though they are usually present.

Thermal diffusion galvanizing, or Sherardizing, provides a zinc diffusion coating on iron- or copper-based materials. Parts and zinc powder are tumbled in a sealed rotating drum. Around , zinc will diffuse into the substrate to form a zinc alloy. The advance surface preparation of the goods can be carried out by shot blasting. The process is also known as "dry galvanizing", because no liquids are involved; this can avoid possible problems caused by hydrogen embrittlement. The dull-grey crystal structure of the zinc diffusion coating has a good adhesion to paint, powder coatings, or rubber. It is a preferred method for coating small, complex-shaped metals, and for smoothing rough surfaces on items formed with sintered metal.

Although galvanizing will inhibit attack of the underlying steel, rusting will be inevitable after some decades of exposure to weather, especially if exposed to acidic conditions. For example, corrugated iron sheet roofing will start to degrade within a few years despite the protective action of the zinc coating. Marine and salty environments also lower the lifetime of galvanized iron because the high electrical conductivity of sea water increases the rate of corrosion, primarily through converting the solid zinc to soluble zinc chloride which simply washes away. Galvanized car frames exemplify this; they corrode much faster in cold environments due to road salt, though they will last longer than unprotected steel. 

Galvanized steel can last for many decades if other supplementary measures are maintained, such as paint coatings and additional sacrificial anodes. The rate of corrosion in non-salty environments is caused mainly by levels of sulfur dioxide in the air. In the most benign natural environments, such as inland low population areas, galvanized steel can last without rust for over 100 years.

This is the most common use for galvanized metal, and hundreds of thousands of tons of steel products are galvanized annually worldwide. In developed countries most larger cities have several galvanizing factories, and many items of steel manufacture are galvanized for protection. Typically these include: street furniture, building frameworks, balconies, verandahs, staircases, ladders, walkways, and more. SGCC hot dip galvanized steel is also used for making steel frames as a basic construction material for steel frame buildings.

In the early 20th century, galvanized piping replaced previously-used cast iron and lead in cold-water plumbing. Typically, galvanized piping rusts from the inside out, building up layers of plaque on the inside of the piping, causing both water pressure problems and eventual pipe failure. These plaques can flake off, leading to visible impurities in water and a slight metallic taste. The life expectancy of galvanized piping is about 70 years, but it may vary by region due to impurities in the water supply and the proximity of electrical grids for which interior piping acts as a pathway (the flow of electricity can accelerate chemical corrosion). Pipe longevity also depends on the thickness of zinc in the original galvanizing, which ranges on a scale from G40 to G210, and whether the pipe was galvanized on both the inside and outside, or just the outside. 

Since World War II, copper and plastic piping have replaced galvanized piping for interior drinking water service, but galvanized steel pipes are still used in outdoor applications requiring steel's superior mechanical strength.
The use of galvanized pipes lends some truth to the urban myth that water purity in outdoor water faucets is lower, but the actual impurities (iron, zinc, calcium) are harmless.

The presence of galvanized piping detracts from the appraised value of housing stock because piping can fail, increasing the risk of water damage. Galvanized piping will eventually need to be replaced if housing stock is to outlast a 50 to 70 year life expectancy, and some jurisdictions require galvanized piping to be replaced before sale. One option to extend the life expectancy of existing galvanized piping is to line it with an epoxy resin.



</doc>
<doc id="12859" url="https://en.wikipedia.org/wiki?curid=12859" title="Golden Rule">
Golden Rule

The Golden Rule (which can be considered a law of reciprocity in some religions) is the principle of treating others as one would wish to be treated. It is a maxim that is found in many religions and cultures.
The maxim may appear as either a positive or negative injunction governing conduct:


The concept occurs in some form in nearly every religion and ethical tradition. It can also be explained from the perspectives of psychology, philosophy, sociology, human evolution, and economics. Psychologically, it involves a person empathizing with others. Philosophically, it involves a person perceiving their neighbor also as "I" or "self". Sociologically, "love your neighbor as yourself" is applicable between individuals, between groups, and also between individuals and groups. In evolution, "reciprocal altruism" is seen as a distinctive advance in the capacity of human groups to survive and reproduce, as their exceptional brains demanded exceptionally long childhoods and ongoing provision and protection even beyond that of the immediate family. In economics, Richard Swift, referring to ideas from David Graeber, suggests that "without some kind of reciprocity society would no longer be able to exist."

Some sources identify the Golden Rule—often considered the central tenet of Christian ethics—as different from the maxim of reciprocity in Christianity. This is captured in "do ut des"—"I give so that you will give in return"—and is a unilateral moral commitment to the well-being of the other without the expectation of anything in return.

The term "Golden Rule", or "Golden law", began to be used widely in the early 17th century in Britain by Anglican theologians and preachers; the earliest known usage is that of Anglicans Charles Gibbon and Thomas Jackson in 1604.

Possibly the earliest affirmation of the maxim of reciprocity, reflecting the ancient Egyptian goddess Ma'at, appears in the story of The Eloquent Peasant, which dates to the Middle Kingdom (c. 2040–1650 BC): "Now this is the command: Do to the doer to make him do." This proverb embodies the "do ut des" principle. A Late Period (c. 664–323 BC) papyrus contains an early negative affirmation of the Golden Rule: "That which you hate to be done to you, do not do to another."

In "Mahābhārata", the ancient epic of India, there is a discourse in which the wise minister Vidura advises the King Yuddhiśhṭhira
In Chapter 32 in the Part on Virtue of the Tirukkuṛaḷ (c. 200 BC – c. 500 AD), Tiruvalluvar says: "Do not do to others what you know has hurt yourself" (K. 316.); "Why does one hurt others knowing what it is to be hurt?" (K. 318). He furthermore opined that it is the determination of the spotless (virtuous) not to do evil, even in return, to those who have cherished enmity and done them evil. (K. 312) The (proper) punishment to those who have done evil (to you), is to put them to shame by showing them kindness, in return and to forget both the evil and the good done on both sides (K. 314)

The Golden Rule in its prohibitive (negative) form was a common principle in ancient Greek philosophy. Examples of the general concept include:

The Pahlavi Texts of Zoroastrianism (c. 300 BC–1000 AD) were an early source for the Golden Rule: "That nature alone is good which refrains from doing to another whatsoever is not good for itself." Dadisten-I-dinik, 94,5, and "Whatever is disagreeable to yourself do not do unto others." Shayast-na-Shayast 13:29

Seneca the Younger (c. 4 BC–65 AD), a practitioner of Stoicism (c. 300 BC–200 AD) expressed the Golden Rule in his essay regarding the treatment of slaves: "Treat your inferior as you would wish your superior to treat you."

According to Simon Blackburn, the Golden Rule "can be found in some form in almost every ethical tradition". 

A rule of altruistic reciprocity was first stated positively in a well-known Torah verse (Hebrew: ):
Hillel the Elder (c. 110 BC – 10 AD), used this verse as a most important message of the Torah for his teachings. Once, he was challenged by a gentile who asked to be converted under the condition that the Torah be explained to him while he stood on one foot. Hillel accepted him as a candidate for conversion to Judaism but, drawing on , briefed the man:
Hillel recognized brotherly love as the fundamental principle of Jewish ethics. Rabbi Akiva agreed and suggested that the principle of love must have its foundation in Genesis chapter 1, which teaches that all men are the offspring of Adam, who was made in the image of God (Sifra, Ḳedoshim, iv.; Yer. Ned. ix. 41c; Genesis Rabba 24). According to Jewish rabbinic literature, the first man Adam represents the "unity of mankind". This is echoed in the modern preamble of the Universal Declaration of Human Rights. And it is also taught, that Adam is last in order according to the evolutionary character of God's creation:Why was only a single specimen of man created first? To teach us that he who destroys a single soul destroys a whole world and that he who saves a single soul saves a whole world; furthermore, so no race or class may claim a nobler ancestry, saying, 'Our father was born first'; and, finally, to give testimony to the greatness of the Lord, who caused the wonderful diversity of mankind to emanate from one type. And why was Adam created last of all beings? To teach him humility; for if he be overbearing, let him remember that the little fly preceded him in the order of creation.

The Jewish Publication Society's edition of Leviticus states:Thou shalt not hate thy brother. in thy heart; thou shalt surely rebuke thy neighbour, and not bear sin because of him. 18 Thou shalt not take vengeance, nor bear any grudge against the children of thy people, but thou shalt love thy neighbour as thyself: I am the LORD.This Torah verse represents one of several versions of the "Golden Rule", which itself appears in various forms, positive and negative. It is the earliest written version of that concept in a positive form.

At the turn of the eras, the Jewish rabbis were discussing the scope of the meaning of Leviticus 19:18 and 19:34 extensively:

Commentators summed up foreigners (= Samaritans), proselytes (= 'strangers who resides with you') (Rabbi Akiva, bQuid 75b) or Jews (Rabbi Gamaliel, yKet 3, 1; 27a) to the scope of the meaning.

On the verse, "Love your fellow as yourself," the classic commentator Rashi quotes from Torat Kohanim, an early Midrashic text regarding the famous dictum of Rabbi Akiva: "Love your fellow as yourself – Rabbi Akiva says this is a great principle of the Torah."

Israel's postal service quoted from the previous Leviticus verse when it commemorated the Universal Declaration of Human Rights on a 1958 postage stamp.

The "Golden Rule" was given by Jesus of Nazareth (Matthew 7:12 NCV, see also ). The common English phrasing is "Do unto others as you would have them do unto you". A similar form of the phrase appeared in a Catholic catechism around 1567 (certainly in the reprint of 1583).
The Golden Rule is stated positively numerous times in the Hebrew Pentateuch as well as the Prophets and Writings. ("Forget about the wrong things people do to you, and do not try to get even. Love your neighbor as you love yourself."; see also Great Commandment) and ("But treat them just as you treat your own citizens. Love foreigners as you love yourselves, because you were foreigners one time in Egypt. I am the Lord your God.").

The Old Testament Deuterocanonical books of Tobit and Sirach, accepted as part of the Scriptural canon by Catholic Church, Eastern Orthodoxy, and the Non-Chalcedonian Churches, express a negative form of the golden rule:

Two passages in the New Testament quote Jesus of Nazareth espousing the positive form of the Golden rule:

A similar passage, a parallel to the Great Commandment, is 

The passage in the book of Luke then continues with Jesus answering the question, "Who is my neighbor?", by telling the parable of the Good Samaritan, indicating that "your neighbor" is anyone in need. This extends to all, including those who are generally considered hostile.

Jesus' teaching goes beyond the negative formulation of not doing what one would not like done to themselves, to the positive formulation of actively doing good to another that, if the situations were reversed, one would desire that the other would do for them. This formulation, as indicated in the parable of the Good Samaritan, emphasizes the needs for positive action that brings benefit to another, not simply restraining oneself from negative activities that hurt another. Taken as a rule of judgment, both formulations of the golden rule, the negative and positive, are equally applicable.

In one passage of the New Testament, Paul the Apostle refers to the golden rule:

The Arabian peninsula was known to not practice the golden rule prior to the advent of Islam. "Pre-Islamic Arabs regarded the survival of the tribe, as most essential and to be ensured by the ancient rite of blood vengeance"

However, this all changed when Muhammad came on the scene:

From the hadith, the collected oral and written accounts of Muhammad and his teachings during his lifetime:

Ali ibn Abi Talib (4th Caliph in Sunni Islam, and first Imam in Shia Islam) says:

Also,
Buddha (Siddhartha Gautama, c. 623–543 BC) made this principle one of the cornerstones of his ethics in the 6th century BC. It occurs in many places and in many forms throughout the Tripitaka.

The Golden Rule is paramount in the Jainist philosophy and can be seen in the doctrines of Ahimsa and Karma. As part of the prohibition of causing any living beings to suffer, Jainism forbids inflicting upon others what is harmful to oneself.

The following quotation from the Acaranga Sutra sums up the philosophy of Jainism:

Saman Suttam of Jinendra Varni gives further insight into this precept:-

The same idea is also presented in V.12 and VI.30 of the "Analects" (c. 500 BC), which can be found in the online Chinese Text Project. The phraseology differs from the Christian version of the Golden Rule. It does not presume to do anything unto others, but merely to avoid doing what would be harmful. It does not preclude doing good deeds and taking moral positions, but there is slim possibility for a Confucian missionary outlook, such as one can justify with the Christian Golden Rule.

Mozi regarded the golden rule as a corollary to the cardinal virtue of impartiality, and encouraged egalitarianism and selflessness in relationships.

Do not do unto others whatever is injurious to yourself. -- Shayast-na-Shayast 13.29

The writings of the Bahá'í Faith encourages everyone to treat others as they would treat themselves and even prefer others over oneself:

The Way to Happiness expresses the Golden Rule both in its negative/prohibitive form and in its positive form. The negative/prohibitive form is expressed in Precept 19 as:

The positive form is expressed in Precept 20 as:

The "Declaration Toward a Global Ethic" from the Parliament of the World’s Religions (1993) proclaimed the Golden Rule ("We must treat others as we wish others to treat us") as the common principle for many religions. The Initial Declaration was signed by 143 leaders from all of the world's major faiths, including Baha'i Faith, Brahmanism, Brahma Kumaris, Buddhism, Christianity, Hinduism, Indigenous, Interfaith, Islam, Jainism, Judaism, Native American, Neo-Pagan, Sikhism, Taoism, Theosophist, Unitarian Universalist and Zoroastrian. In the folklore of several cultures the Golden Rule is depicted by the allegory of the long spoons.

Various sources identify the Golden Rule as a humanist principle:

In the view of Greg M. Epstein, a Humanist chaplain at Harvard University, " 'do unto others' ... is a concept that essentially no religion misses entirely. "But not a single one of these versions of the golden rule requires a God"". At least the Biblical accounts, however, portray the obligation to love one's neighbor as oneself as a corollary of a more basic obligation to love God with one's entire being.

According to Marc H. Bornstein, and William E. Paden, the Golden Rule is arguably the most essential basis for the modern concept of human rights, in which each individual has a right to just treatment, and a reciprocal responsibility to ensure justice for others.

However Leo Damrosch argued that the notion that the Golden Rule pertains to "rights" per se is a contemporary interpretation and has nothing to do with its origin. The development of human "rights" is a modern political ideal that began as a philosophical concept promulgated through the philosophy of Jean Jacques Rousseau in 18th century France, among others. His writings influenced Thomas Jefferson, who then incorporated Rousseau's reference to "inalienable rights" into the United States Declaration of Independence in 1776. Damrosch argued that to confuse the Golden Rule with human rights is to apply contemporary thinking to ancient concepts.

There has been research published arguing that some 'sense' of fair play and the Golden Rule may be stated and rooted in terms of neuroscientific and neuroethical principles.

Philosophers, such as Immanuel Kant and Friedrich Nietzsche, have objected to the rule on a variety of grounds. The most serious among these is its application. How does one know how others want to be treated? The obvious way is to ask them, but this cannot be done if one assumes they have not reached a particular and relevant understanding.

George Bernard Shaw wrote, "Do not do unto others as you would that they should do unto you. Their tastes may be different." This suggests that if your values are not shared with others, the way you want to be treated will not be the way they want to be treated. Hence, the Golden Rule of "do unto others" is "dangerous in the wrong hands," according to philosopher Iain King, because "some fanatics have no aversion to death: the Golden Rule might inspire them to kill others in suicide missions."

Immanuel Kant famously criticized the golden rule for not being sensitive to differences of situation, noting that a prisoner duly convicted of a crime could appeal to the golden rule while asking the judge to release him, pointing out that the judge would not want anyone else to send him to prison, so he should not do so to others. Kant's "Categorical Imperative", introduced in "Groundwork of the Metaphysic of Morals", is often confused with the Golden Rule.

In his book "How to Make Good Decisions and Be Right All the Time", philosopher Iain King has argued that "(although) the idea of mirroring your treatment of others with their treatment of you is very widespread indeed… most ancient wisdoms express this negatively – advice on what you should not do, rather than what you should." He argues this creates a bias in favour of inertia which allows bad actions and states of affairs to persist. The positive formulation, meanwhile, can be "incendiary", since it "can lead to cycles of tit-for-tat reciprocity," unless it is accompanied by a corrective mechanism, such as a concept of forgiveness. Therefore, he concludes that there can be no viable formulation of the Golden Rule, unless it is heavily qualified by other maxims.

Walter Terence Stace, in "The Concept of Morals" (1937), wrote:

Marcus George Singer observed that there are two importantly different ways of looking at the golden rule: as requiring (1) that you perform specific actions that you want others to do to you or (2) that you guide your behavior in the same general ways that you want others to. Counter-examples to the golden rule typically are more forceful against the first than the second.

In his book on the golden rule, Jeffrey Wattles makes the similar observation that such objections typically arise while applying the golden rule in certain general ways (namely, ignoring differences in taste, in situation, and so forth). But if we apply the golden rule to our own method of using it, asking in effect if we would want other people to apply the golden rule in such ways, the answer would typically be no, since it is quite predictable that others' ignoring of such factors will lead to behavior which we object to. It follows that we should not do so ourselves—according to the golden rule. In this way, the golden rule may be self-correcting. An article by Jouni Reinikainen develops this suggestion in greater detail.

It is possible, then, that the golden rule can itself guide us in identifying which differences of situation are morally relevant. We would often want other people to ignore any prejudice against our race or nationality when deciding how to act towards us, but would also want them to not ignore our differing preferences in food, desire for aggressiveness, and so on. This principle of "doing unto others, wherever possible, as "they" would be done by..." has sometimes been termed the platinum rule.

Charles Kingsley's "The Water Babies" (1863) includes a character named Mrs Do-As-You-Would-Be-Done-By (and another, Mrs Be-Done-By-As-You-Did).




</doc>
<doc id="12861" url="https://en.wikipedia.org/wiki?curid=12861" title="Governor of New York">
Governor of New York

The Governor of New York is the chief executive of the U.S. state of New York. The governor is the head of the executive branch of New York's state government and the commander-in-chief of the state's military and naval forces.

The current governor is Andrew Cuomo, a Democrat. Cuomo won the November 2010 gubernatorial election and was sworn in as the 56th governor of the state of New York on January 1, 2011. Cuomo was re-elected on November 5, 2014, defeating his Republican challenger Robert Astorino.

The governor has a duty to enforce state laws, and the power to either approve or veto bills passed by the New York State Legislature, to convene the legislature, and to grant pardons, except in cases of treason and impeachment. Unlike the other government departments that compose the executive branch of government, the governor is the head of the state Executive Department. The officeholder is afforded the courtesy style of "His/Her Excellency" while in office.

The governor of New York is often considered a potential candidate for President. Ten governors have been major-party candidates for president, and four, Martin Van Buren, Grover Cleveland, Theodore Roosevelt, and Franklin D. Roosevelt have won. Six New York governors have gone on to serve as vice president. Additionally two Governors of New York, John Jay and Charles Evans Hughes, have served as Chief Justice of the United States.

Under the New York State Constitution, a person must be at least 30 years of age, a United States citizen, and a resident of the state of New York for at least five years prior to being elected to serve as governor.

The office of Governor was established by the first New York State Constitution in 1777 to coincide with the calendar year. An 1874 amendment extended the term of office to three years, but the 1894 constitution reduced it to two years. The most recent constitution of 1938 extended the term to the current four years.

The Constitution of New York has provided since 1777 for the election of a Lieutenant Governor of New York, who also acts as President of the State Senate, to the same term (keeping the same term lengths as the governor throughout all the constitutional revisions). Originally, in the event of the death, resignation or impeachment of the governor, or absence from the state, the lieutenant governor would take on the governor's duties and powers. Since the 1938 constitution, the lieutenant governor explicitly becomes governor upon such vacancy in the office.

Should the office of lieutenant governor become vacant, the president "pro tempore" of the state senate performs the duties of a lieutenant governor until the governor can take back the duties of the office, or the next election; likewise, should both offices become vacant, the president "pro tempore" acts as governor, with the office of lieutenant governor remaining vacant. Although no provision exists in the constitution for it, precedent set in 2009 allows the governor to appoint a lieutenant governor should a vacancy occur. Should the president "pro tempore" be unable to fulfill the duties, the speaker of the assembly is next in the line of succession. The lieutenant governor is elected on the same ticket as the governor, but nominated separately.

Line of succession in full




</doc>
<doc id="12862" url="https://en.wikipedia.org/wiki?curid=12862" title="Glasnevin">
Glasnevin

Glasnevin (, also known as "Glas Naedhe", meaning "stream of O'Naeidhe" after an ancient chieftain) is a largely residential neighbourhood of Dublin, Ireland. Dublin City University has its main campus and other facilities in the area.

Glasnevin is also a civil parish in the ancient barony of Coolock.

A mainly residential neighbourhood, Glasnevin is located on the Northside of the city of Dublin (about 3 km north of Dublin City centre). It was established on the northern bank of the River Tolka, and now extends north and south of it. At least two streams flow into the Tolka in the area, and can be seen near the Catholic church. One comes from the vicinity of Glasnevin Cemetery and one from the north. In addition, a major diversion from the Wad River comes from the Ballymun area.

Glasnevin is bordered to the north by Finglas, northeast by Ballymun and Santry, Whitehall to the east, Phibsboro and Drumcondra to the south and Cabra to the west.

Glasnevin is part of the Dáil Éireann constituency of Dublin Central and Dublin North-West

Glasnevin seems to have been founded by Saint Mobhi (sometimes known as St Berchan) in the sixth (or perhaps fifth) century as a monastery. His monastery continued to be used for many years afterwards - St. Colman is recorded as having paid homage to its founder when he returned from abroad to visit Ireland a century after St Mobhi's death in 544. St. Columba of Iona is thought to have studied under St. Mobhi, but left Glasnevin following an outbreak of plague and journeyed north to open the House at Derry; there is a long street (Iona Road) in Glasnevin named in his honour and the church on Iona Road is called Saint Columba's.

A settlement grew up around the monastery, which survived until the Viking invasions in the eighth century. After raids on monasteries at Glendalough and Clondalkin, the monasteries at Glasnevin and Finglas were attacked and destroyed.

By 822 Glasnevin, along with Grangegorman and Clonken or Clonkene (now known as Deansgrange), had become parts of the grange (farm) of Christ Church Cathedral and it seems to have maintained this connection up to the time of the Reformation.

The Battle of Clontarf was fought on the banks of the River Tolka in 1014 (a field called the "bloody acre" is supposed to be part of the site). The Irish defeated the Danes in a battle, in which 7,000 Danes and 4,000 Irish died.

The 12th century saw the Normans (who had conquered England and Wales in the eleventh century) invade Ireland. As local rulers continued fighting amongst themselves the Norman King of England Henry II was invited to intervene. He arrived in 1171, took control of much land, and then parcelled it out amongst his supporters. Glasnevin ended up under the jurisdiction of Finglas Abbey. Later, Laurence O'Toole, Archbishop of Dublin, took responsibility for Glasnevin and it became the property of the Priory of the Most Holy Trinity (Christ Church Cathedral).

In 1240 a church and tower was reconstructed on the site of the Church of St. Mobhi in the monastery. The returns of the church for 1326 stated that 28 tenants resided in Glasnevin. The church was enlarged in 1346, along with a small hall known as the Manor Hall.

When King Henry VIII broke from Rome an era of religious repression began. During the Dissolution of the Monasteries, Catholic Church property and land was appropriated to the new Church of England, and monasteries (including the one at Glasnevin) were forcibly closed and fell into ruin. Glasnevin had at this stage developed as a village, with its principal landmark and focal point being its "bull-ring" noted in 1542.

By 1667 Glasnevin had expanded - but not by very much; it is recorded as containing 24 houses. The development of the village was given a fresh impetus when Sir John Rogerson built his country residence - "The Glen" or "Glasnevin House" - outside the village.

The plantations of Ireland saw the settlement of Protestant English families on land previously held by Catholics. Lands at Glasnevin were leased to such families and a Protestant church was erected there in 1707. It was built on the site of the old Catholic Church and was named after St. Mobhi. The church was largely rebuilt in the mid-18th century. The attached churchyard became a graveyard for both Protestants and Catholics. It is said that Robert Emmet is buried there, this claim being made because once somebody working in the graveyard there dug up a headless body.

By now Glasnevin was an area for families of distinction - in spite of a comment attributed to the Protestant Archbishop King of Dublin that "when any couple had a mind to be wicked, they would retire to Glasnevin". In a letter, dated 1725 he described Glasnevin as "the receptacle for thieves and rogues. The first search when anything was stolen, was there, and when any couple had a mind to retire to be wicked there was their harbour. But since the church was built, and service regularly settled, all these evils are banished. Good houses are built in it, and the place civilised." Glasnevin National School was also built during this period.

In the 1830's, the civil parish population was recorded as 1,001, of whom 559 resided in the village. Glasnevin was described as a parish in the barony of Coolock, pleasantly situated and the residence of many families of distinction.

On 1 June 1832, Charles Lindsay, Bishop of Kildare and Leighlin and the William John released their holdings of Sir John Rogerson's lands at Glasnevin, (including Glasnevin House) to George Hayward Lindsay. This transfer included the sum of 1,500 Pounds Sterling. Although this does not specifically cite the marriage of George Hayward Lindsay to Lady Mary Catherine Gore, George Lindsay almost certainly came into the lands at Glasnevin as a result of his marriage.

When Drumcondra began to rapidly expand in the 1870s, the residents of Glasnevin sought to protect their district and opposed being merged with the neighbouring suburb. One of the objectors was the property-owner, Dr Gogarty, the father of the Irish poet, Oliver St. John Gogarty.

Glasnevin became a township in 1878 and became part of the City of Dublin in 1900 under the Dublin Boundaries Act, which received the Royal Assent on August 6, 1900.

George Hayward Lindsay's eldest son, Lieutenant Colonel Henry Gore Lindsay, was in possession of his father's lands at Glasnevin when the area began to be developed at the beginning of the twentieth-century. The development of his lands after 1903/04 marked the start of the gradual development of the area.

Glasnevin remained relatively undeveloped until the opening up of the Carroll Estate in 1914, which saw the creation of the redbrick residential roads running down towards Drumcondra. The process was accelerated by Dublin Corporation in the 1920s and the present shape of the suburb was firmly in place by 1930. Nevertheless, until comparatively recent years, a short stroll up the Old Finglas Road brought you rapidly into open countryside.

The start of the 20th century also saw the opening of a short lived railway station on the Drumcondra and North Dublin Link Railway line from Glasnevin Junction to Connolly Station (then Amiens Street). It opened in 1906 and closed at the end of 1907. Glasnevin railway station opened on 1 April 1901 and closed on 1 December 1910.

The village has changed a lot over the years, and is now part of Dublin city. It is now populated by a mix of young families, senior citizens and students attending Dublin City University.

As well as the amenities of the National Botanic Gardens (Ireland) and local parks, the national meteorological office Met Éireann, the Fisheries Board, the National Standards Authority of Ireland, Sustainable Energy Ireland, the National Metrology Laboratory (NML), the Department of Defence and the national enterprise and trade board Enterprise Ireland are all located in the area.

The house and lands of the poet Thomas Tickell were sold in 1790 to the Irish Parliament and given to the Royal Dublin Society for them to establish Ireland's first Botanic Gardens. The gardens were the first location in Ireland where the infection responsible for the 1845–1847 potato famine was identified. Throughout the famine research to stop the infection was undertaken at the gardens.

The which border the River Tolka also adjoin the Prospect Cemetery. In 2002 the Botanic Gardens gained a new two-storey complex which included a new cafe and a large lecture theatre. The Irish National Herbarium is also located at the botanic gardens.

Prospect Cemetery is located in Glasnevin, although better known as Glasnevin Cemetery, the most historically notable burial place in the country and the last resting place, among a host of historical figures, of Michael Collins, Eamon DeValera, Charles Stewart Parnell and also Arthur Griffith. This graveyard led to Glasnevin being known as "the dead centre of Dublin". It opened in 1832 and is the final resting place for thousands of ordinary citizens, as well as many Irish patriots.

Approaching Glasnevin via Phibsboro is what is known as "Hart's Corner" but which about 200 years ago was called Glasmanogue, and was then a well-known stage on the way to Finglas. At an earlier date the name possessed a wider signification and was applied to a considerable portion of the adjoining district.

At the start of the 18th century a large house, called Delville - known at first as The Glen - was built on the site of the present Bon Secours Hospital, Dublin. Its name was an amalgamation of the surnames of two tenants, Dr. Helsam and Dr. Patrick Delany (as Heldeville), both Fellows of Trinity College.

When Delany married his first wife he acquired sole ownership, but it became famous as the home of Delany and his second wife - Mary Pendarves. She was a widow whom Delany married in 1743, and was an accomplished letter writer.

They couple were friends of Dean Jonathan Swift and, through him, of Alexander Pope. Pope encouraged the Delaneys to develop a garden in a style then becoming popular in England - moving away from the very formal, geometric layout that was common. He redesigned the house in the style of a villa and had the gardens laid out in the latest Dutch fashion creating what was almost certainly Ireland's first naturalistic garden.

The house was, under Mrs Delany, a centre of Dublin's intellectual life. Swift is said to have composed many of his campaigning pamphlets while staying there. He and his life - long companion Stella were both in the habit of visiting, and Swift satirised the grounds which he considered too small for the size of the house. Through her correspondence with her sister, Mrs Dewes, Mary wrote of Swift in 1733: "he calls himself my master and corrects me when I speak bad English or do not pronounce my words distinctly".

Patrick Delany died in 1768 at the age of 82, prompting his widow to sell Delville and return to her native England until her death twenty years later.

Glasnevin is also a parish in the Fingal South West deanery of the Roman Catholic Archdiocese of Dublin. It is served by the Church of Lady of Dolours. The church underwent some refurbishment work inside and in its grounds and car park during the first half of 2011. A timber church, which originally stood on Berkeley Road, was moved to a riverside site on Botanic Avenue early in the twentieth century. The altar in this church was from Newgate prison in Dublin. It served as the parish church until it was replaced, in 1972, by a structure resembling a pyramid when viewed from Botanic Avenue. The previous church was known locally as "The Woodener" or "The Wooden" and the new building is still known to older residents as "The new Woodener" or "The Wigwam".

In 1975 the new headquarters of Met Éireann, the Irish Meteorological Office, opened just off Glasnevin Hill, on the former site of Marlborough House. The Met Éireann building too was built in a somewhat pyramidal shape and is recognised as one of the most significant, smaller commercial buildings, to be erected in Dublin in the 1970s.

Griffith Avenue, which runs through Glasnevin, Drumcondra and Marino. The avenue spans three electoral constituencies. It was named after Arthur Griffith who was the founder and third leader of Sinn Féin and also served as President of Dáil Éireann. Arthur Griffith also was buried in Glasnevin Cemetery.

The Gaelic games of Gaelic football, hurling, camogie and Gaelic handball are all organised locally by Na Fianna CLG, while soccer is played by local clubs Iona FC(now defunct), Tolka Rovers, Glasnevin FC and Glasnaion FC. They celebrated their 125th anniversary in 2006. Basketball is organised by Tolka Rovers. Tennis is played in Charleville Lawn Tennis Club, founded in 1894 by a small group of tennis enthusiasts headed by a Mrs McConnell. Charleville took its name from the original location at the corner of the Charleville and Cabra Roads. The move to its present location on Whitworth Road took place in 1904. The club boasts a membership of 400 senior and junior members and the club has won many Dublin Lawn Tennis Council titles, above the average for a club of their size. Hockey is also played in Botanic Hockey club on the Old Finglas Road. Glasnevin Boxing Club and Football(soccer) club has a clubhouse on Mobhi road.

Scouting has a strong tradition in Glasnevin with 1st Dublin (L.H.O) Scout Troop located on the corner of Griffith Avenue and Ballygall Road East. The Scout group celebrated 100 years of Scouting in 2011 making it one of the longest established Scout Groups in the world.

There are several primary schools in Glasnevin, including Lindsay Road National School, Glasnevin National School, Glasnevin Educate Together National school, North Dublin National School Project, Scoil Mobhi, St.Brigids GNS, St.Columbas NS and St.Vincents CBS.

There are several Roman Catholic secondary schools in the area St Vincent's (Christian Brothers) School, Scoil Chaitríona and St Mary's Secondary School.

Billy Whelan, one of the eight Manchester United players who lost their lives in the Munich air disaster of 6 February 1958, was born locally on 1 April 1935. He is buried in Glasnevin Cemetery.





</doc>
<doc id="12863" url="https://en.wikipedia.org/wiki?curid=12863" title="George Abbot (author)">
George Abbot (author)

George Abbot or Abbott (1604 — 2 February 1649) was an English lay writer, known as "The Puritan", and a politician who sat in the House of Commons in two periods between 1640 and 1649. He is known also for his part in defending Caldecote House against royalist forces in the early days of the English Civil War.

Abbott was the son of George Abbott of York (died 1607) and his wife Joan Penkeston. While "Alumni Cantabrigienses" states that he matriculated at King's College, Cambridge in 1622, the "Oxford Dictionary of National Biography" discounts the identification, for lack of evidence. He owned property in Baddesley Clinton, Warwickshire, and was a good friend of Richard Vines, minister at Caldecote some way to the east. In April 1640, he was elected Member of Parliament for Tamworth in the Short Parliament. 
In the English Civil War, Abbot worked closely in Warwickshire with his stepfather William Purefoy, and made a notable defence, with his mother Joan, of the Purefoy house at Caldecote, Warwickshire, gaining the family coverage in the London press. On 15 August 1642, with eight men, his mother and maids, he held out for a time against Prince Rupert of the Rhine, with about 18 troops of horses and dragoons. In the aftermath of the Battle of Edgehill, in October of the same year, Richard Baxter moved to Coventry, and Abbot was one of those hearing him preach there. Baxter in writing on the Sabbath referred to "my dear friend Mr. George Abbot". In his memoirs "Reliquiæ Baxterianæ", Baxter placed Abbot's defence of Caldecote House, where barns were burnt, in local context: royalists under Spencer Compton, 2nd Earl of Northampton were attacking Warwick Castle, defended by John Bridges, and Coventry, defended by John Barker.

Abbot was re-elected MP for Tamworth in 1645 for the Long Parliament and held the seat until his death in 1649. He died unmarried in his 44th year, and was buried in Caldecote church where his monument describes his defence of Caldecote.

By his will, Abbot endowed a free school at Caldecote. It was supported by land left to it at Baddesley Ensor.

Abbot was a lay theologian and scholar. His "Whole Booke of Job Paraphrased, or made easy for any to understand" (1640), was written in a terse style, and his "Vindiciae Sabbathi" (1641) influenced the Sabbatarian controversy. His "The Whole Book of Psalms Paraphrased" (1650) was published posthumously by Richard Vines, and dedicated to Joan Purefoy, his mother.

Abbot has been confused with others of the same name and has been described as a clergyman, which he never was. His writings have been incorrectly attributed in the bibliographical authorities to a relation of George Abbot the archbishop of Canterbury. One of the sons of Sir Morris Abbot called George was also an MP in the Long Parliament but for the constituency of Guildford.

 

 


</doc>
<doc id="12866" url="https://en.wikipedia.org/wiki?curid=12866" title="Globular cluster">
Globular cluster

A globular cluster is a spherical collection of stars that orbits a galactic core as a satellite. Globular clusters are very tightly bound by gravity, which gives them their spherical shapes and relatively high stellar densities toward their centers. The name of this category of star cluster is derived from the Latin "globulus"—a small sphere. A globular cluster is sometimes known more simply as a "globular".

Globular clusters are found in the halo of a galaxy and contain considerably more stars and are much older than the less dense open clusters, which are found in the disk of a galaxy. Globular clusters are fairly common; there are about 150 to 158 currently known globular clusters in the Milky Way, with perhaps 10 to 20 more still undiscovered. Larger galaxies can have more: Andromeda Galaxy, for instance, may have as many as 500. Some giant elliptical galaxies (particularly those at the centers of galaxy clusters), such as M87, have as many as 13,000 globular clusters.

Every galaxy of sufficient mass in the Local Group has an associated group of globular clusters, and almost every large galaxy surveyed has been found to possess a system of globular clusters. The Sagittarius Dwarf galaxy and the disputed Canis Major Dwarf galaxy appear to be in the process of donating their associated globular clusters (such as Palomar 12) to the Milky Way. This demonstrates how many of this galaxy's globular clusters might have been acquired in the past.

Although it appears that globular clusters contain some of the first stars to be produced in the galaxy, their origins and their role in galactic evolution are still unclear. It does appear clear that globular clusters are significantly different from dwarf elliptical galaxies and were formed as part of the star formation of the parent galaxy rather than as a separate galaxy.

The first known globular cluster, now called M22, was discovered in 1665 by Abraham Ihle, a German amateur astronomer. However, given the small aperture of early telescopes, individual stars within a globular cluster were not resolved until Charles Messier observed M4 in 1764. The first eight globular clusters discovered are shown in the table. Subsequently, Abbé Lacaille would list NGC 104, NGC 4833, M55, M69, and NGC 6397 in his 1751–52 catalogue. The "M" before a number refers to Charles Messier's catalogue, while "NGC" is from the New General Catalogue by John Dreyer.

When William Herschel began his comprehensive survey of the sky using large telescopes in 1782 there were 34 known globular clusters. Herschel discovered another 36 himself and was the first to resolve virtually all of them into stars. He coined the term "globular cluster" in his "Catalogue of a Second Thousand New Nebulae and Clusters of Stars" published in 1789.

The number of globular clusters discovered continued to increase, reaching 83 in 1915, 93 in 1930 and 97 by 1947. A total of 152 globular clusters have now been discovered in the Milky Way galaxy, out of an estimated total of 180 ± 20. These additional, undiscovered globular clusters are believed to be hidden behind the gas and dust of the Milky Way.

Beginning in 1914, Harlow Shapley began a series of studies of globular clusters, published in about 40 scientific papers. He examined the RR Lyrae variables in the clusters (which he assumed were Cepheid variables) and used their period–luminosity relationship for distance estimates. Later, it was found that RR Lyrae variables are fainter than Cepheid variables, which caused Shapley to overestimate the distances of the clusters.
Of the globular clusters within the Milky Way, the majority are found in a halo around the galactic core, and the large majority are located in the celestial sky centered on the core. In 1918, this strongly asymmetrical distribution was used by Shapley to make a determination of the overall dimensions of the galaxy. By assuming a roughly spherical distribution of globular clusters around the galaxy's center, he used the positions of the clusters to estimate the position of the Sun relative to the galactic center. While his distance estimate was in significant error (although within the same order of magnitude as the currently accepted value), it did demonstrate that the dimensions of the galaxy were much greater than had been previously thought. His error was due to interstellar dust in the Milky Way, which absorbs and diminishes the amount of light from distant objects, such as globular clusters, that reaches the Earth, thus making them appear to be more distant than they are.

Shapley's measurements also indicated that the Sun is relatively far from the center of the galaxy, also contrary to what had previously been inferred from the apparently nearly even distribution of ordinary stars. In reality, most ordinary stars lie within the galaxy's disk and those stars that lie in the direction of the galactic centre and beyond are thus obscured by gas and dust, whereas globular clusters lie outside the disk and can be seen at much further distances.

Shapley was subsequently assisted in his studies of clusters by Henrietta Swope and Helen Battles Sawyer (later Hogg). In 1927–29, Shapley and Sawyer categorized clusters according to the degree of concentration each system has toward its core. The most concentrated clusters were identified as Class I, with successively diminishing concentrations ranging to Class XII. This became known as the Shapley–Sawyer Concentration Class (it is sometimes given with numbers [Class 1–12] rather than Roman numerals.) In 2015, a new type of globular cluster was proposed on the basis of observational data, the dark globular clusters.
The formation of globular clusters remains a poorly understood phenomenon and it remains uncertain whether the stars in a globular cluster form in a single generation or are spawned across multiple generations over a period of several hundred million years. In many globular clusters, most of the stars are at approximately the same stage in stellar evolution, suggesting that they formed at about the same time. However, the star formation history varies from cluster to cluster, with some clusters showing distinct populations of stars. An example of this is the globular clusters in the Large Magellanic Cloud (LMC) that exhibit a bimodal population. During their youth, these LMC clusters may have encountered giant molecular clouds that triggered a second round of star formation. This star-forming period is relatively brief, compared to the age of many globular clusters.
It has also been proposed that the reason for this multiplicity in stellar populations could have a dynamical origin. In the Antennae galaxy, for example, the Hubble Space Telescope has observed clusters of clusters, regions in the galaxy that span hundreds of parsecs, where many of the clusters will eventually collide and merge. Many of them present a significant range in ages, hence possibly metallicities, and their merger could plausibly lead to clusters with a bimodal or even multiple distribution of populations. 

Observations of globular clusters show that these stellar formations arise primarily in regions of efficient star formation, and where the interstellar medium is at a higher density than in normal star-forming regions. Globular cluster formation is prevalent in starburst regions and in interacting galaxies. Research indicates a correlation between the mass of a central supermassive black holes (SMBH) and the extent of the globular cluster systems of elliptical and lenticular galaxies. The mass of the SMBH in such a galaxy is often close to the combined mass of the galaxy's globular clusters.

No known globular clusters display active star formation, which is consistent with the view that globular clusters are typically the oldest objects in the Galaxy, and were among the first collections of stars to form. Very large regions of star formation known as super star clusters, such as Westerlund 1 in the Milky Way, may be the precursors of globular clusters.

Globular clusters are generally composed of hundreds of thousands of low-metal, old stars. The type of stars found in a globular cluster are similar to those in the bulge of a spiral galaxy but confined to a volume of only a few million cubic parsecs. They are free of gas and dust and it is presumed that all of the gas and dust was long ago turned into stars.

Globular clusters can contain a high density of stars; on average about 0.4 stars per cubic parsec, increasing to 100 or 1000 stars per cubic parsec in the core of the cluster. 
The typical distance between stars in a globular cluster is about 1 light year, but at its core, the separation is comparable to the size of the Solar System (100 to 1000 times closer than stars near the Solar System).

Globular clusters are not thought to be favorable locations for the survival of planetary systems. Planetary orbits are dynamically unstable within the cores of dense clusters because of the perturbations of passing stars. A planet orbiting at 1 astronomical unit around a star that is within the core of a dense cluster such as 47 Tucanae would only survive on the order of 10 years. There is a planetary system orbiting a pulsar (PSR B1620−26) that belongs to the globular cluster M4, but these planets likely formed after the event that created the pulsar.

Some globular clusters, like Omega Centauri in the Milky Way and G1 in M31, are extraordinarily massive, with several million solar masses () and multiple stellar populations. Both can be regarded as evidence that supermassive globular clusters are in fact the cores of dwarf galaxies that are consumed by the larger galaxies. About a quarter of the globular cluster population in the Milky Way may have been accreted along with their host dwarf galaxy.

Several globular clusters (like M15) have extremely massive cores which may harbor black holes, although simulations suggest that a less massive black hole or central concentration of neutron stars or massive white dwarfs explain observations equally well.

Globular clusters normally consist of Population II stars, which have a low proportion of elements other than hydrogen and helium when compared to Population I stars such as the Sun. Astronomers refer to these heavier elements as metals and to the proportions of these elements as the metallicity. These elements are produced by stellar nucleosynthesis and then are recycled into the interstellar medium, where they enter the next generation of stars. Hence the proportion of metals can be an indication of the age of a star, with older stars typically having a lower metallicity.

The Dutch astronomer Pieter Oosterhoff noticed that there appear to be two populations of globular clusters, which became known as "Oosterhoff groups". The second group has a slightly longer period of RR Lyrae variable stars. Both groups have weak lines of metallic elements. But the lines in the stars of Oosterhoff type I (OoI) cluster are not quite as weak as those in type II (OoII). Hence type I are referred to as "metal-rich" (for example, Terzan 7), while type II are "metal-poor" (for example, ESO 280-SC06).

These two populations have been observed in many galaxies, especially massive elliptical galaxies. Both groups are nearly as old as the universe itself and are of similar ages, but differ in their metal abundances. Many scenarios have been suggested to explain these subpopulations, including violent gas-rich galaxy mergers, the accretion of dwarf galaxies, and multiple phases of star formation in a single galaxy. In the Milky Way, the metal-poor clusters are associated with the halo and the metal-rich clusters with the bulge.

In the Milky Way it has been discovered that the large majority of the low metallicity clusters are aligned along a plane in the outer part of the galaxy's halo. This result argues in favor of the view that type II clusters in the galaxy were captured from a satellite galaxy, rather than being the oldest members of the Milky Way's globular cluster system as had been previously thought. The difference between the two cluster types would then be explained by a time delay between when the two galaxies formed their cluster systems.

Globular clusters have a very high star density, and therefore close interactions and near-collisions of stars occur relatively often. Due to these chance encounters, some exotic classes of stars, such as blue stragglers, millisecond pulsars and low-mass X-ray binaries, are much more common in globular clusters. A blue straggler is formed from the merger of two stars, possibly as a result of an encounter with a binary system. The resulting star has a higher temperature than comparable stars in the cluster with the same luminosity, and thus differs from the main sequence stars formed at the beginning of the cluster.
Astronomers have searched for black holes within globular clusters since the 1970s. The resolution requirements for this task, however, are exacting, and it is only with the Hubble space telescope that the first confirmed discoveries have been made. In independent programs, a intermediate-mass black hole has been suggested to exist based on HST observations in the globular cluster M15 and a black hole in the Mayall II cluster in the Andromeda Galaxy. Both x-ray and radio emissions from Mayall II appear to be consistent with an intermediate-mass black hole.

They are the first black holes discovered that were intermediate in mass between the conventional stellar-mass black hole and the supermassive black holes discovered at the cores of galaxies. The mass of these intermediate mass black holes is proportional to the mass of the clusters, following a pattern previously discovered between supermassive black holes and their surrounding galaxies.

Claims of intermediate mass black holes have been met with some skepticism. The heaviest objects in globular clusters are expected to migrate to the cluster center due to mass segregation. As pointed out in two papers by Holger Baumgardt and collaborators, the mass-to-light ratio should rise sharply towards the center of the cluster, even without a black hole, in both M15 and Mayall II.

The Hertzsprung-Russell diagram (HR-diagram) is a graph of a large sample of stars that plots their visual
absolute magnitude against their color index. The
color index, B−V, is the difference between the magnitude of the star in blue light, or B, and the magnitude in visual light (green-yellow), or V. Large positive values indicate a red star with a cool surface temperature, while negative values imply a blue star with a hotter surface.

When the stars near the Sun are plotted on an HR diagram, it displays a distribution of stars of various masses, ages, and compositions. Many of the stars lie relatively close to a sloping curve with increasing absolute magnitude as the stars are hotter, known as main-sequence stars. However the diagram also typically includes stars that are in later stages of their evolution and have wandered away from this main-sequence curve.

As all the stars of a globular cluster are at approximately the same distance from us, their absolute magnitudes differ from their visual magnitude by about the same amount. The main-sequence stars in the globular cluster will fall along a line that is believed to be comparable to similar stars in the solar neighborhood. The accuracy of this assumption is confirmed by comparable results obtained by comparing the magnitudes of nearby short-period variables, such as RR Lyrae stars and cepheid variables, with those in the cluster.

By matching up these curves on the HR diagram the absolute magnitude of main-sequence stars in the cluster can also be determined. This in turn provides a distance estimate to the cluster, based on the visual magnitude of the stars. The difference between the relative and absolute magnitude, the "distance modulus", yields this estimate of the distance.

When the stars of a particular globular cluster are plotted on an HR diagram, in many cases nearly all of the stars fall upon a relatively well-defined curve. This differs from the HR diagram of stars near the Sun, which lumps together stars of differing ages and origins. The shape of the curve for a globular cluster is characteristic of a grouping of stars that were formed at approximately the same time and from the same materials, differing only in their initial mass. As the position of each star in the HR diagram varies with age, the shape of the curve for a globular cluster can be used to measure the overall age of the star population.

However, the above-mentioned historic process of determining the age and distance to globular clusters is not as robust as first thought, since the morphology and luminosity of globular cluster stars in color-magnitude diagrams are influenced by numerous parameters, many of which are still being actively researched. Certain clusters even display populations that are absent from other globular clusters (e.g., blue hook stars), or feature multiple populations. The historical paradigm that all globular clusters consist of stars born at exactly the same time, or sharing exactly the same chemical abundance, has likewise been overturned (e.g., NGC 2808). Further, the morphology of the cluster stars in a color-magnitude diagram, and that includes the brightnesses of distance indicators such as RR Lyrae variable members, can be influenced by observational biases. One such effect is called blending, and it arises because the cores of globular clusters are so dense that in low-resolution observations multiple (unresolved) stars may appear as a single target. Thus the brightness measured for that seemingly single star (e.g., an RR Lyrae variable) is erroneously too bright, given those unresolved stars contributed to the brightness determined. Consequently, the computed distance is wrong, and more importantly, certain researchers have argued that the blending effect can introduce a systematic uncertainty into the cosmic distance ladder, and may bias the estimated age of the Universe and the Hubble constant.
The most massive main-sequence stars will also have the highest absolute magnitude, and these will be the first to evolve into the giant star stage. As the cluster ages, stars of successively lower masses will also enter the giant star stage. Thus the age of a single population cluster can be measured by looking for the stars that are just beginning to enter the giant star stage. This forms a "knee" in the HR diagram, bending to the upper right from the main-sequence line. The absolute magnitude at this bend is directly a function of the age of globular cluster, so an age scale can be plotted on an axis parallel to the magnitude.

In addition, globular clusters can be dated by looking at the temperatures of the coolest white dwarfs. Typical results for globular clusters are that they may be as old as 12.7 billion years. This is in contrast to open clusters which are only tens of millions of years old.

The ages of globular clusters place a bound on the age limit of the entire universe. This lower limit has been a significant constraint in cosmology. Historically, astronomers were faced with age estimates of globular clusters that appeared older than cosmological models would allow. However, better measurements of cosmological parameters through deep sky surveys and satellites such as the Hubble Space Telescope appear to have resolved this issue.

Evolutionary studies of globular clusters can also be used to determine changes due to the starting composition of the gas and dust that formed the cluster. That is, the evolutionary tracks change with changes in the abundance of heavy elements. The data obtained from studies of globular clusters are then used to study the evolution of the Milky Way as a whole.

In globular clusters a few stars known as blue stragglers are observed, apparently continuing the main sequence in the direction of brighter, bluer stars. The origins of these stars is still unclear, but most models suggest that these stars are the result of mass transfer in multiple star systems.

In contrast to open clusters, most globular clusters remain gravitationally bound for time periods comparable to the life spans of the majority of their stars. However, a possible exception is when strong tidal interactions with other large masses result in the dispersal of the stars.

After they are formed, the stars in the globular cluster begin to interact gravitationally with each other. As a result, the velocity vectors of the stars are steadily modified, and the stars lose any history of their original velocity. The characteristic interval for this to occur is the relaxation time. This is related to the characteristic length of time a star needs to cross the cluster as well as the number of stellar masses in the system. The value of the relaxation time varies by cluster, but the mean value is on the order of 10 years.
Although globular clusters generally appear spherical in form, ellipticities can occur due to tidal interactions. Clusters within the Milky Way and the Andromeda Galaxy are typically oblate spheroids in shape, while those in the Large Magellanic Cloud are more elliptical.

Astronomers characterize the morphology of a globular cluster by means of standard radii. These are the core radius ("r"), the half-light radius ("r"), and the tidal (or Jacobi) radius ("r"). The overall luminosity of the cluster steadily decreases with distance from the core, and the core radius is the distance at which the apparent surface luminosity has dropped by half. A comparable quantity is the half-light radius, or the distance from the core within which half the total luminosity from the cluster is received. This is typically larger than the core radius.

Note that the half-light radius includes stars in the outer part of the cluster that happen to lie along the line of sight, so theorists will also use the half-mass radius ("r")—the radius from the core that contains half the total mass of the cluster. When the half-mass radius of a cluster is small relative to the overall size, it has a dense core. An example of this is Messier 3 (M3), which has an overall visible dimension of about 18 arc minutes, but a half-mass radius of only 1.12 arc minutes.

Almost all globular clusters have a half-light radius of less than 10 pc, although there are well-established globular clusters with very large radii (i.e. NGC 2419 (R = 18 pc) and Palomar 14 (R = 25 pc)).

Finally the tidal radius, or Hill sphere, is the distance from the center of the globular cluster at which the external gravitation of the galaxy has more influence over the stars in the cluster than does the cluster itself. This is the distance at which the individual stars belonging to a cluster can be separated away by the galaxy. The tidal radius of M3 is about 40 arc minutes, or about 113 pc at the distance of 10.4 kpc.

In measuring the luminosity curve of a given globular cluster as a function of distance from the core, most clusters in the Milky Way increase steadily in luminosity as this distance decreases, up to a certain distance from the core, then the luminosity levels off. Typically this distance is about 1–2 parsecs from the core. However about 20% of the globular clusters have undergone a process termed "core collapse". In this type of cluster, the luminosity continues to increase steadily all the way to the core region. An example of a core-collapsed globular is
M15.
Core-collapse is thought to occur when the more massive stars in a globular cluster encounter their less massive companions. Over time, dynamic processes cause individual stars to migrate from the center of the cluster to the outside. This results in a net loss of kinetic energy from the core region, leading the remaining stars grouped in the core region to occupy a more compact volume. When this gravothermal instability occurs, the central region of the cluster becomes densely crowded with stars and the surface brightness of the cluster forms a power-law cusp. (Note that a core collapse is not the only mechanism that can cause such a luminosity distribution; a massive black hole at the core can also result in a luminosity cusp.) Over a lengthy period of time this leads to a concentration of massive stars near the core, a phenomenon called mass segregation.

The dynamical heating effect of binary star systems works to prevent an initial core collapse of the cluster. When a star passes near a binary system, the orbit of the latter pair tends to contract, releasing energy. Only after the primordial supply of binaries is exhausted due to interactions can a deeper core collapse proceed. In contrast, the effect of tidal shocks as a globular cluster repeatedly passes through the plane of a spiral galaxy tends to significantly accelerate core collapse.

The different stages of core-collapse may be divided into three phases. During a globular cluster's adolescence, the process of core-collapse begins with stars near the core. However, the interactions between binary star systems prevents further collapse as the cluster approaches middle age. Finally, the central binaries are either disrupted or ejected, resulting in a tighter concentration at the core.

The interaction of stars in the collapsed core region causes tight binary systems to form. As other stars interact with these tight binaries, they increase the energy at the core, which causes the cluster to re-expand. As the mean time for a core collapse is typically less than the age of the galaxy, many of a galaxy's globular clusters may have passed through a core collapse stage, then re-expanded.

The Hubble Space Telescope has been used to provide convincing observational evidence of this stellar mass-sorting process in globular clusters. Heavier stars slow down and crowd at the cluster's core, while lighter stars pick up speed and tend to spend more time at the cluster's periphery. The globular star cluster 47 Tucanae, which is made up of about 1 million stars, is one of the densest globular clusters in the Southern Hemisphere. This cluster was subjected to an intensive photographic survey, which allowed astronomers to track the motion of its stars. Precise velocities were obtained for nearly 15,000 stars in this cluster.

A 2008 study by John Fregeau of 13 globular clusters in the Milky Way shows that three of them have an unusually large number of X-ray sources, or X-ray binaries, suggesting the clusters are middle-aged. Previously, these globular clusters had been classified as being in old age because they had very tight concentrations of stars in their centers, another test of age used by astronomers. The implication is that most globular clusters, including the other ten studied by Fregeau, are not in middle age as previously thought, but are actually in 'adolescence'.

The overall luminosities of the globular clusters within the Milky Way and the Andromeda Galaxy can be modeled by means of a gaussian curve. This gaussian can be represented by means of an average magnitude M and a variance σ. This distribution of globular cluster luminosities is called the Globular Cluster Luminosity Function (GCLF). (For the Milky Way, M = , σ = magnitudes.) The GCLF has also been used as a "standard candle" for measuring the distance to other galaxies, under the assumption that the globular clusters in remote galaxies follow the same principles as they do in the Milky Way.

Computing the interactions between the stars within a globular cluster requires solving what is termed the N-body problem. That is, each of the stars within the cluster continually interacts with the other "N"−1 stars, where "N" is the total number of stars in the cluster. The naive CPU computational "cost" for a dynamic simulation increases in proportion to "N" (each of N objects must interact pairwise with each of the other N objects), so the potential computing requirements to accurately simulate such a cluster can be enormous. An efficient method of mathematically simulating the N-body dynamics of a globular cluster is done by subdividing into small volumes and velocity ranges, and using probabilities to describe the locations of the stars. The motions are then described by means of a formula called the Fokker–Planck equation. This can be solved by a simplified form of the equation, or by running Monte Carlo simulations and using random values. However the simulation becomes more difficult when the effects of binaries and the interaction with external gravitation forces (such as from the Milky Way galaxy) must also be included.

The results of N-body simulations have shown that the stars can follow unusual paths through the cluster, often forming loops and often falling more directly toward the core than would a single star orbiting a central mass. In addition, due to interactions with other stars that result in an increase in velocity, some of the stars gain sufficient energy to escape the cluster. Over long periods of time this will result in a dissipation of the cluster, a process termed evaporation. The typical time scale for the evaporation of a globular cluster is 10 years. In 2010 it became possible to directly compute, star by star, N-body simulations of a globular cluster over the course of its lifetime.

Binary stars form a significant portion of the total population of stellar systems, with up to half of all stars occurring in binary systems. Numerical simulations of globular clusters have demonstrated that binaries can hinder and even reverse the process of core collapse in globular clusters. When a star in a cluster has a gravitational encounter with a binary system, a possible result is that the binary becomes more tightly bound and kinetic energy is added to the solitary star. When the massive stars in the cluster are sped up by this process, it reduces the contraction at the core and limits core collapse.

The ultimate fate of a globular cluster must be either to accrete stars at its core, causing its steady contraction, or gradual shedding of stars from its outer layers.

The distinction between cluster types is not always
clear-cut, and objects have been found that blur the
lines between the categories. For example, BH 176
in the southern part of the Milky Way has properties
of both an open and a globular cluster.

In 2005, astronomers discovered a completely new type of star cluster in the Andromeda Galaxy, which is, in several ways, very similar to globular clusters. The new-found clusters contain hundreds of thousands of stars, a similar number to that found in globular clusters. The clusters share other characteristics with globular clusters such as stellar populations and metallicity. What distinguishes them from the globular clusters is that they are much larger – several hundred light-years across – and hundreds of times less dense. The distances between the stars are, therefore, much greater within the newly discovered extended clusters. Parametrically, these clusters lie somewhere between a globular cluster and a dwarf spheroidal galaxy.

How these clusters are formed is not yet known, but their formation might well be related to that of globular clusters. Why M31 has such clusters, while the Milky Way does not, is not yet known. It is also unknown if any other galaxy contains these types of clusters, but it would be very unlikely that M31 is the sole galaxy with extended clusters.

When a globular cluster has a close encounter with a large mass, such as the core region of a galaxy, it undergoes a tidal interaction. The difference in the pull of gravity between the part of the cluster nearest the mass and the pull on the furthest part of the cluster results in a tidal force. A "tidal shock" occurs whenever the orbit of a cluster takes it through the plane of a galaxy.

As a result of a tidal shock, streams of stars can be pulled away from the cluster halo, leaving only the core part of the cluster. These tidal interaction effects create tails of stars that can extend up to several degrees of arc away from the cluster. These tails typically both precede and follow the cluster along its orbit. The tails can accumulate significant portions of the original mass of the cluster, and can form clumplike features.

The globular cluster Palomar 5, for example, is near the apogalactic point of its orbit after passing through the Milky Way. Streams of stars extend outward toward the front and rear of the orbital path of this cluster, stretching out to distances of 13,000 light-years. Tidal interactions have stripped away much of the mass from Palomar 5, and further interactions as it passes through the galactic core are expected to transform it into a long stream of stars orbiting the Milky Way halo.

Tidal interactions add kinetic energy into a globular cluster, dramatically increasing the evaporation rate and shrinking the size of the cluster. Not only does tidal shock strip off the outer stars from a globular cluster, but the increased evaporation accelerates the process of core collapse. The same physical mechanism may be at work in dwarf spheroidal galaxies such as the Sagittarius Dwarf, which appears to be undergoing tidal disruption due to its proximity to the Milky Way.

There are many globular clusters with a retrograde orbit round the Milky Way Galaxy. A hypervelocity globular cluster was discovered around Messier 87 in 2014, having a velocity in excess of the escape velocity of M87.

Astronomers are searching for exoplanets of stars in globular star clusters.

In 2000, the results of a search for giant planets in the globular cluster 47 Tucanae were announced. The lack of any successful discoveries suggests that the abundance of elements (other than hydrogen or helium) necessary to build these planets may need to be at least 40% of the abundance in the Sun. Terrestrial planets are built from heavier elements such as silicon, iron and magnesium. The very low abundance of these elements in globular clusters means that the member stars have a far lower likelihood of hosting Earth-mass planets, when compared to stars in the neighborhood of the Sun. Hence the halo region of the Milky Way galaxy, including globular cluster members, are unlikely to host habitable terrestrial planets.

In spite of the lower likelihood of giant planet formation, just such an object has been found in the globular cluster Messier 4. This planet was detected orbiting a pulsar in the binary star system PSR B1620-26. The eccentric and highly inclined orbit of the planet suggests it may have been formed around another star in the cluster, then was later "exchanged" into its current arrangement. The likelihood of close encounters between stars in a globular cluster can disrupt planetary systems, some of which break loose to become free floating planets. Even close orbiting planets can become disrupted, potentially leading to orbital decay and an increase in orbital eccentricity and tidal effects.






</doc>
<doc id="12867" url="https://en.wikipedia.org/wiki?curid=12867" title="George Vancouver">
George Vancouver

Captain George Vancouver (22 June 1757 – 10 May 1798) was a British officer of the Royal Navy, best known for his 1791–95 expedition, which explored and charted North America's northwestern Pacific Coast regions, including the coasts of contemporary Alaska, British Columbia, Washington, and Oregon. He also explored the Hawaiian Islands and the southwest coast of Australia.

In Canada, Vancouver Island and the city of Vancouver are named for him, as are Vancouver, Washington, in the United States, Mount Vancouver on the Yukon/Alaska border, and New Zealand's sixth highest mountain.

George Vancouver was born in the seaport town of King's Lynn (Norfolk, England) on 22 June 1757 as the sixth, and youngest, child of John Jasper Vancouver, a Deputy Collector of Customs, and Bridget Berners.

In 1771, at the age of 13, George Vancouver entered the Royal Navy as a "young gentleman," a future candidate for midshipman. He was selected to serve as a midshipman aboard , on James Cook's second voyage (1772–1775) searching for "Terra Australis". He also accompanied Cook's third voyage (1776–1780), this time aboard "Resolution"'s companion ship, , and was present during the first European sighting and exploration of the Hawaiian Islands. Upon his return to Britain in October 1780, Vancouver was commissioned as a lieutenant and posted aboard the sloop initially on escort and patrol duty in the English Channel and North Sea. He accompanied the ship when it left Plymouth on 11 February 1782 for the West Indies. On 7 May 1782 he was appointed fourth Lieutenant of the 74-gun ship of the line HMS "Fame" which was at the time part of the British West Indies Fleet and assigned to patrolling the French-held Leeward Islands. Vancouver returned to England in June 1783.

In the late 1780s the Spanish Empire commissioned an expedition to the Pacific Northwest. The 1789 Nootka Crisis developed, and Spain and Britain came close to war over ownership of the Nootka Sound on contemporary Vancouver Island, and of greater importance, the right to colonise and settle the Pacific Northwest coast. Henry Roberts had recently taken command of the survey ship (a new vessel named in honour of the ship on Cook's voyage), which was to be used on another round-the-world voyage, and Roberts selected Vancouver as his first lieutenant, but they were then diverted to other warships due to the crisis. Vancouver went with Joseph Whidbey to the 74-gun ship of the line . When the first Nootka Convention ended the crisis in 1790, Vancouver was given command of "Discovery" to take possession of Nootka Sound and to survey the coasts.

Departing England with two ships, HMS "Discovery" and , on 1 April 1791, Vancouver commanded an expedition charged with exploring the Pacific region. In its first year the expedition travelled to Cape Town, Australia, New Zealand, Tahiti, and Hawaii, collecting botanical samples and surveying coastlines along the way. He formally claimed at Possession Point, King George Sound Western Australia, now the town of Albany, Western Australia for the British. Proceeding to North America, Vancouver followed the coasts of present-day Oregon and Washington northward. In April 1792 he encountered American Captain Robert Gray off the coast of Oregon just prior to Gray's sailing up the Columbia River.

Vancouver entered the Strait of Juan de Fuca, between Vancouver Island and the Washington state mainland on 29 April 1792. His orders included a survey of every inlet and outlet on the west coast of the mainland, all the way north to Alaska. Most of this work was in small craft propelled by both sail and oar; manoeuvring larger sail-powered vessels in uncharted waters was generally impractical and dangerous.

Vancouver named many features for his officers, friends, associates, and his ship "Discovery", including:

Vancouver was the second European to enter Burrard Inlet on 13 June 1792, naming it for his friend Sir Harry Burrard. It is the present day main harbour area of the City of Vancouver beyond Stanley Park. George Vancouver surveyed Howe Sound and Jervis Inlet over the next nine days. Then, on his 35th birthday on 22 June 1792, he returned to Point Grey, the present-day location of the University of British Columbia. Here he unexpectedly met a Spanish expedition led by Dionisio Alcalá Galiano and Cayetano Valdés y Flores. Vancouver was ""mortified"" ("his word") to learn they already had a crude chart of the Strait of Georgia based on the 1791 exploratory voyage of José María Narváez the year before, under command of Francisco de Eliza. For three weeks they cooperatively explored the Georgia Strait and the Discovery Islands area before sailing separately towards Nootka Sound.

After the summer surveying season ended, in August 1792, Vancouver went to Nootka, then the region's most important harbour, on contemporary Vancouver Island. Here he was to receive any British buildings and lands returned by the Spanish from claims by Francisco de Eliza for the Spanish crown. The Spanish commander, Juan Francisco Bodega y Quadra, was very cordial and he and Vancouver exchanged the maps they had made, but no agreement was reached; they decided to await further instructions. At this time, they decided to name the large island on which Nootka was now proven to be located as "Quadra and Vancouver Island". Years later, as Spanish influence declined, the name was shortened to simply Vancouver Island.

While at Nootka Sound Vancouver acquired Robert Gray's chart of the lower Columbia River. Gray had entered the river during the summer before sailing to Nootka Sound for repairs. Vancouver realised the importance of verifying Gray's information and conducting a more thorough survey. In October 1792, he sent Lieutenant William Robert Broughton with several boats up the Columbia River. Broughton got as far as the Columbia River Gorge, sighting and naming Mount Hood.

Vancouver sailed south along the coast of Spanish Alta California, visiting Chumash villages at Point Conception and near Mission San Buenaventura. Vancouver spent the winter in continuing exploration of the Sandwich Islands, the contemporary islands of Hawaii.

The next year, 1793, he returned to British Columbia and proceeded further north, unknowingly missing the overland explorer Alexander Mackenzie by only 48 days. He got to 56°30'N, having explored north from Point Menzies in Burke Channel to the northwest coast of Prince of Wales Island. He sailed around the latter island, as well as circumnavigating Revillagigedo Island and charting parts of the coasts of Mitkof, Zarembo, Etolin, Wrangell, Kuiu and Kupreanof Islands. With worsening weather, he sailed south to Alta California, hoping to find Bodega y Quadra and fulfil his territorial mission, but the Spaniard was not there. He again spent the winter in the Sandwich Islands.

In 1794, he first went to Cook Inlet, the northernmost point of his exploration, and from there followed the coast south. Boat parties charted the east coasts of Chichagof and Baranof Islands, circumnavigated Admiralty Island, explored to the head of Lynn Canal, and charted the rest of Kuiu Island and nearly all of Kupreanof Island. He then set sail for Great Britain by way of Cape Horn, returning in September 1795, thus completing a circumnavigation of South America.

Impressed by the view from Richmond Hill, Vancouver retired to Petersham, London.

Vancouver faced difficulties when he returned home to England. The accomplished and politically well-connected naturalist Archibald Menzies complained that his servant had been pressed into service during a shipboard emergency; sailing master Joseph Whidbey had a competing claim for pay as expedition astronomer; and Thomas Pitt, 2nd Baron Camelford, whom Vancouver had disciplined for numerous infractions and eventually sent home in disgrace, proceeded to harass him publicly and privately.

Pitt's allies, including his cousin, Prime Minister William Pitt the Younger, attacked Vancouver in the press. Thomas Pitt took a more direct approach; on 29 August 1796 he sent Vancouver a letter heaping many insults on the head of his former captain, and challenging him to a duel. Vancouver gravely replied that he was unable "in a private capacity to answer for his public conduct in his official duty," and offered instead to submit to formal examination by flag officers. Pitt chose instead to stalk Vancouver, ultimately assaulting him on a London street corner. The terms of their subsequent legal dispute required both parties to keep the peace, but nothing stopped Vancouver's civilian brother Charles from interposing and giving Pitt blow after blow until onlookers restrained the attacker. Charges and counter-charges flew in the press, with the wealthy Camelford faction having the greater firepower until Vancouver, ailing from his long naval service, died.

Vancouver, at one time among of Britain's greatest explorers and navigators, died in obscurity on 10 May 1798 at the age of 40, less than three years after completing his voyages and expeditions. No official cause of death was stated, as the medical records pertaining to Vancouver were destroyed; one doctor named John Naish claimed Vancouver died from kidney failure, while others believed it was a hyperthyroid condition. His grave is in the churchyard of St Peter's Church, Petersham, in the London Borough of Richmond upon Thames, England. The Hudson's Bay Company placed a memorial plaque in the church in 1841. His grave in Portland stone, renovated in the 1960s, is now Grade II listed in view of its historical associations.

Vancouver determined that the Northwest Passage did not exist at the latitudes that had long been suggested. His charts of the North American northwest coast were so extremely accurate that they served as the key reference for coastal navigation for generations. Robin Fisher, the academic Vice-President of Mount Royal University in Calgary and author of two books on Vancouver, states:
However, Vancouver failed to discover two of the largest and most important rivers on the Pacific coast, the Fraser River and the Columbia River. He also missed the Skeena River near Prince Rupert in northern British Columbia. Vancouver did eventually learn of the river before he finished his survey—from Robert Gray, captain of the American merchant ship that conducted the first Euroamerican sailing of the Columbia River on 11 May 1792, after first sighting it on an earlier voyage in 1788. However it and the Fraser River never made it onto Vancouver's charts.
Stephen R. Bown, noted in "Mercator's World" magazine (November/December 1999) that:

While it is difficult to comprehend how Vancouver missed the Fraser River, much of this river's delta was subject to flooding and summer freshet which prevented the captain from spotting any of its great channels as he sailed the entire shoreline from Point Roberts, Washington to Point Grey in 1792. The Spanish expeditions to the Pacific Northwest, with the 1791 Francisco de Eliza expedition preceding Vancouver by a year, had also missed the Fraser River although they knew from its muddy plume that there was a major river located nearby.

Vancouver generally established a good rapport with both Indigenous peoples and European trappers. Historical records show Vancouver enjoyed good relations with native leaders both in Hawaii – where King Kamehameha I ceded Hawaii to Vancouver in 1794 – as well as the Pacific Northwest and California. Vancouver's journals exhibit a high degree of sensitivity to natives. He wrote of meeting the Chumash people, and of his exploration of a small island on the Californian coast on which an important burial site was marked by a sepulchre of "peculiar character" lined with boards and fragments of military instruments lying near a square box covered with mats. Vancouver states:

Vancouver also displayed contempt in his journals towards unscrupulous western traders who provided guns to natives by writing:

Robin Fisher notes that Vancouver's "relationships with aboriginal groups were generally peaceful; indeed, his detailed survey would not have been possible if they had been hostile." While there were hostile incidents at the end of Vancouver's last season – the most serious of which involved a clash with Tlingits at Behm Canal in southeast Alaska in 1794 – these were the exceptions to Vancouver's exploration of the US and Canadian Northwest coast.

Despite a long history of warfare between Britain and Spain, Vancouver maintained excellent relations with his Spanish counterparts and even fêted a Spanish sea captain aboard his ship during his 1792 trip to the Vancouver region.


Many places around the world have been named after George Vancouver, including:






Many collections were made on the voyage: one was donated by Archibald Menzies to the British Museum 1796; another made by surgeon George Goodman Hewett (1765–1834) was donated by A. W. Franks to the British Museum in 1891. An account of these has been published.

 Canada Post issued a $1.55 postage stamp to commemorate the 250th anniversary of Vancouver's birth, on 22 June 2007. The stamp has an embossed image of Vancouver seen from behind as he gazes forward towards a mountainous coastline. This may be the first Canadian stamp not to show the subject's face.

The City of Vancouver in Canada organised a celebration to commemorate the 250th anniversary of Vancouver's birth, in June 2007 at the Vancouver Maritime Museum. The one-hour festivities included the presentation of a massive 63 by 114 centimetre carrot cake, the firing of a gun salute by the Royal Canadian Artillery's 15th Field Regiment and a performance by the Vancouver Firefighter's Band.
Vancouver's then-mayor, Sam Sullivan, officially declared 22 June 2007 to be "George Day".

The Musqueam (xʷməθkʷəy̓əm) Elder sɁəyeɬəq (Larry Grant) attended the festivities and acknowledged that some of his people might disapprove of his presence, but also noted:

There has been some debate about the origins of the Vancouver name. It is now commonly accepted that the name Vancouver derives from the expression van Coevorden, meaning "(originating) from Coevorden", a city in the northeast of the Netherlands. This city is apparently named after the "Coeverden" family of the 13th–15th century.

In the 16th century, a number of businessmen from the Coevorden area (and the rest of the Netherlands) moved to England. Some of them were known as "Van Coeverden". Others adopted the surname Oxford, as in oxen fording (a river), which is approximately the English translation of "Coevorden". However, it is not the exact name of the noble family mentioned in the history books that claim Vancouver's noble lineage: that name was Coeverden not Coevorden.

In the 1970s, Adrien Mansvelt, a former consul general of the Netherlands based in Vancouver, published a collation of information in both historical and genealogical journals and in the "Vancouver Sun" newspaper. Mansvelt's theory was later presented by the city during the Expo 86 World's Fair, as historical fact. The information was then used by historian W. Kaye Lamb in his book "A Voyage of Discovery to the North Pacific Ocean and Round the World, 1791–1795" (1984).

W. Kaye Lamb, in summarising Mansvelt's 1973 research, observes evidence of close family ties between the Vancouver family of Britain and the Van Coeverden family of the Netherlands as well as George Vancouver's own words from his diaries in referring to his Dutch ancestry:

In 2006 John Robson, a librarian at the University of Waikato, conducted his own research into George Vancouver's ancestry, which he published in an article published in the British Columbia History journal. Robson theorises that Vancouver's forebears may have been Flemish rather than Dutch; he believes that Vancouver is descended from the Vangover family of Ipswich and Colchester in Suffolk. Those towns had a significant Flemish population in the 16th and 17th centuries.

George Vancouver named the south point of what is now Couverden Island, Alaska, as "Point Couverden" during his exploration of the North American Pacific coast, in honour of his family's hometown of Coevorden. It is located at the western point of entry to Lynn Canal in southeastern Alaska.

The Admiralty instructed Vancouver to publish a narrative of his voyage which he started to write in early 1796 in Petersham. At the time of his death the manuscript covered the period up to mid-1795. The work, "A Voyage of Discovery to the North Pacific Ocean, and Round the World", was completed by his brother John and published in three volumes in the autumn of 1798. A second edition was published in 1801 in six volumes. 

A modern annotated edition (1984) by W. Kaye Lamb was renamed "The Voyage of George Vancouver 1791–1795", and published in four volumes by the Hakluyt Society of London, England.





</doc>
<doc id="12872" url="https://en.wikipedia.org/wiki?curid=12872" title="Great Vowel Shift">
Great Vowel Shift

The Great Vowel Shift was a major series of changes in the pronunciation of the English language that took place, beginning in southern England, primarily between 1350 and the 1600s and 1700s, today influencing effectively all dialects of English. Through this vowel shift, all Middle English long vowels changed their pronunciation. In addition, some consonant sounds changed as well, particularly those that became silent; the term "Great Vowel Shift" is sometimes used to include these consonant changes as well.

English spelling was first becoming standardized in the 15th and 16th centuries, and the Great Vowel Shift is responsible for the fact that English spellings now often strongly deviate in their representation of English pronunciations. The Great Vowel Shift was first studied by Otto Jespersen (1860–1943), a Danish linguist and Anglicist, who coined the term.

The causes of the Great Vowel Shift have been a source of intense scholarly debate and as yet there is no firm consensus. The greatest changes occurred during the 15th and 16th centuries. Some scholars have argued that the rapid migration of peoples from northern England to the southeast following the Black Death caused a mixing of accents that forced a change in the standard London vernacular. Others argue that the influx of French loanwords was a major factor in the shift. Yet others assert that because of the increasing prestige of French pronunciations among the middle classes (perhaps related to the English aristocracy's switching from French to English around this time), a process of hypercorrection may have started a shift that unintentionally resulted in vowel pronunciations that were less like French instead of more. An opposing theory states that the wars with France and general anti-French sentiments caused hypercorrection in order to deliberately make English sound less like French.

The main difference between the pronunciation of Middle English in the year 1400 and Modern English (Received Pronunciation) is in the value of the long vowels. Long vowels in Middle English had "continental" values, much like those in Italian and Standard German, but in standard Modern English, they have entirely different pronunciations. These differing pronunciations of English vowel letters do not stem from the Great Shift as such, but from the fact that English spelling failed to be adapted to the changes. In fact, German had undergone vowel changes quite similar to the Great Shift in a slightly earlier period, but the spelling was changed accordingly (e.g. Middle High German "bīzen" → modern German "beißen" "to bite").

This timeline shows the main vowel changes that occurred between late Middle English in the year 1400 and Received Pronunciation in the mid-20th century by using representative words. The Great Vowel Shift occurred in the lower half of the table, between 1400 and 1600~1700. The changes that happened after 1700 are not considered part of the Great Vowel Shift. Pronunciation is given in the International Phonetic Alphabet:

Before the Great Vowel Shift, Middle English in Southern England had seven long vowels, . The vowels occurred in the words "bite", "meet", "meat", "mate", "boat", "boot", "out" and many more.

The words had very different pronunciations in Middle English from their pronunciations in Modern English. Long "i" in "bite" was pronounced as so Middle English "bite" sounded like Modern English "beet" ; long "e" in "meet" was pronounced as so Middle English "meet" sounded similar to Modern English "mate" ; long "a" in "mate" was pronounced as , with a vowel like Scottish English "ah" in "father" or General American short "o" in "cot" ; and long "o" in "boot" was pronounced as , similar to modern "oa" in General American "boat" . In addition, Middle English had a long in "beat", like modern short "e" in "bed" but pronounced longer, and a long in "boat".

After around 1300, the long vowels of Middle English began changing in pronunciation. The two close vowels, , became diphthongs (vowel breaking), and the other five, , underwent an increase in tongue height (raising).

They occurred over several centuries and can be divided into two phases. The first phase affected the close vowels and the close-mid vowels : were raised to , and became the diphthongs or . The second phase affected the open vowel and the open-mid vowels : were raised, in most cases changing to .

The Great Vowel Shift changed vowels without merger so Middle English before the vowel shift had the same number of vowel phonemes as Early Modern English after the vowel shift. After the Great Vowel Shift, some vowel phonemes began merging. Immediately after the Great Vowel Shift, the vowels of "meet" and "meat" were different, but they are merged in Modern English, and both words are pronounced as . However, during the 16th and the 17th centuries, there were many different mergers, and some mergers can be seen in individual Modern English words like "great", which is pronounced with the vowel as in "mate" rather than the vowel as in "meat".

This is a simplified picture of the changes that happened between late Middle English and today's English. Pronunciations in 1400, 1500, 1600, and 1900 are shown. To hear recordings of the sounds, click the phonetic symbols.

Before labial consonants and also after , did not shift, and remains as in "soup" and "room" (its Middle English spelling was "roum").

The first phase of the Great Vowel Shift affected the Middle English close-mid vowels , as in "beet" and "boot", and the close vowels , as in "bite" and "out". The close-mid vowels became close , and the close vowels became diphthongs. The first phase was complete in 1500, meaning that by that time, words like "beet" and "boot" had lost their Middle English pronunciation, and were pronounced with the same vowels as in Modern English. The words "bite" and "out" were pronounced with diphthongs, but not the same diphthongs as in Modern English.
Scholars agree that the Middle English close vowels became diphthongs around the year 1500, but disagree about what diphthongs they changed to. According to Lass, the words "bite" and "out" after diphthongization were pronounced as and , similar to American English "bait" and "oat" . Later, the diphthongs shifted to , then , and finally to Modern English . This sequence of events is supported by the testimony of orthoepists before Hodges in 1644.

However, many scholars such as , , and argue for theoretical reasons that, contrary to what 16th-century witnesses report, the vowels were actually immediately centralized and lowered to .

Evidence from northern English and Scots (see below) suggests that the close-mid vowels were the first to shift. As the Middle English vowels were raised towards , they forced the original Middle English out of place and caused them to become diphthongs . This type of sound change, in which one vowel's pronunciation shifts so that it is pronounced like a second vowel, and the second vowel is forced to change its pronunciation, is called a push chain.

However, according to professor Jürgen Handke, for some time, there was a phonetic split between words with the vowel and the diphthong , in words where the Middle English shifted to the Modern English . For an example, "high" was pronounced with the vowel , and "like" and "my" were pronounced with the diphthong . Therefore, for logical reasons, the close vowels could have diphthongized before the close-mid vowels raised. Otherwise, "high" would probably rhyme with "thee" rather than "my". This type of chain is called a drag chain.

The second phase of the Great Vowel Shift affected the Middle English open vowel , as in "mate", and the Middle English open-mid vowels , as in "meat" and "boat". Around 1550, Middle English was raised to . Then, after 1600, the new was raised to , with the Middle English open-mid vowels raised to close-mid . 
During the first and the second phases of the Great Vowel Shift, long vowels were shifted without merging with other vowels, but after the second phase, several vowels merged. The later changes also involved the Middle English diphthong , as in "day", which had monophthongised to , and merged with Middle English as in "mate" or as in "meat".

During the 16th and 17th centuries, several different pronunciation variants existed among different parts of the population for words like "meet", "meat", "mate", and "day". In each pronunciation variant, different pairs or trios of words were merged in pronunciation. Four different pronunciation variants are shown in the table below. The fourth pronunciation variant gave rise to Modern English pronunciation. In Modern English, "meet" and "meat" are merged in pronunciation and both have the vowel , and "mate" and "day" are merged with the diphthong , which developed from the 16th-century long vowel .
Modern English typically has the "meet"–"meat" merger: both "meet" and "meat" are pronounced with the vowel . Words like "great" and "steak", however, have merged with "mate" and are pronounced with the vowel , which developed from the shown in the table above.

The Great Vowel Shift affected other dialects as well as the standard English of southern England but in different ways. In Northern England, the long back vowels remained unaffected because the long mid back vowel had undergone an earlier shift. Similarly, the Scots language in Scotland had a different vowel system before the Great Vowel Shift, as had shifted to in Early Scots. In the Scots equivalent of the Great Vowel Shift, the long vowels , and shifted to , and by the Middle Scots period and remained unaffected.

The first step in the Great Vowel Shift in Northern and Southern English is shown in the table below. The Northern English developments of Middle English and were different from Southern English. In particular, the Northern English vowels in "bite", in "feet", and in "boot" shifted, while the vowel in "house" did not:
The vowel systems of Northern and Southern Middle English immediately before the Great Vowel Shift were different in one way. In Northern Middle English, the back close-mid vowel in "boot" had already shifted to front (a sound change known as fronting), like the long "" in German "hear". Thus, Southern English had a back close-mid vowel , but Northern English did not:

In both Northern and Southern English, the first step of the Great Vowel Shift raised the close-mid vowels to become close. Northern Middle English had two close-mid vowels – in "feet" and in "boot" – which were raised to and . Later on, Northern English changed to , so that "boot" has the same vowel as "feet". Southern Middle English had two close-mid vowels – in "feet" and in "boot" – which were raised to and .

In Southern English, the close vowels in "bite" and in "house" shifted to become diphthongs, but in Northern English, in "bite" shifted but in "house" did not.

If the difference between the Northern and Southern vowel shifts is caused by the vowel systems at the time of the Great Vowel Shift, did not shift because there was no back mid vowel in Northern English. In Southern English, shifting of to could have caused diphthongisation of original , but because Northern English had no back close-mid vowel to shift, the back close vowel did not diphthongise.

Not all words underwent certain phases of the Great Vowel Shift. Examples are "father", which failed to become , and "broad", which failed to become . The word "room", which was spelled as "roum" in Middle English, retains its Middle English pronunciation. It is an exception to the shifting of to because it is followed by "m", a labial consonant.

The class "ea" did not take the step to in several words. The presence of in "swear" and "bear" caused the vowel quality to be retained but not in the cases of "hear" and "near".
Shortening of long vowels at various stages produced further complications: "ea" is again a good example by shortening commonly before coronal consonants such as "d" and "th", thus: "dead", "head", "threat", "wealth" etc. (That is known as the bred–bread merger.) The "oo" was shortened from to , in many cases before "k", "d" and less commonly "t": "book", "foot", "good", etc. Some words subsequently changed from to : "blood", "flood". Similar but older shortening occurred for some instances of "ou": "could".

Some loanwords, such as "soufflé" and "umlaut", have retained a spelling from their origin language that may seem similar to the previous examples; but, since they were not a part of English at the time of the Great Vowel Shift, they are not actually exceptions to the shift.

During this same period, there were a number of consonant changes, particularly changes that were in combination with the vowel changes, or cases of silencing of consonants. An often-cited example is the word "knight", which in Middle English was pronounced . The "k" and the "gh" were silenced, but additionally the "i" changed its value to . This vowel shift, of course, was not independent of the consonant shift as it was the combined sound that actually made the transition to .

The following are examples of some of the most common consonant-related shifts that occurred:
The printing press was introduced to England in the 1470s by William Caxton and later Richard Pynson. The adoption and use of the printing press accelerated the process of standardization of English spelling, which continued into the 16th century. The standard spellings were those of Middle English pronunciation, and spelling conventions continued from Old English. However, the Middle English spellings were retained into Modern English while the Great Vowel Shift was taking place, which caused some of the peculiarities of Modern English spelling in relation to vowels.




</doc>
<doc id="12874" url="https://en.wikipedia.org/wiki?curid=12874" title="Gilbert Arthur à Beckett">
Gilbert Arthur à Beckett

Gilbert Arthur à Beckett (1837 – October 15, 1891) was an English writer.

Beckett was born at Portland House Hammersmith, on 7 April 1837, the eldest son of the civil servant and humorist Gilbert Abbott à Beckett and the composer Mary Anne à Beckett, daughter of Joseph Glossop, clerk of the cheque to the hon. corps of gentlemen-at-arms.
His brother was Arthur William à Beckett. 
He graduated from Christ Church, Oxford, as a Westminster scholar in 1860.
He was entered at Lincoln's Inn on 15 October 1857, but gave his attention chiefly to drama, producing "Diamonds and Hearts" at the Haymarket Theatre in 1867; this was followed by other light comedies. 
His adaptation of a French operetta by Émile Jonas called "The Two Harlequins" opened the new Gaiety Theatre, London in 1868, together with his distant cousin, W. S. Gilbert's, "Robert the Devil" and another piece.

Beckett's pieces include numerous burlesques and pantomimes, the libretti of "Savonarola" (Hamburg, 1884) and "The Canterbury Pilgrims" (Drury Lane, 1884) for the music of Dr. C. V. Stanford.
With the composer Alfred Cellier, Beckett wrote the operetta "Two Foster Brothers" (St. George's Hall, 1877).

In 1879, he had been asked by Tom Taylor, the editor of "Punch", to follow the example of his younger brother Arthur, and become a regular member of the staff of "Punch." 
Three years later he was 'appointed to the Table.' 
The "Punch" dinners 'were his greatest pleasure, and he attended them with regularity, although the paralysis of the legs, the result of falling down the stairway of Gower Street station, rendered his locomotion, and especially the mounting of Mr. Punch's staircase, a matter of painful exertion'. 
To "Punch" he contributed both prose and verse; he wrote, in greater part, the admirable parody of a boy's sensational shocker (March 1882), and he developed Jerrold's idea of humorous bogus advertisements under the heading 'How we advertise now.' 
The idea of one of Sir John Tenniel's best cartoons for "Punch," entitled 'Dropping the Pilot,' illustrative of Bismarck's resignation in 1889, was due to him.

Apart from his work on 'Punch,' he wrote songs and music for the German Reeds' entertainment, while in 1873 and 1874 he was collaborator in two dramatic productions which evoked a considerable amount of public attention.
On 3 March 1873, "The Happy Land" was given at the Court Theatre, 1873, a daring political satire and burlesque of W. S. Gilbert's "The Wicked World". 
In this amusing piece of banter three statesmen (Gladstone, Lowe, and Ayrton) were represented as visiting Fairyland in order to impart to the inhabitants the secrets of popular government. The actors representing 'Mr. G.,' 'Mr. L.,' and 'Mr. A.' were dressed so as to resemble the ministers satirised, and the representation elicited a question in the House of Commons and an official visit of the Lord Chamberlain to the theatre, with the result that the actors had to change their 'make-up.'

In the following year, he furnished the 'legend' to Herman Merivale's tragedy 'The White Pilgrim,' first given at the Court in February 1874. 
At the close of his life he furnished the 'lyrics' and most of the book for the operetta 'La Cigale,' which at the time of his death was nearing its four hundredth performance at the Lyric Theatre.

In 1889, he suffered a great shock from the death by drowning of his only son, and he died in London on 15 October 1891, and was buried in Mortlake cemetery.

"Punch" devoted some appreciative stanzas to his memory, bearing the epigraph 'Wearing the white flower of a blameless life' (24 Oct. 1891). His portrait appeared in the well-known drawing of 'The Mahogany Tree' ("Punch", Jubilee Number, 18 July 1887), and likenesses were also given in the 'Illustrated London News' and in Spielmann's 'History of Punch' (1895). 
He married Emily, eldest daughter of William Hunt, J.P., of Bath, and his only daughter Minna married in 1896 Mr. Hugh Clifford, C.M.G., governor of Labuan and British North Borneo.



</doc>
<doc id="12875" url="https://en.wikipedia.org/wiki?curid=12875" title="Glaucus (disambiguation)">
Glaucus (disambiguation)

Glaucus (; , "greyish blue" or "bluish green" and "glimmering") is a Greek name. In modern Greek usage, the name is usually transliterated "Glafkos". It may refer to:










</doc>
<doc id="12878" url="https://en.wikipedia.org/wiki?curid=12878" title="George Gordon, 1st Earl of Aberdeen">
George Gordon, 1st Earl of Aberdeen

George Gordon, 1st Earl of Aberdeen (3 October 163720 April 1720), was a Lord Chancellor of Scotland.

Gordon, born on 3 October 1637, the second son of Sir John Gordon, 1st Baronet, of Haddo, Aberdeenshire, (executed in 1644);

At the Restoration the sequestration of his father's lands was annulled, and in 1665 he succeeded by the death of his elder brother as the "3rd Baronet Gordon, of Haddo" and to the family estates. He returned home in 1667, was admitted advocate in 1668 and gained a high legal reputation. He represented Aberdeenshire in the Parliament of Scotland of 1669 and in the following assemblies, during his first session strongly opposing the projected union of the two legislatures. In November 1678 he was made a Privy Counsellor for Scotland, and in 1680 was raised to the bench as Lord Haddo. He was a leading member of the Duke of York's administration, was created a Lord of the Articles in June and in November 1681 Lord President of the Privy Council. The same year he is reported as moving in the council for the torture of witnesses.

In 1682 he was made Lord Chancellor of Scotland, and was created, on 13 November, Earl of Aberdeen, Viscount Formartine, and Lord Haddo, Methlick, Tarves and Kellie, in the Scottish peerage, being appointed also Sheriff Principal of Aberdeenshire and Midlothian.

Burnet reflected unfavourably upon him, writing of him, "...a proud and covetous man ... the new chancellor exceeded all that had gone before him.

He executed the laws enforcing religious conformity with severity, and filled the parish churches, but resisted the excessive measures of tyranny prescribed by the English government; and in consequence of an intrigue of the Duke of Queensberry and Lord Perth, who gained the duchess of Portsmouth with a present of £27,000, he was dismissed in 1684.

After his fall he was subjected to various petty prosecutions by his victorious rivals with the view of discovering some act of maladministration on which to found a charge against him, but the investigations only served to strengthen his credit. He took an active part in parliament in 1685 and 1686, but remained a non-juror during the whole of William's reign, being frequently fined for his non-attendance, and took the oaths for the first time after Anne's accession, on 11 May 1703.

In the great affair of the Union in 1707, while protesting against the completion of the treaty till the act declaring the Scots aliens should be repealed, he refused to support the opposition to the measure itself and refrained from attending parliament when the treaty was settled.

He is described by John Mackay as, "...very knowing in the laws and constitution of his country and is believed to be the solidest statesman in Scotland, a fine orator, speaks slow but sure.

His person was said to be deformed, and his "want of mine or deportment" was alleged as a disqualification for the office of Lord Chancellor.

He married Anne Lockhart, daughter and (eventual) sole heiress of George Lockhart of Tarbrax and Anne Lockhart. They had several children:

His only surviving son, William, succeeded him as 2nd earl of Aberdeen. He died on 20 April 1720, having amassed a large fortune.

Attribution


</doc>
<doc id="12879" url="https://en.wikipedia.org/wiki?curid=12879" title="George Hamilton-Gordon, 4th Earl of Aberdeen">
George Hamilton-Gordon, 4th Earl of Aberdeen

George Hamilton-Gordon, 4th Earl of Aberdeen, (28 January 178414 December 1860), styled Lord Haddo from 1791 to 1801, was a British politician, diplomat and landowner, successively a Tory, Conservative and Peelite, who served as Prime Minister from 1852 until 1855 in a coalition between the Whigs and Peelites, with Radical and Irish support. The Aberdeen ministry was filled with powerful and talented politicians, whom Aberdeen was largely unable to control and direct. Despite trying to avoid this happening, it took Britain into the Crimean War, and fell when its conduct became unpopular, after which Aberdeen retired from politics.

Aberdeen's career was dominated by foreign policy, but his experience did not prevent the slide towards the Crimean War. His personal life was marked by the loss of both parents by the time he was eleven, and of his first wife after only seven years of a happy marriage. His daughters died young, and his relations with his sons were difficult. Before his marriage he travelled extensively in Europe, including Greece, and he had a serious interest in the classical civilisations and their archaeology. On his return to Britain in 1805 he devoted much time and energy to improving conditions on his Scottish estates.

After the death of his wife in 1812 he became a diplomat, almost immediately being given the important embassy to Vienna while still in his twenties. His rise in politics was equally rapid and lucky, and "two accidents — Canning's death and Wellington's impulsive acceptance of the Canningite resignations" led to him becoming Foreign Secretary to the Duke of Wellington in 1828 despite "an almost ludicrous lack of official experience"; he had been a minister for less than six months. After holding the position for two years, followed by another cabinet role, by 1841 his experience led to his appointment as Foreign Secretary again under Robert Peel for a longer term. This was despite his being a "notoriously bad speaker", which mattered far less in the House of Lords, and having a "dour, awkward, occasionally sarcastic exterior". Nonetheless his Peelite colleague, later himself Prime Minister, William Ewart Gladstone, said of him that he was "the man in public life of all others whom I have . I say emphatically . I have others, but never like him".

Born in Edinburgh on 28 January 1784, he was the eldest son of George Gordon, Lord Haddo, son of George Gordon, 3rd Earl of Aberdeen. His mother was Charlotte, youngest daughter of William Baird of Newbyth. He lost his father in 1791 and his mother in 1795 and was brought up by Henry Dundas, 1st Viscount Melville and William Pitt the Younger. He was educated at Harrow, and St John's College, Cambridge, where he graduated with a Master of Arts in 1804. Before this, however, he had become Earl of Aberdeen on his grandfather's death in 1801, and had travelled all over Europe. On his return to England, he founded the Athenian Society. In 1805, he married Lady Catherine Elizabeth, daughter of John Hamilton, 1st Marquess of Abercorn.

In December 1805 Lord Aberdeen took his seat as a Tory Scottish representative peer in the House of Lords. In 1808, he was created a Knight of the Thistle. Following the death of his wife from tuberculosis in 1812 he joined the Foreign Service. He was appointed Ambassador Extraordinary and Minister Plenipotentiary to Austria, and signed the Treaty of Töplitz between Britain and Austria in Vienna in October 1813. In the company of the Austrian Emperor, Francis II he was an observer at the decisive Coalition victory of the Battle of Leipzig in October 1813; he had met Napoleon in his earlier travels. He became one of the central diplomatic figures in European diplomacy at this time, and he was one of the British representatives at the Congress of Châtillon in February 1814, and at the negotiations which led to the Treaty of Paris in May of that year.

Aberdeen was greatly affected by the aftermath of war which he witnessed at first hand. He wrote home:

The near approach of war and its effects are horrible beyond what you can conceive. The whole road from Prague to [Teplitz] was covered with waggons full of wounded, dead, and dying. The shock and disgust and pity produced by such scenes are beyond what I could have supposed possible...the scenes of distress and misery have sunk deeper in my mind. I have been quite haunted by them.
Returning home he was created a peer of the United Kingdom as Viscount Gordon, of Aberdeen in the County of Aberdeen (1814), and made a member of the Privy Council. In July 1815 he married his former sister-in-law Harriet, daughter of John Douglas, and widow of James Hamilton, Viscount Hamilton; the marriage was much less happy than his first. During the ensuing thirteen years Aberdeen took a less prominent part in public affairs.

Lord Aberdeen served as Chancellor of the Duchy of Lancaster between January and June 1828 and subsequently as Foreign Secretary until 1830 under the Duke of Wellington. He resigned with Wellington over the Reform Bill of 1832.
He was Secretary of State for War and the Colonies between 1834 and 1835, and again Foreign Secretary between 1841 and 1846 under Sir Robert Peel. It was during his second stint as Foreign Secretary, that he had the harbor settlement of 'Little Hong Kong', on the south side of Hong Kong Island named after him. It was probably the most productive period of his career, that he settled two disagreements with the US – the Northeast Boundary dispute by the Webster-Ashburton Treaty (1842), and the Oregon dispute by the Oregon Treaty of 1846. He also worked successfully to improve relationships with France, where Guizot had become a personal friend. He enjoyed the trust of Queen Victoria, which was still important for a Foreign Secretary. He again followed his leader and resigned with Peel over the issue of the Corn Laws.

After Peel's death in 1850 he became the recognised leader of the Peelites. In July 1852, a general election of Parliament was held which resulted in the election of 325 Tory/Conservative party members to Parliament. This represented 42.7% seats in Parliament. The main opposition to the Tory/Conservative Party was the Whig Party, which elected 292 members of the party to the Parliament in July 1852. Although occupying fewer seats than the Tory/Conservatives, the Whigs had a chance to draw support from the minor parties and independents who were also elected in July 1852. Lord Aberdeen as the leader of the Peelites was one of 38 Peelites elected to members of Parliament independently of the Tory/Conservative Party.

While the Peelites agreed with the Whigs on issues dealing with the international trade, there were other issues which the Peelites disagreed with the Whigs. Indeed, Lord Aberdeen's own dislike of the Ecclesiastical Titles Assumption Bill, the rejection of which he failed to secure in 1851, prevented him from joining the Whig government of Lord John Russell in 1851. Additionally, 113 of the members of Parliament elected in July 1852 were Free Traders. These members agreed with the Peelites on the repeal of the "Corn Laws," but they felt that the tariffs on "all" consumer products should be removed.

Furthermore, 63 members of Parliament elected in 1852, were members of the "Irish Brigade," who voted with the Peelites and the Whigs for the repeal of the Corn Laws because they sought an end the Great Irish Famine by means of cheaper wheat and bread prices for the poor and middle classes in Ireland. Currently, however, neither the Free Traders and the Irish Brigade had disagreements with the Whigs that prevented them from joining with the Whigs form a government. Accordingly, the Tory/Conservative Party leader the Earl of Derby was asked to form a "minority government". However, the Earl of Derby appointed Benjamin Disraeli as the Chancellor of the Exchequer for the minority government.

When in December 1852, the new Chancellor of the Exchequer submitted his budget to Parliament on behalf of the minority government, the Peelites, the Free Traders and the Irish Brigade were all alienated by the proposed budget. Accordingly, each of these groups suddenly forgot their differences with the Whig Party and voted with the Whigs against the proposed budget. The vote was 286 in favour of the budget and 305 votes against the budget. Because the leadership of the minority government had made the vote on the budget vote a "vote of confidence" in the minority government, the defeat of the Disraeli budget was a "vote of no confidence" in the minority government and meant the downfall of the minority government. Accordingly, Lord Aberdeen was asked to form a new government.

 Following the downfall of the Tory/Conservative minority government under Lord Derby in December 1852, Lord Aberdeen formed a new government from the coalition of Free Traders, Peelites and Whigs that had voted no confidence in the minority government. Lord Aberdeen was able to put together a coalition government of these groups that held 53.8% of the seats of Parliament. Thus Lord Aberdeen, a Peelite, became Prime Minister and headed a coalition ministry of Whigs and Peelites.

Although united on international trade issues and on questions of domestic reform, his cabinet which also contained Lord Palmerston and Lord John Russell, who were certain to differ on questions of foreign policy. Charles Greville says in his "Memoirs", "In the present cabinet are five or six first-rate men of equal, or nearly equal, pretensions, none of them likely to acknowledge the superiority or defer to the opinions of any other, and every one of these five or six considering himself abler and more important than their premier"; and Sir James Graham wrote, "It is a powerful team, but it will require good driving"; this Aberdeen was unable to provide. During the administration, much trouble was caused by the rivalry between these two, and over the course of it Palmerston managed to out-manoeuvre Russell to emerge as the Whig heir-apparent. The cabinet also included a single Radical Sir William Molesworth, but much later, when justifying to the Queen his own new appointments, Gladstone told her: "For instance, even in Ld Aberdeen's Govt, in 52, Sir William Molesworth had been selected, at that time, a very advanced Radical, but who was perfectly harmless, & took little, or no part ... He said these people generally became very moderate, when they were in office", which she admitted had been the case.

One of the foreign policy issues on which Palmerston and Russell disagreed was the type of relationship that Britain should have with France and especially France's ruler, Louis-Napoléon Bonaparte. Bonaparte was the nephew of the famous Napoleon Bonaparte, who had become dictator and then Emperor of France from 1804 until 1814. Bonaparte had been elected to a three-year term as President of the Second Republic of France on 20 December 1848. The Constitution of the Second Republic limited the President to a single term in office. Thus, Louis Bonaparte would be unable to succeed himself and after 20 December 1851 would no longer be President. Thus, on 2 December 1851, shortly before the end of his single three-year term in office was to expire, Bonaparte staged a coup against the Second Republic in France, disbanded the elected Constituent Assembly, arrested some of the Republican leaders and declared himself Emperor Napoleon III of France. This coup upset many democrats in England as well as in France. Some British government officials felt that Louis Bonaparte was seeking foreign adventure in the spirit of his uncle—Napoleon I. Consequently, these officials felt that any close association with Bonaparte would eventually lead Britain into another series of wars, like the wars with France and Napoleon dating from 1793 until 1815. British relations with France had scarcely improved since 1815. As prime minister, the Earl of Aberdeen was one of these officials, who feared France and Bonaparte.

However, other British government officials were beginning to worry more about the rising political dominance of the Russian Empire in eastern Europe and the corresponding decline of the Ottoman Empire. Lord Palmerston, who at the time of Louis Bonaparte's 2 December 1851 coup was serving as the Secretary of State for Foreign Affairs in the Whig government of Prime Minister Lord John Russell. Without informing the rest of the cabinet or Queen Victoria, Palmerston had sent a private note to the French ambassador endorsing Louis Bonaparte's coup and congratulating Louis Bonaparte, himself, on the coup. Queen Victoria and members of the Russell government demanded that Palmerston be dismissed as Foreign Minister. Russell requested Palmerston's resignation and Palmerston reluctantly provided it.

In February 1852, Palmerston took revenge on Russell by voting with the Conservatives in a "no confidence" vote against the Russell government. This brought an end to the Russell Whig government and set the stage for a general election in July 1852 which eventually brought the Conservatives to power in a minority government under the Earl of Derby. Another problem facing the Earl of Aberdeen in the formation of his new government in December 1852, was Lord John Russell himself. Russell was the leader of the Whig Party, the largest group in the coalition government. Consequently, Lord Aberdeen, was required to appoint Russell as the Secretary of State for Foreign Affairs, which he had done on 29 December 1852. However, Russell sometimes liked to use this position to speak for the whole government, as if he were the prime minister. In 1832, Russell had been nicknamed "Finality John" because of his statement that the 1832 Reform Act had just been approved by both the House of Commons and the House of Lords would be the "final" expansion of the vote in Britain. There would be no further extension of the ballot to the common people of Britain. However, as political pressure in favour of further reform had risen over the twenty years since 1832, Russell had changed his mind. While the Whigs were still part of the Opposition under the minority government of the Earl of Derby, Russell had said, in January 1852, that he intended to introduce a new reform bill into the House of Commons which would equalise the populations of the districts from which members of Parliament were elected. Probably as a result of their continuing feud, Palmerston declared himself against this Reform Bill of 1852. As a result, support for the Reform Bill of 1852 dwindled and Russell was forced to change his mind again and not introduce the any Reform Bill in 1852.

In order to form the coalition government, the Earl of Aberdeen had been required to appoint both Palmerston and Russell to his cabinet. Because of the controversy surrounding Palmerston's removal as Secretary of State for Foreign Affairs because of his letter to the French ambassador endorsing Louis Bonaparte's 2 December 1851 coup, Palmerston could not now be appointed Foreign Minister again so soon after his removal from the same position. Thus on 28 December 1852, the Earl of Aberdeen appointed Palmerston as Home Secretary and appointed Russell as Foreign Minister.

Given the differences of opinion within the Lord Aberdeen cabinet over the direction of foreign policy with regard to relations between Britain and the French under Napoleon III, it is not surprising that debate raged within the government as Louis Bonaparte, now assuming the title of Emperor Napoleon III of France. As Prime Minister of the Peelite/Whig coalition government, the Earl of Aberdeen eventually led Britain into war on the side of the French/Ottomans against the Russian Empire. This war would eventually be called Crimean War, but the entire foreign policy negotiations surrounding the dismemberment of the Ottoman Empire, which would continue throughout the middle and end of nineteenth century the problem would be referred to as the "Eastern Question". Aberdeen was ordinarily a sympathiser with Russian interests against French/Napoleonic interests. Thus, he was really not in favour of the entrance of into the Crimean War. However, he was following the pressure that was being exerted on him from some members of his cabinet, including Palmerston, who in this rare instance was actually being supported by John Russell, both of whom were in favour of a more aggressive policy against perceived Russian expansion. Aberdeen, unable to control Palmerston, acquiesced. However the Eastern Question and the resulting Crimean War proved to be the downfall of his government.

The Eastern Question began as early as the 2 December 1852 with the Napoleonic coup against the Second Republic of France. As he was forming his new imperial government, Napoleon III sent an ambassador to the Ottoman Empire with instructions to assert France's right to protect Christian sites in Jerusalem and the Holy Land. The Ottoman Empire agreed to this condition to avoid conflict or potential war with France. Aberdeen, as Foreign Secretary, had himself tacitly authorised the construction of the first Anglican church in Jerusalem in 1845, following his predecessor's commission in 1838 of the first European Consul in Jerusalem on Britain's behalf, which lead to series of successive appointments by other nations. Both resulted from Lord Shaftesbury's canvassing with substantial public support. Nevertheless, Britain became increasingly worried about the situation in Turkey and Prime Minister Aberdeen sent Lord Stratford de Redcliffe, a diplomat with vast experience in Turkey, as a special envoy to the Ottoman Empire to guard British interests. Russia protested the Turkish agreement with the French as a violation of the Treaty of Küçük Kaynarca of 1778—the treaty which ended the Russo-Turkish War (1768–74). Under this treaty, the Russians had been granted the exclusive right to protect the Christian sites in the Holy Land. Accordingly. on 7 May 1853, the Russians sent Prince Alexander Sergeyevich Menshikov, one their premier statesmen to negotiate a settlement of the issue. Prince Menshikov called the attention of the Turks to the fact that during the Russo-Turkish War, the Russians had occupied the Turkish controlled provinces of Wallachia and Moldavia on the north bank of the Danube River, but he reminded them that pursuant to the Treaty of Küçük Kaynarca, however, the Russians had returned these "Danubian provinces" to Ottoman control in exchange for the right to protect the Christian sites in the Holy Land. Accordingly, the Turks reversed themselves and agreed with the Russians.

The French sent one of their premier ships-of-the-line, the "Charlemagne" to the Black Sea as a show of force. In light of the French show of force, the Turks, again, reversed themselves and recognised the French right to protect the Christian sites. Lord Stratford de Redcliffe was advising the Ottomans during this time and later, it was alleged, that he had been instrumental in persuading the Turks to reject the Russian arguments.

As war became inevitable, Aberdeen wrote to Russell:
The abstract justice of the cause, although indisputable, is but a poor consolation for the inevitable calamities of all war, or for a decision which I am not without fear may prove to have been impolitic and unwise. My conscience upbraids me the more, because seeing, as I did from the first, all that was to be apprehended, it is possible that by a little more energy and vigour, not on the Danube, but in Downing Street, it might have been prevented.

In response this latest change of mind by the Ottomans/Turks, the Russians, on 2 July 1853 occupied the Turkish-satellite states of Wallachia and Moldavia, as they had during the Russo-Turkish War of 1768–1774. Almost immediately, the Russian troops deployed along the northern banks of the Danube River, implying that they may cross the river. Aberdeen ordered the British Fleet to Constantinople and later into the Black Sea. On 23 October 1853, the Ottoman Empire declared war on Russia. A Russian naval raid on Sinope, on 30 November 1853, resulted in the destruction of the Turkish fleet in the battle of Sinope. When Russia ignored an Anglo-French ultimatum to abandon the Danubian provinces, Britain and France declared war on Russia on 28 March 1854. In September 1854, British and French troops landed on the Crimean peninsula at Eupatoria north of Sevastopol. The Allied troops then moved across the Alma River on 20 September 1854 at the battle of Alma and set siege to the fort of Sevastopol.

A Russian attack on the allied supply base at Balaclava on 25 October 1854 was rebuffed. The Battle of Balaclava is noted for its famous (or rather infamous) Charge of the Light Brigade. On 5 November 1854, Russian forces tried to relieve the siege at Sevastopol and tried to defeat the Allied armies in the field in the Battle of Inkerman. However, this attempt failed and the Russians were rebuffed. Dissatisfaction as to the course of the war arose in England. As reports returned detailing the mismanagement of the conflict arose Parliament began to investigate. On 29 January 1855, John Arthur Roebuck introduced a motion for the appointment of a select committee to enquire into the conduct of the war. This motion was carried by the large majority of 305 in favour and 148 against.
Treating this as a vote of no confidence in his government, Aberdeen resigned, and retired from active politics, speaking for the last time in the House of Lords in 1858. In visiting Windsor Castle to resign, he told the Queen: "Nothing could have been better, he said than the feeling of the members towards each other. Had it not been for the incessant attempts of Ld John Russell to keep up party differences, it must be acknowledged that the experiment of a coalition had succeeded admirably. We discussed future possibilities & agreed that nothing remained to be done, but to offer the Govt to Ld Derby...". The Queen continued to criticise Lord John Russell, for his behaviour for the rest of his life, on his death in 1878 her journal records that he was: "A man of much talent, who leaves a name behind him, kind, & good, with a great knowledge of the constitution, who behaved very well, on many trying occasions; but he was impulsive, very selfish (as shown on many occasions, especially during Ld Aberdeen's administration) vain, & often reckless & imprudent".

Lord Aberdeen married Lady Catherine Hamilton (10 January 1784 – 29 February 1812; daughter of Lord Abercorn) on 28 July 1805. They have four children.

He remarried Harriet Douglas (paternal granddaughter of James Douglas, 14th Earl of Morton and maternal granddaughter of Edward Lascelles, 1st Earl of Harewood) on 8 July 1815. They have five children:

The Countess of Aberdeen died in August 1833. Lord Aberdeen died at Argyll House, St. James's, London, on 14 December 1860, and was buried in the family vault at Stanmore church. In 1994 the novelist, columnist and politician Ferdinand Mount used George Gordon's life as the basis for a historical novel, "Umbrella".

Apart from his political career Aberdeen was also a scholar of the classical civilisations, who published "An Inquiry into the Principles of Beauty in Grecian Architecture" (London, 1822) and was referred to by his cousin Lord Byron in his "English Bards and Scotch Reviewers" (1809) as "the travell'd thane, Athenian Aberdeen." He was appointed Chancellor of the University of Aberdeen in 1827 and was President of the Society of Antiquaries of London.

Aberdeen's biographer Muriel Chamberlain summarises "Religion never came easy to him". In his Scots landowning capacity "North of the border, he considered himself "ex officio" a Presbyterian"; in England "he privately considered himself an Anglican", as early as 1840 told Gladstone he preferred what Aberdeen called "the sister church [of England]" and when in London worshipped at St James's Piccadilly. He was ultimately buried in the Anglican parish church at Stanmore, Middlesex.

He was a member of the General Assembly of the Church of Scotland from 1818 to 1828 and exercised his existing rights to present ministers to parishes on his Scottish estates through a time when the right of churches to veto the appointment or 'call' of a minister became so contentious as to lead in 1843 to the schism known as "the Disruption" when a third of ministers broke away to form the Free Church of Scotland. In the House of Lords, in 1840 and 1843, he raised two Compromise Bills to allow presbyteries but not congregations the right of veto. The first failed to pass (and was voted against by the General Assembly) but the latter, raised post-schism, became law for Scotland and remained in force until patronage of Scots livings was abolished in 1874.

It was under his prime ministership that the revival of the Convocations of Canterbury and York began, though they did not obtain their potential power till 1859.

He is said in the last few months of his life, post Crimean War, to have declined to contribute to building a church on his Scotland estates because of sense of guilt in having "shed much blood", citing biblically King David's being forbidden to build the Temple in Jerusalem.




</doc>
<doc id="12880" url="https://en.wikipedia.org/wiki?curid=12880" title="GnuCash">
GnuCash

GnuCash is an accounting program that implements a double-entry bookkeeping system. It was initially aimed at developing capabilities similar to Intuit, Inc.'s Quicken application, but also has features for small business accounting. Recent development has been focused on adapting to modern desktop support-library requirements.

GnuCash is part of the GNU Project, and runs on Linux, GNU, OpenBSD, FreeBSD, Solaris, macOS, and other Unix-like platforms. A Microsoft Windows (2000 or newer) port was made available starting with the 2.2.0 series.

Programming on GnuCash began in 1997, and its first stable release was in 1998. Small Business Accounting was added in 2001. A Mac installer became available in 2004. A Windows port was released in 2007.

In May 2012, the development of GnuCash for Android was announced. This is an expense-tracking companion app for GnuCash, as opposed to a stand-alone accounting package.



GnuCash is written primarily in C, with a small fraction in Scheme. One of the available features is pure fixed-point arithmetic to avoid rounding errors which would arise with floating-point arithmetic. This feature was introduced with version 1.6.

The Android App for GnuCash is written in Java and does not share any code with the PC software.

Users on the GnuCash mailing list have reported using it for United States 501(c)3 non-profit organizations successfully. However, the reports need to be exported and edited.

Slaw, a Canadian legal webzine, offered this advice to lawyers just starting out in practice, especially those who are trying to pay off student loans, "The GnuCash software...should present a great alternative for lawyers looking for a solid accounting system at low cost. Do not believe that open source software is somehow second-class."

In April 2011, the Minnesota State Bar Association made their GnuCash trust accounting guide freely available in PDF format.

As of July 2018, Sourceforge shows a count of over 6.3 million downloads of the stable releases starting from November 1999 Also, Sourceforge shows that current downloads are running at ~7,000 per week.

Open_Hub's analysis based on commits up to May 2018 (noninclusive) concluded that the project has a mature, well established code base with increasing year-over-year development activity. Moreover, "Over the past twelve months, 51 developers contributed new code to GnuCash. This is one of the largest open-source teams in the world, and is in the top 2% of all project teams on Open Hub."



</doc>
<doc id="12881" url="https://en.wikipedia.org/wiki?curid=12881" title="George Robert Aberigh-Mackay">
George Robert Aberigh-Mackay

George Robert Aberigh-Mackay (25 July 184812 January 1881), Anglo-Indian writer, was the son of the Reverend James Aberigh-Mackay D.D., B.D. and his first wife Lucretia Livingston née Reed. He was educated at Magdalen College School, Oxford and Cambridge University. Entering the Indian education department in 1870, he became professor of English literature in Delhi College in 1873, tutor to the Raja of Rutlam in 1876, and principal of the Rajkumar College at Indore in 1877. On 8 January 1881 he developed symptoms of tetanus after playing polo and tennis on the previous 2 days, and died on 12 January 1881 in Indore.

He is best known for his book "Twenty-one Days in India" (1878–1879), a satire upon Anglo-Indian society and modes of thought. This book gave promise of a successful literary career, but the author died at the age of thirty-three.
Aberigh-Mackay wrote also an extensive manual giving first-hand data about the princely states and their rulers.

George Robert Aberigh-Mackay married Mary Ann Louisa Cherry on the 13th of October 1873 at Simla, Bengal, India.
Children of George Robert Aberigh-Mackay & Mary Ann Louisa Cherry



</doc>
<doc id="12882" url="https://en.wikipedia.org/wiki?curid=12882" title="Gallon">
Gallon

The gallon () is a unit of measurement for fluid capacity in both the US customary units and the British imperial systems of measurement. Three significantly different sizes are in current use: the imperial gallon defined as litres (4 imperial quarts or 8 imperial pints), which is used in the United Kingdom, Canada, and some Caribbean nations; the US gallon defined as 231 cubic inches (4 US liquid quarts or 8 US liquid pints) or about , which is used in the US and some Latin American and Caribbean countries; and the least-used US dry gallon defined as .

The IEEE standard symbol for the gallon is gal.

The gallon currently has one definition in the imperial system, and two definitions (liquid and dry) in the US customary system. Historically, there were many definitions and redefinitions.

There were more than a few systems of liquid measurements in the pre-1884 United Kingdom.

The imperial (UK) gallon, now defined as exactly litres (about 277.42 cubic inches), is used in some Commonwealth countries and was originally based on the volume of 10 pounds (approximately 4.54 kg) of water at . The imperial fluid ounce is defined as of an imperial gallon; there are four quarts in a gallon, two pints in a quart, and 20 Imperial fluid ounces in an imperial pint.

The US gallon is legally defined as 231 cubic inches, which is exactly . A US liquid gallon of water weighs about at , making it about 16.6% lighter than the imperial gallon. There are four quarts in a gallon, two pints in a quart and 16 US fluid ounces in a US pint, which makes the US fluid ounce equal to of a US gallon. In order to overcome the effects of expansion and contraction with temperature when using a gallon to specify a quantity of material for purposes of trade, it is common to define the temperature at which the material will occupy the specified volume. For example, the volume of petroleum products and alcoholic beverages are both referenced to in government regulations.

This dry measure is one-eighth of a US "Winchester" bushel of cubic inches; it is therefore equal to exactly 268.8025 cubic inches or about . The US dry gallon is not used in commerce, and is not listed in the relevant statute, which jumps from the dry quart to the peck.

Gallons used in fuel economy expression in Canada and the United Kingdom are Imperial gallons.

Despite its status as a U.S. territory, and unlike American Samoa, the Northern Mariana Islands, Guam, and the U.S. Virgin Islands, Puerto Rico ceased selling gasoline by the US gallon in 1980.

The gallon was removed from the list of legally defined primary units of measure catalogued in the EU directive 80/181/EEC for trading and official purposes, with effect from 31 December 1994. Under the directive the gallon could still be used – but only as a supplementary or secondary unit. One of the effects of this directive was that the United Kingdom amended its own legislation to replace the gallon with the litre as a primary unit of measure in trade and in the conduct of public business, effective from 30 September 1995.

Ireland also passed legislation in response to the EU directive, with the effective date being 31 December 1993. Though the gallon has ceased to be the legally defined primary unit, it can still be legally used in both the UK and Ireland as a supplementary unit.

The United Arab Emirates started selling gasoline by the litre in 2010, along with Guyana, and Panama in 2013.
The two former had used the Imperial gallon and the latter the US gallon until they switched.
Myanmar (Burma) switched from Imperial gallon to litre sales before 2014.

The Imperial gallon continues to be used as a unit of measure in Anguilla, Antigua and Barbuda, the Bahamas, the British Virgin Islands, the Cayman Islands, Dominica, Grenada, Montserrat, St. Kitts & Nevis, St. Lucia, and St. Vincent & the Grenadines.

Other than the United States, the US gallon is used in Liberia, Belize, Colombia, The Dominican Republic, Ecuador, El Salvador, Guatemala, Haiti, Honduras, Nicaragua, and Peru, but only for the sale of gasoline. All other products are sold in litres and its multiples and submultiples.

Antigua and Barbuda planned to switch over to using litres by 2015, but the switch-over had not been effected.

In the Turks & Caicos Islands, both the U.S. gallon and Imperial gallon are used, due to an increase in tax duties disguised by levying the same duty on the 3.79 L U.S. gallon as was previously levied on the 4.55 L Imperial gallon.

Both the US liquid and imperial gallon are divided into four quarts ("quart"er gallons), which in turn are divided into two pints. These pints are divided into two cups (though the imperial cup is rarely used now), which in turn are divided into two gills (gills are also rarely used). Thus a gallon is equal to four quarts, eight pints, sixteen cups or thirty-two gills. The imperial gill is further divided into five fluid ounces, whereas the US gill is divided into four fluid ounces. Thus an imperial fluid ounce is of an imperial pint or of an imperial gallon, while a US fluid ounce is of a US pint or of a US gallon.

The imperial gallon, quart, pint, cup and gill are approximately 20% larger than their US counterparts and are therefore not interchangeable. The imperial fluid ounce, on the other hand, is only 4% smaller than the US fluid ounce and therefore they are often used interchangeably.

In the US, liquor is often sold in "fifths", which are approximately one-fifth of a US gallon.

The term derives most immediately from "galun, galon" in Old Northern French, but the usage was common in several languages, for example in Old French and (bowl) in Old English. This suggests a common origin in Romance Latin, but the ultimate source of the word is unknown. The gallon originated as the base of systems for measuring wine, and beer in England. The sizes of gallon used in these two systems were different from each other: the first was based on the wine gallon (equal in size to the US gallon), and the second on either the ale gallon or the larger imperial gallon.

By the end of the 18th century, three definitions of the gallon were in common use: 

The "corn" or "dry gallon" was used in the United States until recently for grain and other dry commodities. It is one-eighth of the (Winchester) bushel, originally a cylindrical measure of inches in diameter and 8 inches in depth. That made the dry gallon . The bushel, which like dry quart and pint still sees some use, was later defined to be 2150.42 cubic inches exactly, making its gallon exactly ().
In previous centuries, there had been a corn gallon of around 271 to 272 cubic inches.

The "wine", "fluid", or "liquid gallon" has been the standard US gallon since the early 19th century. The wine gallon, which some sources relate to the volume occupied by eight medieval merchant pounds of wine, was at one time defined as the volume of a cylinder 6 inches deep and 7 inches in diameter, i.e. . It had been redefined during the reign of Queen Anne, in 1706, as 231 cubic inches exactly, which is the result of the earlier definition with π approximated to . Although the wine gallon had been used for centuries for import duty purposes there was no legal standard of it in the Exchequer and a smaller gallon was actually in use, so this statute became necessary. It remains the US definition today.

In 1824, Britain adopted a close approximation to the "ale gallon" known as the "imperial gallon" and abolished all other gallons in favour of it. Inspired by the kilogram-litre relationship, the imperial gallon was based on the volume of 10 pounds of distilled water weighed in air with brass weights with the barometer standing at 30 inches of mercury and at a temperature of . In 1963, this definition was refined as the space occupied by 10 pounds of distilled water of density weighed in air of density against weights of density (the original "brass" was refined as the density of brass alloys vary depending on metallurgical composition). This works out at approximately (). The metric definition of exactly cubic decimetres (also after the litre was redefined in 1964, ≈ ) was adopted shortly afterwards in Canada, but from 1976 the conventional value of was used in the United Kingdom until the Canadian convention was adopted in 1985.

Historically, gallons of various sizes were used in many parts of Western Europe. In these localities, it has been replaced as the unit of capacity by the litre.


</doc>
<doc id="12883" url="https://en.wikipedia.org/wiki?curid=12883" title="Gini coefficient">
Gini coefficient

In economics, the Gini coefficient ( ; sometimes expressed as a Gini ratio or a normalized Gini index) is a measure of statistical dispersion intended to represent the income or wealth distribution of a nation's residents, and is the most commonly used measurement of inequality. It was developed by the Italian statistician and sociologist Corrado Gini and published in his 1912 paper "Variability and Mutability" ().

The Gini coefficient measures the inequality among values of a frequency distribution (for example, levels of income). A Gini coefficient of zero expresses perfect equality, where all values are the same (for example, where everyone has the same income). A Gini coefficient of 1 (or 100%) expresses maximal inequality among values (e.g., for a large number of people, where only one person has all the income or consumption, and all others have none, the Gini coefficient will be very nearly one). However, a value greater than one may occur if some persons represent negative contribution to the total (for example, having negative income or wealth). For larger groups, values close to or above 1 are very unlikely in practice. Given the normalization of both the cumulative population and the cumulative share of income used to calculate the Gini coefficient, the measure is not overly sensitive to the specifics of the income distribution, but rather only on how incomes vary relative to the other members of a population. The exception to this is in the redistribution of wealth resulting in a minimum income for all people. When the population is sorted, if their income distribution were to approximate a well-known function, then some representative values could be calculated.

The Gini coefficient was proposed by Gini as a measure of inequality of income or wealth. For OECD countries, in the late 20th century, considering the effect of taxes and transfer payments, the income Gini coefficient ranged between 0.24 and 0.49, with Slovenia being the lowest and Chile the highest. African countries had the highest pre-tax Gini coefficients in 2008–2009, with South Africa the world's highest, variously estimated to be 0.63 to 0.7, although this figure drops to 0.52 after social assistance is taken into account, and drops again to 0.47 after taxation. The global income Gini coefficient in 2005 has been estimated to be between 0.61 and 0.68 by various sources.

There are some issues in interpreting a Gini coefficient. The same value may result from many different distribution curves. The demographic structure should be taken into account. Countries with an aging population, or with a baby boom, experience an increasing pre-tax Gini coefficient even if real income distribution for working adults remains constant. Scholars have devised over a dozen variants of the Gini coefficient.

The Gini coefficient is a single number aimed at measuring how far a country’s wealth distribution deviates from totally equal distribution of productivity. 

The Gini coefficient is usually defined mathematically based on the Lorenz curve, which plots the proportion of the total income of the population (y axis) that is cumulatively earned by the bottom x% of the population (see diagram). The line at 45 degrees thus represents perfect equality of incomes. The Gini coefficient can then be thought of as the ratio of the area that lies between the line of equality and the Lorenz curve (marked "A" in the diagram) over the total area under the line of equality (marked "A" and "B" in the diagram); i.e., . It is also equal to and to due to the fact that (since the axes scale from 0 to 1).

If all people have non-negative income (or wealth, as the case may be), the Gini coefficient can theoretically range from 0 (complete equality) to 1 (complete inequality); it is sometimes expressed as a percentage ranging between 0 and 100. In practice, both extreme values are not quite reached. If negative values are possible (such as the negative wealth of people with debts), then the Gini coefficient could theoretically be more than 1. Normally the mean (or total) is assumed positive, which rules out a Gini coefficient less than zero.

An alternative approach is to define the Gini coefficient as half of the relative mean absolute difference, which is mathematically equivalent to the Lorenz curve definition.
The mean absolute difference is the average absolute difference of all pairs of items of the population, and the relative mean absolute difference is the mean absolute difference divided by the average, to normalize for scale. if "x" is the wealth or income of person "i", and there are "n" persons, then the Gini coefficient "G" is given by:

When the income (or wealth) distribution is given as a continuous probability distribution function "p(x)", where "p(x)dx" is the fraction of the population with income "x" to "x+dx", then the Gini coefficient is again half of the relative mean absolute difference:

where μ is the mean of the distribution formula_3 and the lower limits of integration may be replaced by zero when all incomes are positive.

The most equal society will be one in which every person receives the same income (); the most unequal society will be one in which a single person receives 100% of the total income and the remaining people receive none ().

While the income distribution of any particular country need not follow simple functions, these functions give a qualitative understanding of the income distribution in a nation given the Gini coefficient.

An informative simplified case just distinguishes two levels of income, low and high. If the high income group is "u" % of the population and earns a fraction "f" % of all income, then the Gini coefficient is . An actual more graded distribution with these same values "u" and "f" will always have a higher Gini coefficient than .

The proverbial case where the richest 20% have 80% of all income (see Pareto principle) would lead to an income Gini coefficient of at least 60%.

An often cited case that 1% of all the world's population owns 50% of all wealth, means a wealth Gini coefficient of at least 49%.

In some cases, this equation can be applied to calculate the Gini coefficient without direct reference to the Lorenz curve. For example, (taking "y" to mean the income or wealth of a person or household):

Since the Gini coefficient is half the relative mean absolute difference, it can also be calculated using formulas for the relative mean absolute difference. For a random sample "S" consisting of values "y", "i" = 1 to "n", that are indexed in non-decreasing order ("y" ≤ "y"), the statistic:

There does not exist a sample statistic that is in general an unbiased estimator of the population Gini coefficient, like the relative mean absolute difference.

For a discrete probability distribution with probability mass function "f ( y )", "i" = 1 to "n", where "f ( y )" is the fraction of the population with income or wealth "y ">0, the Gini coefficient is:

When the population is large, the income distribution may be represented by a continuous probability density function "f(x)" where "f(x) dx" is the fraction of the population with wealth or income in the interval "dx" about "x". If "F(x)" is the cumulative distribution function for "f(x)", then the Lorenz curve "L(F)" may then be represented as a function parametric in "L(x)" and "F(x)" and the value of "B" can be found by integration:

The Gini coefficient can also be calculated directly from the cumulative distribution function of the distribution "F"("y"). Defining μ as the mean of the distribution, and specifying that "F"("y") is zero for all negative values, the Gini coefficient is given by:
The latter result comes from integration by parts. (Note that this formula can be applied when there are negative values if the integration is taken from minus infinity to plus infinity.)

The Gini coefficient may be expressed in terms of the quantile function "Q"("F") (inverse of the cumulative distribution function: Q(F(x))=x)

For some functional forms, the Gini index can be calculated explicitly. For example, if "y" follows a lognormal distribution with the standard deviation of logs equal to formula_17, then formula_18 where formula_19 is the error function ( since formula_20, where formula_21 is the cumulative standard normal distribution). In the table below, some examples are shown. The Dirac delta distribution represents the case where everyone has the same wealth (or income); it implies that there are no variations at all between incomes.

Sometimes the entire Lorenz curve is not known, and only values at certain intervals are given. In that case, the Gini coefficient can be approximated by using various techniques for interpolating the missing values of the Lorenz curve. If ("X", "Y") are the known points on the Lorenz curve, with the "X" indexed in increasing order ("X" < "X"), so that:
If the Lorenz curve is approximated on each interval as a line between consecutive points, then the area B can be approximated with trapezoids and:

is the resulting approximation for G. More accurate results can be obtained using other methods to approximate the area B, such as approximating the Lorenz curve with a quadratic function across pairs of intervals, or building an appropriately smooth approximation to the underlying distribution function that matches the known data. If the population mean and boundary values for each interval are also known, these can also often be used to improve the accuracy of the approximation.

The Gini coefficient calculated from a sample is a statistic and its standard error, or confidence intervals for the population Gini coefficient, should be reported. These can be calculated using bootstrap techniques but those proposed have been mathematically complicated and computationally onerous even in an era of fast computers. Ogwang (2000) made the process more efficient by setting up a "trick regression model" in which respective income variables in the sample are ranked with the lowest income being allocated rank 1. The model then expresses the rank (dependent variable) as the sum of a constant "A" and a normal error term whose variance is inversely proportional to "y";

Ogwang showed that "G" can be expressed as a function of the weighted least squares estimate of the constant "A" and that this can be used to speed up the calculation of the jackknife estimate for the standard error. Giles (2004) argued that the standard error of the estimate of "A" can be used to derive that of the estimate of "G" directly without using a jackknife at all. This method only requires the use of ordinary least squares regression after ordering the sample data. The results compare favorably with the estimates from the jackknife with agreement improving with increasing sample size.

However it has since been argued that this is dependent on the model's assumptions about the error distributions (Ogwang 2004) and the independence of error terms (Reza & Gastwirth 2006) and that these assumptions are often not valid for real data sets. It may therefore be better to stick with jackknife methods such as those proposed by Yitzhaki (1991) and Karagiannis and Kovacevic (2000). The debate continues.

Guillermina Jasso and Angus Deaton independently proposed the following formula for the Gini coefficient:

where formula_25 is mean income of the population, P is the income rank P of person i, with income X, such that the richest person receives a rank of 1 and the poorest a rank of N. This effectively gives higher weight to poorer people in the income distribution, which allows the Gini to meet the Transfer Principle. Note that the Jasso-Deaton formula rescales the coefficient so that its value is 1 if all the formula_26 are zero except one. Note however Allison's reply on the need to divide by N² instead.

FAO explains another version of the formula.

The Gini coefficient and other standard inequality indices reduce to a common form. Perfect equality—the absence of inequality—exists when and only when the inequality ratio, formula_27, equals 1 for all j units in some population (for example, there is perfect income equality when everyone's income formula_28 equals the mean income formula_29, so that formula_30 for everyone). Measures of inequality, then, are measures of the average deviations of the formula_30 from 1; the greater the average deviation, the greater the inequality. Based on these observations the inequality indices have this common form:

where "p" weights the units by their population share, and "f"("r") is a function of the deviation of each unit's "r" from 1, the point of equality. The insight of this generalised inequality index is that inequality indices differ because they employ different functions of the distance of the inequality ratios (the "r") from 1."

Gini coefficients of income are calculated on market income as well as disposable income basis. The Gini coefficient on market income—sometimes referred to as a pre-tax Gini coefficient—is calculated on income before taxes and transfers, and it measures inequality in income without considering the effect of taxes and social spending already in place in a country. The Gini coefficient on disposable income—sometimes referred to as after-tax Gini coefficient—is calculated on income after taxes and transfers, and it measures inequality in income after considering the effect of taxes and social spending already in place in a country.

The difference in Gini indices between OECD countries, on after-taxes and transfers basis, is significantly narrower. For OECD countries, over 2008–2009 period, Gini coefficient on pre-taxes and transfers basis for total population ranged between 0.34 and 0.53, with South Korea the lowest and Italy the highest. Gini coefficient on after-taxes and transfers basis for total population ranged between 0.25 and 0.48, with Denmark the lowest and Mexico the highest. For United States, the country with the largest population in OECD countries, the pre-tax Gini index was 0.49, and after-tax Gini index was 0.38, in 2008–2009. The OECD averages for total population in OECD countries was 0.46 for pre-tax income Gini index and 0.31 for after-tax income Gini Index. Taxes and social spending that were in place in 2008–2009 period in OECD countries significantly lowered effective income inequality, and in general, "European countries—especially Nordic and Continental welfare states—achieve lower levels of income inequality than other countries."

Using the Gini can help quantify differences in welfare and compensation policies and philosophies. However it should be borne in mind that the Gini coefficient can be misleading when used to make political comparisons between large and small countries or those with different immigration policies (see limitations of Gini coefficient section).

The Gini coefficient for the entire world has been estimated by various parties to be between 0.61 and 0.68. The graph shows the values expressed as a percentage in their historical development for a number of countries.
According to UNICEF, Latin America and the Caribbean region had the highest net income Gini index in the world at 48.3, on unweighted average basis in 2008. The remaining regional averages were: sub-Saharan Africa (44.2), Asia (40.4), Middle East and North Africa (39.2), Eastern Europe and Central Asia (35.4), and High-income Countries (30.9). Using the same method, the United States is claimed to have a Gini index of 36, while South Africa had the highest income Gini index score of 67.8.

The table below presents the estimated world income Gini coefficients over the last 200 years, as calculated by Milanovic. Taking income distribution of all human beings, the worldwide income inequality has been constantly increasing since the early 19th century. There was a steady increase in the global income inequality Gini score from 1820 to 2002, with a significant increase between 1980 and 2002. This trend appears to have peaked and begun a reversal with rapid economic growth in emerging economies, particularly in the large populations of BRIC countries.

More detailed data from similar sources plots a continuous decline since 1988. This is attributed to globalization increasing incomes for billions of poor people, mostly in India and China. Developing countries like Brazil have also improved basic services like health care, education, and sanitation; others like Chile and Mexico have enacted more progressive tax policies.

Gini coefficient is widely used in fields as diverse as sociology, economics, health science, ecology, engineering and agriculture. For example, in social sciences and economics, in addition to income Gini coefficients, scholars have published education Gini coefficients and opportunity Gini coefficients.

Education Gini index estimates the inequality in education for a given population. It is used to discern trends in social development through educational attainment over time. From a study of 85 countries by three Economists of World Bank Vinod Thomas, Yan Wang, Xibo Fan, estimate Mali had the highest education Gini index of 0.92 in 1990 (implying very high inequality in education attainment across the population), while the United States had the lowest education inequality Gini index of 0.14. Between 1960 and 1990, South Korea, China and India had the fastest drop in education inequality Gini Index. They also claim education Gini index for the United States slightly increased over the 1980–1990 period.

Similar in concept to income Gini coefficient, opportunity Gini coefficient measures inequality of opportunity. The concept builds on Amartya Sen's suggestion that inequality coefficients of social development should be premised on the process of enlarging people's choices and enhancing their capabilities, rather than on the process of reducing income inequality. Kovacevic in a review of opportunity Gini coefficient explains that the coefficient estimates how well a society enables its citizens to achieve success in life where the success is based on a person's choices, efforts and talents, not his background defined by a set of predetermined circumstances at birth, such as, gender, race, place of birth, parent's income and circumstances beyond the control of that individual.

In 2003, Roemer reported Italy and Spain exhibited the largest opportunity inequality Gini index amongst advanced economies.

In 1978, Anthony Shorrocks introduced a measure based on income Gini coefficients to estimate income mobility. This measure, generalized by Maasoumi and Zandvakili, is now generally referred to as Shorrocks index, sometimes as Shorrocks mobility index or Shorrocks rigidity index. It attempts to estimate whether the income inequality Gini coefficient is permanent or temporary, and to what extent a country or region enables economic mobility to its people so that they can move from one (e.g. bottom 20%) income quantile to another (e.g. middle 20%) over time. In other words, Shorrocks index compares inequality of short-term earnings such as annual income of households, to inequality of long-term earnings such as 5-year or 10-year total income for same households.

Shorrocks index is calculated in number of different ways, a common approach being from the ratio of income Gini coefficients between short-term and long-term for the same region or country.

A 2010 study using social security income data for the United States since 1937 and Gini-based Shorrocks indices concludes that income mobility in the United States has had a complicated history, primarily due to mass influx of women into the American labor force after World War II. Income inequality and income mobility trendsc have been different for men and women workers between 1937 and the 2000s. When men and women are considered together, the Gini coefficient-based Shorrocks index trends imply long-term income inequality has been substantially reduced among all workers, in recent decades for the United States. Other scholars, using just 1990s data or other short periods have come to different conclusions. For example, Sastre and Ayala, conclude from their study of income Gini coefficient data between 1993 and 1998 for six developed economies, that France had the least income mobility, Italy the highest, and the United States and Germany intermediate levels of income mobility over those 5 years.

The Gini coefficient has features that make it useful as a measure of dispersion in a population, and inequalities in particular. It is a ratio analysis method making it easier to interpret. It also avoids references to a statistical average or position unrepresentative of most of the population, such as per capita income or gross domestic product. For a given time interval, Gini coefficient can therefore be used to compare diverse countries and different regions or groups within a country; for example states, counties, urban versus rural areas, gender and ethnic groups. Gini coefficients can be used to compare income distribution over time, thus it is possible to see if inequality is increasing or decreasing independent of absolute incomes.

Other useful features of the Gini coefficient include:

A Gini index value above 50 is considered high; countries including Brazil, Colombia, South Africa, Botswana, and Honduras can be found in this category. A Gini index value of 30 or above is considered medium; countries including Vietnam, Mexico, Poland, The United States, Argentina, Russia and Uruguay can be found in this category. A Gini index value lower than 30 is considered low; countries including Austria, Germany, Denmark, Slovenia, Sweden and Ukraine can be found in this category.

The Gini coefficient is a relative measure. Its proper use and interpretation is controversial. It is possible for the Gini coefficient of a developing country to rise (due to increasing inequality of income) while the number of people in absolute poverty decreases. This is because the Gini coefficient measures relative, not absolute, wealth. Changing income inequality, measured by Gini coefficients, can be due to structural changes in a society such as growing population (baby booms, aging populations, increased divorce rates, extended family households splitting into nuclear families, emigration, immigration) and income mobility. Gini coefficients are simple, and this simplicity can lead to oversights and can confuse the comparison of different populations; for example, while both Bangladesh (per capita income of $1,693) and the Netherlands (per capita income of $42,183) had an income Gini coefficient of 0.31 in 2010, the quality of life, economic opportunity and absolute income in these countries are very different, i.e. countries may have identical Gini coefficients, but differ greatly in wealth. Basic necessities may be available to all in a developed economy, while in an undeveloped economy with the same Gini coefficient, basic necessities may be unavailable to most or unequally available, due to lower absolute wealth.
Even when the total income of a population is the same, in certain situations two countries with different income distributions can have the same Gini index (e.g. cases when income Lorenz Curves cross). Table A illustrates one such situation. Both countries have a Gini coefficient of 0.2, but the average income distributions for household groups are different. As another example, in a population where the lowest 50% of individuals have no income and the other 50% have equal income, the Gini coefficient is 0.5; whereas for another population where the lowest 75% of people have 25% of income and the top 25% have 75% of the income, the Gini index is also 0.5. Economies with similar incomes and Gini coefficients can have very different income distributions. Bellù and Liberati claim that to rank income inequality between two different populations based on their Gini indices is sometimes not possible, or misleading.

A Gini index does not contain information about absolute national or personal incomes. Populations can have very low income Gini indices, yet simultaneously very high wealth Gini index. By measuring inequality in income, the Gini ignores the differential efficiency of use of household income. By ignoring wealth (except as it contributes to income) the Gini can create the appearance of inequality when the people compared are at different stages in their life. Wealthy countries such as Sweden can show a low Gini coefficient for disposable income of 0.31 thereby appearing equal, yet have very high Gini coefficient for wealth of 0.79 to 0.86 thereby suggesting an extremely unequal wealth distribution in its society. These factors are not assessed in income-based Gini.
Gini index has a downward-bias for small populations. Counties or states or countries with small populations and less diverse economies will tend to report small Gini coefficients. For economically diverse large population groups, a much higher coefficient is expected than for each of its regions. Taking world economy as one, and income distribution for all human beings, for example, different scholars estimate global Gini index to range between 0.61 and 0.68.
As with other inequality coefficients, the Gini coefficient is influenced by the granularity of the measurements. For example, five 20% quantiles (low granularity) will usually yield a lower Gini coefficient than twenty 5% quantiles (high granularity) for the same distribution. Philippe Monfort has shown that using inconsistent or unspecified granularity limits the usefulness of Gini coefficient measurements.

The Gini coefficient measure gives different results when applied to individuals instead of households, for the same economy and same income distributions. If household data is used, the measured value of income Gini depends on how the household is defined. When different populations are not measured with consistent definitions, comparison is not meaningful.

Deininger and Squire (1996) show that income Gini coefficient based on individual income, rather than household income, are different. For example, for the United States, they find that the individual income-based Gini index was 0.35, while for France it was 0.43. According to their individual focused method, in the 108 countries they studied, South Africa had the world's highest Gini coefficient at 0.62, Malaysia had Asia's highest Gini coefficient at 0.5, Brazil the highest at 0.57 in Latin America and Caribbean region, and Turkey the highest at 0.5 in OECD countries.
Expanding on the importance of life-span measures, the Gini coefficient as a point-estimate of equality at a certain time, ignores life-span changes in income. Typically, increases in the proportion of young or old members of a society will drive apparent changes in equality, simply because people generally have lower incomes and wealth when they are young than when they are old. Because of this, factors such as age distribution within a population and mobility within income classes can create the appearance of inequality when none exist taking into account demographic effects. Thus a given economy may have a higher Gini coefficient at any one point in time compared to another, while the Gini coefficient calculated over individuals' lifetime income is actually lower than the apparently more equal (at a given point in time) economy's. Essentially, what matters is not just inequality in any particular year, but the composition of the distribution over time.

Kwok claims income Gini coefficient for Hong Kong has been high (0.434 in 2010), in part because of structural changes in its population. Over recent decades, Hong Kong has witnessed increasing numbers of small households, elderly households and elderly living alone. The combined income is now split into more households. Many old people are living separately from their children in Hong Kong. These social changes have caused substantial changes in household income distribution. Income Gini coefficient, claims Kwok, does not discern these structural changes in its society. Household money income distribution for the United States, summarized in Table C of this section, confirms that this issue is not limited to just Hong Kong. According to the US Census Bureau, between 1979 and 2010, the population of United States experienced structural changes in overall households, the income for all income brackets increased in inflation-adjusted terms, household income distributions shifted into higher income brackets over time, while the income Gini coefficient increased.

Another limitation of Gini coefficient is that it is not a proper measure of egalitarianism, as it is only measures income dispersion. For example, if two equally egalitarian countries pursue different immigration policies, the country accepting a higher proportion of low-income or impoverished migrants will report a higher Gini coefficient and therefore may appear to exhibit more income inequality.

Some countries distribute benefits that are difficult to value. Countries that provide subsidized housing, medical care, education or other such services are difficult to value objectively, as it depends on quality and extent of the benefit. In absence of free markets, valuing these income transfers as household income is subjective. The theoretical model of Gini coefficient is limited to accepting correct or incorrect subjective assumptions.

In subsistence-driven and informal economies, people may have significant income in other forms than money, for example through subsistence farming or bartering. These income tend to accrue to the segment of population that is below-poverty line or very poor, in emerging and transitional economy countries such as those in sub-Saharan Africa, Latin America, Asia and Eastern Europe. Informal economy accounts for over half of global employment and as much as 90 per cent of employment in some of the poorer sub-Saharan countries with high official Gini inequality coefficients. Schneider et al., in their 2010 study of 162 countries, report about 31.2%, or about $20 trillion, of world's GDP is informal. In developing countries, the informal economy predominates for all income brackets except for the richer, urban upper income bracket populations. Even in developed economies, between 8% (United States) to 27% (Italy) of each nation's GDP is informal, and resulting informal income predominates as a livelihood activity for those in the lowest income brackets. The value and distribution of the incomes from informal or underground economy is difficult to quantify, making true income Gini coefficients estimates difficult. Different assumptions and quantifications of these incomes will yield different Gini coefficients.

Gini has some mathematical limitations as well. It is not additive and different sets of people cannot be averaged to obtain the Gini coefficient of all the people in the sets.

Given the limitations of Gini coefficient, other statistical methods are used in combination or as an alternative measure of population dispersity. For example, "entropy measures" are frequently used (e.g. the Atkinson index or the Theil Index and Mean log deviation as special cases of the generalized entropy index). These measures attempt to compare the distribution of resources by intelligent agents in the market with a maximum entropy random distribution, which would occur if these agents acted like non-intelligent particles in a closed system following the laws of statistical physics.

The Gini coefficient is sometimes alternatively "defined" as twice the area between the receiver operating characteristic (ROC) curve and its diagonal, in which case the AUC (Area Under the ROC Curve) measure of performance is given by formula_33. The Gini coefficient is also closely related to Mann–Whitney U.

The Gini index is also related to Pietra index—both of which are a measure of statistical heterogeneity and are derived from Lorenz curve and the diagonal line.

In certain fields such as ecology, inverse Simpson's index formula_34 is used to quantify diversity, and this should not be confused with the Simpson index formula_35. These indicators are related to Gini. The inverse Simpson index increases with diversity, unlike Simpson index and Gini coefficient which decrease with diversity. The Simpson index is in the range [0, 1], where 0 means maximum and 1 means minimum diversity (or heterogeneity). Since diversity indices typically increase with increasing heterogeneity, Simpson index is often transformed into inverse Simpson, or using the complement formula_36, known as Gini-Simpson Index.

Although the Gini coefficient is most popular in economics, it can in theory be applied in any field of science that studies a distribution. For example, in ecology the Gini coefficient has been used as a measure of biodiversity, where the cumulative proportion of species is plotted against cumulative proportion of individuals. In health, it has been used as a measure of the inequality of health related quality of life in a population. In education, it has been used as a measure of the inequality of universities. In chemistry it has been used to express the selectivity of protein kinase inhibitors against a panel of kinases. In engineering, it has been used to evaluate the fairness achieved by Internet routers in scheduling packet transmissions from different flows of traffic.

The Gini coefficient is sometimes used for the measurement of the discriminatory power of rating systems in credit risk management.

A 2005 study accessed US census data to measure home computer ownership, and used the Gini coefficient to measure inequalities amongst whites and African Americans. Results indicated that although decreasing overall, home computer ownership inequality is substantially smaller among white households.

A 2016 peer-reviewed study titled Employing the Gini coefficient to measure participation inequality in treatment-focused Digital Health Social Networks illustrated that the Gini coefficient was helpful and accurate in measuring shifts in inequality, however as a standalone metric it failed to incorporate overall network size.

The discriminatory power refers to a credit risk model's ability to differentiate between defaulting and non-defaulting clients. The formula formula_37, in calculation section above, may be used for the final model and also at individual model factor level, to quantify the discriminatory power of individual factors. It is related to accuracy ratio in population assessment models.




</doc>
<doc id="12884" url="https://en.wikipedia.org/wiki?curid=12884" title="Government Communications Headquarters">
Government Communications Headquarters

The Government Communications Headquarters (GCHQ) is an intelligence and security organisation responsible for providing signals intelligence (SIGINT) and information assurance to the government and armed forces of the United Kingdom. Based in "The Doughnut" in the suburbs of Cheltenham, GCHQ is the responsibility of the country's Secretary of State for Foreign and Commonwealth Affairs, but it is not a part of the Foreign Office and its director ranks as a Permanent Secretary.

GCHQ was originally established after the First World War as the Government Code and Cypher School (GC&CS) and was known under that name until 1946. During the Second World War it was located at Bletchley Park, where it was responsible for breaking of the German Enigma codes. Currently there are two main components of the GCHQ, the Composite Signals Organisation (CSO), which is responsible for gathering information, and the National Cyber Security Centre (NCSC), which is responsible for securing the UK's own communications. The Joint Technical Language Service (JTLS) is a small department and cross-government resource responsible for mainly technical language support and translation and interpreting services across government departments. It is co-located with GCHQ for administrative purposes.

In 2013, GCHQ received considerable media attention when the former National Security Agency contractor Edward Snowden revealed that the agency was in the process of collecting all online and telephone data in the UK via the Tempora programme. Snowden's revelations began a spate of ongoing disclosures of global surveillance. "The Guardian" newspaper was then forced to destroy all incriminating files given to them by Snowden because of the threats of lawsuits from the UK Government.

GCHQ is led by the Director of GCHQ, currently Jeremy Fleming, and a Corporate Board, made up of executive and non-executive directors. Reporting to the Corporate Board is:

During the First World War, the British Army and Royal Navy had separate signals intelligence agencies, MI1b and NID25 (initially known as Room 40) respectively. In 1919, the Cabinet's Secret Service Committee, chaired by Lord Curzon, recommended that a peace-time code breaking agency should be created, a task given to the Director of Naval Intelligence, Hugh Sinclair. Sinclair merged staff from NID25 and MI1b into the new organisation, which initially consisted of around 25–30 officers and a similar number of clerical staff. It was titled the "Government Code and Cypher School", a cover-name chosen by Victor Forbes of the Foreign Office. Alastair Denniston, who had been a member of NID25, was appointed as its operational head. It was initially under the control of the Admiralty and located in Watergate House, Adelphi, London. Its public function was "to advise as to the security of codes and cyphers used by all Government departments and to assist in their provision", but also had a secret directive to "study the methods of cypher communications used by foreign powers". GC&CS officially formed on 1 November 1919, and produced its first decrypt on 19 October.

Before the Second World War, GC&CS was a relatively small department. By 1922, the main focus of GC&CS was on diplomatic traffic, with "no service traffic ever worth circulating" and so, at the initiative of Lord Curzon, it was transferred from the Admiralty to the Foreign Office. GC&CS came under the supervision of Hugh Sinclair, who by 1923 was both the Chief of SIS and Director of GC&CS. In 1925, both organisations were co-located on different floors of Broadway Buildings, opposite St. James's Park. Messages decrypted by GC&CS were distributed in blue-jacketed files that became known as "BJs". In the 1920s, GC&CS was successfully reading Soviet Union diplomatic ciphers. However, in May 1927, during a row over clandestine Soviet support for the General Strike and the distribution of subversive propaganda, Prime Minister Stanley Baldwin made details from the decrypts public.

During the Second World War, GC&CS was based largely at Bletchley Park, in present-day Milton Keynes, working on understanding the German Enigma machine and Lorenz ciphers. In 1940, GC&CS was working on the diplomatic codes and ciphers of 26 countries, tackling over 150 diplomatic cryptosystems. Senior staff included Alastair Denniston, Oliver Strachey, Dilly Knox, John Tiltman, Edward Travis, Ernst Fetterlein, Josh Cooper, Donald Michie, Alan Turing, Gordon Welchman, Joan Clarke, Max Newman, William Tutte, I. J. (Jack) Good, Peter Calvocoressi and Hugh Foss.

An outstation in the Far East, the Far East Combined Bureau was set up in Hong Kong in 1935, and moved to Singapore in 1939. Subsequently, with the Japanese advance down the Malay Peninsula, the Army and RAF codebreakers went to the Wireless Experimental Centre in Delhi, India. The Navy codebreakers in FECB went to Colombo, Ceylon, then to Kilindini, near Mombasa, Kenya.

GC&CS was renamed the "Government Communications Headquarters" in June 1946.

GCHQ was at first based in Eastcote, but in 1951 moved to the outskirts of Cheltenham, setting up two sites there – Oakley and Benhall. Duncan Campbell and Mark Hosenball revealed the existence of GCHQ in 1976 in an article for "Time Out"; as a result, Hosenball was deported from the UK. GCHQ had a very low profile in the media until 1983 when the trial of Geoffrey Prime, a KGB mole within GCHQ, created considerable media interest.

Since the days of the Second World War, US and British intelligence have shared information. For the GCHQ this means that it shares information with, and gets information from, the National Security Agency (NSA) in the United States.

In 1984, GCHQ was the centre of a political row when the Conservative government of Margaret Thatcher prohibited its employees from belonging to a trade union. It was claimed that joining a union would be in conflict with national security. A number of mass national one-day strikes were held to protest this decision, seen as a first step to wider bans on trade unions. Appeals to British Courts and European Commission of Human Rights were unsuccessful. The government offered a sum of money to each employee who agreed to give up their union membership. Appeal to the ILO resulted in a decision that government's actions were in violation of Freedom of Association and Protection of the Right to Organise Convention. 

The ban was eventually lifted by the incoming Labour government in 1997, with the Government Communications Group of the Public and Commercial Services Union (PCS) being formed to represent interested employees at all grades. In 2000, a group of 14 former GCHQ employees, who had been dismissed after refusing to give up their union membership, were offered re-employment, which three of them accepted.

The Intelligence Services Act 1994 placed the activities of the intelligence agencies on a legal footing for the first time, defining their purpose, and the British Parliament's Intelligence and Security Committee was given a remit to examine the expenditure, administration and policy of the three intelligence agencies. The objectives of GCHQ were defined as working as "in the interests of national security, with particular reference to the defence and foreign policies of Her Majesty's government; in the interests of the economic wellbeing of the United Kingdom; and in support of the prevention and the detection of serious crime". During the introduction of the Intelligence Agency Act in late 1993, the former Prime Minister Jim Callaghan had described GCHQ as a "full blown bureaucracy", adding that future bodies created to provide oversight of the intelligence agencies should "investigate whether all the functions that GCHQ carries out today are still necessary."

In 1993, in the wake of the "Squidgygate" affair, GCHQ denied "intercepting, recording or disclosing" the telephone calls of the British Royal family.

In late 1993 civil servant Michael Quinlan advised a deep review of the work of GCHQ following the conclusion of his "Review of Intelligence Requirements and Resources", which had imposed a 3% cut on the agency. The Chief Secretary to the Treasury, Jonathan Aitken, subsequently held face to face discussions with the intelligence agency directors to assess further savings in the wake of Quinlan's review. Aldrich (2010) suggests that Sir John Adye, the then Director of GCHQ performed badly in meetings with Aitken, leading Aitken to conclude that GCHQ was "suffering from out-of-date methods of management and out-of-date methods for assessing priorities". GCHQ's budget was £850 million in 1993, (£ as of ) compared to £125 million for the Security Service and SIS (MI5 and MI6). In December 1994 the businessman Roger Hurn was commissioned to begin a review of GCHQ, which was concluded in March 1995. Hurn's report recommended a cut of £100 million in GCHQ's budget; such a large reduction had not been suffered by any British intelligence agency since the end of World War II. The J Division of GCHQ, which had collected SIGINT on Russia, disappeared as result of the cuts. The cuts had been mostly reversed by 2000 in the wake of threats from violent non-state actors, and risks from increased terrorism, organised crime and illegal access to nuclear, chemical and biological weapons.

David Omand became the Director of GCHQ in 1996, and greatly restructured the agency in the face of new and changing targets and rapid technological change. Omand introduced the concept of "Sinews" (or "SIGINT New Systems") which allowed more flexible working methods, avoiding overlaps in work by creating fourteen domains, each with a well-defined working scope. The tenure of Omand also saw the planning and the creation of The Doughnut, GCHQ's modern headquarters. Located on a 176-acre site in Benhall, near Cheltenham, The Doughnut would be the largest building constructed for secret intelligence operations outside the United States.

Operations at GCHQ's Chum Hom Kwok listening station in Hong Kong ended in 1994. GCHQ's Hong Kong operations were extremely important to their relationship with the NSA, who contributed investment and equipment to the station. In anticipation of the transfer of Hong Kong to the Chinese government in 1997, the Hong Kong stations operations were moved to Geraldton in Australia.

Operations that used GCHQ's intelligence-gathering capabilities in the 1990s included the monitoring of communications of Iraqi soldiers in the Gulf War, of dissident republican terrorists and the Real IRA, of the various factions involved in the Yugoslav Wars, and of the criminal Kenneth Noye. In the mid 1990s GCHQ began to assist in the investigation of cybercrime.

At the end of 2003, GCHQ moved to a new circular HQ (popularly known as "The Doughnut"). At the time, it was the second-largest public-sector building project in Europe, with an estimated cost of £337 million. The new building, which was designed by Gensler and constructed by Carillion, became the base for all of GCHQ's Cheltenham operations.

The public spotlight fell on GCHQ in late 2003 and early 2004 following the sacking of Katharine Gun after she leaked to "The Observer" a confidential email from agents at the United States' National Security Agency addressed to GCHQ agents about the wiretapping of UN delegates in the run-up to the 2003 Iraq war.

GCHQ gains its intelligence by monitoring a wide variety of communications and other electronic signals. For this, a number of stations have been established in the UK and overseas. The listening stations are at Cheltenham itself, Bude, Scarborough, Ascension Island, and with the United States at Menwith Hill. Ayios Nikolaos Station in Cyprus is run by the British Army for GCHQ.

In March 2010, GCHQ was criticised by the Intelligence and Security Committee for problems with its IT security practices and failing to meet its targets for work targeted against cyber attacks.

As revealed by Edward Snowden in "The Guardian", GCHQ spied on foreign politicians visiting the 2009 G-20 London Summit by eavesdropping phonecalls and emails and monitoring their computers, and in some cases even ongoing after the summit via keyloggers that had been installed during the summit.

According to Edward Snowden, GCHQ has two principal umbrella programs for collecting communications:

GCHQ also has had access to the US internet monitoring programme PRISM since at least June 2010. PRISM is said to give the National Security Agency and FBI easy access to the systems of nine of the world's top internet companies, including Google, Facebook, Microsoft, Apple, Yahoo, and Skype.

In February 2014, "The Guardian", based on documents provided by Snowden, revealed that GCHQ had indiscriminately collected 1.8 million private Yahoo webcam images from users across the world. In the same month NBC and The Intercept, based on documents released by Snowden, revealed the Joint Threat Research Intelligence Group and the CNE units within GCHQ. Their mission was cyber operations based on "dirty tricks" to shut down enemy communications, discredit, and plant misinformation on enemies. These operations were 5% of all GCHQ operations according to a conference slideshow presented by the GCHQ.

Soon after becoming Director of GCHQ in 2014, Robert Hannigan wrote an article in the "Financial Times" on the topic of internet surveillance, stating that "however much [large US technology companies] may dislike it, they have become the command and control networks of choice for terrorists and criminals" and that GCHQ and its sister agencies "cannot tackle these challenges at scale without greater support from the private sector", arguing that most internet users "would be comfortable with a better and more sustainable relationship between the [intelligence] agencies and the tech companies". Since the 2013 global surveillance disclosures, large US technology companies have improved security and become less co-operative with foreign intelligence agencies, including those of the UK, generally requiring a US court order before disclosing data. However the head of the UK technology industry group techUK rejected these claims, stating that they understood the issues but that disclosure obligations "must be based upon a clear and transparent legal framework and effective oversight rather than, as suggested, a deal between the industry and government".

In 2015, documents obtained by "The Intercept" from US National Security Agency whistleblower Edward Snowden revealed that GCHQ had carried out a mass-surveillance operation, codenamed KARMA POLICE, since about 2008. The KARMA POLICE operation swept up the IP address of Internet users visiting websites. The program was established with no public scrutiny or oversight. KARMA POLICE is a powerful spying tool in conjunction with other GCHQ programs, because IP addresses could be cross-referenced with other data. The goal of the program, according to the documents, was "either (a) a web browsing profile for every visible user on the internet, or (b) a user profile for every visible website on the internet."

In 2015, GCHQ admitted for the first time in court that it conducts computer hacking.

In 2017, US Press Secretary Sean Spicer alleged that GCHQ had conducted surveillance on US President Donald Trump, basing the allegation on statements made by a media commentator during a Fox News segment. The US government formally apologised for the allegations and promised they would not be repeated. However, surveillance of Russian agents did pick up contacts made by Trump's campaign team in the run up to his election, which was passed on to US agencies.

As well as a mission to gather intelligence, GCHQ has for a long-time had a corresponding mission to assist in the protection of the British government's own communications. When the Government Code and Cypher School (GC&CS) was created in 1919, its overt task was providing security advice. GC&C's Security section was located in Mansfield College, Oxford during the Second World War.

In April 1946, GC&CS became GCHQ, and the now GCHQ Security section moved from Oxford to join the rest of the organisation at Eastcote later that year.

From 1952 to 1954, the intelligence mission of GCHQ relocated to Cheltenham; the Security section remained at Eastcote, and in March 1954 became a separate, independent organisation: the London Communications Security Agency (LCSA), which in 1958 was renamed to the London Communications-Electronic Security Agency (LCESA).

In April 1965, GPO and MOD units merged with LCESA to become the Communications-Electronic Security Department (CESD).

In October 1969, CESD was merged into GCHQ and becoming Communications-Electronic Security Group (CESG).

In 1977 CESG relocated from Eastcote to Cheltenham.

CESG continued as the UK National Technical Authority for information assurance, including cryptography. CESG did not manufacture security equipment, but worked with industry to ensure the availability of suitable products and services, while GCHQ itself funded research into such areas, for example to the Centre for Quantum Computing at Oxford University and the Heilbronn Institute at the University of Bristol.

In the 21st century, CESG ran a number of assurance schemes such as CHECK, CLAS, Commercial Product Assurance (CPA) and CESG Assisted Products Service (CAPS).

In late 1969 the concept for public key encryption was developed and proven by James H. Ellis, who had worked for CESG (and before it, CESD) since 1965. Ellis lacked the necessary number theory expertise necessary to build a workable system. Subsequently, a feasible implementation scheme via an asymmetric key algorithm was invented by another staff member Clifford Cocks, a mathematics graduate. This fact was kept secret until 1997.

In 2016, the National Cyber Security Centre was established under GCHQ, but located in London, as the UK's authority on cyber security. It absorbed and replaced CESG as well as activities that had previously existed outside GCHQ: the Centre for Cyber Assessment (CCA), Computer Emergency Response Team UK (CERT UK) and the cyber-related responsibilities of the Centre for the Protection of National Infrastructure (CPNI).

The Joint Technical Language Service (JTLS) was established in 1955, drawing on members of the small Ministry of Defence technical language team and others, initially to provide standard English translations for organisational expressions in any foreign language, discover the correct English equivalents of technical terms in foreign languages and discover the correct expansions of abbreviations in any language. The remit of the JTLS has expanded in the ensuing years to cover technical language support and interpreting and translation services across the UK Government and to local public sector services in Gloucestershire and surrounding counties. The JTLS also produces and publishes foreign language working aids under crown copyright and conducts research into machine translation and on-line dictionaries and glossaries. The JTLS is co-located with GCHQ for administrative purposes.

GCHQ operates in partnership with equivalent agencies worldwide in a number of bi-lateral and multi-lateral relationships. The principal of these is with the United States (National Security Agency), Canada (Communications Security Establishment), Australia (Australian Signals Directorate) and New Zealand (Government Communications Security Bureau), through the mechanism of the UK-US Security Agreement, a broad intelligence-sharing agreement encompassing a range of intelligence collection methods. Relationships are alleged to include shared collection methods, such as the system described in the popular media as ECHELON, as well as analysed product.

GCHQ's legal basis is enshrined in the Intelligence Services Act 1994 Section 3 as follows:

Activities that involve interception of communications are permitted under the Regulation of Investigatory Powers Act 2000; this kind of interception can only be carried out after a warrant has been issued by a Secretary of State. The Human Rights Act 1998 requires the intelligence agencies, including GCHQ, to respect citizens' rights as described in the European Convention on Human Rights.

The Prime Minister nominates cross-party Members of Parliament to an Intelligence and Security Committee. The remit of the Committee includes oversight of intelligence and security activities and reports are made directly to Parliament. Its functions were increased under the Justice and Security Act 2013 to provide for further access and investigatory powers.

Judicial oversight of GCHQ's conduct is exercised by the Investigatory Powers Tribunal. The UK also has an independent Intelligence Services Commissioner and Interception of Communications Commissioner, both of whom are former senior judges.

The Investigatory Powers Tribunal ruled in December 2014 that GCHQ does not breach the European Convention of Human Rights, and that its activities are compliant with Articles 8 (right to privacy) and 10 (freedom of expression) of the European Convention of Human Rights. However, the Tribunal stated in February 2015 that one particular aspect, the data-sharing arrangement that allowed UK Intelligence services to request data from the US surveillance programmes Prism and Upstream, had been in contravention of human rights law prior to this until two paragraphs of additional information, providing details about the procedures and safeguards, were disclosed to the public in December 2014.

Furthermore, the IPT ruled that the legislative framework in the United Kingdom does not permit mass surveillance and that while GCHQ collects and analyses data in bulk, it does not practice mass surveillance. This complements independent reports by the Interception of Communications Commissioner, and a special report made by the Intelligence and Security Committee of Parliament; although several shortcomings and potential improvements to both oversight and the legislative framework were highlighted.

Despite the inherent secrecy around much of GCHQ's work, investigations carried out by the UK government after the Snowden disclosures have admitted various abuses by the security services. A report by the Intelligence and Security Committee (ISC) in 2015 revealed that a small number of staff at UK intelligence agencies had been found to misuse their surveillance powers, in one case leading to the dismissal of a member of staff at GCHQ, although there were no laws in place at the time to make these abuses a criminal offence.

Later that year, a ruling by the Investigatory Powers Tribunal found that GCHQ acted unlawfully in conducting surveillance on two human rights organisations. The closed hearing found the government in breach of its internal surveillance policies in accessing and retaining the communications of the Egyptian Initiative for Personal Rights and the Legal Resources Centre in South Africa. This was only the second time in the IPT's history that it had made a positive determination in favour of applicants after a closed session.

At another IPT case in 2015, GCHQ conceded that "from January 2010, the regime for the interception/obtaining, analysis, use, disclosure and destruction of legally privileged material has not been in accordance with the law for the purposes of Article 8(2) of the European convention on human rights and was accordingly unlawful". This admission was made in connection with a case brought against them by Abdel Hakim Belhaj, a Libyan opponent of the former Gaddafi regime, and his wife Fatima Bouchar. The couple accused British ministers and officials of participating in their unlawful abduction, kidnapping and removal to Libya in March 2004, while Gaddafi was still in power.

In 2015 there was a complaint the Green Party MP Caroline Lucas that British intelligence services, including GCHQ, had been spying on MPs allegedly "in defiance of laws prohibiting it." GCHQ had introduced a policy in March 2015 that did not require approval by the Prime Minister, or any minister, before deliberately targeting the communications of a parliamentarian.

Then-Home Secretary, Theresa May, had told Parliament in 2014 that:
The Investigatory Powers Tribunal investigated the complaint, and ruled that contrary to the allegation, there was no law that gave the communications of parliament any special protection. The Wilson Doctrine merely acts as a political convention.

A controversial GCHQ case determined the scope of judicial review of prerogative powers (the Crown's residual powers under common law). This was "Council of Civil Service Unions v Minister for the Civil Service" [1985] AC 374 (often known simply as the "GCHQ case"). In this case, a prerogative Order in Council had been used by the prime minister (who is the Minister for the Civil Service) to ban trade union activities by civil servants working at GCHQ. This order was issued without consultation. The House of Lords had to decide whether this was reviewable by judicial review. It was held that executive action is not immune from judicial review simply because it uses powers derived from common law rather than statute (thus the prerogative is reviewable).

The following is a list of the heads of the operational heads of GCHQ and GC&CS:


The following are the stations and former stations that continued to operate through the Cold War:

The GCHQ Certified Training (GCT) scheme was established to certify two main levels of cyber security training. There are also degree and masters level courses. These are:


The GCT scheme was designed to help organisations find the right training that also met GCHQ's exacting standards. It was designed to assure high quality cyber security training courses where the training provider had also undergone rigorous quality checks. The GCT process is carried out by APMG as the independent certification body. The GCT scheme is part of the National Cyber Security Programme established by the Government to develop knowledge, skills and capability in all aspects of cyber security in the U.K. It is based off the IISP Skills Framework.

The historical drama film "The Imitation Game" (2014) featured Benedict Cumberbatch portraying Alan Turing's efforts to break the Enigma code as part of the Government Code and Cypher School, the forerunner of GCHQ.





</doc>
<doc id="12888" url="https://en.wikipedia.org/wiki?curid=12888" title="Francis Gary Powers">
Francis Gary Powers

Francis Gary Powers (August 17, 1929 – August 1, 1977)—often referred to as simply Gary Powers—was an American pilot whose Central Intelligence Agency (CIA) U-2 spy plane was shot down while flying a reconnaissance mission in Soviet Union airspace, causing the 1960 U-2 incident.

Powers was born August 17, 1929, in Jenkins, Kentucky, the son of Oliver Winfield Powers (1904–1970), a coal miner, and his wife Ida Melinda Powers (; 1905–1991). His family eventually moved to Pound, Virginia, just across the state border. He was the second born and only male of six children.

His family lived in a mining town, and because of the hardships associated with the life in such a town, his father wanted Powers to become a doctor. He hoped his son would achieve the higher earnings of such a profession and felt the life of a doctor would involve less hardship than any job in his hometown.

Graduating with a bachelor's degree from Milligan College in Tennessee in June 1950, he enlisted in the United States Air Force in October. He was commissioned as a Second Lieutenant in December 1952 after completing his advanced training with USAF Pilot Training Class 52-H at Williams Air Force Base, Arizona. Powers was then assigned to the 468th Strategic Fighter Squadron at Turner Air Force Base, Georgia, as an F-84 Thunderjet pilot.

In January 1956 he was recruited by the CIA. He married Barbara Gay Moore in Newnan, Georgia, on April 2, 1955. In May 1956 he began U-2 training at Watertown Strip, Nevada. His training was complete by August 1956 and his unit, the Second Weather Observational Squadron (Provisional) or Detachment 10-10, was deployed to Incirlik Air Base, Turkey. By 1960, Powers was already a veteran of many covert aerial reconnaissance missions.

Powers was discharged from the Air Force in 1956 with the rank of captain. He then joined the CIA's U-2 program at the civilian grade of GS-12. U-2 pilots flew espionage missions at altitudes above , above the reach of Soviet air defenses. The U-2 was equipped with a state-of-the-art camera designed to take high-resolution photos from the edge of the stratosphere over hostile countries, including the Soviet Union. U-2 missions systematically photographed military installations and other important sites.

The primary mission of the U-2s was overflying the Soviet Union. The border surveillance and atomic sampling, though vital, were secondary. Additionally, the U-2 flew special missions. If there was a trouble spot in the Middle East, the U-2s observed it. Beginning on September 27, 1956 and continuing until 1960, the United States was spying not only on most of the countries in the Middle East but also on its own allies.

Soviet intelligence had been aware of encroaching U-2 flights at least since 1958 if not sooner but lacked effective countermeasures until 1960. On May 1, 1960, Powers's U-2A, "56-6693", departed from a military airbase in Peshawar, Pakistan, with support from the U.S. Air Station at Badaber (Peshawar Air Station). This was to be the first attempt "to fly all the way across the Soviet Union...but it was considered worth the gamble. The planned route would take us deeper into Russia than we had ever gone, while traversing important targets never before photographed."

Powers was shot down by an S-75 Dvina (SA-2 Guideline) surface-to-air missile over Sverdlovsk. A total of 14 were launched, one of which hit a MiG-19 jet fighter which was sent to intercept the U-2 but could not reach a high enough altitude. Its pilot, Sergei Safronov, ejected but died of his injuries. Another Soviet aircraft, a newly manufactured Su-9 in transit flight, also attempted to intercept Powers's U-2. The unarmed Su-9 was directed to ram the U-2 but missed because of the large differences in speed (the Su-9 flew above Mach 1.1, while the U-2 flew at approximately Mach 0.6).

The first of three SA-2 Guideline (S-75 Dvina) surface-to-air missiles launched at the U-2 near Kosulino in the Ural Region hit the aircraft. "What was left of the plane began spinning, only upside down, the nose pointing upward toward the sky, the tail down toward the ground." Powers was unable to activate the plane's self-destruct mechanism before he was thrown out of the plane after releasing the canopy and his seat belt. While descending under his parachute, Powers had time to scatter his escape map, and rid himself of part of his suicide device, a silver dollar coin suspended around his neck containing a poison-laced injection pin, though he kept the poison pin. "Yet I was still hopeful of escape." He hit the ground hard, was immediately captured, and taken to Lubyanka Prison in Moscow. Powers did note a second chute after landing on the ground, "some distance away and very high, a lone red and white parachute".

When the U.S. government learned of Powers's disappearance over the Soviet Union, they issued a cover statement claiming a "weather plane" had strayed off course after its pilot had "difficulties with his oxygen equipment". What CIA officials did not realize was that the plane crashed almost fully intact, and the Soviets recovered its equipment. Powers was interrogated extensively by the KGB for months before he made a confession and a public apology for his part in espionage.

Powers tried to limit the information he shared with the KGB to that which could be determined from the remains of his plane's wreckage. He was hampered by information appearing in the western press. A KGB major stated "there's no reason for you to withhold information. We'll find it out anyway. Your press will give it to us." However, he limited his divulging of CIA contacts to one individual, with a pseudonym of "Collins". At the same time, he repeatedly stated the maximum altitude for the U-2 was , significantly lower than its actual flight ceiling.

The incident set back talks between Khrushchev and Eisenhower. Powers's interrogations ended on June 30, and his solitary confinement ended on July 9. On August 17, 1960, his trial began for espionage before the military division of the Supreme Court of the USSR. Lieutenant General Borisoglebsky, Major General Vorobyev, and Major General Zakharov presided. Roman Rudenko acted as prosecutor in his capacity of Procurator General of the Soviet Union. Mikhail I. Grinev served as Powers's defense counsel. In attendance were his parents and sister, and his wife Barbara and her mother. His father brought along his attorney Carl McAfee, while the CIA provided two additional attorneys.

On August 19, 1960, Powers was convicted of espionage, "a grave crime covered by Article 2 of the Soviet Union's law 'On Criminality Responsibility for State Crimes'". His sentence consisted of 10 years' confinement, three of which were to be in a prison, with the remainder in a labor camp. The US Embassy "News Bulletin" stated, according to Powers, "as far as the government was concerned, I had acted in accordance with the instructions given me and would receive my full salary while imprisoned".

He was held in Vladimir Central Prison, about east of Moscow, in building number 2 from September 9, 1960 until February 8, 1962. His cellmate was Zigurd Kruminsh, a Latvian political prisoner. Gary kept a diary and a journal while confined. Additionally he took up carpet weaving from his cellmate to pass the time. He could send and receive a limited number of letters from his family. The prison now contains a small museum with an exhibit on Powers, who allegedly developed a good rapport with Russian prisoners there. Some pieces of the plane and Powers's uniform are on display at the Monino Airbase museum near Moscow.

On February 10, 1962, Powers was exchanged, along with U.S. student Frederic Pryor, in a well-publicized spy swap at the Glienicke Bridge in Berlin. The exchange was for Soviet KGB Colonel Vilyam Fisher, known as "Rudolf Abel", who had been caught by the FBI and tried and jailed for espionage. Powers credited his father with the swap idea. When released, Powers's total time in captivity was 1 year, 9 months, and 10 days.

In 2010, CIA documents were released indicating that U.S. officials did not believe Powers's account of the incident at the time, because it was contradicted by a classified National Security Agency (NSA) report which alleged that the U-2 had descended from before changing course and disappearing from radar. However, newly released declassified CIA documents confirm the accuracy of Powers's report. The NSA report remains classified.

Powers initially received a cold reception on his return home. He was criticized for not activating his aircraft's self-destruct charge to destroy the camera, photographic film, and related classified parts. He was also criticized for not using a CIA-issued "suicide pill" to kill himself (a coin with shellfish toxin embedded in its grooves, revealed during CIA testimony to the Church Committee in 1975).

He was debriefed extensively by the CIA, Lockheed Corporation, and the Air Force, after which a statement was issued by CIA director John McCone that "Mr. Powers lived up to the terms of his employment and instructions in connection with his mission and in his obligations as an American." On March 6, 1962, he appeared before a Senate Armed Services Select Committee hearing chaired by Senator Richard Russell, Jr. which included Senators Prescott Bush, Leverett Saltonstall, Robert Byrd, Margaret Chase Smith, John Stennis, Strom Thurmond, and Barry Goldwater. During the hearing, Senator Saltonstall stated, "I commend you as a courageous, fine young American citizen who lived up to your instructions and who did the best you could under very difficult circumstances." Senator Bush declared, "I am satisfied he has conducted himself in exemplary fashion and in accordance with the highest traditions of service to one's country, and I congratulate him upon his conduct in captivity." Senator Goldwater sent him a handwritten note: "You did a good job for your country."

Powers and his wife Barbara divorced in January 1963. He started a relationship with Claudia Edwards "Sue" Downey, whom he had met while working briefly at CIA Headquarters. They were married on October 26, 1963. Their son Francis Gary Powers II was born on June 5, 1965.

During a speech in March 1964, former CIA Director Allen Dulles said of Powers, "He performed his duty in a very dangerous mission and he performed it well, and I think I know more about that than some of his detractors and critics know, and I am glad to say that to him tonight."

Powers worked for Lockheed as a test pilot from 1962 to 1970, though the CIA paid his salary. In 1970, he wrote the book "Operation Overflight" with co-author Curt Gentry. Lockheed fired him, because "the book's publication had ruffled some feathers at Langley." Powers became a helicopter traffic pilot reporter for KNBC News Channel 4.

Powers was conducting a traffic report for KNBC Channel 4 over West Los Angeles on August 1, 1977 when his helicopter crashed, killing him and his cameraman George Spears. They had been covering brush fires in Santa Barbara County in the KNBC helicopter and were heading back from them.

His Bell 206 JetRanger helicopter ran out of fuel and crashed at the Sepulveda Dam recreational area in Encino, California, several miles short of its intended landing site at Burbank Airport. The National Transportation Safety Board report attributed the probable cause of the crash to pilot error. According to Powers's son, an aviation mechanic had repaired a faulty fuel gauge without informing Powers, who subsequently misread it.

At the last moment, he noticed children playing in the area and directed the helicopter elsewhere to avoid landing on them. He might have landed safely if not for the last-second deviation, which compromised his autorotative descent.
Powers was survived by his wife, children Claudia Dee and Francis Gary Powers Jr., and five sisters. He is buried in Arlington National Cemetery as an Air Force veteran.

Powers received the CIA's Intelligence Star in 1965 after his return from the Soviet Union. Powers was originally scheduled to receive it in 1963 along with other pilots involved in the CIA's U-2 program, but the award was postponed for political reasons. In 1970, Powers published his first—and only—book review, on a work about aerial reconnaissance, "Unarmed and Unafraid" by Glenn Infield, in the monthly magazine "Business & Commercial Aviation". "The subject has great interest to me," he said, in submitting his review.

In 1998, newly declassified information revealed that Powers's mission had been a joint USAF/CIA operation. In 2000, on the 40th anniversary of the U-2 Incident, his family was presented with his posthumously awarded Prisoner of War Medal, Distinguished Flying Cross, and National Defense Service Medal. In addition, CIA Director George Tenet authorized Powers to posthumously receive the CIA's coveted Director's Medal for extreme fidelity and extraordinary courage in the line of duty.

On June 15, 2012, Powers was posthumously awarded the Silver Star medal for "demonstrating 'exceptional loyalty' while enduring harsh interrogation in the Lubyanka Prison in Moscow for almost two years." Air Force Chief of Staff General Norton Schwartz presented the decoration to Powers's grandchildren, Trey Powers, 9, and Lindsey Berry, 29, in a Pentagon ceremony.

Powers's son, Francis Gary Powers Jr., founded the Cold War Museum in 1996. Affiliated with the Smithsonian Institution, it was essentially a traveling exhibit until it found a permanent home in 2011 on a former Army communications base outside Washington, D.C.





</doc>
<doc id="12890" url="https://en.wikipedia.org/wiki?curid=12890" title="Gospel of James">
Gospel of James

The Gospel of James, also known as the Infancy Gospel of James or the Protoevangelium of James, is an apocryphal gospel probably written about AD 145, which expands backward in time the infancy stories contained in the Gospels of Matthew and Luke, and presents a narrative concerning the birth and upbringing of Mary herself. It is the oldest source outside the New Testament to assert the virginity of Mary not only prior to, but during (and after) the birth of Jesus. The ancient manuscripts that preserve the book have different titles, including "The Birth of Mary", "The Story of the Birth of Saint Mary, Mother of God," and "The Birth of Mary; The Revelation of James."

The document presents itself as written by James: "I, James, wrote this history in Jerusalem." The purported author is thus James, the brother of Jesus, but scholars have concluded that the work was not written by the person to whom it is attributed, but was composed some time in the mid to late 2nd century.

That conclusion is based on the style of the language and the fact that the author describes certain activities as contemporary Jewish customs that probably did not exist. For example, the work suggests there were consecrated temple virgins in Judaism, similar to the Vestal Virgins in pagan Rome, but that is never directly stated to have been a practice in mainstream Judaism. Conversely, some Eastern Orthodox Christians and Roman Catholics argue that the Old Testament shows that the idea of Mary being a consecrated virgin is plausible and claim the practice of consecrated virginity was within Judaism since the days of the prophet Samuel (), and until the time of the Maccabees (). A similar claim is also made by a number of Rabbinic sources.

The consensus is that it was actually composed in the latter half of the 2nd century. The first mention of it is by Origen of Alexandria in the early 3rd century, who says the text, like that of a Gospel of Peter, was of dubious, recent appearance and shared with that book the claim that the "brethren of the Lord" were sons of Joseph by a former wife.

Although a number of Church councils condemned it as an inauthentic writing of the New Testament, this did little to diminish its popularity. Pope Innocent I condemned this Gospel of James in his third epistle "ad Exuperium" in 405 AD, and the so-called Gelasian Decree also excluded it as canonical around 500 AD. Thomas Aquinas, in his "Summa Theologiae" rejects the Protevangelium of James teaching that midwives were present at Christ's birth, and invokes Jerome as contending that the words of the canonical gospels show that Mary was both mother and midwife, that she wrapped up the child with swaddling clothes and laid him in a manger. And thus concludes, "These words prove the falseness of the apocryphal ravings."

The Gospel of James is one of several surviving Infancy Gospels that give an idea of the miracle literature that was created to satisfy the hunger of early Christians for more detail about the early life of their Saviour. In Greek such an infancy gospel was termed a "protevangelion", a "pre-Gospel" narrating events of Jesus' life before those recorded in the four canonical gospels. Such a work was intended to be "apologetic, doctrinal, or simply to satisfy one's curiosity". The literary genre that these works represent shows stylistic features that suggest dates in the 2nd century and later. Other infancy gospels in this tradition include The Infancy Gospel of Thomas, the Gospel of Pseudo-Matthew (based on the Protevangelium of James and on the Infancy Gospel of Thomas), and the so-called Arabic Infancy Gospel; all of which were regarded by the Church as apocryphal.

Some indication of the popularity of the "Infancy Gospel of James" may be drawn from the fact that over 150 Greek manuscripts containing it have survived. The "Gospel of James" was translated into Syriac, Ethiopic, Coptic, Georgian, Old Slavonic, Armenian, Arabic, Irish and Latin. Though no early Latin versions are known, it was relegated to the apocrypha in the Gelasian decretal, so it must have been known in the West by the fifth century though the vast majority of the manuscripts come from the 10th century or later. The earliest known manuscript of the text, a papyrus dating to the third or early 4th century, was found in 1958; it is kept in the Bodmer Library, Geneva (Papyrus Bodmer 5). Of the surviving Greek manuscripts, the fullest text is a 10th-century codex in the Bibliothèque Nationale, Paris (Paris 1454).

The Gospel of James is in three equal parts, of eight chapters each:

One of the work's high points is the Lament of Anna. A primary theme is the work and grace of God in Mary's life, Mary's personal purity, and her perpetual virginity before, during and after the birth of Jesus, as confirmed by the midwife after she gave birth, and tested by Salome who is perhaps intended to be Salome, later the disciple of Jesus who is mentioned in the Gospel of Mark as being one of the women at the crucifixion.

This is also the earliest text that explicitly claims that Joseph was a widower, with children, at the time that Mary is entrusted to his care. This feature is mentioned in the text of Origen, who adduces it to demonstrate that the 'brethren of the Lord' were sons of Joseph by a former wife.

Among further traditions not present in the four canonical gospels are the birth of Jesus in a cave, the martyrdom of John the Baptist's father Zechariah during the Massacre of the Innocents and Joseph's being elderly when Jesus was born. The Nativity reported as taking place in a cave remained in the popular imagination; many Early Renaissance Sienese and Florentine paintings of the Nativity continued to show such a setting, which is practically universal in Byzantine, Greek and Russian icons of the Nativity. The Gospel also describes the narrative of Mary’s early childhood in the holiest part of the temple, which was later also mentioned in the Qur’an.





</doc>
<doc id="12891" url="https://en.wikipedia.org/wiki?curid=12891" title="Gene therapy">
Gene therapy

In the medicine field, gene therapy (also called human gene transfer) is the therapeutic delivery of nucleic acid into a patient's cells as a drug to treat disease. The first attempt at modifying human DNA was performed in 1980 by Martin Cline, but the first successful nuclear gene transfer in humans, approved by the National Institutes of Health, was performed in May 1989. The first therapeutic use of gene transfer as well as the first direct insertion of human DNA into the nuclear genome was performed by French Anderson in a trial starting in September 1990.

Between 1989 and February 2016, over 2,300 clinical trials had been conducted, more than half of them in phase I.

Not all medical procedures that introduce alterations to a patient's genetic makeup can be considered gene therapy. Bone marrow transplantation and organ transplants in general have been found to introduce foreign DNA into patients. Gene therapy is defined by the precision of the procedure and the intention of direct therapeutic effect.

Gene therapy was conceptualized in 1972, by authors who urged caution before commencing human gene therapy studies.

The first attempt, an unsuccessful one, at gene therapy (as well as the first case of medical transfer of foreign genes into humans not counting organ transplantation) was performed by Martin Cline on 10 July 1980. Cline claimed that one of the genes in his patients was active six months later, though he never published this data or had it verified and even if he is correct, it's unlikely it produced any significant beneficial effects treating beta-thalassemia.

After extensive research on animals throughout the 1980s and a 1989 bacterial gene tagging trial on humans, the first gene therapy widely accepted as a success was demonstrated in a trial that started on 14 September 1990, when Ashi DeSilva was treated for ADA-SCID.

The first somatic treatment that produced a permanent genetic change was performed in 1993.

Gene therapy is a way to fix a genetic problem at its source. The polymers are either translated into proteins, interfere with target gene expression, or possibly correct genetic mutations.

The most common form uses DNA that encodes a functional, therapeutic gene to replace a mutated gene. The polymer molecule is packaged within a "vector", which carries the molecule inside cells.

Early clinical failures led to dismissals of gene therapy. Clinical successes since 2006 regained researchers' attention, although as of 2014, it was still largely an experimental technique. These include treatment of retinal diseases Leber's congenital amaurosis and choroideremia, X-linked SCID, ADA-SCID, adrenoleukodystrophy, chronic lymphocytic leukemia (CLL), acute lymphocytic leukemia (ALL), multiple myeloma, haemophilia, and Parkinson's disease. Between 2013 and April 2014, US companies invested over $600 million in the field.

The first commercial gene therapy, Gendicine, was approved in China in 2003 for the treatment of certain cancers. In 2011 Neovasculgen was registered in Russia as the first-in-class gene-therapy drug for treatment of peripheral artery disease, including critical limb ischemia.
In 2012 Glybera, a treatment for a rare inherited disorder, became the first treatment to be approved for clinical use in either Europe or the United States after its endorsement by the European Commission.

Following early advances in genetic engineering of bacteria, cells, and small animals, scientists started considering how to apply it to medicine. Two main approaches were considered – replacing or disrupting defective genes. Scientists focused on diseases caused by single-gene defects, such as cystic fibrosis, haemophilia, muscular dystrophy, thalassemia, and sickle cell anemia. Glybera treats one such disease, caused by a defect in lipoprotein lipase.

DNA must be administered, reach the damaged cells, enter the cell and either express or disrupt a protein. Multiple delivery techniques have been explored. The initial approach incorporated DNA into an engineered virus to deliver the DNA into a chromosome. Naked DNA approaches have also been explored, especially in the context of vaccine development.

Generally, efforts focused on administering a gene that causes a needed protein to be expressed. More recently, increased understanding of nuclease function has led to more direct DNA editing, using techniques such as zinc finger nucleases and CRISPR. The vector incorporates genes into chromosomes. The expressed nucleases then knock out and replace genes in the chromosome. As of 2014 these approaches involve removing cells from patients, editing a chromosome and returning the transformed cells to patients.

Gene editing is a potential approach to alter the human genome to treat genetic diseases, viral diseases, and cancer. As of 2016 these approaches were still years from being medicine.

Gene therapy may be classified into two types:

In somatic cell gene therapy (SCGT), the therapeutic genes are transferred into any cell other than a gamete, germ cell, gametocyte, or undifferentiated stem cell. Any such modifications affect the individual patient only, and are not inherited by offspring. Somatic gene therapy represents mainstream basic and clinical research, in which therapeutic DNA (either integrated in the genome or as an external episome or plasmid) is used to treat disease.

Over 600 clinical trials utilizing SCGT are underway in the US. Most focus on severe genetic disorders, including immunodeficiencies, haemophilia, thalassaemia, and cystic fibrosis. Such single gene disorders are good candidates for somatic cell therapy. The complete correction of a genetic disorder or the replacement of multiple genes is not yet possible. Only a few of the trials are in the advanced stages.

In germline gene therapy (GGT), germ cells (sperm or egg cells) are modified by the introduction of functional genes into their genomes. Modifying a germ cell causes all the organism's cells to contain the modified gene. The change is therefore heritable and passed on to later generations. Australia, Canada, Germany, Israel, Switzerland, and the Netherlands prohibit GGT for application in human beings, for technical and ethical reasons, including insufficient knowledge about possible risks to future generations and higher risks versus SCGT. The US has no federal controls specifically addressing human genetic modification (beyond FDA regulations for therapies in general).

The delivery of DNA into cells can be accomplished by multiple methods. The two major classes are recombinant viruses (sometimes called biological nanoparticles or viral vectors) and naked DNA or DNA complexes (non-viral methods).

In order to replicate, viruses introduce their genetic material into the host cell, tricking the host's cellular machinery into using it as blueprints for viral proteins. Retroviruses go a stage further by having their genetic material copied into the genome of the host cell. Scientists exploit this by substituting a virus's genetic material with therapeutic DNA. (The term 'DNA' may be an oversimplification, as some viruses contain RNA, and gene therapy could take this form as well.) A number of viruses have been used for human gene therapy, including retroviruses, adenoviruses, herpes simplex, vaccinia, and adeno-associated virus. Like the genetic material (DNA or RNA) in viruses, therapeutic DNA can be designed to simply serve as a temporary blueprint that is degraded naturally or (at least theoretically) to enter the host's genome, becoming a permanent part of the host's DNA in infected cells.

Non-viral methods present certain advantages over viral methods, such as large scale production and low host immunogenicity. However, non-viral methods initially produced lower levels of transfection and gene expression, and thus lower therapeutic efficacy. Later technology remedied this deficiency.

Methods for non-viral gene therapy include the injection of naked DNA, electroporation, the gene gun, sonoporation, magnetofection, the use of oligonucleotides, lipoplexes, dendrimers, and inorganic nanoparticles.

Some of the unsolved problems include:

Three patients' deaths have been reported in gene therapy trials, putting the field under close scrutiny. The first was that of Jesse Gelsinger, who died in 1999 because of immune rejection response. One X-SCID patient died of leukemia in 2003. In 2007, a rheumatoid arthritis patient died from an infection; the subsequent investigation concluded that the death was not related to gene therapy.

In 1972 Friedmann and Roblin authored a paper in "Science" titled "Gene therapy for human genetic disease?" Rogers (1970) was cited for proposing that "exogenous good DNA" be used to replace the defective DNA in those who suffer from genetic defects.

In 1984 a retrovirus vector system was designed that could efficiently insert foreign genes into mammalian chromosomes.

The first approved gene therapy clinical research in the US took place on 14 September 1990, at the National Institutes of Health (NIH), under the direction of William French Anderson. Four-year-old Ashanti DeSilva received treatment for a genetic defect that left her with ADA-SCID, a severe immune system deficiency. The defective gene of the patient's blood cells was replaced by the functional variant. Ashanti’s immune system was partially restored by the therapy. Production of the missing enzyme was temporarily stimulated, but the new cells with functional genes were not generated. She led a normal life only with the regular injections performed every two months. The effects were successful, but temporary.

Cancer gene therapy was introduced in 1992/93 (Trojan et al. 1993). The treatment of glioblastoma multiforme, the malignant brain tumor whose outcome is always fatal, was done using a vector expressing antisense IGF-I RNA (clinical trial approved by NIH protocolno.1602 November 24, 1993, and by the FDA in 1994). This therapy also represents the beginning of cancer immunogene therapy, a treatment which proves to be effective due to the anti-tumor mechanism of IGF-I antisense, which is related to strong immune and apoptotic phenomena.

In 1992 Claudio Bordignon, working at the Vita-Salute San Raffaele University, performed the first gene therapy procedure using hematopoietic stem cells as vectors to deliver genes intended to correct hereditary diseases. In 2002 this work led to the publication of the first successful gene therapy treatment for adenosine deaminase deficiency (ADA-SCID). The success of a multi-center trial for treating children with SCID (severe combined immune deficiency or "bubble boy" disease) from 2000 and 2002, was questioned when two of the ten children treated at the trial's Paris center developed a leukemia-like condition. Clinical trials were halted temporarily in 2002, but resumed after regulatory review of the protocol in the US, the United Kingdom, France, Italy, and Germany.

In 1993 Andrew Gobea was born with SCID following prenatal genetic screening. Blood was removed from his mother's placenta and umbilical cord immediately after birth, to acquire stem cells. The allele that codes for adenosine deaminase (ADA) was obtained and inserted into a retrovirus. Retroviruses and stem cells were mixed, after which the viruses inserted the gene into the stem cell chromosomes. Stem cells containing the working ADA gene were injected into Andrew's blood. Injections of the ADA enzyme were also given weekly. For four years T cells (white blood cells), produced by stem cells, made ADA enzymes using the ADA gene. After four years more treatment was needed.

Jesse Gelsinger's death in 1999 impeded gene therapy research in the US. As a result, the FDA suspended several clinical trials pending the reevaluation of ethical and procedural practices.

The modified cancer gene therapy strategy of antisense IGF-I RNA (NIH n˚ 1602) using antisense / triple helix anti-IGF-I approach was registered in 2002 by Wiley gene therapy clinical trial - n˚ 635 and 636. The approach has shown promising results in the treatment of six different malignant tumors: glioblastoma, cancers of liver, colon, prostate, uterus, and ovary (Collaborative NATO Science Programme on Gene Therapy USA, France, Poland n˚ LST 980517 conducted by J. Trojan) (Trojan et al., 2012). This anti-gene antisense/triple helix therapy has proven to be efficient, due to the mechanism stopping simultaneously IGF-I expression on translation and transcription levels, strengthening anti-tumor immune and apoptotic phenomena.

Sickle-cell disease can be treated in mice. The mice – which have essentially the same defect that causes human cases – used a viral vector to induce production of fetal hemoglobin (HbF), which normally ceases to be produced shortly after birth. In humans, the use of hydroxyurea to stimulate the production of HbF temporarily alleviates sickle cell symptoms. The researchers demonstrated this treatment to be a more permanent means to increase therapeutic HbF production.

A new gene therapy approach repaired errors in messenger RNA derived from defective genes. This technique has the potential to treat thalassaemia, cystic fibrosis and some cancers.

Researchers created liposomes 25 nanometers across that can carry therapeutic DNA through pores in the nuclear membrane.

In 2003 a research team inserted genes into the brain for the first time. They used liposomes coated in a polymer called polyethylene glycol, which unlike viral vectors, are small enough to cross the blood–brain barrier.

Short pieces of double-stranded RNA (short, interfering RNAs or siRNAs) are used by cells to degrade RNA of a particular sequence. If a siRNA is designed to match the RNA copied from a faulty gene, then the abnormal protein product of that gene will not be produced.

Gendicine is a cancer gene therapy that delivers the tumor suppressor gene p53 using an engineered adenovirus. In 2003, it was approved in China for the treatment of head and neck squamous cell carcinoma.

In March researchers announced the successful use of gene therapy to treat two adult patients for X-linked chronic granulomatous disease, a disease which affects myeloid cells and damages the immune system. The study is the first to show that gene therapy can treat the myeloid system.

In May a team reported a way to prevent the immune system from rejecting a newly delivered gene. Similar to organ transplantation, gene therapy has been plagued by this problem. The immune system normally recognizes the new gene as foreign and rejects the cells carrying it. The research utilized a newly uncovered network of genes regulated by molecules known as microRNAs. This natural function selectively obscured their therapeutic gene in immune system cells and protected it from discovery. Mice infected with the gene containing an immune-cell microRNA target sequence did not reject the gene.

In August scientists successfully treated metastatic melanoma in two patients using killer T cells genetically retargeted to attack the cancer cells.

In November researchers reported on the use of VRX496, a gene-based immunotherapy for the treatment of HIV that uses a lentiviral vector to deliver an antisense gene against the HIV envelope. In a phase I clinical trial, five subjects with chronic HIV infection who had failed to respond to at least two antiretroviral regimens were treated. A single intravenous infusion of autologous CD4 T cells genetically modified with VRX496 was well tolerated. All patients had stable or decreased viral load; four of the five patients had stable or increased CD4 T cell counts. All five patients had stable or increased immune response to HIV antigens and other pathogens. This was the first evaluation of a lentiviral vector administered in a US human clinical trial.

In May researchers announced the first gene therapy trial for inherited retinal disease. The first operation was carried out on a 23-year-old British male, Robert Johnson, in early 2007.

Leber's congenital amaurosis is an inherited blinding disease caused by mutations in the RPE65 gene. The results of a small clinical trial in children were published in April. Delivery of recombinant adeno-associated virus (AAV) carrying RPE65 yielded positive results. In May two more groups reported positive results in independent clinical trials using gene therapy to treat the condition. In all three clinical trials, patients recovered functional vision without apparent side-effects.

In September researchers were able to give trichromatic vision to squirrel monkeys. In November 2009, researchers halted a fatal genetic disorder called adrenoleukodystrophy in two children using a lentivirus vector to deliver a functioning version of ABCD1, the gene that is mutated in the disorder.

An April paper reported that gene therapy addressed achromatopsia (color blindness) in dogs by targeting cone photoreceptors. Cone function and day vision were restored for at least 33 months in two young specimens. The therapy was less efficient for older dogs.

In September it was announced that an 18-year-old male patient in France with beta-thalassemia major had been successfully treated. Beta-thalassemia major is an inherited blood disease in which beta haemoglobin is missing and patients are dependent on regular lifelong blood transfusions. The technique used a lentiviral vector to transduce the human ß-globin gene into purified blood and marrow cells obtained from the patient in June 2007. The patient's haemoglobin levels were stable at 9 to 10 g/dL. About a third of the hemoglobin contained the form introduced by the viral vector and blood transfusions were not needed. Further clinical trials were planned. Bone marrow transplants are the only cure for thalassemia, but 75% of patients do not find a matching donor.

Cancer immunogene therapy using modified antigene, antisense/triple helix approach was introduced in South America in 2010/11 in La Sabana University, Bogota (Ethical Committee 14 December 2010, no P-004-10). Considering the ethical aspect of gene diagnostic and gene therapy targeting IGF-I, the IGF-I expressing tumors i.e. lung and epidermis cancers were treated (Trojan et al. 2016).

In 2007 and 2008, a man (Timothy Ray Brown) was cured of HIV by repeated hematopoietic stem cell transplantation (see also allogeneic stem cell transplantation, allogeneic bone marrow transplantation, allotransplantation) with double-delta-32 mutation which disables the CCR5 receptor. This cure was accepted by the medical community in 2011. It required complete ablation of existing bone marrow, which is very debilitating.

In August two of three subjects of a pilot study were confirmed to have been cured from chronic lymphocytic leukemia (CLL). The therapy used genetically modified T cells to attack cells that expressed the CD19 protein to fight the disease. In 2013, the researchers announced that 26 of 59 patients had achieved complete remission and the original patient had remained tumor-free.

Human HGF plasmid DNA therapy of cardiomyocytes is being examined as a potential treatment for coronary artery disease as well as treatment for the damage that occurs to the heart after myocardial infarction.

In 2011 Neovasculgen was registered in Russia as the first-in-class gene-therapy drug for treatment of peripheral artery disease, including critical limb ischemia; it delivers the gene encoding for VEGF. Neovasculogen is a plasmid encoding the CMV promoter and the 165 amino acid form of VEGF.

The FDA approved Phase 1 clinical trials on thalassemia major patients in the US for 10 participants in July. The study was expected to continue until 2015.

In July 2012, the European Medicines Agency recommended approval of a gene therapy treatment for the first time in either Europe or the United States. The treatment used Alipogene tiparvovec (Glybera) to compensate for lipoprotein lipase deficiency, which can cause severe pancreatitis. The recommendation was endorsed by the European Commission in November 2012 and commercial rollout began in late 2014. Alipogene tiparvovec was expected to cost around $1.6 million per treatment in 2012, revised to $1 million in 2015, making it the most expensive medicine in the world at the time. As of 2016, only one person had been treated with drug.

In December 2012, it was reported that 10 of 13 patients with multiple myeloma were in remission "or very close to it" three months after being injected with a treatment involving genetically engineered T cells to target proteins NY-ESO-1 and LAGE-1, which exist only on cancerous myeloma cells.

In March researchers reported that three of five adult subjects who had acute lymphocytic leukemia (ALL) had been in remission for five months to two years after being treated with genetically modified T cells which attacked cells with CD19 genes on their surface, i.e. all B-cells, cancerous or not. The researchers believed that the patients' immune systems would make normal T-cells and B-cells after a couple of months. They were also given bone marrow. One patient relapsed and died and one died of a blood clot unrelated to the disease.

Following encouraging Phase 1 trials, in April, researchers announced they were starting Phase 2 clinical trials (called CUPID2 and SERCA-LVAD) on 250 patients at several hospitals to combat heart disease. The therapy was designed to increase the levels of SERCA2, a protein in heart muscles, improving muscle function. The FDA granted this a Breakthrough Therapy Designation to accelerate the trial and approval process. In 2016 it was reported that no improvement was found from the CUPID 2 trial.

In July researchers reported promising results for six children with two severe hereditary diseases had been treated with a partially deactivated lentivirus to replace a faulty gene and after 7–32 months. Three of the children had metachromatic leukodystrophy, which causes children to lose cognitive and motor skills. The other children had Wiskott-Aldrich syndrome, which leaves them to open to infection, autoimmune diseases, and cancer. Follow up trials with gene therapy on another six children with Wiskott-Aldrich syndrome were also reported as promising.

In October researchers reported that two children born with adenosine deaminase severe combined immunodeficiency disease (ADA-SCID) had been treated with genetically engineered stem cells 18 months previously and that their immune systems were showing signs of full recovery. Another three children were making progress. In 2014 a further 18 children with ADA-SCID were cured by gene therapy. ADA-SCID children have no functioning immune system and are sometimes known as "bubble children."

Also in October researchers reported that they had treated six hemophilia sufferers in early 2011 using an adeno-associated virus. Over two years later all six were producing clotting factor.

In January researchers reported that six choroideremia patients had been treated with adeno-associated virus with a copy of REP1. Over a six-month to two-year period all had improved their sight. By 2016, 32 patients had been treated with positive results and researchers were hopeful the treatment would be long-lasting. Choroideremia is an inherited genetic eye disease with no approved treatment, leading to loss of sight.

In March researchers reported that 12 HIV patients had been treated since 2009 in a trial with a genetically engineered virus with a rare mutation (CCR5 deficiency) known to protect against HIV with promising results.

Clinical trials of gene therapy for sickle cell disease were started in 2014. There is a need for high quality randomised controlled trials assessing the risks and benefits involved with gene therapy for people with sickle cell disease.

In February LentiGlobin BB305, a gene therapy treatment undergoing clinical trials for treatment of beta thalassemia gained FDA "breakthrough" status after several patients were able to forgo the frequent blood transfusions usually required to treat the disease.

In March researchers delivered a recombinant gene encoding a broadly neutralizing antibody into monkeys infected with simian HIV; the monkeys' cells produced the antibody, which cleared them of HIV. The technique is named immunoprophylaxis by gene transfer (IGT). Animal tests for antibodies to ebola, malaria, influenza, and hepatitis were underway.

In March, scientists, including an inventor of CRISPR, Jennifer Doudna, urged a worldwide moratorium on germline gene therapy, writing "scientists should avoid even attempting, in lax jurisdictions, germline genome modification for clinical application in humans" until the full implications "are discussed among scientific and governmental organizations".

In October, researchers announced that they had treated a baby girl, Layla Richards, with an experimental treatment using donor T-cells genetically engineered using TALEN to attack cancer cells. One year after the treatment she was still free of her cancer (a highly aggressive form of acute lymphoblastic leukaemia [ALL]). Children with highly aggressive ALL normally have a very poor prognosis and Layla's disease had been regarded as terminal before the treatment.

In December, scientists of major world academies called for a moratorium on inheritable human genome edits, including those related to CRISPR-Cas9 technologies but that basic research including embryo gene editing should continue.

In April the Committee for Medicinal Products for Human Use of the European Medicines Agency endorsed a gene therapy treatment called Strimvelis and the European Commission approved it in June. This treats children born with adenosine deaminase deficiency and who have no functioning immune system. This was the second gene therapy treatment to be approved in Europe.

In October, Chinese scientists reported they had started a trial to genetically modify T-cells from 10 adult patients with lung cancer and reinject the modified T-cells back into their bodies to attack the cancer cells. The T-cells had the PD-1 protein (which stops or slows the immune response) removed using CRISPR-Cas9.

A 2016 Cochrane systematic review looking at data from four trials on topical cystic fibrosis transmembrane conductance regulator (CFTR) gene therapy does not support its clinical use as a mist inhaled into the lungs to treat cystic fibrosis patients with lung infections. One of the four trials did find weak evidence that liposome-based CFTR gene transfer therapy may lead to a small respiratory improvement for people with CF. This weak evidence is not enough to make a clinical recommendation for routine CFTR gene therapy.

In February Kite Pharma announced results from a clinical trial of CAR-T cells in around a hundred people with advanced Non-Hodgkin lymphoma.

In March, French scientists reported on clinical research of gene therapy to treat sickle-cell disease.

In August, the FDA approved tisagenlecleucel for acute lymphoblastic leukemia. Tisagenlecleucel is an adoptive cell transfer therapy for B-cell acute lymphoblastic leukemia; T cells from a person with cancer are removed, genetically engineered to make a specific T-cell receptor (a chimeric T cell receptor, or "CAR-T") that reacts to the cancer, and are administered back to the person. The T cells are engineered to target a protein called CD19 that is common on B cells. This is the first form of gene therapy to be approved in the United States. In October, a similar therapy called axicabtagene ciloleucel was approved for non-Hodgkin lymphoma.

In December the results of using an adeno-associated virus with blood clotting factor VIII to treat nine haemophilia A patients were published. Six of the seven patients on the high dose regime increased the level of the blood clotting VIII to normal levels. The low and medium dose regimes had no effect on the patient's blood clotting levels.

In December, the FDA approved Luxturna, the first "in vivo" gene therapy, for the treatment of blindness due to Leber's congenital amaurosis. The price of this treatment was 850,000 US dollars for both eyes.

Speculated uses for gene therapy include:

Gene therapy techniques have the potential to provide alternative treatments for those with infertility. Recently, successful experimentation on mice has proven that fertility can be restored by using the gene therapy method, CRISPR. Spermatogenical stem cells from another organism were transplanted into the testes of an infertile male mouse. The stem cells re-established spermatogenesis and fertility.

Athletes might adopt gene therapy technologies to improve their performance. Gene doping is not known to occur, but multiple gene therapies may have such effects. Kayser et al. argue that gene doping could level the playing field if all athletes receive equal access. Critics claim that any therapeutic intervention for non-therapeutic/enhancement purposes compromises the ethical foundations of medicine and sports.

Genetic engineering could be used to cure diseases, but also to change physical appearance, metabolism, and even improve physical capabilities and mental faculties such as memory and intelligence. Ethical claims about germline engineering include beliefs that every fetus has a right to remain genetically unmodified, that parents hold the right to genetically modify their offspring, and that every child has the right to be born free of preventable diseases. For parents, genetic engineering could be seen as another child enhancement technique to add to diet, exercise, education, training, cosmetics, and plastic surgery. Another theorist claims that moral concerns limit but do not prohibit germline engineering.

Possible regulatory schemes include a complete ban, provision to everyone, or professional self-regulation. The American Medical Association’s Council on Ethical and Judicial Affairs stated that "genetic interventions to enhance traits should be considered permissible only in severely restricted situations: (1) clear and meaningful benefits to the fetus or child; (2) no trade-off with other characteristics or traits; and (3) equal access to the genetic technology, irrespective of income or other socioeconomic characteristics."

As early in the history of biotechnology as 1990, there have been scientists opposed to attempts to modify the human germline using these new tools, and such concerns have continued as technology progressed. With the advent of new techniques like CRISPR, in March 2015 a group of scientists urged a worldwide moratorium on clinical use of gene editing technologies to edit the human genome in a way that can be inherited. In April 2015, researchers sparked controversy when they reported results of basic research to edit the DNA of non-viable human embryos using CRISPR. A committee of the American National Academy of Sciences and National Academy of Medicine gave qualified support to human genome editing in 2017 once answers have been found to safety and efficiency problems "but only for serious conditions under stringent oversight."

Regulations covering genetic modification are part of general guidelines about human-involved biomedical research. There are no international treaties which are legally binding in this area, but there are recommendations for national laws from various bodies.

The Helsinki Declaration (Ethical Principles for Medical Research Involving Human Subjects) was amended by the World Medical Association's General Assembly in 2008. This document provides principles physicians and researchers must consider when involving humans as research subjects. The Statement on Gene Therapy Research initiated by the Human Genome Organization (HUGO) in 2001 provides a legal baseline for all countries. HUGO’s document emphasizes human freedom and adherence to human rights, and offers recommendations for somatic gene therapy, including the importance of recognizing public concerns about such research.

No federal legislation lays out protocols or restrictions about human genetic engineering. This subject is governed by overlapping regulations from local and federal agencies, including the Department of Health and Human Services, the FDA and NIH's Recombinant DNA Advisory Committee. Researchers seeking federal funds for an investigational new drug application, (commonly the case for somatic human genetic engineering,) must obey international and federal guidelines for the protection of human subjects.

NIH serves as the main gene therapy regulator for federally funded research. Privately funded research is advised to follow these regulations. NIH provides funding for research that develops or enhances genetic engineering techniques and to evaluate the ethics and quality in current research. The NIH maintains a mandatory registry of human genetic engineering research protocols that includes all federally funded projects.

An NIH advisory committee published a set of guidelines on gene manipulation. The guidelines discuss lab safety as well as human test subjects and various experimental types that involve genetic changes. Several sections specifically pertain to human genetic engineering, including Section III-C-1. This section describes required review processes and other aspects when seeking approval to begin clinical research involving genetic transfer into a human patient. The protocol for a gene therapy clinical trial must be approved by the NIH's Recombinant DNA Advisory Committee prior to any clinical trial beginning; this is different from any other kind of clinical trial.
As with other kinds of drugs, the FDA regulates the quality and safety of gene therapy products and supervises how these products are used clinically. Therapeutic alteration of the human genome falls under the same regulatory requirements as any other medical treatment. Research involving human subjects, such as clinical trials, must be reviewed and approved by the FDA and an Institutional Review Board.

Gene therapy is the basis for the plotline of the film "I Am Legend" and the TV show "Will Gene Therapy Change the Human Race?". In 1994, gene therapy was a plot element in The Erlenmeyer Flask, The X-Files' first season finale. It is also used in Stargate as a means of allowing humans to use Ancient technology.




</doc>
<doc id="12893" url="https://en.wikipedia.org/wiki?curid=12893" title="Galatea">
Galatea

Galatea is an ancient Greek name meaning "she who is milk-white".

Galatea, Galathea or Gallathea may refer to:










</doc>
<doc id="12896" url="https://en.wikipedia.org/wiki?curid=12896" title="Gulf of Oman">
Gulf of Oman

The Gulf of Oman or Sea of Oman ( "khalīj ʿUmān"; "daryāye ʿUmān") is a strait (and not an actual gulf) that connects the Arabian Sea with the Strait of Hormuz, which then runs to the Persian Gulf. It borders Iran and Pakistan on the north, Oman on the south, and the United Arab Emirates on the west.

The International Hydrographic Organization defines the limits of the Gulf of Oman as follows:


In 2018, scientists confirmed the Gulf of Oman contains one of the world's largest marine dead zones, where the ocean contains little or no oxygen and marine wildlife can not exist. The dead zone encompasses nearly the entire 63,700-square-mile Gulf of Oman. The cause is a combination of increased ocean warming, and increased runoff of nitrogen and phosphorus from fertilizers.





</doc>
<doc id="12898" url="https://en.wikipedia.org/wiki?curid=12898" title="Grammatical case">
Grammatical case

Case is a special grammatical category of a noun, pronoun, adjective, participle or numeral whose value reflects the grammatical function performed by that word in a phrase, clause or sentence. In some languages, nouns, pronouns, adjectives, determiners, participles, prepositions, numerals, articles and their modifiers take different inflected forms, depending on their case. As a language evolves, cases can merge (for instance, in Ancient Greek, the locative case merged with the dative case), a phenomenon formally called syncretism.

English has largely lost its case system although personal pronouns still have three cases, which are simplified forms of the nominative, accusative and genitive cases. They are used with personal pronouns: subjective case (I, you, he, she, it, we, they, who, whoever), objective case (me, you, him, her, it, us, them, whom, whomever) and possessive case (my, mine; your, yours; his; her, hers; its; our, ours; their, theirs; whose; whosever). Forms such as "I", "he" and "we" are used for the subject ("I kicked the ball"), and forms such as "me", "him" and "us" are used for the object ("John kicked me").

Languages such as Sanskrit, Ancient Greek, Latin, Armenian, Hungarian, Hindi, Tibetan, Czech, Slovak, Turkish, Tamil, Romanian, Russian, Polish, Croatian, Serbian, Estonian, Finnish, Icelandic, Belarusian, Ukrainian, Lithuanian, Basque and most Caucasian languages have extensive case systems, with nouns, pronouns, adjectives, and determiners all inflecting (usually by means of different suffixes) to indicate their case. The number of cases differs between languages: Esperanto has two; German, Icelandic and Swedish have four; Turkish, Latin and Russian each have at least six; Armenian, Czech, Polish, Serbo-Croatian, Ukrainian and Lithuanian all have seven; Sanskrit has eight; Estonian has fourteen and Finnish has fifteen, Hungarian has eighteen and Tsez has sixty-four. 

Commonly encountered cases include nominative, accusative, dative and genitive. A role that one of those languages marks by case is often marked in English with a preposition. For example, the English prepositional phrase "with (his) foot" (as in "John kicked the ball with his foot") might be rendered in Russian using a single noun in the instrumental case or in Ancient Greek as (, meaning "the foot") with both words (the definite article, and the noun () "foot") changing to dative form.

More formally, case has been defined as "a system of marking dependent nouns for the type of relationship they bear to their heads". Cases should be distinguished from thematic roles such as "agent" and "patient". They are often closely related, and in languages such as Latin, several thematic roles have an associated case, but cases are a morphological notion, but thematic roles are a semantic one. Languages having cases often exhibit free word order, as thematic roles are not required to be marked by position in the sentence.

The English word "case" used in this sense comes from the Latin , which is derived from the verb , "to fall", from the Proto-Indo-European root "". The Latin word is a calque of the Greek , , lit. "falling, fall". The sense is that all other cases are considered to have "fallen" away from the nominative. This picture is also reflected in the word "declension", from Latin , "to lean", from the PIE root "".

The equivalent to "case" in several other European languages also derives from "casus", including in French, in Spanish and in German. The Russian word ("padyézh") is a calque from Greek and similarly contains a root meaning "fall", and the German and Czech simply mean "fall", and are used for both the concept of grammatical case and to refer to physical falls. The Finnish equivalent is , which can also mean "position" or "support".

Although not very prominent in modern English, cases featured much more saliently in Old English and other ancient Indo-European languages, such as Latin, Ancient Greek , and Sanskrit. Historically, the Indo-European languages had eight morphological cases, though modern languages typically have fewer, using prepositions and word order to convey information that had previously been conveyed using distinct noun forms. Among modern languages, cases still feature prominently in most of the Balto-Slavic languages (except Macedonian and Bulgarian), with most having six to eight cases, as well as Icelandic, German and Modern Greek, which have four. In German, cases are mostly marked on articles and adjectives, and less so on nouns. In Icelandic, articles, adjectives, personal names and nouns are all marked for case, making it, among other things, the living Germanic language that could be said to most closely resemble Proto-Germanic.

The eight historical Indo-European cases are as follows, with examples either of the English case or of the English syntactic alternative to case:

All of the above are just rough descriptions; the precise distinctions vary from language to language, and are often quite complex. Case is based fundamentally on changes to the noun to indicate the noun's role in the sentence. This is not how English works; word order and prepositions are used to achieve this.

Modern English has largely abandoned the inflectional case system of Indo-European in favor of analytic constructions. The personal pronouns of Modern English retain morphological case more strongly than any other word class (a remnant of the more extensive case system of Old English). For other pronouns, and all nouns, adjectives, and articles, grammatical function is indicated only by word order, by prepositions, and by the "Saxon genitive" ("-'s").

Taken as a whole, English personal pronouns are typically said to have three morphological cases:


Most English personal pronouns have five forms: the nominative and oblique case forms, the possessive case, which has both a "determiner" form (such as "my", "our") and a distinct "independent" form (such as "mine", "ours") (with two exceptions: the third person singular masculine and the third person singular neuter "it", which use the same form for both determiner and independent ["his car", "it is his"]), and a distinct "reflexive" or "intensive" form (such as "myself", "ourselves"). The interrogative personal pronoun "who" exhibits the greatest diversity of forms within the modern English pronoun system, having definite nominative, oblique, and genitive forms ("who", "whom", "whose") and equivalently coordinating indefinite forms ("whoever", "whomever", and "whosever").

Though English "pronouns" can have subject and object forms (he/him, she/her), "nouns" show only a singular/plural and a possessive/non-possessive distinction (e.g. "chair", "chairs", "chair's", "chairs"'). Note that "chair" does not change form between "the chair is here" (subject) and "I saw the chair" (direct object), a distinction made by word order and context.

Cases can be ranked in the following hierarchy, where a language that does not have a given case will tend not to have any cases to the right of the missing case:

This is, however, only a general tendency. Many forms of Central German, such as Colognian and Luxembourgish, have a dative case but lack a genitive. In Irish nouns, the nominative and accusative have fallen together, whereas the dative–locative has remained separate in some paradigms; Irish also has genitive and vocative cases. In Punjabi, the accusative, genitive, and dative have merged to an oblique case, but the language still retains vocative, locative, and ablative cases. Old English had an instrumental case, but not a locative or prepositional.

The "traditional" case order (nom-gen-dat-acc) was expressed for the first time in "The Art of Grammar" in the 2nd century AD:
Latin grammars, such as "Ars grammatica", followed the Greek tradition, but added the ablative case of Latin. Later other European languages also followed that Graeco-Roman tradition.

However, for some languages, such as Russian or Latin, due to case syncretism the order may be changed for convenience, where the accusative or the vocative cases are placed after the nominative and before the genitive. For example:
In the most common case concord system, only the head-word (the noun) in a phrase is marked for case. This system appears in many Papuan languages as well as in Turkic, Mongolian, Quechua, Dravidian, Indo-Aryan, and other languages. In Basque and various Amazonian and Australian languages, only the phrase-final word (not necessarily the noun) is marked for case. In many Indo-European, Balto-Finnic, and Semitic languages, case is marked on the noun, the determiner, and usually the adjective. Other systems are less common. In some languages, there is double-marking of a word as both genitive (to indicate semantic role) and another case such as accusative (to establish concord with the head noun).

Declension is the process or result of altering nouns to the correct grammatical cases. Languages with rich nominal inflection (use grammatical cases for many purposes) typically have a number of identifiable declension classes, or groups of nouns with a similar pattern of case inflection or declension. Sanskrit has six declension classes, whereas Latin is traditionally considered to have five, and Ancient Greek three declension classes. For example, Slovak has fifteen noun declension classes, five for each gender (the number may vary depending on which paradigms are counted or omitted, this mainly concerns those that modify declension of foreign words; refer to article).

In Indo-European languages, declension patterns may depend on a variety of factors, such as gender, number, phonological environment, and irregular historical factors. Pronouns sometimes have separate paradigms. In some languages, particularly Slavic languages, a case may contain different groups of endings depending on whether the word is a noun or an adjective. A single case may contain many different endings, some of which may even be derived from different roots. For example, in Polish, the genitive case has "-a, -u, -ów, -i/-y, -e-" for nouns, and "-ego, -ej, -ich/-ych" for adjectives. To a lesser extent, a noun's animacy or humanness may add another layer of complexity. For example, in Russian:

vs.
and

An example of a Belarusian case inflection is given below, using the singular forms of the Belarusian term for "country," which belongs to Belarusian's first declension class.

In German, grammatical case is largely preserved in the articles and adjectives, but nouns have lost many of their original endings. Below is an example of case inflection in German using the masculine definite article and one of the German words for "sailor".


Modern Greek has four cases: nominative, genitive, accusative, and vocative. For neuters and most groups of feminines and plural masculines, the genitive case differs from the other three. Below is an example of the declension of (sky), which has a different form in the singular of all four cases, together with the appropriate article in both the singular and the plural:


Ancient Greek had one additional case, the dative. At some point, it was replaced with the preposition , followed by the accusative. This became necessary when pronunciation simplified, merging the two long vowels eta and omega to short. The result was that dative did not sound much different from the accusative in the singular of the first two groups. However, the dative case is still used in many expressions.

With time, only the sigma of was left and got attached to the article, except when an article is not used and it becomes instead. Note that this is not a different case from the accusative.

Below is an example with the dative case of the word (city):


Cases in Japanese are marked by particles placed after the nouns. A distinctive feature of Japanese is the presence of two cases, which are roughly equivalent to the nominative case in other languages: one representing the sentence topic, the other representing the subject. The most important case markers are the following:


Cases in Korean are marked by particles placed after the nouns, similar to Japanese. Like Japanese, the nominative case has two distinctions, one representing the topic of a sentence and the other the subject. In informal speech, nominative (이/가 and 은/는) and accusative (을/를) particles are often omitted, while dative (에게) and ablative (에서) are shortened to simply 에, if the meaning of the sentence can easily be inferred from context. Most common case markers are the following:


An example of a Latin case inflection is given below, using the singular forms of the Latin term for "cook," which belongs to Latin's second declension class.


Latvian nouns have seven grammatical cases: nominative, genitive, dative, accusative, instrumental, locative and vocative. The instrumental case is always identical to the accusative in the singular and to the dative in the plural. It is used as a free-standing case (without a preposition) only in highly restricted contexts in modern Latvian.

An example of a Latvian case inflection is given below, using the singular forms of the Latvian term for "man," which belongs to the first declension class.


In Lithuanian, only the inflection usually changes in the seven different grammatical cases:


An example of a Polish case inflection is given below, using the singular forms of the Polish terms for "human" () and "monkey" ()


Hungarian declension is relatively simple with regular suffixes attached to the vast majority of nouns. The following table lists a few of the many cases used in Hungarian.

Romanian is the only modern major Romance language with a case system for all nouns, whereas all other Romance languages dropped the cases for nouns replacing them by prepositions. An example of Romanian case inflection is given below, using the singular form of the word "boy":


An example of a Russian case inflection is given below (with explicit stress marks), using the singular forms of the Russian term for "sailor," which belongs to Russian's first declension class.

Up to ten additional cases are identified by linguists, although today all of them are either incomplete (do not apply to all nouns or do not form full word paradigm with all combinations of gender and number) or degenerate (appear identical to one of the main six cases). The most recognized additional cases are locative (), partitive (), and two forms of vocative — old () and neo-vocative (). Sometimes, so called count-form (for some countable nouns after numerals) is considered to be a sub-case. See details.

Grammatical case was analyzed extensively in Sanskrit. The grammarian Pāṇini identified six semantic roles or "kāraka", which by default are related to the following eight Sanskrit cases in order:

For example, in the following sentence "leaf" is the agent ("kartā", nominative case), "tree" is the source ("apādāna", ablative case), and "ground" is the locus ("adhikaraṇa", locative case). The declensions are reflected in the morphemes "-āt", "-am", and "-au" respectively.
However, the cases may be deployed for other than the default thematic roles. A notable example is the passive construction. In the following sentence, "Devadatta" is the "kartā", but appears in the instrumental case, and "rice", the "karman", object, is in the nominative case (as subject of the verb). The declensions are reflected in the morphemes "-ena" and "-am".
The Tamil case system is analyzed in native and missionary grammars as consisting of a finite number of cases. The usual treatment of Tamil case (Arden 1942) is one in which there are seven cases: nominative (first case), accusative (second case), instrumental (third), dative (fourth), ablative (fifth), genitive (sixth), and locative (seventh). In traditional analyses, there is always a clear distinction made between post-positional morphemes and case endings. The vocative is sometimes given a place in the case system as an eighth case, but vocative forms do not participate in usual morphophonemic alternations and do not govern the use of any postpositions. Modern grammarians, however, argue that this eight-case classification is coarse and artificial and that Tamil usage is best understood if each suffix or combination of suffixes is seen as marking a separate case.

Telugu has eight cases.
As languages evolve, case systems change. In early Ancient Greek, for example, the genitive and ablative cases became combined, giving five cases, rather than the six retained in Latin. In modern Hindi, the Sanskrit cases have been reduced to two: a direct case (for subjects and direct objects) and an oblique case. In English, apart from the pronouns discussed above, case has vanished altogether except for the possessive/non-possessive dichotomy in nouns.

The evolution of the treatment of case relationships can be circular. Adpositions can become unstressed and sound like they are an unstressed syllable of a neighboring word. A postposition can thus merge into the stem of a head noun, developing various forms depending on the phonological shape of the stem. Affixes can then be subject to various phonological processes such as assimilation, vowel centering to the schwa, phoneme loss, and fusion, and these processes can reduce or even eliminate the distinctions between cases. Languages can then compensate for the resulting loss of function by creating adpositions, thus coming full circle.

Recent experiments in agent-based modeling have shown how case systems can emerge and evolve in a population of language users. The experiments demonstrate that language users may introduce new case markers to reduce the cognitive effort required for semantic interpretation, hence facilitating communication through language. Case markers then become generalized through analogical reasoning and reuse.

Languages are categorized into several case systems, based on their "morphosyntactic alignment"—how they group verb agents and patients into cases:


The following are systems that some languages use to mark case instead of, or in addition to, declension:


With a few exceptions, most languages in the Uralic family make extensive use of cases. Finnish has 15 cases according to the traditional understanding (or up to 30 depending on the interpretation). However, only 12 are commonly used in speech (see Finnish noun cases). Estonian has 14 and Hungarian has 18, both with additional archaic cases used for some words.

Some languages have very many cases. For example, Tsez, a Northeast Caucasian language, has 64 cases.

The original version of John Quijada's constructed language Ithkuil has 81 noun cases, and its descendent Ilaksh and Ithkuil after the 2011 revision both have 96 noun cases.

The lemma form of words, which is the form chosen by convention as the canonical form of a word, is usually the most unmarked or basic case, which is typically the nominative, trigger, or absolutive case, whichever a language may have.






</doc>
<doc id="12899" url="https://en.wikipedia.org/wiki?curid=12899" title="Gestapo">
Gestapo

The Gestapo (), abbreviation of Geheime Staatspolizei (Secret State Police), was the official secret police of Nazi Germany and German-occupied Europe.

The force was created by Hermann Göring in 1933 by combining the various security police agencies of Prussia into one organisation. Beginning on 20 April 1934 it passed to the administration of "Schutzstaffel" (SS) national leader Heinrich Himmler, who in 1936 was appointed Chief of German Police ("Chef der Deutschen Polizei") by Hitler. The Gestapo at this time becoming a national rather than a Prussian state agency as a suboffice of the "Sicherheitspolizei" (SiPo) (Security Police). Then from 27 September 1939 forward, it was administered by the "Reichssicherheitshauptamt" (RSHA) (Reich Main Security Office) and was considered a sister organisation to the SS "Sicherheitsdienst" (SD) (Security Service). During World War II, the Gestapo played a key role in the Nazi plan to exterminate the Jews of Europe.

As part of the agreement in which Adolf Hitler became Chancellor of Germany, Hermann Göring—future commander of the Luftwaffe and the number two man in the Nazi Party—was named Interior Minister of Prussia. This gave Göring command of the largest police force in Germany. Soon afterward, Göring detached the political and intelligence sections from the police and filled their ranks with Nazis. On 26 April 1933, Göring merged the two units as the "Geheime Staatspolizei", which was abbreviated by a post office clerk for a franking stamp and became known as the "Gestapo". He originally wanted to name it the Secret Police Office ("Geheimes Polizeiamt"), but the German initials, "GPA", were too similar to those of the Soviet "Gosudarstvennoye Politicheskoye Upravlenie" or "State Political Directorate", known as the GPU.

The first commander of the Gestapo was Rudolf Diels, a protégé of Göring. Diels was appointed with the title of chief of "Abteilung Ia" (Department 1a) of the Political Police of the Prussian Interior Ministry. Diels was best known as the primary interrogator of Marinus van der Lubbe after the "Reichstag" fire. In late 1933, the "Reich" Interior Minister Wilhelm Frick wanted to integrate all the police forces of the German states under his control. Göring outflanked him by removing the Prussian political and intelligence departments from the state interior ministry. Göring took over the Gestapo in 1934 and urged Hitler to extend the agency's authority throughout Germany. This represented a radical departure from German tradition, which held that law enforcement was (mostly) a "Land" (state) and local matter. In this, he ran into conflict with Heinrich Himmler, who was police chief of the second most powerful German state, Bavaria. Frick did not have the muscle to take on Göring by himself so he allied with Himmler. With Frick's support, Himmler (pushed on by his right-hand man, Reinhard Heydrich) took over the political police of state after state. Soon only Prussia was left.

Concerned that Diels was not ruthless enough to effectively counteract the power of the "Sturmabteilung" (SA), Göring handed over control of the Gestapo to Himmler on 20 April 1934. Also on that date, Hitler appointed Himmler chief of all German police outside Prussia. Heydrich, named chief of the Gestapo by Himmler on 22 April 1934, also continued as head of the SS Security Service ("Sicherheitsdienst"; SD). Himmler and Heydrich both immediately began installing their own personnel in select positions, several of whom were directly from the Bavarian Political Police like Heinrich Müller, Franz Josef Huber, and Josef Meisinger. Many of the Gestapo employees in the newly established offices were young and highly educated in a wide-variety of academic fields and moreover, represented a new generation of National Socialist adherents, who were hard-working, efficient, and prepared to carry the Nazi state forward through the persecution of their political opponents.

By the spring of 1934 Himmler's SS controlled the SD and the Gestapo, but for him, there was still a problem, as technically the SS (and the Gestapo by proxy) was subordinated to the SA, which was under the command of Ernst Röhm. Himmler wanted to free himself entirely from Röhm, whom he viewed as an obstacle. Röhm's position was menacing as more than 4.5 million men fell under his command once the militias and veterans organisations were absorbed by the SA, a fact which fuelled Röhm's aspirations; his dream of fusing the SA and "Reichswehr" together was undermining Hitler's relationships with the leadership of Germany's armed forces. Several Nazi chieftains, among them Göring, Joseph Goebbels, Rudolf Hess, and Himmler, began a concerted campaign to convince Hitler to take action against Röhm. Both the SD and Gestapo released information concerning an imminent putsch by the SA. Once persuaded, Hitler acted by setting Himmler's SS into action, who then proceeded to murder over 100 of Hitler's identified antagonists. While members of the Gestapo did not participate in the killing, they supplied the information which implicated the SA and ultimately enabled Himmler and Heydrich to emancipate themselves entirely from the organisation. For the Gestapo, the next two years following the Night of the Long Knives, a term describing the putsch against Röhm and the SA, were characterised by "behind-the-scenes political wrangling over policing".

On 17 June 1936, Hitler decreed the unification of all police forces in Germany and named Himmler as Chief of German Police. This action effectively merged the police into the SS and removed it from Frick's control. Himmler was nominally subordinate to Frick as police chief, but as "Reichsführer-SS", he answered only to Hitler. This move also gave Himmler operational control over Germany's entire detective force. The Gestapo became a national state agency. Himmler also gained authority over all of Germany's uniformed law enforcement agencies, which were amalgamated into the new "Ordnungspolizei" (Orpo: Order Police), which became a national agency under SS general Kurt Daluege. Shortly thereafter, Himmler created the "Kriminalpolizei" (Kripo: Criminal Police), merging it with the Gestapo into the "Sicherheitspolizei" (SiPo: Security Police), under Heydrich's command. Heinrich Müller was at that time the Gestapo operations chief. He answered to Heydrich; Heydrich answered only to Himmler and Himmler answered only to Hitler.

The Gestapo had the authority to investigate cases of treason, espionage, sabotage and criminal attacks on the Nazi Party and Germany. The basic Gestapo law passed by the government in 1936 gave the Gestapo "carte blanche" to operate without judicial review—in effect, putting it above the law. The Gestapo was specifically exempted from responsibility to administrative courts, where citizens normally could sue the state to conform to laws. As early as 1935, a Prussian administrative court had ruled that the Gestapo's actions were not subject to judicial review. The SS officer Werner Best, one-time head of legal affairs in the Gestapo, summed up this policy by saying, "As long as the police carries out the will of the leadership, it is acting legally."

On 27 September 1939, the security and police agencies of Nazi Germany—with the exception of the Orpo—were consolidated into the Reich Main Security Office (RSHA), headed by Heydrich. The Gestapo became "Amt IV" (Department IV) of RSHA and Müller became the Gestapo Chief, with Heydrich as his immediate superior. After Heydrich's 1942 assassination, Himmler assumed the leadership of the RSHA until January 1943, when Ernst Kaltenbrunner was appointed chief. Müller remained the Gestapo Chief. His direct subordinate Adolf Eichmann headed the Gestapo's Office of Resettlement and then its Office of Jewish Affairs ("Referat IV B4" or Sub-Department IV, Section B4). During the Holocaust, Eichmann and his agency coordinated the mass deportation of European Jews to the Nazis' extermination camps.

The power of the Gestapo included the use of what was called, "Schutzhaft"—"protective custody", a euphemism for the power to imprison people without judicial proceedings. An oddity of the system was that the prisoner had to sign his own "Schutzhaftbefehl", an order declaring that the person had requested imprisonment—presumably out of fear of personal harm. In addition, thousands of political prisoners throughout Germany—and from 1941, throughout the occupied territories under the Night and Fog Decree—simply disappeared while in Gestapo custody.

The Polish government in exile in London during World War II received sensitive military information about Nazi Germany from agents and informants throughout Europe. After Germany conquered Poland in the autumn of 1939, Gestapo officials believed that they had neutralised Polish intelligence activities. However, certain Polish information about the movement of German police and SS units to the East during the German invasion of the Soviet Union in the autumn of 1941 was similar to information British intelligence secretly obtained through intercepting and decoding German police and SS messages sent by radio telegraphy.

In 1942, the Gestapo discovered a cache of Polish intelligence documents in Prague and were surprised to see that Polish agents and informants had been gathering detailed military information and smuggling it out to London, via Budapest and Istanbul. The Poles identified and tracked German military trains to the Eastern front and identified four Orpo battalions sent to conquered areas of the Soviet Union in October 1941 that engaged in war crimes and mass murder.

Polish agents also gathered detailed information about the morale of German soldiers in the East. After uncovering a sample of the information the Poles had reported, Gestapo officials concluded that Polish intelligence activity represented a very serious danger to Germany. As late as 6 June 1944, Heinrich Müller—concerned about the leakage of information to the Allies—set up a special unit called "Sonderkommando Jerzy" that was meant to root out the Polish intelligence network in western and southwestern Europe.

Early in the regime's existence, harsh measures were meted out to political opponents and those who resisted Nazi doctrine (e.g., the Communists), a role the SA performed until the SD and Gestapo undermined their influence and took control of security in the Reich. Because the Gestapo seemed omniscient and omnipotent, the atmosphere of fear they created led to an overestimation of their reach and strength; a faulty assessment which hampered the operational effectiveness of underground resistance organisations. Antipathy to Hitler and his regime was not tolerated, so the Gestapo had an important role to play in monitoring and prosecuting all who opposed Nazi rule, whether openly or covertly.

Many parts of Germany (where religious dissent existed upon the Nazi seizure of power) saw a rapid transformation; a change as noted by the Gestapo in conservative towns such as Würzburg, where people acquiesced to the regime either through accommodation, collaboration, or simple compliance. Increasing religious objections to Nazi policies led the Gestapo to carefully monitor church organisations. For the most part, members of the church did not offer political resistance but simply wanted to ensure that organizational doctrine remained intact.

However, the Nazi regime sought to suppress any source of ideology other than its own, and set out to muzzle or crush the churches in the so-called "Kirchenkampf". When Church leaders (clergy) voiced their misgiving about the euthanasia program and Nazi racial policies, Hitler intimated that he considered them "traitors to the people" and went so far as to call them "the destroyers of Germany". The extreme anti-Semitism and neo-Pagan heresies of the Nazis caused some Christians to outright resist, and Pope Pius XI to issue the encyclical Mit Brennender Sorge denouncing Nazism and warning Catholics against joining or supporting the Party. Some pastors, like the Protestant clergyman Dietrich Bonhoeffer, paid for their opposition with their lives.

In an effort to counter the strength and influence of spiritual resistance, Nazi records reveal that the Gestapo's "Referat B1" monitored the activities of bishops very closely—instructing that agents be set up in every diocese, that the bishops' reports to the Vatican should be obtained and that the bishops' areas of activity must be found out. Deans were to be targeted as the "eyes and ears of the bishops" and a "vast network" established to monitor the activities of ordinary clergy: "The importance of this enemy is such that inspectors of security police and of the security service will make this group of people and the questions discussed by them their special concern".

In "Dachau: The Official History 1933–1945", Paul Berben wrote that clergy were watched closely, and frequently denounced, arrested and sent to concentration camps: "One priest was imprisoned in Dachau for having stated that there were good folk in England too; another suffered the same fate for warning a girl who wanted to marry an S.S. man after abjuring the Catholic faith; yet another because he conducted a service for a deceased communist". Others were arrested simply on the basis of being "suspected of activities hostile to the State" or that there was reason to "suppose that his dealings might harm society". Over 2700 Catholic, Protestant and Orthodox clergy were imprisoned at Dachau alone. After Heydrich (who was staunchly anti-Catholic and anti-Christian) was assassinated in Prague, his successor, Ernst Kaltenbrunner, relaxed some of the policies and then disbanded Department IVB (religious opponents) of the Gestapo.

Between June 1942 and March 1943, student protests were calling for an end to the Nazi regime. These included the non-violent resistance of Hans and Sophie Scholl, two leaders of the White Rose student group. However, resistance groups and those who were in moral or political opposition to the Nazis were stalled by the fear of reprisals from the Gestapo. Fearful of an internal overthrow, the forces of the Gestapo were unleashed on the opposition. The first five months of 1943 witnessed thousands of arrests and executions as the Gestapo exercised their powers over the German public. Student opposition leaders were executed in late February, and a major opposition organisation, the Oster Circle, was destroyed in April 1943. Efforts to resist the Nazi regime amounted to very little and had only minor chances of success, particularly since the broad percentage of the German people did not support oppositional movements.

Between 1934 and 1938, opponents of the Nazi regime and their fellow travellers began to emerge. Among the first to speak out were religious dissenters but following in their wake were educators, aristocratic businessmen, office workers, teachers, and others from nearly every walk of life. Most people quickly learned that open opposition was dangerous since Gestapo informants and agents were widespread. Yet a significant number of them still worked against the National Socialist government.

During May 1935, the Gestapo broke up and arrested members of the "Markwitz Circle", a group of former socialists in contact with Otto Strasser, who sought Hitler's downfall. From the mid-1930s into the early 1940s—various groups made up of communists, idealists, working-class people, and far-right conservative opposition organisations covertly fought against Hitler's government, and several of them fomented plots that included Hitler's assassination. Nearly all of them, including: the Römer Group, Robby Group, Solf Circle, "Schwarze Reichswehr", the Party of the Radical Middle Class, "Jungdeutscher Orden", "Schwarze Front" and "Stahlhelm" were either discovered or infiltrated by the Gestapo. This led to corresponding arrests, being sent to concentration camps and execution. One of the methods employed by the Gestapo to contend with these resistance factions was 'protective detention' which facilitated the process in expediting dissenters to concentration camps and against which there was no legal defence.

Early efforts to resist the Nazis with aid from abroad were hindered when the opposition's peace feelers to the Western Allies did not meet with success. This was partly because of the Venlo incident of 9 November 1939, in which SD and Gestapo agents, posing as anti-Nazis in the Netherlands, kidnapped two British Secret Intelligence Service (SIS) officers after having lured them to a meeting to discuss peace terms. This prompted Winston Churchill to ban any further contact with the German opposition. Later, the British and Americans did not want to deal with anti-Nazis because they were fearful that the Soviet Union would believe they were attempting to make deals behind their back.

The German opposition was in an unenviable position by the late spring and early summer of 1943. On one hand, it was next to impossible for them to overthrow Hitler and the party; on the other, the Allied demand for an unconditional surrender meant no opportunity for a compromise peace, which left the military and conservative aristocrats who opposed the regime no option (in their eyes) other than continuing the military struggle. Despite fear of the Gestapo after mass arrests and executions in the spring, the opposition still plotted and planned. One of the more famous schemes, Operation Valkyrie, involved a number of senior German officers and was carried out by Colonel Claus Schenk Graf von Stauffenberg. In an attempt to assassinate Hitler, Stauffenberg planted a bomb underneath a conference table inside the Wolf's Lair field headquarters. Known as the 20 July plot, this assassination attempt failed and Hitler was only slightly injured. Reports indicate that the Gestapo was caught unaware of this plot as they did not have sufficient protections in place at the appropriate locations nor did they take any preventative steps. Stauffenberg and his group were shot on 21 July 1944; meanwhile, his fellow conspirators were rounded up by the Gestapo and sent to a concentration camp. Thereafter, there was a show trial overseen by Roland Freisler, followed by their execution.

Some Germans were convinced that it was their duty to apply all possible expedients to end the war as quickly as possible. Sabotage efforts were undertaken by members of the "Abwehr" (military intelligence) leadership, as they recruited people known to oppose the Nazi regime. The Gestapo cracked down ruthlessly on dissidents in Germany, just as they did everywhere else. Opposition became more difficult. Arrests, torture, and executions were common. Terror against "state enemies" had become a way of life to such a degree that the Gestapo's presence and methods were eventually normalised in the minds of people living in Nazi Germany.

In January 1933, Hermann Göring, Hitler's minister without portfolio, was appointed the head of the Prussian Police and began filling the political and intelligence units of the Prussian Secret Police with Nazi Party members. A year after the organisations inception, Göring wrote in a British publication about having created the organisation on his own initiative and how he was "chiefly responsible" for the elimination of the Marxist and Communist threat to Germany and Prussia. Describing the activities of the organisation, Göring boasted about the utter ruthlessness required for Germany's recovery, the establishment of concentration camps for that purpose, and even went on to claim that excesses were committed in the beginning, recounting how beatings took place here and there. On 26 April 1933, he reorganised the force's "Amt III" as the "Gestapa" (better-known by the "sobriquet" Gestapo), a secret state police intended to serve the Nazi cause. Less than two weeks later in early May 1933, the Gestapo moved into their Berlin headquarters at Prinz-Albrecht-Straße 8.

With its 1936 merging with the Kripo (National criminal police) to form sub-units of the "Sicherheitspolizei" (SiPo; Security Police), the Gestapo was classified as a government agency. Himmler by his appointment to "Chef der Deutschen Polizei" (Chief of German Police), along with serving as "Reichsführer-SS" made him independent of Interior Minister Wilhelm Frick's nominal control.

The SiPo was placed under the direct command of Reinhard Heydrich who was already chief of the Nazi Party's intelligence service, the "Sicherheitsdienst" (SD). The idea was to fully combine the party agency, the SD, with the SiPo, the state agency. SiPo members were encouraged to become members of the SS. However, in practise, the SiPo and the SD came into jurisdictional and operational conflict. Gestapo and Kripo had many experienced, professional policemen and investigators, who considered the SD to be a less competent agency, amateurs who were "good Nazis but bad detectives".

In September 1939, the SiPo together with the SD were merged into the newly created "Reichssicherheitshauptamt" (RSHA: Reich Main Security Office). Both the Gestapo and Kripo became distinct departments within the RSHA. Although the "Sicherheitspolizei" was officially disbanded, the term SiPo was figuratively used to describe any RSHA personnel throughout the remainder of the war. In lieu of naming convention changes, the original construct of the SiPo, Gestapo, and Kripo cannot be fully comprehended as "discrete entities", since they ultimately formed "a conglomerate in which each was wedded to each other and the SS through its Security Service, the SD".

The creation of the RSHA represented the formalisation, at the top level, of the relationship under which the SD served as the intelligence agency for the security police. A similar co-ordination existed in the local offices. Within Germany and areas which were incorporated within the Reich for the purpose of civil administration, local offices of the Gestapo, criminal police, and SD were formally separate. They were subject to co-ordination by inspectors of the security police and SD on the staffs of the local higher SS and police leaders, however, and one of the principal functions of the local SD units was to serve as the intelligence agency for the local Gestapo units. In the occupied territories, the formal relationship between local units of the Gestapo, criminal police, and SD was slightly closer.

The Gestapo became known as RSHA "Amt IV" ("Department or Office IV") with Heinrich Müller as its chief. In January 1943, Himmler appointed Ernst Kaltenbrunner RSHA chief; almost seven months after Heydrich had been assassinated. The specific internal departments of "Amt IV" were as follows:



Central administrative office of the Gestapo, responsible for card files of all personnel including all officials.

Administration for regions outside the "Reich".


In 1941 "Referat N", the central command office of the Gestapo was formed. However, these internal departments remained and the Gestapo continued to be a department under the RSHA umbrella. The local offices of the Gestapo, known as Gestapo "Leitstellen" and "Stellen", answered to a local commander known as the "Inspekteur der Sicherheitspolizei und des SD" ("Inspector of the Security Police and Security Service") who, in turn, was under the dual command of "Referat N" of the Gestapo and also his local SS and Police Leader.

The Gestapo also maintained offices at all Nazi concentration camps, held an office on the staff of the SS and Police Leaders, and supplied personnel as needed to formations such as the "Einsatzgruppen". Personnel assigned to these auxiliary duties were often removed from the Gestapo chain of command and fell under the authority of branches of the SS.

The Gestapo maintained police detective ranks which were used for all officers, both those who were and who were not concurrently SS members.

Sources:

Median annual wage for an industrial worker was 1,495 RM in 1939. In the same year the median salary for a privately employed white-collar worker was 2,772 RM.

In 1933, there was no purge of the German police forces. The vast majority of Gestapo officers came from the police forces of the Weimar Republic; members of the SS, the SA, and the NSDAP also joined the Gestapo but were less numerous. By March 1937, the Gestapo employed an estimated 6,500 people in fifty-four regional offices across the Reich. Additional staff were added in March 1938 consequent the annexation of Austria and again in October 1938 with the acquisition of the Sudetenland. In 1939, only 3,000 out of the total of 20,000 Gestapo men held SS ranks, and in most cases, these were honorary. One man who served in the Prussian Gestapo in 1933 recalled that most of his co-workers "were by no means Nazis. For the most part they were young professional civil service officers..." The Nazis valued police competence more than politics, so in general in 1933, almost all of the men who served in the various state police forces under the Weimar Republic stayed on in their jobs. In Würzburg, which is one of the few places in Germany where most of the Gestapo records survived, every member of the Gestapo was a career policeman or had a police background.

The Canadian historian Robert Gellately wrote that most Gestapo men were not Nazis, but at the same time were not opposed to the Nazi regime, which they were willing to serve, in whatever task they were called upon to perform. Over time, membership in the Gestapo included ideological training, particularly once Werner Best assumed a leading role for training in April 1936. Employing biological metaphors, Best emphasised a doctrine which encouraged members of the Gestapo to view themselves as 'doctors' to the national body in the struggle against "pathogens" and "diseases"; among the implied sicknesses were "communists, Freemasons, and the churches—and above and behind all these stood the Jews". Heydrich thought along similar lines and advocated both defensive and offensive measures on the part of the Gestapo, so as to prevent any subversion or destruction of the National Socialist body.

Whether trained as police originally or not, Gestapo agents themselves were shaped by their socio-political environment. Historian George C. Browder contends that there was a four-part process (authorisation, bolstering, routinisation, and dehumanisation) in effect which legitimised the psycho-social atmosphere conditioning members of the Gestapo to radicalised violence. Browder also describes a sandwich effect, where from above; Gestapo agents were subjected to ideologically oriented racism and criminal biological theories; and from below, the Gestapo was transformed by SS personnel who did not have the proper police training, which showed in their propensity for unrestrained violence. This admixture certainly shaped the Gestapo's public image which they sought to maintain in lieu of their increasing workload; an image which helped them identify and eliminate enemies of the Nazi state.

From June 1936, a concerted effort was made to recruit policemen of the SiPo into the SS, and SS members into the Kripo and especially the Gestapo, but with limited success; by 1939 only a small percentage of Gestapo agents were SS members. With the formation of RSHA in September 1939, Gestapo officers who also held SS rank began to wear the wartime grey SS uniform when on duty in the "Hauptamt" or regional headquarters ("Abschnitte"). Hollywood notwithstanding, after 1939 the black SS uniform was only worn by "Allgemeine SS" reservists. Outside the central offices, Gestapo agents working out of the "Stapostellen" and "Stapoleitstellen" continued to wear civilian suits in keeping with the secretive nature of their work.

There were strict protocols protecting the identity of Gestapo field personnel. When asked for identification, an operative was only required to present his warrant disc. This identified the operative as Gestapo without revealing personal identity and agents, except when ordered to do so by an authorised official, were not required to show picture identification, something all non-Gestapo people were expected to do. Nevertheless, the sight of dark leather coats and black SS uniforms along with the very mention of the word "Gestapo" elicited fear among the general population.

Beginning in 1940, the grey SS uniform was worn by Gestapo in occupied countries of the east, even those who were not actually SS members, because agents in civilian clothes had been shot by members of the "Wehrmacht" thinking they were partisans.

Unlike the rest of the SS, the right-side collar patch of the RSHA was plain black without insignia, as was the uniform cuffband. Gestapo agents in uniform did not wear SS shoulderboards, but rather police-pattern shoulderboards piped or underlaid in "poison green" ("giftgrün"). A diamond-shaped black patch with "SD" in white was worn on the lower left sleeve even by SiPo men who were not in the SD. Sometimes this "Raute" (diamond) was piped in white; there is some debate over whether this may or may not have indicated Gestapo personnel.

Contrary to popular belief, the Gestapo was not the all-pervasive, omnipotent agency in German society. In Germany proper, many towns and cities had fewer than 50 official Gestapo personnel. For example, in 1939 Stettin and Frankfurt am Main only had a total of 41 Gestapo men combined. In Düsseldorf, the local Gestapo office of only 281 men were responsible for the entire Lower Rhine region, which comprised 4 million people. "V-men", as undercover Gestapo agents were known, were used to infiltrate Social Democratic and Communist opposition groups, but this was more the exception, not the rule. The Gestapo office in Saarbrücken had 50 full-term informers in 1939. The District Office in Nuremberg, which had the responsibility for all of northern Bavaria, employed a total of 80–100 full-term informers between 1943 and 1945. The majority of Gestapo informers were not full-term informers working undercover, but were rather ordinary citizens who chose to denounce other people to the Gestapo.

According to Canadian historian Robert Gellately's analysis of the local offices established, the Gestapo was—for the most part—made up of bureaucrats and clerical workers who depended upon denunciations by citizens for their information. Gellately argued that it was because of the widespread willingness of Germans to inform on each other to the Gestapo that Germany between 1933 and 1945 was a prime example of panopticism. The Gestapo—at times—was overwhelmed with denunciations and most of its time was spent sorting out the credible from the less credible denunciations. Many of the local offices were understaffed and overworked, struggling with the paper load caused by so many denunciations. Gellately has also suggested that the Gestapo was "a reactive organisation" "... which was constructed within German society and whose functioning was structurally dependent on the continuing co-operation of German citizens".

After 1939, when many Gestapo personnel were called up for war-related work such as service with the "Einsatzgruppen", the level of overwork and understaffing at the local offices increased. For information about what was happening in German society, the Gestapo continued to be mostly dependent upon denunciations. 80% of all Gestapo investigations were started in response to information provided by denunciations by ordinary Germans; while 10% were started in response to information provided by other branches of the German government and another 10% started in response to information that the Gestapo itself unearthed. The information supplied by denunciations often led the Gestapo in determining who was arrested.

The popular picture of the Gestapo with its spies everywhere terrorising German society has been rejected by many historians as a myth invented after the war as a cover for German society's widespread complicity in allowing the Gestapo to work. Work done by social historians such as Detlev Peukert, Robert Gellately, Reinhard Mann, Inge Marssolek, René Otto, Klaus-Michael Mallamann and Paul Gerhard, which by focusing on what the local offices were doing has shown the Gestapos almost total dependence on denunciations from ordinary Germans, and very much discredited the older "Big Brother" picture with the Gestapo having its eyes and ears everywhere. For example, of the 84 cases in Würzburg of "Rassenschande" ("race defilement"—sexual relations with non-Aryans), 45 (54%) were started in response to denunciations by ordinary people, two (2%) by information provided by other branches of the government, 20 (24%) via information gained during interrogations of people relating to other matters, four (5%) from information from (Nazi) NSDAP organisations, two (2%) during "political evaluations" and 11 (13%) have no source listed while none were started by Gestapos own "observations" of the people of Würzburg.

An examination of 213 denunciations in Düsseldorf showed that 37% were motivated by personal conflicts, no motive could be established in 39%, and 24% were motivated by support for the Nazi regime. The Gestapo always showed a special interest in denunciations concerning sexual matters, especially cases concerning "Rassenschande" with Jews or between Germans and foreigners, in particular Polish slave workers; the Gestapo applied even harsher methods to the foreign workers in the country, especially those from Poland, Jews, Catholics and homosexuals. As time went by, anonymous denunciations to the Gestapo caused trouble to various NSDAP officials, who often found themselves being investigated by the Gestapo.

Of the political cases, 61 people were investigated for suspicion of belonging to the KPD, 44 for the SPD and 69 for other political parties. Most of the political investigations took place between 1933 and 1935 with the all-time high of 57 cases in 1935. After that year, political investigations declined with only 18 investigations in 1938, 13 in 1939, two in 1941, seven in 1942, four in 1943 and one in 1944. The "other" category associated with non-conformity included everything from a man who drew a caricature of Hitler to a Catholic teacher suspected of being lukewarm about teaching National Socialism in his classroom. The "administrative control" category concerned whose were breaking the law concerning residency in the city. The "conventional criminality" category concerned economic crimes such as money laundering, smuggling and homosexuality.

Normal methods of investigation included various forms of blackmail, threats and extortion to secure "confessions". Beyond that, sleep deprivation and various forms of harassment were used as investigative methods. Failing that, torture and planting evidence were common methods of resolving a case, especially if the case concerned someone Jewish. Brutality on the part of interrogators—often prompted by denunciations and followed with roundups—enabled the Gestapo to uncover numerous resistance networks; it also made them seem like they knew everything and could do anything they wanted.

While the total numbers of Gestapo officials was limited when contrasted against the represented populations, the average "Volksgenosse" (Nazi term for the "member of the German people") was typically not under observation, so the statistical ratio between Gestapo officials and inhabitants is "largely worthless and of little significance" according to some recent scholars. As historian Eric Johnson remarked, "The Nazi terror was selective terror", with its focus upon political opponents, ideological dissenters (clergy and religious organisations), career criminals, the Sinti and Roma population, handicapped persons, homosexuals and above all, upon the Jews. "Selective terror" by the Gestapo, as mentioned by Johnson, is also supported by historian Richard Evans who states that, "Violence and intimidation rarely touched the lives of most ordinary Germans. Denunciation was the exception, not the rule, as far as the behaviour of the vast majority of Germans was concerned." The involvement of ordinary Germans in denunciations also needs to be put into perspective so as not to exonerate the Gestapo. As Evans makes clear, "...it was not the ordinary German people who engaged in surveillance, it was the Gestapo; nothing happened until the Gestapo received a denunciation, and it was the Gestapo's active pursuit of deviance and dissent that was the only thing that gave denunciations meaning." The Gestapo's effectiveness remained in the ability to "project" omnipotence...they co-opted the assistance of the German population by using denunciations to their advantage; proving in the end a powerful, ruthless and effective organ of terror under the Nazi regime that was seemingly everywhere. Lastly, the Gestapo's effectiveness, while aided by denunciations and the watchful eye of ordinary Germans, was more the result of the co-ordination and co-operation amid the various police organs within Germany, the assistance of the SS, and the support provided by the various Nazi Party organisations; all of them together forming an organised persecution network.

Between 14 November 1945 and 3 October 1946, the Allies established an International Military Tribunal (IMT) to try 22 of 24 major Nazi war criminals and six groups for crimes against peace, war crimes and crimes against humanity. Nineteen of the 22 were convicted, and twelve of them (Bormann [in absentia], Frank, Frick, Göring, Jodl, Kaltenbrunner, Keitel, Ribbentrop, Rosenberg, Sauckel, Seyss-Inquart, Streicher), were each given the death penalty; the remaining three (Funk, Hess, Raeder) received life terms. At that time, the Gestapo was condemned as a criminal organisation, along with the SS. However, Gestapo leader Heinrich Müller was never tried, as he disappeared at the end of the war.

Leaders, organisers, investigators and accomplices participating in the formulation or execution of a common plan or conspiracy to commit the crimes specified were declared responsible for all acts performed by any persons in execution of such plan. The official positions of defendants as heads of state or holders of high government offices were not to free them from responsibility or mitigate their punishment; nor was the fact that a defendant acted pursuant to an order of a superior to excuse him from responsibility, although it might be considered by the IMT in mitigation of punishment.

At the trial of any individual member of any group or organisation, the IMT was authorised to declare (in connection with any act of which the individual was convicted) that the group or organisation to which he belonged was a criminal organisation. When a group or organisation was thus declared criminal, the competent national authority of any signatory had the right to bring persons to trial for membership in that organisation, with the criminal nature of the group or organisation assumed proved.

These groups—the Nazi party and government leadership, the German General staff and High Command (OKW); the "Sturmabteilung" (SA); the "Schutzstaffel" (SS), including the "Sicherheitsdienst" (SD); and the Gestapo—had an aggregate membership exceeding two million, making a large number of their members liable to trial when the organisations were convicted.

The trials began in November 1945. On 1 October 1946, the IMT rendered its judgement on 21 top Nazi figures: 18 were sentenced to death or to long prison terms, and three acquitted. The IMT also convicted three of the groups: the Nazi leadership corps, the SS (including the SD) and the Gestapo. Gestapo members Hermann Göring, Ernst Kaltenbrunner and Arthur Seyss-Inquart were individually convicted. Three groups were acquitted of collective war crimes charges, but this did not relieve individual members of those groups from conviction and punishment under the denazification programme. Members of the three convicted groups were subject to apprehension by Britain, the United States, the Soviet Union and France.

In 1997, Cologne transformed the former regional Gestapo headquarters in Cologne—the EL-DE Haus—into a museum to document the Gestapo's actions.


Informational notes
Citations
Bibliography



</doc>
<doc id="12900" url="https://en.wikipedia.org/wiki?curid=12900" title="Grammatical conjugation">
Grammatical conjugation

In linguistics, conjugation () is the creation of derived forms of a verb from its principal parts by inflection (alteration of form according to rules of grammar). Conjugation may be affected by person, number, gender, tense, aspect, mood, voice, case, and other grammatical categories such as possession, definiteness, politeness, causativity, clusivity, interrogativity, transitivity, valency, polarity, telicity, volition, mirativity, evidentiality, animacy, associativity, pluractionality, reciprocity, agreement, polypersonal agreement, incorporation, noun class, noun classifiers, and verb classifiers in some languages. Agglutinative and polysynthetic languages tend to have the most complex conjugations albeit some fusional languages such as Archi can also have extremely complex conjugation. Typically the principal parts are the root and/or several modifications of it (stems). All the different forms of the same verb constitute a lexeme, and the canonical form of the verb that is conventionally used to represent that lexeme (as seen in dictionary entries) is called a lemma.

The term conjugation is applied only to the inflection of verbs, and not of other parts of speech (inflection of nouns and adjectives is known as declension). Also it is often restricted to denoting the formation of finite forms of a verb – these may be referred to as "conjugated forms", as opposed to non-finite forms, such as the infinitive or gerund, which tend not to be marked for most of the grammatical categories.

Conjugation is also the traditional name for a group of verbs that share a similar conjugation pattern in a particular language (a "verb class"). For example, Latin is said to have four conjugations of verbs. This means that any regular Latin verb can be conjugated in any person, number, tense, mood, and voice by knowing which of the four conjugation groups it belongs to, and its principal parts. A verb that does not follow all of the standard conjugation patterns of the language is said to be an irregular verb. The system of all conjugated variants of a particular verb or class of verbs is called a verb paradigm; this may be presented in the form of a conjugation table.

Indo-European languages usually inflect verbs for several grammatical categories in complex paradigms, although some, like English, have simplified verb conjugation to a large extent. Below is the conjugation of the verb "to be" in the present tense (of the infinitive, if it exists, and indicative moods), in English, German, Yiddish, Dutch, Afrikaans, Icelandic, Faroese, Swedish, Norwegian, Latvian, Bulgarian, Bosnian, Serbian, Croatian, Polish, Slovenian, Macedonian, Urdu or Hindi, Persian, Latin, French, Italian, Spanish, Portuguese, Russian, Albanian, Armenian, Irish, Ukrainian, Ancient Attic Greek and Modern Greek. This is usually the most irregular verb. The similarities in corresponding verb forms may be noticed. Some of the conjugations may be disused, like the English "thou"-form, or have additional meanings, like the English "you"-form, which can also stand for second person singular or be impersonal.

Verbal agreement or concord is a morpho-syntactic construct in which properties of the subject and/or objects of a verb are indicated by the verb form. Verbs are then said to agree with their subjects (resp. objects).

Many English verbs exhibit subject agreement of the following sort: whereas "I go", "you go", "we go", "they go" are all grammatical in standard English, "she go" is not (except in the subjunctive, as "They requested that "she go" with them"). Instead, a special form of the verb "to go" has to be used to produce "she goes". On the other hand "I goes", "you goes" etc. are not grammatical in standard English. (Things are different in some English dialects that lack agreement.) A few English verbs have no special forms that indicate subject agreement ("I may", "you may", "she may"), and the verb "to be" has an additional form "am" that can only be used with the pronoun "I" as the subject.

Verbs in written French exhibit more intensive agreement morphology than English verbs: "je suis" (I am), "tu es" ("you are", singular informal), "elle est" (she is), "nous sommes" (we are), "vous êtes" ("you are", plural), "ils sont" (they are). Historically, English used to have a similar verbal paradigm. Some historic verb forms are used by Shakespeare as slightly archaic or more formal variants ("I do", "thou dost", "she doth", typically used by nobility) of the modern forms.

Some languages with verbal agreement can leave certain subjects implicit when the subject is fully determined by the verb form. In Spanish, for instance, subject pronouns do not need to be explicitly present, even though in French, its close relative, they are obligatory. The Spanish equivalent to the French "je suis" (I am) can be simply "soy" (lit. "am"). The pronoun "yo" (I) in the explicit form "yo soy" is only required for emphasis or to clear ambiguity in complex texts.

Some languages have a richer agreement system in which verbs also agree with some or all of their objects. Ubykh exhibits verbal agreement for the subject, direct object, indirect object, benefaction and ablative objects ("a.w3.s.xe.n.t'u.n", "you gave it to him for me").

Basque can show agreement not only for subject, direct object and indirect object, but it also on occasion exhibits agreement for the listener as the implicit benefactor: "autoa ekarri digute" means "they brought us the car" (neuter agreement for listener), but "autoa ekarri ziguten" means "they brought us the car" (agreement for feminine singular listener).

Languages with a rich agreement morphology facilitate relatively free word order without leading to increased ambiguity. The canonical word order in Basque is subject–object–verb. However, all permutations of subject, verb and object are permitted.

In some languages, predicative adjectives and copular complements receive a form of person agreement that is distinct from that used on ordinary predicative verbs. Although this is a form of conjugation in that it refers back to the person of the subject, it is not “verbal” because it always derives from pronouns that have become cliticised to the nouns to which they refer. An example of nonverbal person agreement, along with contrasting verbal conjugation, can be found from Beja (person agreement affixes in bold):


Another example can be found from Ket:


In Turkic, and a few Uralic and Australian Aboriginal languages, predicative adjectives and copular complements take affixes that are identical to those used on predicative verbs, but their negation is different. For example, in Turkish:


Under negation this becomes (negative affixes in bold):


For this reason, the person agreement affixes used with predicative adjectives and nominals in Turkic languages are considered to be nonverbal in character. In some analyses, they are viewed as a form of verbal takeover by a copular strategy.

Common grammatical categories according to which verbs can be conjugated are the following:

Other factors which may affect conjugation are:




</doc>
<doc id="12902" url="https://en.wikipedia.org/wiki?curid=12902" title="Gomoku">
Gomoku

Gomoku, also called "Gobang" or "Five in a Row", is an abstract strategy board game. It is traditionally played with Go pieces (black and white stones) on a Go board, using 15×15 of the 19×19 grid intersections. Because pieces are not moved or removed from the board, Gomoku may also be played as a paper and pencil game. The game is known in several countries under different names.

Players alternate turns placing a stone of their color on an empty intersection. The winner is the first player to form an unbroken chain of five stones horizontally, vertically, or diagonally.

It originated in Japan during the Heian period. The name "Gomoku" is from the Japanese language, in which it is referred to as . "Go" means five, "moku" is a counter word for pieces and "narabe" means "line-up". The game is also popular in Korea, where it is called "omok" (오목 [五目]) which has the same structure and origin as the Japanese name.

The Japanese call this game Go-moku (five stones). In the nineteenth century, the game was introduced to Britain where it was known as Go Bang, said to be a corruption of the Japanese word "goban", said to be adopted from Chinese "k'i pan (qí pán)" "chess-board."

Besides many variations around the world, the Swap2 rule (based on "swap" from Renju) is currently adapted in tournaments among professional players, including Gomoku World Championships.

In Swap2 rule, the first player starts by placing three stones (2 black 1 white, if black goes first) on the board. The second player next can select one of these three options: choose to play black, or to play white and place one more stone, or to place two more stones to change the shape and let the first player choose the color. This is essentially a slightly more elaborate pie rule.

Swap2 solved the low complexity problem and makes the game fairer. Like other rules and variations, 100% fairness can be reached by playing two alternating games for each point.

Most variations are based on either Free-style gomoku or Standard gomoku.


Black (the player who makes the first move) was long known to have a big advantage, even before L. Victor Allis proved that black could force a win (see below). So a number of variations are played with extra rules that aimed to reduce black's advantage.




This game on the 15×15 board is adapted from the paper "Go-Moku and Threat-Space Search".

The opening moves show clearly black's advantage. An open row of three (one that is not blocked by an opponent's stone at either end) has to be blocked immediately, or countered with a threat elsewhere on the board. If not blocked or countered, the open row of three will be extended to an open row of four, which threatens to win in two ways.

White has to block open rows of three at moves 10, 14, 16 and 20, but black only has to do so at move 9.
Move 20 is a blunder for white (it should have been played next to black 19). Black can now force a win against any defence by white, starting with move 21.
There are two forcing sequences for black, depending on whether white 22 is played next to black 15 or black 21. The diagram on the right shows the first sequence. All the moves for white are forced. Such long forcing sequences are typical in gomoku, and expert players can read out forcing sequences of 20 to 40 moves rapidly and accurately.
The diagram on the right shows the second forcing sequence. This diagram shows why white 20 was a blunder; if it had been next to black 19 (at the position of move 32 in this diagram) then black 31 would not be a threat and so the forcing sequence would fail.

World Championships in Gomoku have occurred 2 times in 1989, 1991.
Since 2009 the tournament resumed, the opening rule being played was changed and now is swap2.

List of the tournaments occurred and title holders follows.

People have been applying artificial intelligence techniques on playing gomoku for several decades. In 1994, L. Victor Allis raised the algorithm of proof-number search (pn-search) and dependency-based search (db-search), and proved that when starting from an empty 15×15 board, the first player has a winning strategy using these searching algorithms. This applies to both free-style gomoku and standard gomoku without any opening rules. It seems very likely that black wins on larger boards too. In any size of a board, freestyle gomoku is an "m","n","k"-game, hence it is known that the first player can enforce a win or a draw. In 2001, Allis' winning strategy was also approved for renju, a variation of gomoku, when there was no limitation on the opening stage.

However, neither the theoretical values of all legal positions, nor the opening rules such as Swap2 used by the professional gomoku players have been solved yet, so the topic of gomoku artificial intelligence is still a challenge for computer scientists, such as the problem on how to improve the gomoku algorithms to make them more strategic and competitive. Nowadays, most of the state-of-the-art gomoku algorithms are based on the alpha-beta pruning framework.

Reisch proved that Generalized gomoku is PSPACE-complete. He also observed that the reduction can be adapted to the rules of k-in-a-Row for fixed k. Although he did not specify exactly which values of k are allowed, the reduction would appear to generalize to any k ≥ 5.

There exist several well-known tournaments for gomoku programs since 1989. The Computer Olympiad started with the gomoku game in 1989, but gomoku has not been in the list since 1993. The Renju World Computer Championship was started in 1991, and held for 4 times until 2004. The Gomocup tournament is played since 2000 and taking place every year, still active now, with more than 30 participants from about 10 countries. The Hungarian Computer Go-Moku Tournament was also played twice in 2005. There were also two Computer vs. Human tournaments played in the Czech Republic, in 2006 and 2011. Not until 2017 were the computer programs proved to be able to outperform the world human champion in public competitions. In the Gomoku World Championship 2017, there was a match between the world champion program Yixin and the world champion human player Rudolf Dupszki. Yixin won the match with a score of 2-0.





</doc>
<doc id="12903" url="https://en.wikipedia.org/wiki?curid=12903" title="Gegenschein">
Gegenschein

Gegenschein ( German for "countershine") is a faintly bright spot in the night sky, around the antisolar point. The backscatter of sunlight by interplanetary dust causes this optical phenomenon.

Like the zodiacal light, the gegenschein is sunlight scattered by interplanetary dust. Most of this dust is orbiting the Sun in about the ecliptic plane, with a possible concentration of particles at the Earth–Sun Lagrangian point.

It is distinguished from zodiacal light by its high angle of reflection of the incident sunlight on the dust particles. It forms a slightly more luminous, oval glow directly opposite the Sun within the band of luminous zodiacal light. The intensity of the gegenschein is relatively enhanced, because each dust particle is seen in full phase.

The gegenschein was first described by the French Jesuit astronomer and professor (1692–1776) in 1730. Further observations were made by the German explorer Alexander von Humboldt during his South American journey from 1799 to 1803. It was also Humboldt who gave the phenomenon its German name Gegenschein.

The Danish astronomer Theodor Brorsen published the first thorough investigations of the gegenschein in 1854, stating that Pezenas was the first to see it. T. W. Backhouse discovered it independently in 1876, as did Edward Emerson Barnard in 1882. In modern times, the gegenschein is not visible in most inhabited regions of the world due to light pollution.




</doc>
<doc id="12904" url="https://en.wikipedia.org/wiki?curid=12904" title="Glyph">
Glyph

In typography, a glyph is an elemental symbol within an agreed set of symbols, intended to represent a readable character for the purposes of writing. Glyphs are considered to be unique marks that collectively add up to the spelling of a word or contribute to a specific meaning of what is written, with that meaning dependent on cultural and social usage. In contrast, in most languages written in any variety of the Latin alphabet, the dot on a lower-case "i" is not a glyph because it does not convey any distinction, and an "i" in which the dot has been accidentally omitted is still likely to be recognized correctly. However, in Turkish it is a glyph because that language has two distinct versions of the letter "i", with and without a dot. Also, in Japanese syllabaries, a number of the characters are made up of more than one separate mark, but in general these separate marks are not glyphs because they have no meaning by themselves. However, in some cases, additional marks fulfill the role of diacritics, to differentiate distinct characters. Such additional marks constitute glyphs. In general, a diacritic is a glyph, even if it is contiguous with the rest of the character like a cedilla in French, the ogonek in several languages, or the stroke on a Polish "Ł".

Some characters such as "æ" in Icelandic and the "ß" in German may be regarded as glyphs, yet they were originally ligatures, but over time have become characters in their own right, and these languages treat them as separate letters. However, a ligature such as "ſi", that is treated in some typefaces as a single unit, is arguably not a glyph as this is just a quirk of the typeface, essentially an allographic feature, and includes more than one grapheme. In normal handwriting, even long words are often written "joined up", without the pen leaving the paper, and the form of each written letter will often vary depending on which letters precede and follow it, but that does not make the whole word into a single glyph.

Two or more glyphs which have the same significance, whether used interchangeably or chosen depending on context, are called allographs of each other.

The term has been used in English since 1727, borrowed from "glyphe" (in use by French antiquaries since 1701), from the Greek γλυφή, "glyphē", "carving," and the verb γλύφειν, "glýphein", "to hollow out, engrave, carve" (cognate with Latin "glubere" "to peel" and English "cleave").

The word "hieroglyph" (Greek for sacred writing) has a longer history in English, dating from an early use in an English to Italian dictionary published by John Florio in 1598, referencing the complex and mysterious characters of the Egyptian alphabet.

The word "glyph" first came to widespread European attention with the engravings and lithographs from Frederick Catherwood's drawings of undeciphered glyphs of the Maya civilization in the early 1840s. 

In archaeology, a glyph is a carved or inscribed symbol. It may be a pictogram or ideogram, or part of a writing system such as a syllable, or a logogram.

In typography, a glyph has a slightly different definition: it is "the specific shape, design, or representation of a character". It is a particular graphical representation, in a particular typeface, of an element of written language, which could be a grapheme, or part of a grapheme, or sometimes several graphemes in combination (a composed glyph). If there is more than one allograph of a unit of writing, and the choice between them depends on context or on the preference of the author, they now have to be treated as separate glyphs, because mechanical arrangements have to be available to differentiate between them and to print whichever of them is required. The same is true in computing. In computing as well as typography, the term "character" refers to a grapheme or grapheme-like unit of text, as found in natural language writing systems ("scripts"). In typography and computing, the range of graphemes is broader than in a written language in other ways too: a typographical font often has to cope with a range of different languages each of which contribute their own graphemes, and it may also be required to print other symbols such as dingbats. The range of glyphs required increases correspondingly. In summary, in typography and computing, a glyph is a graphical unit.

In graphonomics, the term glyph is used for a noncharacter, i.e. either a subcharacter or multicharacter pattern. Most typographic glyphs originate from the characters of a typeface. In a typeface each character typically corresponds to a single glyph, but there are exceptions, such as a font used for a language with a large alphabet or complex writing system, where one character may correspond to several glyphs, or several characters to one glyph.




</doc>
<doc id="12905" url="https://en.wikipedia.org/wiki?curid=12905" title="Goth subculture">
Goth subculture

The goth subculture is a music subculture that began in England during the early 1980s, where it developed from the audience of gothic rock, an offshoot of the post-punk genre. The name, goth subculture, derived directly from the music genre. Seminal post-punk and gothic rock artists that helped develop and shape the subculture include Siouxsie and the Banshees, The Cure, Joy Division, and Bauhaus. The goth subculture has survived much longer than others of the same era, and has continued to diversify and spread throughout the world. Its imagery and cultural proclivities indicate influences from the 19th century Gothic literature and gothic horror films. The scene is centered on music festivals, nightclubs and organized meetings, especially in Western Europe.

The goth subculture has associated tastes in music, aesthetics, and fashion. The music preferred by the goth subculture includes a number of different styles, e.g. gothic rock, death rock, post-punk, gothabilly, cold wave, dark wave and ethereal wave. Styles of dress within the subculture draw on punk, new wave and new romantic fashion as well as fashion of earlier periods such as the Victorian and Edwardian eras ("Belle Époque"), or combinations of the above. The style usually includes dark attire (often black), pale face makeup and black hair. The subculture continues to draw interest from a large audience decades after its emergence.

The term "gothic rock" was coined in 1967, by music critic John Stickney to describe a meeting he had with Jim Morrison in a dimly lit wine-cellar which he called "the perfect room to honor the Gothic rock of the Doors". That same year, Velvet Underground with a track like "All Tomorrow's Parties", created a kind of "mesmerizing gothic-rock masterpiece" according to music historian Kurt Loder. In the late 1970s, the "gothic" adjective was used to describe the atmosphere of post-punk bands like Siouxsie and the Banshees, Magazine, and Joy Division. In a live review about a Siouxsie and the Banshees' concert in July 1978, critic Nick Kent wrote that concerning their music, "parallels and comparisons can now be drawn with gothic rock architects like the Doors and, certainly, early Velvet Underground". In March 1979, in his review of Magazine's second album "Secondhand Daylight", Kent noted that there was "a new austere sense of authority" in the music, with a "dank neo-Gothic sound". Later that year, the term was also used by Joy Division's manager, Tony Wilson on 15 September in an interview for the BBC TV programme's "Something Else". Wilson described Joy Division as "gothic" compared to the pop mainstream, right before a live performance of the band. The term was later applied to "newer bands such as Bauhaus who had arrived in the wake of Joy Division and Siouxsie and the Banshees". Bauhaus's first single issued in 1979, "Bela Lugosi's Dead", is generally credited as the starting point of the gothic rock genre.

In 1979, "Sounds" described Joy Division as "Gothic" and "theatrical". In February 1980, "Melody Maker" qualified the same band as "masters of this Gothic gloom". Critic Jon Savage would later say that their singer Ian Curtis wrote "the definitive Northern Gothic statement". However, it was not until the early-1980s that gothic rock became a coherent music subgenre within post-punk, and that followers of these bands started to come together as a distinctly recognizable movement. They may have taken the "goth" mantle from a 1981 article published in UK rock weekly "Sounds": "The face of Punk Gothique", written by Steve Keaton. In a text about the audience of UK Decay, Keaton asked: "Could this be the coming of Punk Gothique? With Bauhaus flying in on similar wings could it be the next big thing?" In July 1982, the opening of the Batcave in London's Soho provided a prominent meeting point for the emerging scene, which would be briefly labelled "positive punk" by the "NME" in a special issue with a front cover in early 1983. The term "Batcaver" was then used to describe old-school goths.
Independent from the British scene, in the late 1970s and early 1980s in California, deathrock developed as a distinct branch of American punk rock, with acts such as Christian Death and 45 Grave.

The bands that defined and embraced the gothic rock genre included Bauhaus, early Adam and the Ants, the Cure, the Birthday Party, Southern Death Cult, Specimen, Sex Gang Children, UK Decay, Virgin Prunes, Killing Joke, and the Damned. Near the peak of this first generation of the gothic scene in 1983, "The Face" Paul Rambali recalled that there were "several strong Gothic characteristics" in the music of Joy Division. In 1984, Joy Division's bassist Peter Hook named Play Dead as one of their heirs: "If you listen to a band like Play Dead, who I really like, Joy Division played the same stuff that Play Dead are playing. They're similar."
By the mid-1980s, bands began proliferating and became increasingly popular, including the Sisters of Mercy, the Mission (known as the Mission UK in the US), Alien Sex Fiend, the March Violets, Xmal Deutschland, the Membranes, and Fields of Nephilim. Record labels like Factory, 4AD and Beggars Banquet released much of this music in Europe, and through a vibrant import music market in the US, the subculture grew, especially in New York and Los Angeles, California, where many nightclubs featured "gothic/industrial" nights. The popularity of 4AD bands resulted in the creation of a similar US label, Projekt, which produces what was colloquially termed ethereal wave, a subgenre of dark wave music.

The 1990s saw further growth for some 1980s bands and the emergence of many new acts, as well as new goth-centric U.S. record labels such as Cleopatra Records, among others. According to Dave Simpson of "The Guardian", "in the 90s, goths all but disappeared as dance music became the dominant youth cult". As a result, the goth "movement went underground and mistaken for cyber goth, Shock rock, Industrial metal, Gothic metal, Medieval folk metal and the latest subgenre, horror punk". Marilyn Manson was seen as a "goth-shock icon" by "Spin".

The Goth subculture of the 1980s drew inspiration from a variety of sources. Some of them were modern or contemporary, others were centuries-old or ancient. Michael Bibby and Lauren M. E. Goodlad liken the subculture to a bricolage. Among the music subcultures that influenced it were Punk, New wave, and Glam. But it also drew inspiration from B movies, Gothic literature, horror films, vampire cults, Neo-noir science fiction films such as Ridley Scott's "Blade Runner", and traditional mythology. Among the mythologies that proved influential in Goth were Celtic mythology, Christian mythology, Egyptian mythology, and various traditions of Paganism.

The figures that the movement counted among its historic canon of ancestors were equally diverse. They included the Pre-Raphaelite Brotherhood, Friedrich Nietzsche (1844‒1900), Comte de Lautréamont (1846‒1870), Salvador Dalí (1904‒1989) and Jean-Paul Sartre (1905‒1980). Writers that have had a significant influence on the movement also represent a diverse canon. They include Ann Radcliffe (1764‒1823), John William Polidori (1795‒1821), Edgar Allan Poe (1809‒1849), Sheridan Le Fanu (1814-1873), Bram Stoker (1847‒1912), Oscar Wilde (1854‒1900), H. P. Lovecraft (1890‒1937), Anne Rice (1941‒), William Gibson (1948‒), Ian McEwan (1948‒), Storm Constantine (1956‒), and Poppy Z. Brite (1967‒).

Gothic literature is a genre of fiction that combines romance and dark elements to produce mystery, suspense, terror, horror and the supernatural. According to David H. Richter, settings were framed to take place at "…ruinous castles, gloomy churchyards, claustrophobic monasteries, and lonely mountain roads". Typical characters consisted of the cruel parent, sinister priest, courageous victor, and the helpless heroine, along with supernatural figures such as demons, vampires, ghosts, and monsters. Often, the plot focused on characters ill-fated, internally conflicted, and innocently victimized by harassing malicious figures. In addition to the dismal plot focuses, the literary tradition of the gothic was to also focus on individual characters that were gradually going insane.

English author Horace Walpole, with his 1764 novel "The Castle of Otranto" is one of the first writers who explored this genre. The American Revolutionary War-era "American Gothic" story of the Headless Horseman, immortalized in "The Legend of Sleepy Hollow" (published in 1820) by Washington Irving, marked the arrival in the New World of dark, romantic storytelling. The tale was composed by Irving while he was living in England, and was based on popular tales told by colonial Dutch settlers of the Hudson Valley, New York. The story would be adapted to film in 1922, in 1949 as the animated "The Adventures of Ichabod and Mr. Toad", and again in 1999.

Throughout the evolution of the goth subculture, classic romantic, Gothic and horror literature has played a significant role. E. T. A. Hoffmann (1776–1822), Edgar Allan Poe (1809–1849), Charles Baudelaire (1821–1867), H. P. Lovecraft (1890–1937), and other tragic and romantic writers have become as emblematic of the subculture as the use of dark eyeliner or dressing in black. Baudelaire, in fact, in his preface to "Les Fleurs du mal" ("Flowers of Evil") penned lines that could serve as a sort of goth malediction:

<poem>"C'est l'Ennui! —l'œil chargé d'un pleur involontaire,"
"Il rêve d'échafauds en fumant son houka."
"Tu le connais, lecteur, ce monstre délicat,"
"—Hypocrite lecteur,—mon semblable,—mon frère!"

It is Boredom! — an eye brimming with an involuntary tear,
He dreams of the gallows while smoking his water-pipe.
You know him, reader, this delicate monster,
—Hypocrite reader,—my twin,—my brother!</poem>

The gothic subculture has influenced different artists—not only musicians—but also painters and photographers. In particular their work is based on mystic, morbid and romantic motifs. In photography and painting the spectrum varies from erotic artwork to romantic images of vampires or ghosts. There is a marked preference for dark colours and sentiments, similar to Gothic fiction. At the end of the 19th century, painters like John Everett Millais and John Ruskin invented a new kind of Gothic.

Some people credit Jalacy "Screamin' Jay" Hawkins, perhaps best known for his 1956 song "I Put A Spell On You," as a foundation of modern goth style and music. Some people credit the band Bauhaus' first single "Bela Lugosi's Dead", released in August 1979, with the start of goth subculture.

The British sitcom, The IT Crowd featured a recurring goth character named Richmond Avenal, played by Noel Fielding. Fielding said in an interview that he himself had been a goth at age fifteen and that he had a series of goth girlfriends. This was the first time he dabbled in makeup. Fielding said that he loved his girlfriends dressing him up.

Notable examples of goth icons include several bandleaders: Siouxsie Sioux, of Siouxsie and the Banshees; Robert Smith, of The Cure; Peter Murphy, of Bauhaus; Rozz Williams, of Christian Death; Jonathan Melton aka Jonny Slut, of Specimen (who created the "Batcave" outfit and styling), Ian Curtis, of Joy Division; and Dave Vanian, of The Damned. Some members of Bauhaus were, themselves, fine art students or active artists. Nick Cave was dubbed as "the grand lord of gothic lushness". Nico is also a notable icon of goth fashion and music, with pioneering records like "The Marble Index" and "Desertshore" and the persona she adopted after their release.

One female role model is Theda Bara, the 1910s femme fatale known for her dark eyeshadow. In 1977, Karl Lagerfeld hosted the Soirée Moratoire Noir party, specifying "tenue tragique noire absolument obligatoire" (black tragic dress absolutely required). The event included elements associated with leatherman style.

African and Caribbean influences on gothic style are often missing from conversations: Jalacy "Screamin' Jay" Hawkins used voodoo imagery mixed with "spooky theatrics" to create a unique style, positioning him as one of the first goths. He would often use onstage props that reflected his goth and voodoo style, such as skulls, staffs, candles, tombstones, and bones.

Siouxsie Sioux was particularly influential on the dress style of the Gothic rock scene; Paul Morley of "NME" described Siouxsie and the Banshees' 1980 gig at Futurama: "[Siouxsie was] modeling her newest outfit, the one that will influence how all the girls dress over the next few months. About half the girls at Leeds had used Sioux as a basis for their appearance, hair to ankle." Robert Smith, Musidora, Bela Lugosi, Bettie Page, Vampira, Morticia Addams, Nico, Rozz Williams, David Bowie, Lux Interior, Dave Vanian, are also style icons.

The 1980s established designers such as Drew Bernstein of Lip Service, while the 1990s saw a surge of US-based gothic fashion designers, many of whom continue to evolve the style through the current day. Style magazines such as Gothic Beauty have given repeat features to a select few gothic fashion designers who began their labels in the 1990s, such as Kambriel, Rose Mortem, and Tyler Ondine of Heavy Red.

Gothic fashion is marked by conspicuously dark, antiquated and homogeneous features. It is stereotyped as eerie, mysterious, complex and exotic. A dark, sometimes morbid fashion and style of dress, typical gothic fashion includes a pale complexion with colored black hair and black period-styled clothing. Both male and female goths can wear dark eyeliner and dark fingernail polish, most especially black. Styles are often borrowed from punk fashion and − more currently − from the Victorian and Elizabethan periods. It also frequently expresses pagan, occult or other religious imagery. Gothic fashion and styling may also feature silver jewelry and piercings.
Ted Polhemus described goth fashion as a "profusion of black velvets, lace, fishnets and leather tinged with scarlet or purple, accessorized with tightly laced corsets, gloves, precarious stilettos and silver jewelry depicting religious or occult themes".

In contrast to the LARP-based Victorian and Elizabethan pomposity of the 2000s, the more Romantic side of 1980s trad-goth − mainly represented by females − was characterized by new wave/post-punk-oriented hairstyles (both long and short, partly shaved and teased) and street-compliant clothing, including black frill blouses, midi dresses or tea-length skirts, and floral lace tights, Dr. Martens, spike heels (pumps), and pointed toe buckle boots (winklepickers), sometimes supplemented with accessoires such as bracelets, chokers and bib necklaces. This style, retroactively referred to as "Ethergoth", took its inspiration from Siouxsie Sioux and mid-1980s protagonists from the 4AD roster like Liz Fraser and Lisa Gerrard.

Researcher Maxim W. Furek stated that "Goth is a revolt against the slick fashions of the 1970s disco era and a protest against the colourful pastels and extravagance of the 1980s. Black hair, dark clothing and pale complexions provide the basic look of the Goth Dresser. One can paradoxically argue that the Goth look is one of deliberate overstatement as just a casual look at the heavy emphasis on dark flowing capes, ruffled cuffs, pale makeup and dyed hair demonstrate a modern-day version of late-Victorian excess".

"The New York Times" noted: "The costumes and ornaments are a glamorous cover for the genre's somber themes. In the world of Goth, nature itself lurks as a malign protagonist, causing flesh to rot, rivers to flood, monuments to crumble and women to turn into slatterns, their hair streaming and lipstick askew".

Cintra Wilson declares that the origins of the dark romantic style are found in the "Victorian cult of mourning." Valerie Steele is an expert in the history of the style.

Goth fashion has a reciprocal relationship with the fashion world. In the later part of the first decade of the 21st century, designers such as Alexander McQueen, Anna Sui, Rick Owens, Gareth Pugh, Ann Demeulemeester, Philipp Plein, Hedi Slimane, John Richmond, John Galliano, Olivier Theyskens and Yohji Yamamoto brought elements of goth to runways. This was described as "Haute Goth" by Cintra Wilson in the "New York Times".

Thierry Mugler, Claude Montana, Jean Paul Gaultier and Christian Lacroix have also been associated with the fashion trend. In Spring 2004, Riccardo Tisci, Jean Paul Gaultier, Raf Simons and Stefano Pilati dressed their models as "glamorous ghouls dressed in form-fitting suits and coal-tinted cocktail dresses". Swedish designer Helena Horstedt and jewelry artist Hanna Hedman also practice a goth aesthetic.

Gothic styling often goes hand in hand with aesthetics, authenticity and expression, and is mostly considered to be an "artistical concept". Clothes are frequently self-designed.

In recent times, especially in the course of commercialization of parts of the Goth subculture, many non-involved people developed an interest in dark fashion styles and started to adopt elements of Goth clothing (primarily mass-produced goods from malls) without being connected to subcultural basics, e.g. Gothic music and Gothic lifestyle. Within the Goth movement they have been regularly described as "poseurs" or "mallgoths." (see also section "Identity").

Some of the early gothic rock and deathrock artists adopted traditional horror film images and drew on horror film soundtracks for inspiration. Their audiences responded by adopting appropriate dress and props. Use of standard horror film props like swirling smoke, rubber bats, and cobwebs featured as gothic club décor from the beginning in The Batcave. Such references in bands' music and images were originally tongue-in-cheek, but as time went on, bands and members of the subculture took the connection more seriously. As a result, morbid, supernatural and occult themes became more noticeably serious in the subculture. The interconnection between horror and goth was highlighted in its early days by "The Hunger", a 1983 vampire film starring David Bowie, Catherine Deneuve and Susan Sarandon. The film featured gothic rock group Bauhaus performing "Bela Lugosi's Dead" in a nightclub. Tim Burton created a storybook atmosphere filled with darkness and shadow in some of his films like "Beetlejuice" (1988), "Batman" (1989), "Edward Scissorhands" (1990), and the stop motion films "The Nightmare Before Christmas" (1993), which was produced/co-written by Burton, and "Corpse Bride" (2005), which he co-produced.

As the subculture became well-established, the connection between goth and horror fiction became almost a cliché, with goths quite likely to appear as characters in horror novels and film. For example, "The Craft", "The Crow", "The Matrix" and "Underworld" film series drew directly on goth music and style. The dark comedies "Beetlejuice", "The Faculty", "American Beauty", "Wedding Crashers" and a few episodes of the animated TV show "South Park" portray or parody the goth subculture. In "South Park", several of the fictional schoolchildren are depicted as goths. The goth kids on the show are depicted as finding it annoying to be confused with the Hot Topic "vampire" kids from the episode "The Ungroundable" in season 12. and even more frustrating to be compared with emo kids. The goth kids are usually depicted listening to goth music, writing or reading Gothic poetry, drinking coffee, flipping their hair and smoking.

A prominent American literary influence on the gothic scene was provided by Anne Rice's re-imagining of the vampire in 1976. In "The Vampire Chronicles", Rice's characters were depicted as self-tormentors who struggled with alienation, loneliness, and the human condition. Not only did the characters torment themselves, but they also depicted a surreal world that focused on uncovering its splendor. These Chronicles assumed goth attitudes, but they were not intentionally created to represent the gothic subculture. Their romance, beauty, and erotic appeal attracted
many goth readers, making her works popular from the 1980s through the 1990s. While Goth has embraced Vampire literature both in its 19th-century form and in its later incarnations, Rice's postmodern take on the vampire mythos has had a "special resonance" in the subculture. Her vampire novels feature intense emotions, period clothing, and "cultured decadence". Her vampires are socially alienated monsters, but they are also stunningly attractive. Rice's goth readers tend to envision themselves in much the same terms and view characters like Lestat de Lioncourt as role models.

Richard Wright's novel "Native Son" contains gothic imagery and themes that demonstrate the links between blackness and the gothic; themes and images of "premonitions, curses, prophecies, spells, veils, demonic possessions, graves, skeletons" are present, suggesting gothic influence. Other classic themes of the gothic are present in the novel, such as transgression and unstable identities of race, class, gender, and nationality. 

The re-imagining of the vampire continued with the release of Poppy Z. Brite's book "Lost Souls" in October 1992. Despite the fact that Brite's first novel was criticized by some mainstream sources for allegedly "lack[ing] a moral center: neither terrifyingly malevolent supernatural creatures nor (like Anne Rice's protagonists) tortured souls torn between good and evil, these vampires simply add blood-drinking to the amoral panoply of drug abuse, problem drinking and empty sex practiced by their human counterparts", many of these so-called "human counterparts" identified with the teen angst and goth music references therein, keeping the book in print. Upon release of a special 10th anniversary edition of "Lost Souls", "Publishers Weekly"—the same periodical that criticized the novel's "amorality" a decade prior—deemed it a "modern horror classic" and acknowledged that Brite established a "cult audience".

Neil Gaiman's graphic novel series "The Sandman" influenced goths with characters like the dark, brooding Dream and his sister Death. The 2002 release "21st Century Goth" by Mick Mercer, an author, noted music journalist and leading historian of gothic rock, explored the modern state of the goth scene around the world, including South America, Japan, and mainland Asia. His previous 1997 release, "Hex Files: The Goth Bible", similarly took an international look at the subculture.

In the US, "Propaganda" was a gothic subculture magazine founded in 1982. In Italy, "Ver Sacrum" covers the Italian goth scene, including fashion, sexuality, music, art and literature. Some magazines, such as the now-defunct "Dark Realms" and "Goth Is Dead" included goth fiction and poetry. Other magazines cover fashion (e.g., "Gothic Beauty"); music (e.g., "Severance") or culture and lifestyle (e.g., "Althaus" e-zine).

October 31, 2011 ECW Press published the "Encyclopedia Gothica" written by author and poet Liisa Ladouceur with illustrations done by Gary Pullin. This non-fiction book describes over 600 words and phrases relevant to Goth subculture.

Visual contemporary graphic artists with this aesthetic include Gerald Brom, Dave McKean, and Trevor Brown as well as illustrators Edward Gorey, Charles Addams, Lorin Morgan-Richards, and James O'Barr. The artwork of Polish surrealist painter Zdzisław Beksiński is often described as gothic. British artist Anne Sudworth published a book on gothic art in 2007.

The goth scene continues to exist in the 2010s. In Western Europe, there are large annual festivals mainly in Germany, including Wave-Gotik-Treffen (Leipzig) and M'era Luna (Hildesheim), both annually attracting tens of thousands of attendees. The Lumous Gothic Festival (more commonly known as Lumous) is the largest festival dedicated to the goth subculture in Finland and the northernmost gothic festival in the world. The Ukrainian festival "Deti Nochi: Chorna Rada" (Children of the night) is the biggest gothic event in Ukraine. Goth events like "Ghoul School" and "Release the Bats" promote deathrock and are attended by fans from many countries, and events such as the Drop Dead Festival in the US attract attendees from over 30 countries. The Whitby Goth Weekend is a twice-yearly goth music festival in Whitby, North Yorkshire, England. In the US, events such as Bats Day in the Fun Park celebrate the culture, as well as the Goth Cruise, and the Gothic Cruise.

In the 1980s, goths decorated their walls and ceiling with black fabrics and accessories like rosaries, crosses and plastic roses. Black furniture and cemetery-related objects like candlesticks, death lanterns and skulls. In the 1990s the interior design approach of the 1980s was replaced by a less macabre style.

Since the late 1970s, the UK goth scene refused "traditional standards of sexual propriety" and accepted and celebrated "unusual, bizarre or deviant sexual practices". In the 2000s, many members "... claim overlapping memberships in the queer, polyamorous, bondage-discipline/sadomasochism, and pagan communities".

Though sexual empowerment is not unique to women in the goth scene, it remains an important part of many goth women's experience: The "... [s]cene's celebration of active sexuality" enables goth women "... to resist mainstream notions of passive femininity". They have an "active sexuality" approach which creates "gender egalitarianism" within the scene, as it "allows them to engage in sexual play with multiple partners while sidestepping most of the stigma and dangers that women who engage in such behavior" outside the scene frequently incur, while continuing to "... see themselves as strong".

Men dress up in androgynous way: "... Men 'gender blend,' wearing makeup and skirts". In contrast, the "... women are dressed in sexy feminine outfits" that are "... highly sexualized" and which often combine "... corsets with short skirts and fishnet stockings". Androgyny is common among the scene: "... androgyny in Goth subcultural style often disguises or even functions to reinforce conventional gender roles". It was only "valorised" for male goths, who adopt a "feminine" appearance, including "make-up, skirts and feminine accessories" to "enhance masculinity" and facilitate traditional heterosexual courting roles.

While goth is "considered a music-based scene", "... to be Goth implies much more than shared musical tastes; it is ... an 'aesthetic,' a particular way of seeing and of being seen". Observers have raised the issue of to what degree individuals are truly members of the goth subculture. On one end of the spectrum is the "Uber goth", a person who is described as seeking a pallor so much that he or she applies "... as much white foundation and white powder as possible". On the other end of the spectrum another writer terms "poseurs": "goth wannabes, usually young kids going through a goth phase who do not hold to goth sensibilities but want to be part of the goth crowd..". It has been said that a "mall goth" is a teen who dresses in a goth style and spends time in malls with a Hot Topic store, but who does not know much about the goth subculture or its music, thus making him or her a poseur. In one case, even a well-known performer has been labelled with the pejorative term: a "number of goths, especially those who belonged to this subculture before the late-1980s, reject Marilyn Manson as a poseur who undermines the true meaning of goth".

The BBC described academic research that indicated that goths are "refined and sensitive, keen on poetry and books, not big on drugs or anti-social behaviour". Teens often stay in the subculture "into their adult life", and they are likely to become well-educated and enter professions such as medicine or law. The subculture carries on appealing to teenagers who are looking for meaning and for identity. The scene teaches teens that there are difficult aspects to life that you "have to make an attempt to understand" or explain.

"The Guardian" reported that a "glue binding the [goth] scene together was drug use"; however, in the scene, drug use was varied. Goth is one of the few subculture movements that is not associated with a single drug, in the way that the Hippie subculture is associated with cannabis and the Mod subculture is associated with amphetamines. A 2006 study of young goths found that those with higher levels of goth identification had higher drug use.

The goth scene is often described as non-violent. However, two non peer-reviewed studies by the A.S.H.A. concluded a higher than average propensity toward violence, and for one of the papers, self-harm, within the goth subculture.

In the weeks following the 1999 Columbine High School massacre, media reports about the teen gunmen, Harris and Klebold, portrayed them as part of a gothic cult. An increased suspicion of goth subculture subsequently manifested in the media. This led to a moral panic over teen involvement in goth subculture and a number of other activities, such as violent video games. Harris and Klebold had initially been thought to be members of "The Trenchcoat Mafia"; an informal club within Columbine High School. Later, such characterizations were considered incorrect.

Media reported that the gunman in the 2006 Dawson College shooting in Montreal, Quebec, Kimveer Singh Gill, was interested in goth subculture. Gill's self-professed love of Goth culture was the topic of media interest, and it was widely reported that the word "Goth", in Gill's writings, was a reference to the alternative industrial and goth subculture rather than a reference to gothic rock music. Gill, who committed suicide after the attack, wrote in his online journal: "I'm so sick of hearing about jocks and preps making life hard for the goths and others who look different, or are different". Gill described himself in his profile on Vampirefreaks.com as "... Trench ... the Angel of Death" and he stated that "Metal and Goth kick ass". An image gallery on Gill's Vampirefreaks.com blog had photos of him pointing a gun at the camera or wearing a long black trench coat.

Mick Mercer stated that Gill was "not a Goth. Never a Goth. The bands he listed as his chosen form of ear-bashing were relentlessly metal and standard grunge, rock and goth metal, with some industrial presence". Mercer stated that "Kimveer Gill listened to metal", "He had nothing whatsoever to do with Goth" and further commented "I realise that like many Neos [neophyte], Kimveer Gill may even have believed he somehow was a Goth, because they're [Neophytes] only really noted for spectacularly missing the point".

In part because of public misunderstanding surrounding gothic aesthetics, people in the goth subculture sometimes suffer prejudice, discrimination, and intolerance. As is the case with members of various other subcultures and alternative lifestyles, outsiders sometimes marginalize goths, either by intention or by accident. Actress Christina Hendricks talked of being bullied as a goth in school and how difficult it was for her to deal with societal pressure: "Kids can be pretty judgmental about people who are different. But instead of breaking down and conforming, I stood firm. That is also probably why I was unhappy. My mother was mortified and kept telling me how horrible and ugly I looked. Strangers would walk by with a look of shock on their face, so I never felt pretty. I just always felt awkward". Prejudice moves people into circles of bonding where they share these similar experiences and are accepted. Young goths have to define themselves and learn beauty is an aspect of cultural relativism.

On 11 August 2007, a couple walking through Stubbylee Park in Bacup, Lancashire, England were attacked by a group of teenagers because they were goths. Sophie Lancaster subsequently died from her injuries. On 29 April 2008, two teens, Ryan Herbert and Brendan Harris, were convicted for the murder of Lancaster and given life sentences; three others were given lesser sentences for the assault on her boyfriend Robert Maltby. In delivering the sentence, Judge Anthony Russell stated, "This was a hate crime against these completely harmless people targeted because their appearance was different to yours". He went on to defend the goth community, calling goths "perfectly peaceful, law-abiding people who pose no threat to anybody". Judge Russell added that he "recognised it as a hate crime without Parliament having to tell him to do so and had included that view in his sentencing". Despite this ruling, a bill to add discrimination based on subculture affiliation to the definition of hate crime in British law was not presented to parliament.

In 2013, police in Manchester announced they would be treating attacks on members of alternative subcultures, like goths, the same as they do for attacks based on race, religion, and sexual orientation.

A study published on the "British Medical Journal" concluded that "identification as belonging to the Goth subculture [at some point in their lives] was the best predictor of self harm and attempted suicide [among young teens]", and that it was most possibly due to a selection mechanism (persons that wanted to harm themselves later identified as goths, thus raising the percentage of those persons who identify as goths). According to "The Guardian", some goth teens are at more likely to harm themselves or attempt suicide. A medical journal study of 1,300 Scottish schoolchildren until their teen years found that the 53% of the goth teens had attempted to harm themselves and 47% had attempted suicide. The study found that the "correlation was stronger than any other predictor". The study was based on a sample of 15 teenagers who identified as goths, of which 8 had self-harmed by any method, 7 had self-harmed by cutting, scratching or scoring, and 7 had attempted suicide.

The authors held that most self-harm by teens was done before joining the subculture, and that joining the subculture would actually protect them and help them deal with distress in their lives. The authors insisted on the study being based on small numbers and on the need of replication to confirm the results. The study was criticized for using only a small sample of goth teens and not taking into account other influences and differences between types of goths; by taking a study from a larger number of people.



</doc>
<doc id="12908" url="https://en.wikipedia.org/wiki?curid=12908" title="Global warming potential">
Global warming potential

Global warming potential (GWP) is a relative measure of how much heat a greenhouse gas traps in the atmosphere. It compares the amount of heat trapped by a certain mass of the gas in question to the amount of heat trapped by a similar mass of carbon dioxide. A GWP is calculated over a specific time interval, commonly 20, 100, or 500 years. GWP is expressed as a factor of carbon dioxide (whose GWP is standardized to 1). In the Fifth Assessment Report of the Intergovernmental Panel on Climate Change, methane has a lifetime of 12.4 years and with climate-carbon feedbacks a global warming potential of 86 over 20 years and 34 over 100 years in response to emissions. User related choices such as the time horizon can greatly affect the numerical values obtained for carbon dioxide equivalents. For a change in time horizon from 20 to 100 years, the GWP for methane decreases by a factor of approximately 2.5.
The substances subject to restrictions under the Kyoto protocol are either rapidly increasing their concentrations in Earth's atmosphere or have a large GWP.

The GWP depends on the following factors:
Thus, a high GWP correlates with a large infrared absorption and a long atmospheric lifetime. The dependence of GWP on the wavelength of absorption is more complicated. Even if a gas absorbs radiation efficiently at a certain wavelength, this may not affect its GWP much if the atmosphere already absorbs most radiation at that wavelength. A gas has the most effect if it absorbs in a "window" of wavelengths where the atmosphere is fairly transparent. The dependence of GWP as a function of wavelength has been found empirically and published as a graph.

Because the GWP of a greenhouse gas depends directly on its infrared spectrum, the use of infrared spectroscopy to study greenhouse gases is centrally important in the effort to understand the impact of human activities on global climate change.

Just as radiative forcing provides a simplified means of comparing the various factors that are believed to influence the climate system to one another, global warming potentials (GWPs) are one type of simplified index based upon radiative properties that can be used to estimate the potential future impacts of emissions of different gases upon the climate system in a relative sense. GWP is based on a number of factors, including the radiative efficiency (infrared-absorbing ability) of each gas relative to that of carbon dioxide, as well as the decay rate of each gas (the amount removed from the atmosphere over a given number of years) relative to that of carbon dioxide.

The radiative forcing capacity (RF) is the amount of energy per unit area, per unit time, absorbed by the greenhouse gas, that would otherwise be lost to space. It can be expressed by the formula:

where the subscript "i" represents an interval of 10 inverse centimeters. Abs represents the integrated infrared absorbance of the sample in that interval, and F represents the RF for that interval.

The Intergovernmental Panel on Climate Change (IPCC) provides the generally accepted values for GWP, which changed slightly between 1996 and 2001. An exact definition of how GWP is calculated is to be found in the IPCC's 2001 Third Assessment Report. The GWP is defined as the ratio of the time-integrated radiative forcing from the instantaneous release of 1 kg of a trace substance relative to that of 1 kg of a reference gas:

where TH is the time horizon over which the calculation is considered; a is the radiative efficiency due to a unit increase in atmospheric abundance of the substance (i.e., Wm kg) and [x(t)] is the time-dependent decay in abundance of the substance following an instantaneous release of it at time t=0. The denominator contains the corresponding quantities for the reference gas (i.e. ). The radiative efficiencies a and a are not necessarily constant over time. While the absorption of infrared radiation by many greenhouse gases varies linearly with their abundance, a few important ones display non-linear behaviour for current and likely future abundances (e.g., CO, CH, and NO). For those gases, the relative radiative forcing will depend upon abundance and hence upon the future scenario adopted.

Since all GWP calculations are a comparison to CO which is non-linear, all GWP values are affected. Assuming otherwise as is done above will lead to lower GWPs for other gases than a more detailed approach would. Clarifying this, while increasing CO has less and less effect on radiative absorption as ppm concentrations rise, more powerful greenhouse gases like methane and nitrous oxide have different thermal absorption frequencies to CO that are not filled up (saturated) as much as CO, so rising ppms of these gases are far more significant.

Under the Kyoto Protocol, the Conference of the Parties decided (decision 2/CP.3) that the values of GWP calculated for the IPCC Second Assessment Report are to be used for converting the various greenhouse gas emissions into comparable CO equivalents when computing overall sources and sinks.
The "Global Temperature change Potential" is another way to quantify the ratio change from a substance relative to that of CO, in global mean surface temperature, used for a specific time span.

A substance's GWP depends on the timespan over which the potential is calculated. A gas which is quickly removed from the atmosphere may initially have a large effect, but for longer time periods, as it has been removed, it becomes less important. Thus methane has a potential of 34 over 100 years but 86 over 20 years; conversely sulfur hexafluoride has a GWP of 22,800 over 100 years but 16,300 over 20 years (IPCC Third Assessment Report). The GWP value depends on how the gas concentration decays over time in the atmosphere. This is often not precisely known and hence the values should not be considered exact. For this reason when quoting a GWP it is important to give a reference to the calculation.

The GWP for a mixture of gases can be obtained from the mass-fraction-weighted average of the GWPs of the individual gases.

Commonly, a time horizon of 100 years is used by regulators (e.g., the California Air Resources Board).

Carbon dioxide has a GWP of exactly 1 (since it is the baseline unit to which all other greenhouse gases are compared).

The values given in the table assume the same mass of compound is released; different ratios will result from the conversion of one substance to another. For instance, burning methane to carbon dioxide would reduce the global warming impact, but by a smaller factor than 25:1 because the mass of methane burned is less than the mass of carbon dioxide released (ratio 1:2.74). If you started with 1 tonne of methane which has a GWP of 25, after combustion you would have 2.74 tonnes of CO, each tonne of which has a GWP of 1. This is a net reduction of 22.26 tonnes of GWP, reducing the global warming effect by a ratio of 25:2.74 (approximately 9 times).

The global warming potential of perfluorotributylamine (PFTBA) over a 100-year time horizon has been estimated to be approximately 7100. It has been used by the electrical industry since the mid-20th century for electronic testing and as a heat transfer agent. PFTBA has the highest radiative efficiency (relative effectiveness of greenhouse gases to restrict long-wave radiation from escaping back into space) of any molecule detected in the atmosphere to date. The researchers found an average of 0.18 parts per trillion of PFTBA in Toronto air samples, whereas carbon dioxide exists around 400 parts per million.

Water vapour has a profound infrared absorption spectrum with more and broader absorption bands than CO, and also absorbs non-zero amounts of radiation in its low absorbing spectral regions, (see greenhouse gas (GHG)), its GWP is therefore difficult to calculate. Further, its concentration in the atmosphere depends on air temperature and water availability; using a global average temperature of ~16 °C, for example, creates an average humidity of ~18,000ppm at sea level (CO is ~400ppm and so concentrations of [HO]/[CO] ~ 45x). Another issue with calculating GWP is that, unlike other GHG, water vapor does not decay in the environment, so an average over some time period or some other measure consistent with "time dependent decay," q.v., above, must be used in lieu of the time dependent decay of artificial or excess CO, molecules. Other factors complicating its calculation are the Earth's temperature distribution, and the differing land masses in the Northern and Southern hemispheres.




</doc>
<doc id="12910" url="https://en.wikipedia.org/wiki?curid=12910" title="Grothendieck topology">
Grothendieck topology

In category theory, a branch of mathematics, a Grothendieck topology is a structure on a category "C" which makes the objects of "C" act like the open sets of a topological space. A category together with a choice of Grothendieck topology is called a site.

Grothendieck topologies axiomatize the notion of an open cover. Using the notion of covering provided by a Grothendieck topology, it becomes possible to define sheaves on a category and their cohomology. This was first done in algebraic geometry and algebraic number theory by Alexander Grothendieck to define the étale cohomology of a scheme. It has been used to define other cohomology theories since then, such as l-adic cohomology, flat cohomology, and crystalline cohomology. While Grothendieck topologies are most often used to define cohomology theories, they have found other applications as well, such as to John Tate's theory of rigid analytic geometry.

There is a natural way to associate a site to an ordinary topological space, and Grothendieck's theory is loosely regarded as a generalization of classical topology. Under meager point-set hypotheses, namely sobriety, this is completely accurate—it is possible to recover a sober space from its associated site. However simple examples such as the indiscrete topological space show that not all topological spaces can be expressed using Grothendieck topologies. Conversely, there are Grothendieck topologies which do not come from topological spaces.

The term "Grothendieck topology" has changed in meaning. In it meant what is now called a Grothendieck pretopology, and some authors still use this old meaning. modified the definition to use sieves rather than covers. Much of the time this does not make much difference, as each Grothendieck pretopology determines a unique Grothendieck topology, though quite different pretopologies can give the same topology.

André Weil's famous Weil conjectures proposed that certain properties of equations with integral coefficients should be understood as geometric properties of the algebraic variety that they define. His conjectures postulated that there should be a cohomology theory of algebraic varieties which gave number-theoretic information about their defining equations. This cohomology theory was known as the "Weil cohomology", but using the tools he had available, Weil was unable to construct it.

In the early 1960s, Alexander Grothendieck introduced étale maps into algebraic geometry as algebraic analogues of local analytic isomorphisms in analytic geometry. He used étale coverings to define an algebraic analogue of the fundamental group of a topological space. Soon Jean-Pierre Serre noticed that some properties of étale coverings mimicked those of open immersions, and that consequently it was possible to make constructions which imitated the cohomology functor "H". Grothendieck saw that it would be possible to use Serre's idea to define a cohomology theory which he suspected would be the Weil cohomology. To define this cohomology theory, Grothendieck needed to replace the usual, topological notion of an open covering with one that would use étale coverings instead. Grothendieck also saw how to phrase the definition of covering abstractly; this is where the definition of a Grothendieck topology comes from.

The classical definition of a sheaf begins with a topological space "X". A sheaf associates information to the open sets of "X". This information can be phrased abstractly by letting "O"("X") be the category whose objects are the open subsets "U" of "X" and whose morphisms are the inclusion maps "V" → "U" of open sets "U" and "V" of "X". We will call such maps "open immersions", just as in the context of schemes. Then a presheaf on "X" is a contravariant functor from "O"("X") to the category of sets, and a sheaf is a presheaf which satisfies the gluing axiom. The gluing axiom is phrased in terms of pointwise covering, i.e., formula_1 covers "U" if and only if formula_2. In this definition, formula_3 is an open subset of "X". Grothendieck topologies replace each formula_3 with an entire family of open subsets; in this example, formula_3 is replaced by the family of all open immersions formula_6. Such a collection is called a "sieve". Pointwise covering is replaced by the notion of a "covering family"; in the above example, the set of all formula_7 as "i" varies is a covering family of "U". Sieves and covering families can be axiomatized, and once this is done open sets and pointwise covering can be replaced by other notions which describe other properties of the space "X".

In a Grothendieck topology, the notion of a collection of open subsets of "U" stable under inclusion is replaced by the notion of a sieve. If "c" is any given object in "C", a sieve on "c" is a subfunctor of the functor Hom(−, "c"); (this is the Yoneda embedding applied to "c"). In the case of "O"("X"), a sieve "S" on an open set "U" selects a collection of open subsets of "U" which is stable under inclusion. More precisely, consider that for any open subset "V" of "U", "S"("V") will be a subset of Hom("V", "U"), which has only one element, the open immersion "V" → "U". Then "V" will be considered "selected" by "S" if and only if "S"("V") is nonempty. If "W" is a subset of "V", then there is a morphism "S"("V") → "S"("W") given by composition with the inclusion "W" → "V". If "S"("V") is non-empty, it follows that "S"("W") is also non-empty.

If "S" is a sieve on "X", and "f": "Y" → "X" is a morphism, then left composition by "f" gives a sieve on "Y" called the pullback of "S" along "f", denoted by "f""S". It is defined as the fibered product "S" × Hom(−, "Y") together with its natural embedding in Hom(−, "Y"). More concretely, for each object "Z" of "C", "f""S"("Z") = { "g": "Z" → "Y" | "fg" formula_8"S"("Z") }, and "f""S" inherits its action on morphisms by being a subfunctor of Hom(−, "Y"). In the classical example, the pullback of a collection {"V"} of subsets of "U" along an inclusion "W" → "U" is the collection {"V"∩W}.

A Grothendieck topology "J" on a category "C" is a collection, "for each object c of C", of distinguished sieves on "c", denoted by "J"("c") and called covering sieves of "c". This selection will be subject to certain axioms, stated below. Continuing the previous example, a sieve "S" on an open set "U" in "O"("X") will be a covering sieve if and only if the union of all the open sets "V" for which "S"("V") is nonempty equals "U"; in other words, if and only if "S" gives us a collection of open sets which cover "U" in the classical sense.

The conditions we impose on a Grothendieck topology are:


The base change axiom corresponds to the idea that if {formula_3} covers "U", then {"U" ∩ "V"} should cover "U" ∩ "V". The local character axiom corresponds to the idea that if {"U"} covers "U" and {"V"} covers "U" for each "i", then the collection {"V"} for all "i" and "j" should cover "U". Lastly, the identity axiom corresponds to the idea that any set is covered by all its possible subsets.

In fact, it is possible to put these axioms in another form where their geometric character is more apparent, assuming that the underlying category "C" contains certain fibered products. In this case, instead of specifying sieves, we can specify that certain collections of maps with a common codomain should cover their codomain. These collections are called covering families. If the collection of all covering families satisfies certain axioms, then we say that they form a Grothendieck pretopology. These axioms are:


For any pretopology, the collection of all sieves that contain a covering family from the pretopology is always a Grothendieck topology.

For categories with fibered products, there is a converse. Given a collection of arrows {"X" → "X"}, we construct a sieve "S" by letting "S"("Y") be the set of all morphisms "Y" → "X" that factor through some arrow "X" → "X". This is called the sieve generated by {"X" → "X"}. Now choose a topology. Say that {"X" → "X"} is a covering family if and only if the sieve that it generates is a covering sieve for the given topology. It is easy to check that this defines a pretopology.

(PT 3) is sometimes replaced by a weaker axiom:


(PT 3) implies (PT 3'), but not conversely. However, suppose that we have a collection of covering families that satisfies (PT 0) through (PT 2) and (PT 3'), but not (PT 3). These families generate a pretopology. The topology generated by the original collection of covering families is then the same as the topology generated by the pretopology, because the sieve generated by an isomorphism "Y" → "X" is Hom(−, "X"). Consequently, if we restrict our attention to topologies, (PT 3) and (PT 3') are equivalent.

Let "C" be a category and let "J" be a Grothendieck topology on "C". The pair ("C", "J") is called a site.

A presheaf on a category is a contravariant functor from "C" to the category of all sets. Note that for this definition "C" is not required to have a topology. A sheaf on a site, however, should allow gluing, just like sheaves in classical topology. Consequently, we define a sheaf on a site to be a presheaf "F" such that for all objects "X" and all covering sieves "S" on "X", the natural map Hom(Hom(−, "X"), "F") → Hom("S", "F"), induced by the inclusion of "S" into Hom(−, "X"), is a bijection. Halfway in between a presheaf and a sheaf is the notion of a separated presheaf, where the natural map above is required to be only an injection, not a bijection, for all sieves "S". A morphism of presheaves or of sheaves is a natural transformation of functors. The category of all sheaves on "C" is the topos defined by the site ("C", "J").

Using the Yoneda lemma, it is possible to show that a presheaf on the category "O"("X") is a sheaf on the topology defined above if and only if it is a sheaf in the classical sense.

Sheaves on a pretopology have a particularly simple description: For each covering family {"X" → "X"}, the diagram

must be an equalizer. For a separated presheaf, the first arrow need only be injective.

Similarly, one can define presheaves and sheaves of abelian groups, rings, modules, and so on. One can require either that a presheaf "F" is a contravariant functor to the category of abelian groups (or rings, or modules, etc.), or that "F" be an abelian group (ring, module, etc.) object in the category of all contravariant functors from "C" to the category of sets. These two definitions are equivalent.

Let C be any category. To define the discrete topology, we declare all sieves to be covering sieves. If C has all fibered products, this is equivalent to declaring all families to be covering families. To define the indiscrete topology, we declare only the sieves of the form Hom(−, "X") to be covering sieves. The indiscrete topology is also known as the biggest or chaotic topology, and it is generated by the pretopology which has only isomorphisms for covering families. A sheaf on the indiscrete site is the same thing as a presheaf.

Let C be any category. The Yoneda embedding gives a functor Hom(−, "X") for each object "X" of C. The canonical topology is the biggest (finest) topology such that every representable presheaf, i.e. presheaf of the form Hom(−, "X"), is a sheaf. A covering sieve or covering family for this site is said to be "strictly universally epimorphic". A topology which is less fine than the canonical topology, that is, for which every covering sieve is strictly universally epimorphic, is called subcanonical. Subcanonical sites are exactly the sites for which every presheaf of the form Hom(−, "X") is a sheaf. Most sites encountered in practice are subcanonical.

We repeat the example which we began with above. Let "X" be a topological space. We defined "O"("X") to be the category whose objects are the open sets of "X" and whose morphisms are inclusions of open sets. Note that for an open set "U" and a sieve "S" on "U", the set "S"("V") contains either zero or one element for every open set "V". The covering sieves on an object "U" of "O"("X") are those sieves "S" satisfying the following condition:
This notion of cover matches the usual notion in point-set topology. 

This topology can also naturally be expressed as a pretopology. We say that a family of inclusions {"V" formula_11 "U"} is a covering family if and only if the union formula_12"V" equals "U". This site is called the "'small site associated to a topological space "X".

Let "Spc" be the category of all topological spaces. Given any family of functions {"u" : "V" → "X"}, we say that it is a surjective family or that the morphisms "u" are jointly surjective if formula_12 "u"("V") equals "X". We define a pretopology on "Spc" by taking the covering families to be surjective families all of whose members are open immersions. Let "S" be a sieve on "Spc". "S" is a covering sieve for this topology if and only if:

Fix a topological space "X". Consider the comma category "Spc/X" of topological spaces with a fixed continuous map to "X". The topology on "Spc" induces a topology on "Spc/X". The covering sieves and covering families are almost exactly the same; the only difference is that now all the maps involved commute with the fixed maps to "X". This is the big site associated to a topological space X . Notice that "Spc" is the big site associated to the one point space. This site was first considered by Jean Giraud.

Let "M" be a manifold. "M" has a category of open sets "O"("M") because it is a topological space, and it gets a topology as in the above example. For two open sets "U" and "V" of "M", the fiber product "U" × "V" is the open set "U" ∩ "V", which is still in "O"("M"). This means that the topology on "O"("M") is defined by a pretopology, the same pretopology as before.

Let "Mfd" be the category of all manifolds and continuous maps. (Or smooth manifolds and smooth maps, or real analytic manifolds and analytic maps, etc.) "Mfd" is a subcategory of "Spc", and open immersions are continuous (or smooth, or analytic, etc.), so "Mfd" inherits a topology from "Spc". This lets us construct the big site of the manifold "M" as the site "Mfd/M". We can also define this topology using the same pretopology we used above. Notice that to satisfy (PT 0), we need to check that for any continuous map of manifolds "X" → "Y" and any open subset "U" of "Y", the fibered product "U" × "X" is in "Mfd/M". This is just the statement that the preimage of an open set is open. Notice, however, that not all fibered products exist in "Mfd" because the preimage of a smooth map at a critical value need not be a manifold.

The category of schemes, denoted "Sch", has a tremendous number of useful topologies. A complete understanding of some questions may require examining a scheme using several different topologies. All of these topologies have associated small and big sites. The big site is formed by taking the entire category of schemes and their morphisms, together with the covering sieves specified by the topology. The small site over a given scheme is formed by only taking the objects and morphisms which are part of a cover of the given scheme.

The most elementary of these is the Zariski topology. Let "X" be a scheme. "X" has an underlying topological space, and this topological space determines a Grothendieck topology. The Zariski topology on "Sch" is generated by the pretopology whose covering families are jointly surjective families of scheme-theoretic open immersions. The covering sieves "S" for "Zar" are characterized by the following two properties:
Despite their outward similarities, the topology on "Zar" is "not" the restriction of the topology on "Spc"! This is because there are morphisms of schemes which are topologically open immersions but which are not scheme-theoretic open immersions. For example, let "A" be a non-reduced ring and let "N" be its ideal of nilpotents. The quotient map "A" → "A/N" induces a map Spec "A/N" → Spec "A" which is the identity on underlying topological spaces. To be a scheme-theoretic open immersion it must also induce an isomorphism on structure sheaves, which this map does not do. In fact, this map is a closed immersion.

The étale topology is finer than the Zariski topology. It was the first Grothendieck topology to be closely studied. Its covering families are jointly surjective families of étale morphisms. It is finer than the Nisnevich topology, but neither finer nor coarser than the "cdh" and l′ topologies.

There are two flat topologies, the "fppf" topology and the "fpqc" topology. "fppf" stands for ', and in this topology, a morphism of affine schemes is a covering morphism if it is faithfully flat, of finite presentation, and is quasi-finite. "fpqc" stands for ', and in this topology, a morphism of affine schemes is a covering morphism if it is faithfully flat. In both categories, a covering family is defined to be a family that is a cover on Zariski open subsets. In the fpqc topology, any faithfully flat and quasi-compact morphism is a cover. These topologies are closely related to descent. The "fpqc" topology is finer than all the topologies mentioned above, and it is very close to the canonical topology.

Grothendieck introduced crystalline cohomology to study the "p"-torsion part of the cohomology of characteristic "p" varieties. In the "crystalline topology" which is the basis of this theory, the underlying category has objects given by infinitesimal thickenings together with divided power structures. Crystalline sites are examples of sites with no final object.

There are two natural types of functors between sites. They are given by functors which are compatible with the topology in a certain sense.

If ("C", "J") and ("D", "K") are sites and "u" : "C" → "D" is a functor, then "u" is continuous if for every sheaf "F" on "D" with respect to the topology "K", the presheaf "Fu" is a sheaf with respect to the topology "J". Continuous functors induce functors between the corresponding topoi by sending a sheaf "F" to "Fu". These functors are called pushforwards. If formula_14 and formula_15 denote the topoi associated to "C" and "D", then the pushforward functor is formula_16.

"u" admits a left adjoint "u" called the pullback. "u" need not preserve limits, even finite limits.

In the same way, "u" sends a sieve on an object "X" of "C" to a sieve on the object "uX" of "D". A continuous functor sends covering sieves to covering sieves. If "J" is the topology defined by a pretopology, and if "u" commutes with fibered products, then "u" is continuous if and only if it sends covering sieves to covering sieves and if and only if it sends covering families to covering families. In general, it is "not" sufficient for "u" to send covering sieves to covering sieves (see SGA IV 3, 1.9.3).

Again, let ("C", "J") and ("D", "K") be sites and "v" : "C" → "D" be a functor. If "X" is an object of "C" and "R" is a sieve on "vX", then "R" can be pulled back to a sieve "S" as follows: A morphism "f" : "Z" → "X" is in "S" if and only if "v"("f") : "vZ" → "vX" is in "R". This defines a sieve. "v" is cocontinuous if and only if for every object "X" of "C" and every covering sieve "R" of "vX", the pullback "S" of "R" is a covering sieve on "X".

Composition with "v" sends a presheaf "F" on "D" to a presheaf "Fv" on "C", but if "v" is cocontinuous, this need not send sheaves to sheaves. However, this functor on presheaf categories, usually denoted formula_17, admits a right adjoint formula_18. Then "v" is cocontinuous if and only if formula_18 sends sheaves to sheaves, that is, if and only if it restricts to a functor formula_20. In this case, the composite of formula_17 with the associated sheaf functor is a left adjoint of "v" denoted "v". Furthermore, "v" preserves finite limits, so the adjoint functors "v" and "v" determine a geometric morphism of topoi formula_22.

A continuous functor "u" : "C" → "D" is a morphism of sites "D" → "C" ("not" "C" → "D") if "u" preserves finite limits. In this case, "u" and "u" determine a geometric morphism of topoi formula_22. The reasoning behind the convention that a continuous functor "C" → "D" is said to determine a morphism of sites in the opposite direction is that this agrees with the intuition coming from the case of topological spaces. A continuous map of topological spaces "X" → "Y" determines a continuous functor "O"("Y") → "O"("X"). Since the original map on topological spaces is said to send "X" to "Y", the morphism of sites is said to as well.

A particular case of this happens when a continuous functor admits a left adjoint. Suppose that "u" : "C" → "D" and "v" : "D" → "C" are functors with "u" right adjoint to "v". Then "u" is continuous if and only if "v" is cocontinuous, and when this happens, "u" is naturally isomorphic to "v" and "u" is naturally isomorphic to "v". In particular, "u" is a morphism of sites.





</doc>
<doc id="12913" url="https://en.wikipedia.org/wiki?curid=12913" title="Greens">
Greens

Greens may refer to:








</doc>
<doc id="12914" url="https://en.wikipedia.org/wiki?curid=12914" title="Ghost in the Shell">
Ghost in the Shell

Animation studio Production I.G has produced several different anime adaptations of "Ghost in the Shell", starting with the 1995 film of the same name, telling the story of Section 9's investigation of the Puppet Master. The television series "" followed in 2002, telling an alternate story from the manga and first film, featuring Section 9's investigations of government corruption in the Laughing Man and Individual Eleven incidents. A sequel to the 1995 film, "", was released in 2004. In 2006, the film "" retook the story of the television series. 2013 saw the start of the "" original video animation (OVA) series, consisting of four parts through mid-2014. The series was recompiled in early 2015 as a television series titled "Ghost in the Shell: Arise - Alternative Architecture", airing with an additional two episodes (one part). An animated feature film produced by most of the "Arise" staff, titled "", was released on June 20, 2015. A live-action American film of the same name was released on March 31, 2017.

Shirow has stated that he had always wanted the title of his manga to be "Ghost in the Shell", even in Japan, but his original publishers preferred "Mobile Armored Riot Police". He had chosen "Ghost in the Shell" in homage to Arthur Koestler's "The Ghost in the Machine", from which he also drew inspiration.

Primarily set in the mid-twenty-first century in the fictional Japanese city of , otherwise known as , the manga and the many anime adaptations follow the members of Public Security Section 9, a special-operations task-force made up of former military officers and police detectives. Political intrigue and counter-terrorism operations are standard fare for Section 9, but the various actions of corrupt officials, companies, and cyber-criminals in each scenario are unique and require the diverse skills of Section 9's staff to prevent a series of incidents from escalating.
In this post-cyberpunk iteration of a possible future, computer technology has advanced to the point that many members of the public possess cyberbrains, technology that allows them to interface their biological brain with various networks. The level of cyberization varies from simple minimal interfaces to almost complete replacement of the brain with cybernetic parts, in cases of severe trauma. This can also be combined with various levels of prostheses, with a fully prosthetic body enabling a person to become a cyborg. The main character of "Ghost in the Shell", Major Motoko Kusanagi, is such a cyborg, having had a terrible accident befall her as a child that ultimately required her to use a full-body prosthesis to house her cyberbrain. This high level of cyberization, however, opens the brain up to attacks from highly skilled hackers, with the most dangerous being those who will hack a person to bend to their whims.

The original "Ghost in the Shell" manga ran in Japan from April 1989 to November 1990 in Kodansha's manga anthology "Young Magazine", and was released in a "tankōbon" volume on October 5, 1991. "Ghost in the Shell 2: Man-Machine Interface" followed 1997 for 9 issues in "Young Magazine", and was collected in the "Ghost in the Shell: Solid Box" on December 1, 2000. Four stories from "Man-Machine Interface" that were not released in tankobon format from previous releases were later collected in "Ghost in the Shell 1.5: Human-Error Processor", and was published by Kodansha on July 23, 2003. Several art books have also been published for the manga.

Two animated films based on the original manga have been released, both directed by Mamoru Oshii and animated by Production I.G. "Ghost in the Shell" was released in 1995 and follows the "Puppet Master" storyline from the manga. It was re-released in 2008 as "Ghost in the Shell 2.0" with new audio and updated 3D computer graphics in certain scenes. "Innocence", otherwise known as "", was released in 2004, with its story based on a chapter from the first manga.

On September 5, 2014, it was revealed by Production I.G. that a new "Ghost in the Shell" animated film, in Japanese, would be released in 2015 promising to show the "further evolution [of the series]". On January 8, 2015, a short teaser trailer was revealed for the project unveiling a redesigned Major more closely resembling her appearance from the older films, and a plot following the Arise continuity of the franchise. The trailer listed Kazuya Nomura as the director, Kazuchika Kise as the general director and character designer, Toru Okubo as the animation director, Tow Ubukata as the screenplay writer and Cornelius as the composer. The film premiered on June 20, 2015, in Japanese theaters.

In 2008, DreamWorks and producer Steven Spielberg acquired the rights to a live-action film adaptation of the original "Ghost in the Shell" manga. On January 24, 2014, Rupert Sanders was announced as director, with a screenplay by William Wheeler. In April 2016, the full cast was announced, which included Juliette Binoche, Chin Han, Lasarus Ratuere and Kaori Momoi, and Scarlett Johansson in the lead role; the casting of Johansson drew accusations of whitewashing. Principal photography on the film began on location in Wellington, New Zealand, on February 1, 2016. Filming wrapped in June 2016. "Ghost in the Shell" premiered in Tokyo on March 16, 2017, and was released in the United States on March 31, 2017, in 2D, 3D and IMAX 3D. It received mixed reviews, with praise for its visuals and Johansson's performance but criticism for its script.

In 2002, "Ghost in the Shell: Stand Alone Complex" premiered on Animax, presenting a new telling of "Ghost in the Shell" independent from the original manga, focusing on Section 9's investigation of the Laughing Man hacker. It was followed in 2004 by a second season titled "Ghost in the Shell: S.A.C. 2nd GIG", which focused on the Individual Eleven terrorist group. The primary storylines of both seasons were compressed into OVAs broadcast as "Ghost in the Shell: Stand Alone Complex The Laughing Man" in 2005 and "Ghost in the Shell: Stand Alone Complex Individual Eleven" in 2006. Also in 2006, "", featuring Section 9's confrontation with a hacker known as the Puppeteer, was broadcast, serving as a finale to the anime series. for the series and its films was composed by Yoko Kanno.

In addition to the anime, a series of published books, two separate manga adaptations, and several video games for consoles and mobile phones have been released for "Stand Alone Complex".

In 2013, a new iteration of the series titled "" premiered, tackling an original look at the "Ghost in the Shell" world, set before the original manga. It was released as a series of four original video animation (OVA) episodes (with limited theatrical releases) from 2013 to 2014, then recompiled as a 10-episode television series under the title of "Kōkaku Kidōtai: Arise - Alternative Architecture". An additional fifth OVA titled "Pyrophoric Cult", originally premiering in the "Alternative Architecture" broadcast as two original episodes, was released on August 26, 2015. Kazuchika Kise served as the chief director of the series, with Tow Ubukata as head writer. Cornelius was brought onto the project to compose the score for the series, with the Major's new voice actress Maaya Sakamoto also providing vocals for certain tracks.

"Ghost in the Shell: The New Movie", also known as "Ghost in the Shell: Arise − The Movie" or "New Ghost in the Shell", is a 2015 film directed by Kazuya Nomura that serves as a finale to the "Ghost in the Shell: Arise" story arc. The film is a continuation to the plot of the "Pyrophoric Cult" episode of "Arise", and ties up loose ends from that arc.

A manga adaptation was serialized in Kodansha's "Young Magazine", which started on March 13 and ended on August 26, 2013.

"Ghost in the Shell" was developed by Exact and released for the PlayStation on July 17, 1997, in Japan by Sony Computer Entertainment. It is a third-person shooter featuring an original storyline where the character plays a rookie member of Section 9. The video game's soundtrack "Megatech Body" features various electronica artists.

Several video games were also developed to tie into the "Stand Alone Complex" television series, in addition to a first-person shooter by Nexon and Neople titled "", released in 2016.

Bungie's famous 2001 third-person adventure game "Oni" draws substantial inspiration from "Ghost in the Shell" setting and characters.

Kodansha and Production I.G announced on April 7, 2017 that Kenji Kamiyama and Shinji Aramaki will be co-directing a new "Kōkaku Kidōtai" anime production. Its format and release date haven't been announced yet.




</doc>
<doc id="12916" url="https://en.wikipedia.org/wiki?curid=12916" title="Gauss–Legendre algorithm">
Gauss–Legendre algorithm

The Gauss–Legendre algorithm is an algorithm to compute the digits of π. It is notable for being rapidly convergent, with only 25 iterations producing 45 million correct digits of π. However, the drawback is that it is computer memory-intensive and therefore sometimes Machin-like formulas are used instead.

The method is based on the individual work of Carl Friedrich Gauss (1777–1855) and Adrien-Marie Legendre (1752–1833) combined with modern algorithms for multiplication and square roots. It repeatedly replaces two numbers by their arithmetic and geometric mean, in order to approximate their arithmetic-geometric mean.

The version presented below is also known as the Gauss–Euler, Brent–Salamin (or Salamin–Brent) algorithm; it was independently discovered in 1975 by Richard Brent and Eugene Salamin. It was used to compute the first 206,158,430,000 decimal digits of π on September 18 to 20, 1999, and the results were checked with Borwein's algorithm.


The first three iterations give (approximations given up to and including the first incorrect digit):

The algorithm has quadratic convergence, which essentially means that the number of correct digits doubles with each iteration of the algorithm.

The arithmetic–geometric mean of two numbers, a and b, is found by calculating the limit of the sequences

which both converge to the same limit.
If formula_10 and formula_11 then the limit is formula_12 where formula_13 is the complete elliptic integral of the first kind

If formula_15, formula_16. then

where formula_18 is the complete elliptic integral of the second kind:

Gauss knew of both of these results.

For formula_20 and formula_21 such that formula_22 Legendre proved the identity:

The values formula_24 can be substituted into Legendre’s identity and the approximations to K, E can be found by terms in the sequences for the arithmetic geometric mean with formula_10 and formula_26.




</doc>
<doc id="12917" url="https://en.wikipedia.org/wiki?curid=12917" title="Great Internet Mersenne Prime Search">
Great Internet Mersenne Prime Search

The Great Internet Mersenne Prime Search (GIMPS) is a collaborative project of volunteers who use freely available software to search for Mersenne prime numbers.

The GIMPS project was founded by George Woltman, who also wrote the software Prime95 and MPrime for the project. Scott Kurowski wrote the PrimeNet Internet server that supports the research to demonstrate Entropia-distributed computing software, a company he founded in 1997. GIMPS is registered as Mersenne Research, Inc. Kurowski is Executive Vice President and board director of Mersenne Research Inc. GIMPS is said to be one of the first large scale distributed computing projects over the Internet for research purposes.

The project has found a total of sixteen Mersenne primes , fourteen of which were the largest known prime number at their respective times of discovery. The largest known prime is 2 − 1 (or M in short). This prime was discovered on December 26, 2017 by Jonathan Pace.

To perform its testing, the project relies primarily on Lucas–Lehmer primality test, an algorithm that is both specialized to testing Mersenne primes and particularly efficient on binary computer architectures. They also have a trial division phase, used to rapidly eliminate Mersenne numbers with small factors which make up a large proportion of candidates. Pollard's p - 1 algorithm is also used to search for larger factors.

The project began in early January 1996, with a program that ran on i386 computers.
The name for the project was coined by Luther Welsh, one of its earlier searchers and the co-discoverer of the 29th Mersenne prime.
Within a few months, several dozen people had joined, and over a thousand by the end of the first year.
Joel Armengaud, a participant, discovered the primality of M on November 13, 1996.

, GIMPS has a sustained aggregate throughput of approximately 324 TeraFLOPS (or TFLOPS). In November 2012, GIMPS maintained 95 TFLOPS, theoretically earning the GIMPS virtual computer a place among the TOP500 most powerful known computer systems in the world. Also theoretically, in November 2012, the GIMPS held a rank of 330 in the TOP500. The preceding place was then held by an 'HP Cluster Platform 3000 BL460c G7' of Hewlett-Packard. As of November 2014 TOP500 results, these old GIMPS numbers would no longer make the list.

Previously, this was approximately 50 TFLOPS in early 2010, 30 TFLOPS in mid-2008, 20 TFLOPS in mid-2006, and 14 TFLOPS in early 2004.

Although the GIMPS software's source code is publicly available, technically it is not free software, since it has a restriction that users must abide by the project's distribution terms.
Specifically, if the software is used to discover a prime number with at least 100,000,000 decimal digits, the user will only win $50,000 of the $150,000 prize offered by the Electronic Frontier Foundation.

Third-party programs for testing Mersenne numbers, such as Mlucas and Glucas (for non-x86 systems), do not have this restriction.

GIMPS also "reserves the right to change this EULA without notice and with reasonable retroactive effect".""

All Mersenne primes are in the form M, where "p" is the (prime) exponent. The prime number itself is so the smallest prime number in this table is 

M is the rank of the Mersenne prime based on its exponent.

Whenever a possible prime is reported to the server, it is verified first before it is announced. The importance of this was illustrated in 2003, when a false positive was reported to possibly be the 40th Mersenne prime but verification failed.




</doc>
<doc id="12919" url="https://en.wikipedia.org/wiki?curid=12919" title="Game.com">
Game.com

The Game.com is a handheld game console released by Tiger Electronics in August 1997. It features a touchscreen and stylus. The first version of the Game.com includes two slots for game cartridges and can be connected to a 14.4 kbit/s modem, making it the first handheld to include Internet connectivity, hence its name referencing the top level domain .com. A smaller version, the Game.com Pocket Pro, was released in September 1998, followed by a third version known as the Game.com Pocket.

Prior to its release, Tiger Electronics stated that their Game.com system would "change the gaming world as we know it," while a spokesperson stated that it would be "one of this summer's hits." The Game.com was developed to compete directly against Nintendo's portable Game Boy console. Dennis Lynch of the "Chicago Tribune" considered the Game.com to be the "most interesting hand-held device" on display at the 1997 Electronic Entertainment Expo, describing it as a "sort of Game Boy for adults".
The Game.com was released in the United States in August 1997, with a price of $69.95, while an Internet-access cartridge was scheduled for release in the autumn. "Lights Out" was included with the console as a pack-in game and Solitaire was built into the handheld itself. By late September 1997, several games had been released for the console: "Henry", "Indy 500", "Quiz Wiz", "Wheel of Fortune", and "Williams Arcade Classics".
The Game.com featured a design similar to Sega's Game Gear console, with a screen larger than the Game Boy's. The Game.com included a phone directory, a calculator, and a calendar, and had an older target audience. The device was powered by four AA batteries. Tiger produced equivalents to many Game Boy peripherals, such as the "compete.com" serial cable, allowing players to connect their consoles to play multiplayer games or exchange high scores. Branded items such as an AC adapter, earphones, and a carry-case were also made available.

The Game.com's monochrome touchscreen measures approximately one and a half inches by two inches, and is divided into square zones that are imprinted onto the screen itself, to aid players in determining where to apply the stylus. The Game.com touchscreen also had a fairly low sensor resolution along with no back-light, so it lacked precision and made it hard to see the on-screen controls. As with most portable devices in the 1990s, data storage was entirely dependent on a backup battery, and its failure would erase high scores or any other information stored within the console.

While Tiger was able to obtain game licenses like "Wheel of Fortune", "Sonic the Hedgehog", "Mortal Kombat", "Duke Nukem", and "Resident Evil", none of these games sold in great numbers. Development of games, including licensed ones, was done in-house, with the exception of "Centipede". Software development kits were not known to be widely available, and third-party development (essential to the success of most gaming systems) was absent.

Tiger released the Game.com Pocket Pro in September 1998. This was a smaller version of the game.com which had the same specifications as the original except that it had a single cartridge slot, and required only two AA batteries. The Game.com Pocket Pro featured improved screen quality over its predecessor. The initial version of the Pocket Pro featured a front-lit screen (although it was advertised as being back-lit) and is distinguished by its rough-textured black case. A subsequent re-release, dubbed as the Game.com Pocket, omitted the front light and came in four translucent colors: green, blue, pink, and purple.

Both re-releases shared very limited success, and the handheld would be canceled in 2000, along with its exclusive Internet service. Most of the console's problems were due to a small line-up of only 20 games, poor quality of some games, lack of third-party support, poor distribution and marketing. Unlike the original Game Boy, its screen suffered from very slow screen updates (known as ""ghosting""), which makes fast moving objects blur and particularly hurt the fast-moving games Tiger sought licenses for.

Although the Game.com was a commercial failure, similar features were later used successfully in the Nintendo DS. The Game.com was the first to include basic PDA-functions and the first handheld to allow Internet access.

To access the Internet, users had to connect an external dial-up modem to the Game.com via a serial cable and dial into a game.com-exclusive internet service provider (ISP). From there, users could upload saved high scores to the game.com database (games do not feature online play functionalities), or check e-mail and access the web (text-only) if they had the Internet cartridge (sold separately from the modem).

The game.com also supported other ISPs, although set up was a matter of trial-and-error. Both Tiger's now-defunct website and the included manual gave incorrect instructions for setting up a game.com for Internet access. Later, single-cartridge re-releases of the game.com could not access the web nor check e-mail. 

There were 20 games released for the game.com.

Released: 

Cancelled:

The Game.com sold fewer than 300,000 units, and was ranked as third on "GamePro"'s 2007 list of the 10 worst-selling handheld game consoles. At the time of its release in 1997, Chip and Jonathan Carter wrote that the console did not play action games as well as it did with other games, although they praised the console's various options and wrote, "Graphically, we'd have to say this has the potential to perform better than Game Boy. As for sound, game.com delivers better than any other hand-held on the market."

Steven L. Kent, writing for the "Chicago Tribune", wrote a year later that the console had an elegant design, as well as better sound and a higher-definition screen than the Game Boy: "Elegant design, however, has not translated into ideal game play. Though Tiger has produced fighting, racing and shooting games for game.com, the games have noticeably slow frame rates. The racing game looks like a flickering silent picture show." In 2004, Kent included the modem and "some PDA functionality" as the console's strengths, while listing its "Slow processor" and "lackluster library of games" as weaknesses.

Brett Alan Weiss of AllGame wrote, "The game.com, the little system that (almost) could, constantly amazes me with the strength and scope of its sound effects. [...] It's astounding what power comes out of such a tiny little speaker." Cameron Davis of VideoGames.com wrote, "Sure, this is no Game Boy Color-killer, but the Game.Com was never meant to be. To deride it by comparing it with more powerful and established formats would be a bit unfair". Davis also wrote, "The touch screen is pretty sensitive, but it works well - you won't need more than a few seconds to get used to it." However, he criticized the screen's squared zones: "more often than not it proves distracting when you are playing games that don't require it."

In 2009, "PC World" ranked the Game.com at number nine on its list of the 10 worst video game systems ever released, criticizing its Internet aspect, its game library, its low-resolution touchscreen, and its "Silly name that attempted to capitalize on Internet mania." However, "PC World" positively noted its "primitive" PDA features and its solitaire game, considered by the magazine to be the system's best game. In 2016, Motherboard stated that the Game.com was "perhaps one of the worst consoles of all time," due largely to its low screen quality.



</doc>
<doc id="12920" url="https://en.wikipedia.org/wiki?curid=12920" title="General Packet Radio Service">
General Packet Radio Service

General Packet Radio Service (GPRS) is a packet oriented mobile data standard on the 2G and 3G cellular communication network's global system for mobile communications (GSM). GPRS was established by European Telecommunications Standards Institute (ETSI) in response to the earlier CDPD and i-mode packet-switched cellular technologies. It is now maintained by the 3rd Generation Partnership Project (3GPP).

GPRS is typically sold according to the total volume of data transferred during the billing cycle, in contrast with circuit switched data, which is usually billed per minute of connection time, or sometimes by one-third minute increments. Usage above the GPRS bundled data cap may be charged per Mb of data, speed limited, or disallowed.

GPRS is a best-effort service, implying variable throughput and latency that depend on the number of other users sharing the service concurrently, as opposed to circuit switching, where a certain quality of service (QoS) is guaranteed during the connection. In 2G systems, GPRS provides data rates of 56–114 kbit/sec. 2G cellular technology combined with GPRS is sometimes described as "2.5G", that is, a technology between the second (2G) and third (3G) generations of mobile telephony. It provides moderate-speed data transfer, by using unused time division multiple access (TDMA) channels in, for example, the GSM system. GPRS is integrated into GSM Release 97 and newer releases.

The GPRS core network allows 2G, 3G and WCDMA mobile networks to transmit IP packets to external networks such as the Internet. The GPRS system is an integrated part of the GSM network switching subsystem.

GPRS extends the GSM Packet circuit switched data capabilities and makes the following services possible:

If SMS over GPRS is used, an SMS transmission speed of about 30 SMS messages per minute may be achieved. This is much faster than using the ordinary SMS over GSM, whose SMS transmission speed is about 6 to 10 SMS messages per minute.

GPRS supports the following protocols:

When TCP/IP is used, each phone can have one or more IP addresses allocated. GPRS will store and forward the IP packets to the phone even during handover. The TCP restores any packets lost (e.g. due to a radio noise induced pause).

Devices supporting GPRS are grouped into three classes:

Because a Class A device must service GPRS and GSM networks together, it effectively needs two radios. To avoid this hardware requirement, a GPRS mobile device may implement the dual transfer mode (DTM) feature. A DTM-capable mobile can handle both GSM packets and GPRS packets with network coordination to ensure both types are not transmitted at the same time. Such devices are considered pseudo-Class A, sometimes referred to as "simple class A". Some networks have supported DTM since 2007.
USB 3G/GPRS modems have a terminal-like interface over USB with V.42bis, and RFC 1144 data formats. Some models include an external antenna connector. Modem cards for laptop PCs, or external USB modems are available, similar in shape and size to a computer mouse, or a pendrive.

A GPRS connection is established by reference to its access point name (APN). The APN defines the services such as wireless application protocol (WAP)
access, short message service (SMS), multimedia messaging service (MMS), and for Internet communication services such as email and World Wide Web access.

In order to set up a GPRS connection for a wireless modem, a user must specify an APN, optionally a user name and password, and very rarely an IP address, provided by the network operator.

GSM module or GPRS modules are similar to modems, but there’s one difference: the modem is an external piece of equipment, whereas the GSM module or GPRS module can be integrated within an electrical or electronic equipment. It is an embedded piece of hardware. A GSM mobile, on the other hand, is a complete embedded system in itself. It comes with embedded processors dedicated to provide a functional interface between the user and the mobile network.

The upload and download speeds that can be achieved in GPRS depend on a number of factors such as:


The multiple access methods used in GSM with GPRS are based on frequency division duplex (FDD) and TDMA. During a session, a user is assigned to one pair of up-link and down-link frequency channels. This is combined with time domain statistical multiplexing which makes it possible for several users to share the same frequency channel. The packets have constant length, corresponding to a GSM time slot. The down-link uses first-come first-served packet scheduling, while the up-link uses a scheme very similar to reservation ALOHA (R-ALOHA). This means that slotted ALOHA (S-ALOHA) is used for reservation inquiries during a contention phase, and then the actual data is transferred using dynamic TDMA with first-come first-served.

The channel encoding process in GPRS consists of two steps: first, a cyclic code is used to add parity bits, which are also referred to as the Block Check Sequence, followed by coding with a possibly punctured convolutional code. The Coding Schemes CS-1 to CS-4 specify the number of parity bits generated by the cyclic code and the puncturing rate of the convolutional code. In Coding Schemes CS-1 through CS-3, the convolutional code is of rate 1/2, i.e. each input bit is converted into two coded bits. In Coding Schemes CS-2 and CS-3, the output of the convolutional code is punctured to achieve the desired code rate. In Coding Scheme CS-4, no convolutional coding is applied. The following table summarises the options.

The least robust, but fastest, coding scheme (CS-4) is available near a base transceiver station (BTS), while the most robust coding scheme (CS-1) is used when the mobile station (MS) is further away from a BTS.

Using the CS-4 it is possible to achieve a user speed of 20.0 kbit/s per time slot. However, using this scheme the cell coverage is 25% of normal. CS-1 can achieve a user speed of only 8.0 kbit/s per time slot, but has 98% of normal coverage. Newer network equipment can adapt the transfer speed automatically depending on the mobile location.

In addition to GPRS, there are two other GSM technologies which deliver data services: circuit-switched data (CSD) and high-speed circuit-switched data (HSCSD). In contrast to the shared nature of GPRS, these instead establish a dedicated circuit (usually billed per minute). Some applications such as video calling may prefer HSCSD, especially when there is a continuous flow of data between the endpoints.

The following table summarises some possible configurations of GPRS and circuit switched data services.

The multislot class determines the speed of data transfer available in the Uplink and Downlink directions. It is a value between 1 and 45 which the network uses to allocate radio channels in the uplink and downlink direction. Multislot class with values greater than 31 are referred to as high multislot classes.

A multislot allocation is represented as, for example, 5+2. The first number is the number of downlink timeslots and the second is the number of uplink timeslots allocated for use by the mobile station. A commonly used value is class 10 for many GPRS/EGPRS mobiles which uses a maximum of 4 timeslots in downlink direction and 2 timeslots in uplink direction. However simultaneously a maximum number of 5 simultaneous timeslots can be used in both uplink and downlink. The network will automatically configure for either 3+2 or 4+1 operation depending on the nature of data transfer.

Some high end mobiles, usually also supporting UMTS, also support GPRS/EDGE multislot class 32. According to 3GPP TS 45.002 (Release 12), Table B.1, mobile stations of this class support 5 timeslots in downlink and 3 timeslots in uplink with a maximum number of 6 simultaneously used timeslots. If data traffic is concentrated in downlink direction the network will configure the connection for 5+1 operation. When more data is transferred in the uplink the network can at any time change the constellation to 4+2 or 3+3. Under the best reception conditions, i.e. when the best EDGE modulation and coding scheme can be used, 5 timeslots can carry a bandwidth of 5*59.2 kbit/s = 296 kbit/s. In uplink direction, 3 timeslots can carry a bandwidth of 3*59.2 kbit/s = 177.6 kbit/s.

Each multislot class identifies the following:

The different multislot class specification is detailed in the Annex B of the 3GPP Technical Specification 45.002 (Multiplexing and multiple access on the radio path)

The maximum speed of a GPRS connection offered in 2003 was similar to a modem connection in an analog wire telephone network, about 32–40 kbit/s, depending on the phone used. Latency is very high; round-trip time (RTT) is typically about 600–700 ms and often reaches 1s. GPRS is typically prioritized lower than speech, and thus the quality of connection varies greatly.

Devices with latency/RTT improvements (via, for example, the extended UL TBF mode feature) are generally available. Also, network upgrades of features are available with certain operators. With these enhancements the active round-trip time can be reduced, resulting in significant increase in application-level throughput speeds.

GPRS opened in 2000 as a packet-switched data service embedded to the channel-switched cellular radio network GSM. GPRS extends the reach of the fixed Internet by connecting mobile terminals worldwide.

The CELLPAC protocol developed 1991-1993 was the trigger point for starting in 1993 specification of standard GPRS by ETSI SMG. Especially, the CELLPAC Voice & Data functions introduced in a 1993 ETSI Workshop contribution anticipate what was later known to be the roots of GPRS. This workshop contribution is referenced in 22 GPRS related US-Patents. Successor systems to GSM/GPRS like W-CDMA (UMTS) and LTE rely on key GPRS functions for mobile Internet access as introduced by CELLPAC.

According to a study on history of GPRS development Bernhard Walke and his student Peter Decker are the inventors of GPRS – the first system providing worldwide mobile Internet access.




</doc>
<doc id="12922" url="https://en.wikipedia.org/wiki?curid=12922" title="Gnosis">
Gnosis

Gnosis is the common Greek noun for knowledge (γνῶσις, "gnōsis", f.). The term is used in various Hellenistic religions and philosophies. It is best known from Gnosticism, where it signifies a knowledge or insight into humanity’s real nature as divine, leading to the deliverance of the divine spark within humanity from the constraints of earthly existence.

"Gnosis" is a feminine Greek noun which means "knowledge". It is often used for personal knowledge compared with intellectual knowledge (εἶδειν "eídein"), as with the French "connaître" compared with "savoir", the Spanish "conocer" compared with "saber," or the German "kennen" rather than "wissen".

A related term is the adjective "gnostikos", "cognitive", a reasonably common adjective in Classical Greek. Plato uses the plural adjective γνωστικοί – "gnostikoi" and the singular feminine adjective γνωστικὴ ἐπιστήμη – "gnostike episteme" in his "Politikos" where "Gnostike episteme" was also used to indicate one's aptitude. The terms do not appear to indicate any mystic, esoteric or hidden meaning in the works of Plato, but instead expressed a sort of higher intelligence and ability analogous to talent.

In the Hellenistic era the term became associated with the mystery cults.

"Gnosis" is used throughout Greek philosophy as a technical term for experience knowledge (see gnosiology) in contrast to theoretical knowledge or epistemology. The term is also related to the study of knowledge retention or memory (see also cognition), in relation to ontic or ontological, which is how something actually is rather than how something is captured (abstraction) and stored (memory) in the mind.

Irenaeus used the phrase "knowledge falsely so-called" ("pseudonymos gnosis", from 1 Timothy 6:20) for the title of his book "On the Detection and Overthrow of False Knowledge", that contains the adjective "gnostikos", which is the source for the 17th-century English term "Gnosticism".

The Greek word "gnosis" (knowledge) is a standard translation of the Hebrew word "knowledge" (דעת da'ath) in the Septuagint, thus:

Philo also refers to the "knowledge" ("gnosis") and "wisdom" ("sophia") of God.

Paul distinguishes "knowledge" ("gnosis") and "knowledge falsely so-called" ("pseudonymos gnosis").

The fathers of early Christianity used the word "knowledge" ("gnosis") in the New Testament to mean spiritual knowledge or specific knowledge of the divine. This positive usage was to contrast it with how gnostic sectarians used the word. This positive use carried over from Hellenic philosophy into Greek Orthodoxy as a critical characteristic of ascetic practices, through St. Clement of Alexandria, Irenaeus, Hippolytus of Rome, Hegesippus, and Origen.

Cardiognosis ("knowledge of the heart") from Eastern Christianity related to the tradition of the staretz and in Roman Catholic theology is the view that only God knows the condition of one's relationship with God.

Gnosis in Orthodox Christian (primarily Eastern Orthodox) thought is the spiritual knowledge of a saint (one who has obtained theosis) or mystically enlightened human being. Within the cultures of the term's provenance (Byzantine and Hellenic) "Gnosis" was a knowledge or insight into the infinite, divine and uncreated in all and above all, rather than knowledge strictly into the finite, natural or material world. Gnosis is transcendental as well as mature understanding. It indicates direct spiritual, experiential knowledge and intuitive knowledge, mystic rather than that from rational or reasoned thinking. Gnosis itself is gained through understanding at which one can arrive via inner experience or contemplation such as an internal epiphany of intuition and external epiphany such as the Theophany.

In the "Philokalia," it is emphasized that such knowledge is not secret knowledge but rather a maturing, transcendent form of knowledge derived from contemplation ("theoria" resulting from practice of "hesychasm)", since knowledge cannot truly be derived from knowledge, but rather, knowledge can only be derived from "theoria" (to witness, see (vision) or experience). Knowledge, thus plays an important role in relation to "theosis" (deification/personal relationship with God) and "theoria" (revelation of the divine, vision of God). Gnosis, as the proper use of the spiritual or noetic faculty plays an important role in Orthodox Christian theology. Its importance in the economy of salvation is discussed periodically in the "Philokalia" where as direct, personal knowledge of God (noesis; see also Noema) it is distinguished from ordinary epistemological knowledge (episteme—i.e., speculative philosophy).

Gnosticism originated in the late first century CE in nonrabbinical Jewish sects and early Christian sects. In the formation of Christianity, various sectarian groups, labeled "gnostics" by their opponents, emphasised spiritual knowledge ("gnosis") of the Divine spark within, over faith ("pistis") in the teachings and traditions of the various communities of Christians. Gnosticism presents a distinction between the highest, unknowable God, and the demiurge “creator” of the material. The Gnostics considered the most essential part of the process of salvation to be this personal knowledge, in contrast to faith as an outlook in their world view along with faith in the ecclesiastical authority. They were regarded as heretics by the Fathers of the early church.



</doc>
<doc id="12923" url="https://en.wikipedia.org/wiki?curid=12923" title="Georgian">
Georgian

Georgian may refer to:









</doc>
<doc id="12924" url="https://en.wikipedia.org/wiki?curid=12924" title="Georgian architecture">
Georgian architecture

Georgian architecture is the name given in most English-speaking countries to the set of architectural styles current between 1714 and 1830. It is eponymous for the first four British monarchs of the House of Hanover—George I, George II, George III, and George IV—who reigned in continuous succession from August 1714 to June 1830. The style was revived in the late 19th century in the United States as Colonial Revival architecture and in the early 20th century in Great Britain as Neo-Georgian architecture; in both it is also called Georgian Revival architecture. In the United States the term "Georgian" is generally used to describe all buildings from the period, regardless of style; in Britain it is generally restricted to buildings that are "architectural in intention", and have stylistic characteristics that are typical of the period, though that covers a wide range. 

The Georgian style is highly variable, but marked by symmetry and proportion based on the classical architecture of Greece and Rome, as revived in Renaissance architecture. Ornament is also normally in the classical tradition, but typically restrained, and sometimes almost completely absent on the exterior. The period brought the vocabulary of classical architecture to smaller and more modest buildings than had been the case before, replacing English vernacular architecture (or becoming the new vernacular style) for almost all new middle-class homes and public buildings by the end of the period.

Georgian architecture is characterized by its proportion and balance; simple mathematical ratios were used to determine the height of a window in relation to its width or the shape of a room as a double cube. Regularity, as with ashlar (uniformly cut) stonework, was strongly approved, imbuing symmetry and adherence to classical rules: the lack of symmetry, where Georgian additions were added to earlier structures remaining visible, was deeply felt as a flaw, at least before Nash began to introduce it in a variety of styles. Regularity of housefronts along a street was a desirable feature of Georgian town planning. Until the start of the Gothic Revival in the early 19th century, Georgian designs usually lay within the Classical orders of architecture and employed a decorative vocabulary derived from ancient Rome or Greece.

In towns, which expanded greatly during the period, landowners turned into property developers, and rows of identical terraced houses became the norm. Even the wealthy were persuaded to live in these in town, especially if provided with a square of garden in front of the house. There was an enormous amount of building in the period, all over the English-speaking world, and the standards of construction were generally high. Where they have not been demolished, large numbers of Georgian buildings have survived two centuries or more, and they still form large parts of the core of cities such as London, Edinburgh, Dublin, Newcastle upon Tyne and Bristol.

The period saw the growth of a distinct and trained architectural profession; before the mid-century "the high-sounding title, 'architect' was adopted by anyone who could get away with it". This contrasted with earlier styles, which were primarily disseminated among craftsmen through the direct experience of the apprenticeship system. But most buildings were still designed by builders and landlords together, and the wide spread of Georgian architecture, and the Georgian styles of design more generally, came from dissemination through pattern books and inexpensive suites of engravings. Authors such as the prolific William Halfpenny (active 1723–1755) published editions in America as well as Britain. 

A similar phenomenon can be seen in the commonality of housing designs in Canada and the United States (though of a wider variety of styles) from the 19th century down to the 1950s, using pattern books drawn up by professional architects that were distributed by lumber companies and hardware stores to contractors and homebuilders. 
From the mid-18th century, Georgian styles were assimilated into an architectural vernacular that became part and parcel of the training of every architect, designer, builder, carpenter, mason and plasterer, from Edinburgh to Maryland.

Georgian succeeded the English Baroque of Sir Christopher Wren, Sir John Vanbrugh, Thomas Archer, William Talman, and Nicholas Hawksmoor; this in fact continued into at least the 1720s, overlapping with a more restrained Georgian style. The architect James Gibbs was a transitional figure, his earlier buildings are Baroque, reflecting the time he spent in Rome in the early 18th century, but he adjusted his style after 1720. Major architects to promote the change in direction from baroque were Colen Campbell, author of the influential book "Vitruvius Britannicus" (1715–1725); Richard Boyle, 3rd Earl of Burlington and his protégé William Kent; Isaac Ware; Henry Flitcroft and the Venetian Giacomo Leoni, who spent most of his career in England. 
Other prominent architects of the early Georgian period include James Paine, Robert Taylor, and John Wood, the Elder. The European Grand Tour became very common for wealthy patrons in the period, and Italian influence remained dominant, though at the start of the period Hanover Square, Westminster (1713 on), developed and occupied by Whig supporters of the new dynasty, seems to have deliberately adopted German stylisic elements in their honour, especially vertical bands connecting the windows.

The styles that resulted fall within several categories. In the mainstream of Georgian style were both Palladian architecture—and its whimsical alternatives, Gothic and Chinoiserie, which were the English-speaking world's equivalent of European Rococo. From the mid-1760s a range of Neoclassical modes were fashionable, associated with the British architects Robert Adam, James Gibbs, Sir William Chambers, James Wyatt, George Dance the Younger, Henry Holland and Sir John Soane. John Nash was one of the most prolific architects of the late Georgian era known as The Regency style, he was responsible for designing large areas of London. Greek Revival architecture was added to the repertory, beginning around 1750, but increasing in popularity after 1800. Leading exponents were William Wilkins and Robert Smirke. 

In Britain brick or stone are almost invariably used; brick is often disguised with stucco. In America and other colonies wood remained very common, as its availability and cost-ratio with the other materials was more favourable. Raked roofs were mostly covered in earthenware tiles until Richard Pennant, 1st Baron Penrhyn led the development of the slate industry in Wales from the 1760s, which by the end of the century had become the usual material.

Versions of revived Palladian architecture dominated English country house architecture. Houses were increasingly placed in grand landscaped settings, and large houses were generally made wide and relatively shallow, largely to look more impressive from a distance. The height was usually highest in the centre, and the Baroque emphasis on corner pavilions often found on the continent generally avoided. In grand houses, an entrance hall led to steps up to a "piano nobile" or mezzanine floor where the main reception rooms were. Typically the basement area or "rustic", with kitchens, offices and service areas, as well as male guests with muddy boots, came some way above ground, and was lit by windows that were high on the inside, but just above ground level outside. A single block was typical, with perhaps a small court for carriages at the front marked off by railings and a gate, but rarely a stone gatehouse, or side wings around the court.

Windows in all types of buildings were large and regularly placed on a grid; this was partly to minimize window tax, which was in force throughout the period in the United Kingdom. Some windows were subsequently bricked-in. Their height increasingly varied between the floors, and they increasingly began below waist-height in the main rooms, making a small balcony desirable. Before this the internal plan and function of the rooms can generally not be deduced from the outside. To open these large windows the sash window, already developed by the 1670s, became very widespread. Corridor plans became universal inside larger houses.

Internal courtyards became more rare, except beside the stables, and the functional parts of the building were placed at the sides, or in separate buildings nearby hidden by trees. The views to and from the front and rear of the main block were concentrated on, with the side approaches usually much less important. The roof was typically invisible from the ground, though domes were sometimes visible in grander buildings. The roofline was generally clear of ornament except for a balustrade or the top of a pediment. Columns or pilasters, often topped by a pediment, were popular for ornament inside and out, and other ornament was generally geometrical or plant-based, rather than using the human figure. 
Inside ornament was far more generous, and could sometimes be overwhelming. The chimneypiece continued to be the usual main focus of rooms, and was now given a classical treatment, and increasingly topped by a painting or a mirror. Plasterwork ceilings, carved wood, and bold schemes of wallpaint formed a backdrop to increasingly rich collections of furniture, paintings, porcelain, mirrors, and objets d'art of all kinds. Wood-panelling, very common since about 1500, fell from favour around the mid-century, and wallpaper included very expensive imports from China.

Smaller houses in the country, such as vicarages, were simple regular blocks with visible raked roofs, and a central doorway, often the only ornamented area. Similar houses, often referred to as "villas" became common around the fringes of the larger cities, especially London, and detached houses in towns remained common, though only the very rich could afford them in central London.

In towns even most better-off people lived in terraced houses, which typically opened straight onto the street, often with a few steps up to the door. There was often an open space, protected by iron railings, dropping down to the basement level, with a discreet entrance down steps off the street for servants and deliveries; this is known as the "area". This meant that the ground floor front was now removed and protected from the street and encouraged the main reception rooms to move there from the floor above. Where, as often, a new street or set of streets was developed, the road and pavements were raised up, and the gardens or yards behind the houses at a lower level, usually representing the original one.

Town terraced houses for all social classes remained resolutely tall and narrow, each dwelling occupying the whole height of the building. This contrasted with well-off continental dwellings, which had already begun to be formed of wide apartments occupying only one or two floors of a building; such arrangements were only typical in England when housing groups of batchelors, as in Oxbridge colleges, the lawyers in the Inns of Court or The Albany after it was converted in 1802. In the period in question, only in Edinburgh were working-class purpose-built tenements common, though lodgers were common in other cities. A curving crescent, often looking out at gardens or a park, was popular for terraces where space allowed. In early and central schemes of development, plots were sold and built on individually, though there was often an attempt to enforce some uniformity, but as development reached further out schemes were increasingly built as a uniform scheme and then sold. 

The late Georgian period saw the birth of the semi-detached house, planned systematically, as a suburban compromise between the terraced houses of the city and the detached "villas" further out, where land was cheaper. There had been occasional examples in town centres going back to medieval times. Most early suburban examples are large, and in what are now the outer fringes of Central London, but were then in areas being built up for the first time. Blackheath, Chalk Farm and St John's Wood are among the areas contesting being the original home of the semi. Sir John Summerson gave primacy to the Eyre Estate of St John's Wood. A plan for this exists dated 1794, where "the whole development consists of "pairs of semi-detached houses", So far as I know, this is the first recorded scheme of the kind". In fact the French Wars put an end to this scheme, but when the development was finally built it retained the semi-detached form, "a revolution of striking significance and far-reaching effect".

Until the Church Building Act of 1818, the period saw relatively few churches built in Britain, which was already well-supplied, although in the later years of the period the demand for Non-conformist and Roman Catholic places of worship greatly increased. Anglican churches that were built were designed internally to allow maximum audibility, and visibility, for preaching, so the main nave was generally wider and shorter than in medieval plans, and often there were no side-aisles. Galleries were common in new churches. Especially in country parishes, the external appearance generally retained the familiar signifiers of a Gothic church, with a tower or spire, a large west front with one or more doors, and very large windows along the nave, but all with any ornament drawn from the classical vocabulary. Where funds permitted, a classical temple portico with columns and a pediment might be used at the west front. Decoration inside was very limited, but churches filled up with monuments to the prosperous.

In the colonies new churches were certainly required, and generally repeated similar formulae. British Non-conformist churches were often more classical in mood, and tended not to feel the need for a tower or steeple.

The archetypical Georgian church is St Martin-in-the-Fields in London (1720), by Gibbs, who boldly added to the classical temple façade at the west end a large steeple on top of a tower, set back slightly from the main frontage. This formula shocked purists and foreigners, but became accepted and was very widely copied, at home and in the colonies, for example at St Andrew's Church, Chennai in India.

The 1818 Act allocated some public money for new churches required to reflect changes in population, and a commission to allocate it. Building of Commissioners' churches gathered pace in the 1820s, and continued until the 1850s. The early churches, falling into the Georgian period, show a high proportion of Gothic Revival buildings, along with the classically inspired.

Public buildings generally varied between the extremes of plain boxes with grid windows and Italian Late Renaissance palaces, depending on budget. Somerset House in London, designed by Sir William Chambers in 1776 for government offices, was as magnificent as any country house, though never quite finished, as funds ran out. Barracks and other less prestigious buildings could be as functional as the mills and factories that were growing increasingly large by the end of the period. But as the period came to an end many commercial projects were becoming sufficiently large, and well-funded, to become "architectural in intention", rather than having their design left to the lesser class of "surveyors".

Georgian architecture was widely disseminated in the English colonies during the Georgian era. American buildings of the Georgian period were very often constructed of wood with clapboards; even columns were made of timber, framed up, and turned on an oversized lathe. At the start of the period the difficulties of obtaining and transporting brick or stone made them a common alternative only in the larger cities, or where they were obtainable locally. Dartmouth College, Harvard University, and the College of William and Mary, offer leading examples of Georgian architecture in the Americas.

Unlike the Baroque style that it replaced, which was mostly used for palaces and churches, and had little representation in the British colonies, simpler Georgian styles were widely used by the upper and middle classes. Perhaps the best remaining house is the pristine Hammond-Harwood House (1774) in Annapolis, Maryland, designed by the colonial architect William Buckland and modelled on the Villa Pisani at Montagnana, Italy as depicted in Andrea Palladio's "I quattro libri dell'architettura" ("Four Books of Architecture").

After independence, in the former American colonies, Federal style architecture represented the equivalent of Regency architecture, with which it had much in common.

After about 1840, Georgian conventions were slowly abandoned as a number of revival styles, including Gothic Revival, that had originated in the Georgian period, developed and contested in Victorian architecture, and in the case of Gothic became better researched, and closer to their originals. Neoclassical architecture remained popular, and was the opponent of Gothic in the Battle of the Styles of the early Victorian period. In the United States the Federalist Style contained many elements of Georgian style, but incorporated revolutionary symbols.

In the early decades of the twentieth century when there was a growing nostalgia for its sense of order, the style was revived and adapted and in the United States came to be known as the Colonial Revival. In Canada the United Empire Loyalists embraced Georgian architecture as a sign of their fealty to Britain, and the Georgian style was dominant in the country for most of the first half of the 19th century. The Grange, for example, a manor built in Toronto, was built in 1817. In Montreal, English born architect John Ostell worked on a significant number of remarkable constructions in the Georgian style such as the Old Montreal Custom House and the Grand séminaire de Montréal.

The revived Georgian style that emerged in Britain at the beginning of the 20th century is usually referred to as Neo-Georgian; the work of Edwin Lutyens includes many examples. Versions of the Neo-Georgian style were commonly used in Britain for certain types of urban architecture until the late 1950s, Bradshaw Gass & Hope's Police Headquarters in Salford of 1958 being a good example. In both the United States and Britain, the Georgian style is still employed by architects like Quinlan Terry Julian Bicknell and Fairfax and Sammons for private residences.





</doc>
<doc id="12926" url="https://en.wikipedia.org/wiki?curid=12926" title="Goshen, Indiana">
Goshen, Indiana

Goshen is a city in and the county seat of Elkhart County, Indiana, United States. It is the smaller of the two principal cities of the Elkhart-Goshen Metropolitan Statistical Area, which in turn is part of the South Bend-Elkhart-Mishawaka Combined Statistical Area. It is located in the northern part of Indiana near the Michigan border, in a region known as Michiana. Goshen is located 10 miles southeast of Elkhart, 25 miles southeast of South Bend, 120 miles east of Chicago, and 150 miles north of Indianapolis. The population was 31,719 at the 2010 census.

The city is known as a major recreational vehicle and accessories manufacturing center, the home of Goshen College, a small Mennonite liberal arts college, and the Elkhart County 4-H Fair, one of the largest county fairs in the United States.

Goshen was platted in 1831. It was named after the Land of Goshen. The initial settlers consisted entirely of old stock "Yankee" immigrants, who were descended from the English Puritans who settled New England in the 1600s. The New England Yankee population that founded towns such as Goshen considered themselves the "chosen people" and identified with the Israelites of the Old Testament and they thought of North America as their Canaan. They founded a large number of towns and counties across what is known as the Northern Tier of the upper midwest. It was in this context that Goshen was named.

The Yankee migration to Indiana was a result of several factors, one of which was the overpopulation of New England. The old stock Yankee population had large families, often bearing up to ten children in one household. Most people were expected to have their own piece of land to farm, and due to the massive and nonstop population boom, land in New England became scarce as every son claimed his own farmstead. As a result, there was not enough land for every family to have a self-sustaining farm, and Yankee settlers began leaving New England for the Midwestern United States.

They were aided in this effort by the construction and completion of the Erie Canal which made traveling to the region much easier, causing an additional surge in migrants coming from New England. Added to this was the end of the Black Hawk War, which made the region much safer to travel through and settle in for white settlers. These settlers were primarily members of the Congregational Church, though due to the Second Great Awakening, many of them had converted to Methodism, and some had become Baptists before coming to what is now Indiana. The Congregational Church has subsequently gone through many divisions, and some factions, including those in Goshen, are now known as the Church of Christ and the United Church of Christ. When the New Englanders arrived in what is now Elkhart County there was nothing but dense virgin forest and wild prairie. They laid out farms, constructed roads, erected government buildings and established post routes.
On Palm Sunday, April 11, 1965, a large outbreak of tornadoes struck the Midwest. The most famous pair of tornadoes devastated the Midway Trailer Park (now inside the city limits of Goshen), and the Sunnyside Housing Addition in Dunlap, Indiana, but a smaller F4 tornado also struck neighborhoods on the southeast side of Goshen on the same day. Statewide, 137 Hoosiers died in the storms—55 of them in Elkhart County. Days later, President Lyndon B. Johnson visited the Dunlap site.

The Goshen Historic district, added in 1983 to the National Registor of Historic Places is bounded by Pike, RR, Cottage, Plymouth, Main, Purl, the Canal, and Second Sts. with the Elkhart County Courthouse at its center.

In April 2006, Goshen was the site for an immigration march. Officials estimated that from 2000 to 3000 people marched from Linway Plaza to the County Courthouse.

For much of its history, Goshen was a "sundown town", forbidding African Americans from living in, or entering, the town, often under threat of violence. In March 2015, the city acknowledged this part of its past, apologizing and saying that it no longer condones such behavior.

The Elkhart County Courthouse, Fort Wayne Street Bridge, Goshen Carnegie Public Library, Goshen Historic District, William N. Violett House, and Violett-Martin House and Gardens are listed on the National Register of Historic Places.

Goshen is located at . The Elkhart River winds its way through the city and through a dam on the south side making the Goshen Dam Pond. Rock Run Creek also runs through town. The city is divided east/west by Main Street and north/south by Lincoln Avenue.

According to the United States Census Bureau, the city has a total area of , of which is land and is water.

As of the census of 2010, there were 31,719 people, 11,344 households, and 7,580 families residing in the city. The population density was . There were 12,631 housing units at an average density of . The racial makeup of the city was 78.2% White, 2.6% African American, 0.5% Native American, 1.2% Asian, 14.8% from other races, and 2.7% from two or more races. Hispanic or Latino of any race were 28.1% of the population.

There were 11,344 households of which 36.1% had children under the age of 18 living with them, 47.4% were married couples living together, 13.1% had a female householder with no husband present, 6.3% had a male householder with no wife present, and 33.2% were non-families. 27.4% of all households were made up of individuals and 13.2% had someone living alone who was 65 years of age or older. The average household size was 2.67 and the average family size was 3.23.

The median age in the city was 32.4 years. 27.4% of residents were under the age of 18; 11.3% were between the ages of 18 and 24; 26.1% were from 25 to 44; 20% were from 45 to 64; and 14.9% were 65 years of age or older. The gender makeup of the city was 48.9% male and 51.1% female.

As of the census of 2000, there were 29,383 people, 10,675 households, and 7,088 families residing in the city. The population density was 2,227.7 people per square mile (860.1/km²). There were 11,264 housing units at an average density of 854.0 per square mile (329.7/km²). The racial makeup of the city was 83.15% White, 1.53% Black or African American, 0.26% Native American, 1.10% Asian, 0.02% Pacific Islander, 12.00% from other races, and 1.94% from two or more races. 19.33% of the population were Hispanic or Latino of any race.

There were 10,675 households out of which 32.6% had children under the age of 18 living with them, 50.8% were married couples living together, 10.1% had a female householder with no husband present, and 33.6% were non-families. 27.5% of all households were made up of individuals and 12.5% had someone living alone who was 65 years of age or older. The average household size was 2.61 and the average family size was 3.14.

In the city, the population was spread out with 25.9% under the age of 18, 12.9% from 18 to 24, 30.0% from 25 to 44, 17.6% from 45 to 64, and 13.6% who were 65 years of age or older. The median age was 32 years. For every 100 females, there were 100.6 males. For every 100 females age 18 and over, there were 97.7 males.

The median income for a household in the city was $39,383, and the median income for a family was $46,877. Males had a median income of $32,159 versus $23,290 for females. The per capita income for the city was $18,899. About 6.0% of families and 9.3% of the population were below the poverty line, including 11.8% of those under age 18 and 5.3% of those age 65 or over.

Industry in Goshen centers around the automotive and Recreational Vehicle business. There are automotive component manufacturers like Benteler; firms that build custom bodies onto chassis like Supreme, Independent Protection and Showhauler Trucks. RV manufacturing companies include Dutchmen, Forest River and Keystone.

The government consists of a mayor, a clerk treasurer, and a city council. The mayor and clerk are elected in citywide vote. The city council consists of seven members. Five are elected from individual districts. Two are elected at-large.

Goshen Community Schools serves all of the city that is inside of Elkhart Township. This system consists of seven elementary schools, Goshen Middle School, and Goshen High School.

In 2012, U.S. News & World Report ranked Goshen High School as the 12th best high school in Indiana, as well as in the top 6% of high schools in the entire country.

Small parts of the city of Goshen are covered by several other school districts, including Fairfield Community Schools, Middlebury Community Schools, Concord Community Schools, and WaNee Community Schools.

Goshen College, located on the south side of town, has a current enrollment of approximately 800, with 40% being male, and 60% being female. Tuition and fees for the 2017–2018 year were $33,200.

The town has a free lending library, the Goshen Public Library.

Goshen Municipal Airport is a public use airport located three nautical miles (6 km) southeast of the central business district of Goshen. It is owned by the Goshen Board of Aviation Commissioners.

The Interurban Trolley bus connects Goshen to the nearby city of Elkhart and the unincorporated town of Dunlap via Concord and Elkhart-Goshen routes. The routes pass at Elkhart's Amtrak station, allowing passengers to connect to the "Capitol Limited" and "Lake Shore Limited" trains. Riders can also transfer to North Pointe route and Bittersweet/Mishawaka route. The former allows riders to connect to Elkhart's Greyhound bus station, while the later connects the riders to the city of Mishawaka and town of Osceola. The Bittersweet/Mishawaka route also allows them to transfer to TRANSPO Route 9 to connect to destinations throughout the South Bend-Goshen metropolitan region and the South Shore Line's South Bend International Airport station.

Goshen has seven parks and has a few different greenways and trails winding through the city, one of which runs along the old Mill Race and hydraulic canal which was once used to power an old hydroelectric power plant. Plans drawn up in 2005 call for the plant to be reopened and redevelopment to begin along the canal.

The Pumpkinvine Nature Trail runs from Goshen to Middlebury and Shipshewana, along the former Pumpkin Vine Railroad. The trail starts north east of Goshen at Abshire Park. It is one of the recreational highlights of Goshen. Along with the Maple City Greenway and the Millrace trail, they provide many miles of easily accessible trails for walking, running, and biking.

The Elkhart County Fairgrounds are also located in the city, where in late July, the Elkhart County 4-H Fair is held. It is the largest county fair in Indiana and one of the largest 4-H County Fairs in the United States.

The Goshen Air Show is also an annual event that takes place at the Goshen Municipal Airport.

In 2007, Downtown Goshen, Inc., a public-private partnership formed from the merger of Face of the City and the Downtown Action Team, started a First Fridays program. Occurring year round, First Fridays happens on the first Friday of each month with stores open until 9, music and other entertainment, and other events occurring within Goshen's downtown district.

The south side Wal-Mart is rumored to be the first Wal-Mart in the United States to provide a covered stable for its frequent Amish customers. The Amish built the stable with lumber and other supplies donated by Wal-Mart.

"Lonesome Jim" (2005) which was written by former resident James Strouse, directed by Steve Buscemi and starred Liv Tyler and Casey Affleck, was shot in Goshen.





Goshen has two sister cities as designated by Sister Cities International.




</doc>
<doc id="12929" url="https://en.wikipedia.org/wiki?curid=12929" title="Gallipoli">
Gallipoli

The Gallipoli peninsula (; ; , "Chersónisos tis Kallípolis") is located in the southern part of East Thrace, the European part of Turkey, with the Aegean Sea to the west and the Dardanelles strait to the east.

Gallipoli is the Italian form of the Greek name "Καλλίπολις" ("Kallípolis"), meaning "Beautiful City", the original name of the modern town of Gelibolu. In antiquity, the peninsula was known as the Thracian Chersonese (, "Thrakiké Chersónesos"; ).

The peninsula runs in a south-westerly direction into the Aegean Sea, between the Dardanelles (formally known as the Hellespont), and the Gulf of Saros (formally the bay of Melas). In antiquity, it was protected by the Long Wall, a defensive structure built across the narrowest part of the peninsula near the ancient city of Agora. The isthmus traversed by the wall was only 36 stadia in breadth (about 6.5 km), but the length of the peninsula from this wall to its southern extremity, Cape Mastusia, was 420 stadia (about 77.5 km).

In ancient times, the Gallipoli Peninsula was known as the Thracian Chersonesus (from Greek "χερσόνησος", "peninsula") to the Greeks and later the Romans. It was the location of several prominent towns, including Cardia, Pactya, Callipolis (Gallipoli), Alopeconnesus (), Sestos, Madytos, and Elaeus. The peninsula was renowned for its wheat. It also benefited from its strategic importance on the main route between Europe and Asia, as well as from its control of the shipping route from Crimea. The city of Sestos was the main crossing-point on the Hellespont.

According to Herodotus, the Thracian tribe of Dolonci () (or "barbarians" according to Cornelius Nepos) held possession of Chersonesus before the Greek colonization. Then, settlers from Ancient Greece, mainly of Ionian and Aeolian stock, founded about 12 cities on the peninsula in the 7th century BC. The Athenian statesman Miltiades the Elder founded a major Athenian colony there around 560 BC. He took authority over the entire peninsula, building up its defences against incursions from the mainland. It eventually passed to his nephew, the more famous Miltiades the Younger, around 524 BC. The peninsula was abandoned to the Persians in 493 BC after the outbreak of the Greco-Persian Wars (499–478 BC).

The Persians were eventually expelled, after which the peninsula was for a time ruled over by Athens, which enrolled it into the Delian League in 478 BC. The Athenians established a number of cleruchies on the Thracian Chersonese and sent an additional 1,000 settlers around 448 BC. Sparta gained control after the decisive battle of Aegospotami in 404 BC, but the peninsula subsequently reverted to the Athenians. In the 4th century BC, the Thracian Chersonese became the focus of a bitter territorial dispute between Athens and Macedon, whose king Philip II sought possession. It was eventually ceded to Philip in 338 BC.

After the death of Philip's son Alexander the Great in 323 BC, the Thracian Chersonese became the object of contention among Alexander's successors. Lysimachus established his capital Lysimachia here. In 278 BC, Celtic tribes from Galatia in Asia Minor settled in the area. In 196 BC, the Seleucid king Antiochus III seized the peninsula. This alarmed the Greeks and prompted them to seek the aid of the Romans, who conquered the Thracian Chersonese, which they gave to their ally Eumenes II of Pergamon in 188 BC. At the extinction of the Attalid dynasty in 133 BC it passed again to the Romans, who from 129 BC administered it in the Roman province of Asia. It was subsequently made a state-owned territory ("ager publicus") and during the reign of the emperor Augustus it was imperial property.

The Thracian Chersonese was part of the Eastern Roman Empire from its foundation in 330 AD. In 443 AD, Attila the Hun invaded the Gallipoli Peninsula during one of the last stages of his grand campaign that year. He captured both Callipolis and Sestus. Aside from a brief period from 1204 to 1235, when it was controlled by the Republic of Venice, the Byzantine Empire ruled the territory until 1356. During the night between the 1st and the 2nd of March 1354, a strong earthquake destroyed the city of Gallipoli and its city walls, weakening its defenses.

After the devastating 1354 earthquake, the town of Gallipoli was besieged and captured by the Ottomans, making Gallipoli the first Ottoman stronghold in Europe, and the staging area for their expansion across the Balkans. It was recaptured for Byzantium by the Savoyard Crusade in 1366, but the beleaguered Byzantines were forced to hand it back in September 1376. The Greeks living there were allowed to continue their everyday life. In the 19th century, Gallipoli () was a district (kaymakamlik) in the Vilayet of Adrianople, with about thirty thousand inhabitants: comprising Greeks, Turks, Armenians and Jews.

Gallipoli became a major encampment for British and French forces in 1854 during the Crimean War, and the harbour was also a stopping-off point on the way to Constantinople.

British and French engineers constructed in March 1854, a 7-mile line of defence to protect the peninsula from a possible Russian attack and so keep control of the route to the Mediterranean Sea.

Gallipoli did not experience any more wars until the First Balkan War, when the 1913 Battle of Bulair and several minor skirmishes took place here.
A dispatch on 7 July 1913 reported that Ottoman troops treated Gallipoli’s Greeks ‘with marked depravity’ as they ‘destroyed, looted, and burned all the Greek villages near Gallipoli’. Many villages were sacked and destroyed completely and also, some Greeks killed. The cause of this savagery of the Turks was their fear that if Thrace was declared autonomous the Greek population may be found numerically superior to the Muslims.

The Turkish Government, under pre-text that a village was within the firing line, ordered its evacuation within three hours. The residents abandoned everything they possessed, left their village and went to Gallipoli. Seven of the Greek villagers who were two minutes late behind the three hours limit allowed for the evacuation were shot by the soldiers. After the Balkan War was over, the exiles were allowed to return. But as the Government allowed only the Turks to rebuild their houses and furnished them, the exiled Greeks were compelled to remain in Gallipoli.

During World War I, British and colonial forces attacked the peninsula in 1915, seeking to secure a route to relieve their eastern ally, Russia. The Ottomans set up defensive fortifications along the peninsula and the attackers were eventually repulsed.

In early 1915, attempting to seize a strategic advantage in World War I by capturing Constantinople, the British authorised an attack on the peninsula. The first Australian troops landed on early morning 25 April 1915 and after eight months of heavy fighting, the troops were withdrawn around the end of the year.

The campaign was one of the greatest Ottoman victories during the war and is considered a major Allied failure. In Turkey, it is regarded as a defining moment in the nation's history: a final surge in the defence of the motherland as the Ottoman Empire crumbled. The struggle formed the basis for the Turkish War of Independence and the founding of the Republic of Turkey eight years later under Mustafa Kemal Atatürk, who first rose to prominence as a commander at Gallipoli.

The Gallipoli Star was a military decoration created by the Ottoman Empire in 1915 and awarded for the duration of World War I.

The campaign was the first major military action of Australia and New Zealand as independent dominions, and is often considered to mark the birth of national consciousness in those nations. The date of the landing, 25 April, is known as "Anzac Day". It remains the most significant commemoration of military casualties and veterans in Australia and New Zealand.

On the Allied side one of the key promoters of the expedition was Britain's First Lord of the Admiralty, Winston Churchill, whose reputation took years to recover.

Prior to the Allied landings in April 1915, Ottoman Empire deported Greek residents from Gallipoli and surrounding region and from the islands in the sea of Marmara, to the interior where they were at the mercy of hostile Turks. The Greeks had little time to pack and the Ottoman authorities permitted them to take only some bedding and the rest was handed over to the Government. Also, Greek houses and properties were plundered by the Turks. A testimony of a deportee described how the deportees were forced onto crowded steamers, standing room only; how, on disembarking, men of military age were removed (for forced labour in the labour battalions of the Ottoman army) and how the rest were ‘scattered… among the farms like ownerless cattle’.

The Metropolitan of Gallipoli on 17 July 1915, wrote that the extermination of the Christian refugees was methodical. It also mentions that: "The Turks, like beasts of prey, immediately plundered all the Christians' property and carried it off. The inhabitants and refugees of my district are entirely without shelter, awaiting to be sent no one knows where ...". In addition many Greeks died from hunger and there were frequent cases of rape among women and young girls, as well as their conversion to Islam.

Gallipoli was occupied by Greek troops on 4 August 1920 during the Greco-Turkish War of 1919–22, considered part of the Turkish War of Independence, and after the Armistice of Mudros it became a Greek prefecture centre as "Kallipolis". However, Greece was forced to withdraw from Eastern Thrace after the Armistice of Mudanya. Gallipoli was briefly handed over to British troops on 20 October 1922, but was finally returned to Turkish rule on 26 November 1922.

In 1920, after the defeat of the Russian White army of General Pyotr Wrangel, a significant number of emigre soldiers and their families evacuated to Gallipoli from the Crimean Peninsula. From there, many went to European countries, such as Yugoslavia, where they found refuge. A stone monument was erected and a special "Gallipoli cross" was created to commemorate the soldiers, who stayed in Gallipoli. The stone monument was destroyed during an earthquake, but in January 2008 reconstruction of the monument had begun with the consent of the Turkish government.

Between 1923 and 1926 Gallipoli became the centre of Gelibolu Province, comprising the districts of Gelibolu, Eceabat, Keşan and Şarköy. After the dissolution of the province, it became a district centre in Çanakkale Province.




</doc>
<doc id="12935" url="https://en.wikipedia.org/wiki?curid=12935" title="Gram stain">
Gram stain

Gram stain or Gram staining, also called Gram's method, is a method of staining used to distinguish and classify bacterial species into two large groups (gram-positive and gram-negative). The name comes from the Danish bacteriologist Hans Christian Gram, who developed the technique.

Gram staining differentiates bacteria by the chemical and physical properties of their cell walls by detecting peptidoglycan, which is present in the cell wall of Gram-positive bacteria. Gram-negative cells also contain peptidoglycan, but a very small layer of it that is dissolved when the alcohol is added. This is why the cell loses its initial color from the primary stain. Gram-positive bacteria retain the crystal violet dye, and thus are stained violet, while the Gram-negative bacteria do not; after washing, a counterstain is added (commonly safranin or fuchsine) that will stain these Gram-negative bacteria a pink color. Both Gram-positive bacteria and Gram-negative bacteria pick up the counterstain. The counterstain, however, is unseen on Gram-positive bacteria because of the darker crystal violet stain.

The Gram stain is almost always the first step in the preliminary identification of a bacterial organism. While Gram staining is a valuable diagnostic tool in both clinical and research settings, not all bacteria can be definitively classified by this technique. This gives rise to "gram-variable" and "gram-indeterminate" groups.

The method is named after its inventor, the Danish scientist Hans Christian Gram (1853–1938), who developed the technique while working with Carl Friedländer in the morgue of the city hospital in Berlin in 1884. Gram devised his technique not for the purpose of distinguishing one type of bacterium from another but to make bacteria more visible in stained sections of lung tissue. He published his method in 1884, and included in his short report the observation that the typhus bacillus did not retain the stain.

Gram staining is a bacteriological laboratory technique used to differentiate bacterial species into two large groups (gram-positive and gram-negative) based on the physical properties of their cell walls. Gram staining is not used to classify archaea, formerly archaeabacteria, since these microorganisms yield widely varying responses that do not follow their phylogenetic groups.

The Gram stain is not an infallible tool for diagnosis, identification, or phylogeny, and it is of extremely limited use in environmental microbiology. It is used mainly to make a preliminary morphologic identification or to establish that there are significant numbers of bacteria in a clinical specimen. It cannot identify bacteria to the species level, and for most medical conditions, it should not be used as the sole method of bacterial identification. In clinical microbiology laboratories, it is used in combination with other traditional and molecular techniques to identify bacteria. Some organisms are gram-variable (meaning they may stain either negative or positive); some are not stained with either dye used in the Gram technique and are not seen. In a modern environmental or molecular microbiology lab, most identification is done using genetic sequences and other molecular techniques, which are far more specific and informative than differential staining.

Gram staining has been suggested to be as effective a diagnostic tool as PCR in one primary research report regarding gonococcal urethritis.

Gram stains are performed on body fluid or biopsy when infection is suspected. Gram stains yield results much more quickly than culturing, and is especially important when infection would make an important difference in the patient's treatment and prognosis; examples are cerebrospinal fluid for meningitis and synovial fluid for septic arthritis.

Gram-positive bacteria have a thick mesh-like cell wall made of peptidoglycan (50–90% of cell envelope), and as a result are stained purple by crystal violet, whereas gram-negative bacteria have a thinner layer (10% of cell envelope), so do not retain the purple stain and are counter-stained pink by safranin. There are four basic steps of the Gram stain:

Crystal violet (CV) dissociates in aqueous solutions into and chloride () ions. These ions penetrate through the cell wall and cell membrane of both gram-positive and gram-negative cells. The ion interacts with negatively charged components of bacterial cells and stains the cells purple.

Iodide ( or ) interacts with and forms large complexes of crystal violet and iodine (CV–I) within the inner and outer layers of the cell. Iodine is often referred to as a mordant, but is a trapping agent that prevents the removal of the CV–I complex and, therefore, color the cell.

When a decolorizer such as alcohol or acetone is added, it interacts with the lipids of the cell membrane. A gram-negative cell loses its outer lipopolysaccharide membrane, and the inner peptidoglycan layer is left exposed. The CV–I complexes are washed from the gram-negative cell along with the outer membrane. In contrast, a gram-positive cell becomes dehydrated from an ethanol treatment. The large CV–I complexes become trapped within the gram-positive cell due to the multilayered nature of its peptidoglycan. The decolorization step is critical and must be timed correctly; the crystal violet stain is removed from both gram-positive and negative cells if the decolorizing agent is left on too long (a matter of seconds).

After decolorization, the gram-positive cell remains purple and the gram-negative cell loses its purple color. Counterstain, which is usually positively charged safranin or basic fuchsine, is applied last to give decolorized gram-negative bacteria a pink or red color.

Gram-positive bacteria generally have a single membrane ("monoderm") surrounded by a thick peptidoglycan.
This rule is followed by two phyla: "Firmicutes" (except for the classes Mollicutes and Negativicutes) and the "Actinobacteria". In contrast, members of the Chloroflexi (green non-sulfur bacteria) are monoderms but possess a thin or absent (class Dehalococcoidetes) peptidoglycan and can stain negative, positive or indeterminate; members of the Deinococcus-Thermus group, stain positive but are diderms with a thick peptidoglycan. 

Historically, the gram-positive forms made up the phylum Firmicutes, a name now used for the largest group. It includes many well-known genera such as "Bacillus", "Listeria", "Staphylococcus", "Streptococcus", "Enterococcus", and "Clostridium". It has also been expanded to include the Mollicutes, bacteria like "Mycoplasma" that lack cell walls and so cannot be stained by Gram, but are derived from such forms.

Some bacteria have cell walls which are particularly adept at retaining stains. These will appear positive by Gram stain even though they are not closely related to other gram-positive bacteria. These are called acid fast bacteria, and can only be differentiated from other gram-positive bacteria by special staining procedures.

Gram-negative bacteria generally possess a thin layer of peptidoglycan between two membranes ("diderms"). Most bacterial phyla are gram-negative, including the cyanobacteria, green sulfur bacteria, and most Proteobacteria (exceptions being some members of the "Rickettsiales" and the insect-endosymbionts of the "Enterobacteriales").

Some bacteria, after staining with the Gram stain, yield a gram-variable pattern: a mix of pink and purple cells are seen. In cultures of Bacillus, Butyrivibrio, and Clostridium, a decrease in peptidoglycan thickness during growth coincides with an increase in the number of cells that stain gram-negative. In addition, in all bacteria stained using the Gram stain, the age of the culture may influence the results of the stain.

Gram-indeterminate bacteria do not respond predictably to Gram staining and, therefore, cannot be determined as either gram-positive or gram-negative. Examples include many species of "Mycobacterium", including "M. tuberculosis" and "M. leprae".

The term Gram staining is derived from the surname of Hans Christian Gram, the eponym (Gram) is therefore capitalized but not the common noun (stain) as is usual for scientific terms. The adjectives 'gram-positive' and 'gram-negative'; as eponymous adjectives, their initial letter can be either lowercase 'g' or capital 'G', depending on whose style guide (if any) governs the document being written. Lowercase style is used by the US Centers for Disease Control and Prevention and other style regimens such as the AMA style. Dictionaries may use lowercase, uppercase, or both. Uppercase 'Gram-positive' or 'Gram-negative' usage is also common in many scientific journal articles and publications. When articles are submitted to journals, each journal may or may not apply house style to the postprint version. Preprint versions contain whichever style the author happened to use. Even style regimens that use lowercase for the adjectives 'gram-positive' and 'gram-negative' still use capital for 'Gram stain'.




</doc>
<doc id="12936" url="https://en.wikipedia.org/wiki?curid=12936" title="Gram-positive bacteria">
Gram-positive bacteria

Gram-positive bacteria are bacteria that give a positive result in the Gram stain test, which is traditionally used to quickly classify bacteria into two broad categories according to their cell wall.

Gram-positive bacteria take up the crystal violet stain used in the test, and then appear to be purple-coloured when seen through a microscope. This is because the thick peptidoglycan layer in the bacterial cell wall retains the stain after it is washed away from the rest of the sample, in the decolorization stage of the test.

Gram-negative bacteria cannot retain the violet stain after the decolorization step; alcohol used in this stage degrades the outer membrane of Gram-negative cells, making the cell wall more porous and incapable of retaining the crystal violet stain. Their peptidoglycan layer is much thinner and sandwiched between an inner cell membrane and a bacterial outer membrane, causing them to take up the counterstain (safranin or fuchsine) and appear red or pink.

Despite their thicker peptidoglycan layer, Gram-positive bacteria are more receptive to antibiotics than Gram-negative, due to the absence of the outer membrane.

In general, the following characteristics are present in Gram-positive bacteria:


Only some species have a capsule, usually consisting of polysaccharides. Also, only some species are flagellates, and when they do have flagella, have only two basal body rings to support them, whereas Gram-negative have four. Both Gram-positive and Gram-negative bacteria commonly have a surface layer called an S-layer. In Gram-positive bacteria, the S-layer is attached to the peptidoglycan layer. Gram-negative bacteria's S-layer is attached directly to the outer membrane). Specific to Gram-positive bacteria is the presence of teichoic acids in the cell wall. Some of these are lipoteichoic acids, which have a lipid component in the cell membrane that can assist in anchoring the peptidoglycan.

Along with cell shape, Gram staining is a rapid method used to differentiate bacterial species. Such staining, together with growth requirement and antibiotic susceptibility testing, and other macroscopic and physiologic tests, forms the full basis for classification and subdivision of the bacteria (e.g., see figure and pre-1990 versions of "Bergey's Manual").

Historically, the kingdom Monera was divided into four divisions based primarily on Gram staining: Firmicutes (positive in staining), Gracilicutes (negative in staining), Mollicutes (neutral in staining) and Mendocutes (variable in staining). Based on 16S ribosomal RNA phylogenetic studies of the late microbiologist Carl Woese and collaborators and colleagues at the University of Illinois, the monophyly of the Gram-positive bacteria was challenged, with major implications for the therapeutic and general study of these organisms. Based on molecular studies of the 16S sequences, Woese recognised twelve bacterial phyla. Two of these were both Gram-positive and were divided on the proportion of the guanine and cytosine content in their DNA. The high G + C phylum was made up of the Actinobacteria and the low G + C phylum contained the Firmicutes. The Actinobacteria include the "Corynebacterium", "Mycobacterium", "Nocardia" and "Streptomyces" genera. The (low G + C) Firmicutes, have a 45–60% GC content, but this is lower than that of the Actinobacteria.

Although bacteria are traditionally divided into two main groups, Gram-positive and Gram-negative, based on their Gram stain retention property, this classification system is ambiguous as it refers to three distinct aspects (staining result, envelope organization, taxonomic group), which do not necessarily coalesce for some bacterial species. The Gram-positive and Gram-negative staining response is also not a reliable characteristic as these two kinds of bacteria do not form phylogenetic coherent groups. However, although Gram staining response is an empirical criterion, its basis lies in the marked differences in the ultrastructure and chemical composition of the bacterial cell wall, marked by the absence or presence of an outer lipid membrane.

All Gram-positive bacteria are bounded by a single-unit lipid membrane, and, in general, they contain a thick layer (20–80 nm) of peptidoglycan responsible for retaining the Gram stain. A number of other bacteria—that are bounded by a single membrane, but stain Gram-negative due to either lack of the peptidoglycan layer, as in the Mycoplasmas, or their inability to retain the Gram stain because of their cell wall composition—also show close relationship to the Gram-positive bacteria. For the bacterial cells bounded by a single cell membrane, the term "monoderm bacteria" or "monoderm prokaryotes" has been proposed.

In contrast to Gram-positive bacteria, all archetypical Gram-negative bacteria are bounded by a cytoplasmic membrane and an outer cell membrane; they contain only a thin layer of peptidoglycan (2–3 nm) between these membranes. The presence of inner and outer cell membranes defines a new compartment in these cells: the periplasmic space or the periplasmic compartment. These bacteria have been designated as "diderm bacteria." The distinction between the monoderm and diderm bacteria is supported by conserved signature indels in a number of important proteins (viz. DnaK, GroEL). Of these two structurally distinct groups of bacteria, monoderms are indicated to be ancestral. Based upon a number of observations including that the Gram-positive bacteria are the major producers of antibiotics and that, in general, Gram-negative bacteria are resistant to them, it has been proposed that the outer cell membrane in Gram-negative bacteria (diderms) has evolved as a protective mechanism against antibiotic selection pressure. Some bacteria, such as "Deinococcus", which stain Gram-positive due to the presence of a thick peptidoglycan layer and also possess an outer cell membrane are suggested as intermediates in the transition between monoderm (Gram-positive) and diderm (Gram-negative) bacteria. The diderm bacteria can also be further differentiated between simple diderms lacking lipopolysaccharide, the archetypical diderm bacteria where the outer cell membrane contains lipopolysaccharide, and the diderm bacteria where outer cell membrane is made up of mycolic acid.

In general, Gram-positive bacteria are monoderms and have a single lipid bilayer whereas Gram-negative bacteria are diderms and have two bilayers. Some taxa lack peptidoglycan (such as the domain Archaea, the class Mollicutes, some members of the Rickettsiales, and the insect-endosymbionts of the Enterobacteriales) and are Gram-variable. This, however, does not always hold true. The "Deinococcus-Thermus" bacteria have Gram-positive stains, although they are structurally similar to Gram-negative bacteria with two layers. The Chloroflexi have a single layer, yet (with some exceptions) stain negative. Two related phyla to the Chloroflexi, the TM7 clade and the Ktedonobacteria, are also monoderms.

Some Firmicute species are not Gram-positive. These belong to the class Mollicutes (alternatively considered a class of the phylum Tenericutes), which lack peptidoglycan (Gram-indeterminate), and the class Negativicutes, which includes Selenomonas and stain Gram-negative. Additionally, a number of bacterial taxa (viz. Negativicutes, Fusobacteria, Synergistetes, and Elusimicrobia) that are either part of the phylum Firmicutes or branch in its proximity are found to possess a diderm cell structure. However, a conserved signature indel (CSI) in the HSP60 (GroEL) protein distinguishes all traditional phyla of Gram-negative bacteria (e.g., Proteobacteria, Aquificae, Chlamydiae, Bacteroidetes, Chlorobi, Cyanobacteria, Fibrobacteres, Verrucomicrobia, Planctomycetes, Spirochetes, Acidobacteria, etc.) from these other atypical diderm bacteria, as well as other phyla of monoderm bacteria (e.g., Actinobacteria, Firmicutes, Thermotogae, Chloroflexi, etc.). The presence of this CSI in all sequenced species of conventional LPS (lipopolysaccharide)-containing Gram-negative bacterial phyla provides evidence that these phyla of bacteria form a monophyletic clade and that no loss of the outer membrane from any species from this group has occurred.

In the classical sense, six Gram-positive genera are typically pathogenic in humans. Two of these, "Streptococcus" and "Staphylococcus", are cocci (sphere-shaped). The remaining organisms are bacilli (rod-shaped) and can be subdivided based on their ability to form spores. The non-spore formers are "Corynebacterium" and "Listeria" (a coccobacillus), whereas "Bacillus" and "Clostridium" produce spores. The spore-forming bacteria can again be divided based on their respiration: "Bacillus" is a facultative anaerobe, while "Clostridium" is an obligate anaerobe. Also, "Rathybacter", "Leifsonia", and "Clavibacter" are three Gram-positive genera that cause plant disease. Gram-positive bacteria are capable of causing serious and sometimes fatal infections in newborn infants.

Transformation is one of three processes for horizontal gene transfer, in which exogenous genetic material passes from a donor bacterium to a recipient bacterium, the other two processes being conjugation (transfer of genetic material between two bacterial cells in direct contact) and transduction (injection of donor bacterial DNA by a bacteriophage virus into a recipient host bacterium). In transformation, the genetic material passes through the intervening medium, and uptake is completely dependent on the recipient bacterium.

As of 2014 about 80 species of bacteria were known to be capable of transformation, about evenly divided between Gram-positive and Gram-negative bacteria; the number might be an overestimate since several of the reports are supported by single papers. Transformation among Gram-positive bacteria has been studied in medically important species such as "Streptococcus pneumoniae", "Streptococcus mutans", "Staphylococcus aureus" and "Streptococcus sanguinis" and in Gram-positive soil bacterium "Bacillus subtilis".

The adjectives "Gram-positive" and "Gram-negative" derive from the surname of Hans Christian Gram; as eponymous adjectives, their initial letter can be either capital "G" or lower-case "g", depending on which style guide (e.g., that of the CDC), if any, governs the document being written. This is further explained at "Gram staining § Orthographic note".



</doc>
<doc id="12937" url="https://en.wikipedia.org/wiki?curid=12937" title="Gram-negative bacteria">
Gram-negative bacteria

Gram-negative bacteria are bacteria that do not retain the crystal violet stain used in the gram-staining method of bacterial differentiation. They are characterized by their cell envelopes, which are composed of a thin peptidoglycan cell wall sandwiched between an inner cytoplasmic cell membrane and a bacterial outer membrane.

Gram-negative bacteria are found everywhere, in virtually all environments on Earth that support life. The gram-negative bacteria include the model organism "Escherichia coli", as well as many pathogenic bacteria, such as "Pseudomonas aeruginosa", "Neisseria gonorrhoeae", "Chlamydia trachomatis", and "Yersinia pestis". They are an important medical challenge, as their outer membrane protects them from many antibiotics (including penicillin); detergents that would normally damage the peptidoglycans of the (inner) cell membrane; and lysozyme, an antimicrobial enzyme produced by animals that forms part of the innate immune system. Additionally, the outer leaflet of this membrane comprises a complex lipopolysaccharide (LPS) whose lipid A component can cause a toxic reaction when these bacteria are lysed by immune cells. This toxic reaction can include fever, an increased respiratory rate, and low blood pressure — a life-threatening condition known as septic shock.

Several classes of antibiotics have been designed to target gram-negative bacteria, including aminopenicillins, ureidopenicillins, cephalosporins, beta-lactam-betalactamase combinations (e.g. pipercillin-tazobactam), Folate antagonists, quinolones, and carbapenems. Many of these antibiotics also cover gram positive organisms. The drugs that specifically target gram negative organisms include aminoglycosides, monobactams (aztreonam) and Ciprofloxacin.

Gram-negative bacteria display :


Along with cell shape, gram-staining is a rapid diagnostic tool and once was used to group species at the subdivision of Bacteria.
Historically, the kingdom Monera was divided into four divisions based on gram-staining: Firmacutes (+), Gracillicutes (−), Mollicutes (0) and Mendocutes (var.).
Since 1987, the monophyly of the gram-negative bacteria has been disproven with molecular studies. However some authors, such as Cavalier-Smith still treat them as a monophyletic taxon (though not a clade; his definition of monophyly requires a single common ancestor but does not require holophyly, the property that all descendants be encompassed by the taxon) and refer to the group as a subkingdom "Negibacteria".

Bacteria are traditionally divided into the two groups: gram-positive and gram-negative, based on their gram-staining response. Gram-positive bacteria are also referred to as "monoderms" having one membrane, and gram-negative bacteria are also referred to as "diderms", having two membranes. These groups are often thought of as lineages, with gram-negative bacteria more closely related to one another than to gram-positive bacteria. While this is often true, the classification system breaks down in some cases. A given bacteria's staining result, bacterial membrane organization, and lineage groupings do not always match up. Thus, gram-staining cannot be reliably used to assess familial relationships of bacteria. However, staining often gives reliable information about the composition of the cell membrane, distinguishing between the presence or absence of an outer lipid membrane.

Of these two structurally distinct groups of prokaryotic organisms, monoderm prokaryotes are indicated to be ancestral. Based upon a number of different observations including that the gram-positive bacteria are the major reactors to antibiotics and that gram-negative bacteria are, in general, resistant to them, it has been proposed that the outer cell membrane in gram-negative bacteria (diderms) evolved as a protective mechanism against antibiotic selection pressure. Some bacteria such as Deinococcus, which stain gram-positive due to the presence of a thick peptidoglycan layer, but also possess an outer cell membrane are suggested as intermediates in the transition between monoderm (gram-positive) and diderm (gram-negative) bacteria. The diderm bacteria can also be further differentiated between simple diderms lacking lipopolysaccharide; the archetypical diderm bacteria, in which the outer cell membrane contains lipopolysaccharide; and the diderm bacteria, in which the outer cell membrane is made up of mycolic acid (e. g. Mycobacterium).

In addition, a number of bacterial taxa (including Negativicutes, Fusobacteria, Synergistetes, and Elusimicrobia) that are either part of the phylum Firmicutes or branches in its proximity are also found to possess a diderm cell structure. However, a conserved signature indel (CSI) in the HSP60 (GroEL) protein distinguishes all traditional phyla of gram-negative bacteria (e.g., Proteobacteria, Aquificae, Chlamydiae, Bacteroidetes, Chlorobi, Cyanobacteria, Fibrobacteres, Verrucomicrobia, Planctomycetes, Spirochetes, Acidobacteria) from these other atypical diderm bacteria as well as other phyla of monoderm bacteria (e.g., Actinobacteria, Firmicutes, Thermotogae, Chloroflexi). The presence of this CSI in all sequenced species of conventional lipopolysaccharide-containing gram-negative bacterial phyla provides evidence that these phyla of bacteria form a monophyletic clade and that no loss of the outer membrane from any species from this group has occurred.

The proteobacteria are a major phylum of gram-negative bacteria, including "Escherichia coli" ("E. coli"), "Salmonella", "Shigella", and other Enterobacteriaceae, "Pseudomonas", "Moraxella", "Helicobacter", "Stenotrophomonas", "Bdellovibrio", acetic acid bacteria, "Legionella" etc. Other notable groups of gram-negative bacteria include the cyanobacteria, spirochaetes, green sulfur, and green non-sulfur bacteria.

Medically relevant gram-negative cocci include the four types that cause a sexually transmitted disease ("Neisseria gonorrhoeae"), a meningitis ("Neisseria meningitidis"), and respiratory symptoms ("Moraxella catarrhalis", "Haemophilus influenzae").

Medically relevant gram-negative bacilli include a multitude of species. Some of them cause primarily respiratory problems ("Klebsiella pneumoniae", "Legionella pneumophila", "Pseudomonas aeruginosa"), primarily urinary problems ("Escherichia coli", "Proteus mirabilis", "Enterobacter cloacae", "Serratia marcescens"), and primarily gastrointestinal problems ("Helicobacter pylori", "Salmonella enteritidis", "Salmonella typhi").

Gram-negative bacteria associated with hospital-acquired infections include "Acinetobacter baumannii", which cause bacteremia, secondary meningitis, and ventilator-associated pneumonia in hospital intensive-care units.

Transformation is one of three processes for horizontal gene transfer, in which exogenous genetic material passes from bacterium to another, the other two being conjugation (transfer of genetic material between two bacterial cells in direct contact) and transduction (injection of foreign DNA by a bacteriophage virus into the host bacterium). In transformation, the genetic material passes through the intervening medium, and uptake is completely dependent on the recipient bacterium.

As of 2014 about 80 species of bacteria were known to be capable of transformation, about evenly divided between Gram-positive and Gram-negative bacteria; the number might be an overestimate since several of the reports are supported by single papers. Transformation has been studied in medically important Gram-negative bacteria species such as "Helicobacter pylori", "Legionella pneumophila", "Neisseria meningitidis", "Neisseria gonorrhoeae", "Haemophilus influenzae" and "Vibrio cholerae". It has also been studied in gram-negative species found in soil such as "Pseudomonas stutzeri", "Acinetobacter baylyi", and gram-negative plant pathogens such as "Ralstonia solanacearum" and "Xylella fastidiosa".

One of the several unique characteristics of gram-negative bacteria is the structure of the bacterial outer membrane. The outer leaflet of this membrane comprises a complex lipopolysaccharide (LPS) whose lipid portion acts as an endotoxin. If gram-negative bacteria enter the circulatory system, the liposaccharide can cause a toxic reaction. This results in fever, an increased respiratory rate, and low blood pressure. This may lead to life-threatening septic shock.

The outer membrane protects the bacteria from several antibiotics, dyes, and detergents that would normally damage either the inner membrane or the cell wall (made of peptidoglycan). The outer membrane provides these bacteria with resistance to lysozyme and penicillin. The periplasmic space (space between the two cell membranes) also contains enzymes which break down or modify antibiotics. Drugs commonly used to treat gram negative infections include amino, carboxy and ureido penicillins (ampicillin, amoxicillin, pipercillin, ticarcillin) these drugs may be combined with beta-lactamase inhibitors to combat the presence of enzymes that can digest these drugs (known as beta-lactamases) in the peri-plasmic space. Other classes of drugs that have gram negative spectrum include cephalosporins, monobactams (aztreonam), aminogylosides, quinolones, macrolides, chloramphenicol, folate antagonists, and carbapenems.

The pathogenic capability of gram-negative bacteria is often associated with certain components of their membrane, in particular, the LPS. In humans, the presence of LPS triggers an innate immune response, activating the immune system and producing cytokines (hormonal regulators). Inflammation is a common reaction to cytokine production, which can also produce host toxicity. The innate immune response to LPS, however, is not synonymous with pathogenicity, or the ability to cause disease.

The adjectives "Gram-positive" and "Gram-negative" derive from the surname of Hans Christian Gram, a Danish bacteriologist; as eponymous adjectives, their initial letter can be either capital "G" or lower-case "g", depending on which style guide (e.g., that of the CDC), if any, governs the document being written. This is further explained at "Gram staining § Orthographic note".




</doc>
<doc id="12938" url="https://en.wikipedia.org/wiki?curid=12938" title="Greyhound">
Greyhound

The Greyhound is a breed of dog; a sighthound which has been bred for coursing game and Greyhound racing. Since the rise in large-scale adoption of retired racing Greyhounds, the breed has seen a resurgence in popularity as a family pet.

According to Merriam-Webster, a Greyhound is "any of a breed of tall slender graceful smooth-coated dogs characterized by swiftness and keen sight", as well as "any of several related dogs," such as the Italian Greyhound.

It is a gentle and intelligent breed whose combination of long, powerful legs, deep chest, flexible spine and slim build allows it to reach average race speeds exceeding . The Greyhound can reach a full speed of within , or six strides from the boxes, traveling at almost for the first of a race.

Males are usually tall at the withers, and weigh on average . Females tend to be smaller, with shoulder heights ranging from and weights from less than . Greyhounds have very short fur, which is easy to maintain. There are approximately thirty recognized color forms, of which variations of white, brindle, fawn, black, red and blue (gray) can appear uniquely or in combination. Greyhounds are dolichocephalic, with a skull which is relatively long in comparison to its breadth, and an elongated muzzle.

Greyhounds can be aloof and indifferent to strangers, but are affectionate with their own pack. They are generally docile, lazy, easy-going, and calm.

Greyhounds wear muzzles during racing, which can lead some to believe they are aggressive dogs, but this is not true. Muzzles are worn to prevent injuries resulting from dogs nipping one another during or immediately after a race, when the 'hare' has disappeared out of sight and the dogs are no longer racing but remain excited. 

Contrary to popular belief, adult Greyhounds do not need extended periods of daily exercise, as they are bred for sprinting rather than endurance. Greyhound puppies that have not been taught how to utilize their energy, however, can be hyperactive and destructive if not given an outlet, and therefore require more experienced handlers.

Greyhound owners and adoption groups consider Greyhounds wonderful pets. Greyhounds are quiet, gentle and loyal to owners. They are very loving, and enjoy the company of their humans and other dogs. Whether a Greyhound will enjoy the company of other small animals, such as cats, depends on the individual dog's personality. Greyhounds will typically chase small animals; those lacking a high 'prey drive' will be able to coexist happily with toy dog breeds and/or cats. Many owners describe their Greyhounds as "45-mile-per-hour couch potatoes".

Greyhounds live most happily as pets in quiet environments. They do well in families with children, as long as the children are taught to treat the dog properly with politeness and appropriate respect. Greyhounds have a sensitive nature, and gentle commands work best as training methods.

Occasionally, a Greyhound may bark; however, Greyhounds are generally not barkers, which is beneficial in suburban environments, and are usually as friendly to strangers as they are with their own families.

A very common misconception regarding Greyhounds is that they are hyperactive. This is usually not the case with retired racing Greyhounds. Greyhounds can live comfortably as apartment dogs, as they do not require much space and sleep almost 18 hours per day. Due to their calm temperament, Greyhounds can make better "apartment dogs" than smaller, more active breeds.

At most race tracks, Greyhounds are housed in crates. Most such animals know nothing other than being in a crate for the majority of the day. Therefore, crate training a retired Greyhound in a home is generally easy.

Many Greyhound adoption groups recommend that owners keep their Greyhounds on a leash whenever outdoors, except in fully enclosed areas. This is due to their prey-drive, their speed, and the assertion that Greyhounds have no road sense. In some jurisdictions, it is illegal for Greyhounds to be allowed off-lead even in off-lead dog parks. Due to their size and strength, adoption groups recommend that fences be between 4 and 6 feet tall, to prevent Greyhounds from jumping over them.

The original primary use of Greyhounds, both in the British Isles and on the Continent of Europe, was in the coursing of deer. Later, they specialized in competition hare coursing. Some Greyhounds are still used for coursing, although artificial lure sports like lure coursing and racing are far more common and popular. Many leading 300- to 550-yard sprinters have bloodlines traceable back through Irish sires, within a few generations of racers that won events such as the Irish Coursing Derby or the Irish Cup.

Until the early twentieth century, Greyhounds were principally bred and trained for hunting and coursing. During the 1920s, modern greyhound racing was introduced into the United States and England (Belle Vue Stadium, Manchester, July 1926), as well as Northern Ireland (Celtic Park (Belfast), April 1927) and the Republic of Ireland (Shelbourne Park, Dublin). 

Australia also has a significant racing culture. However, the 2015 live baiting scandal and adverse media coverage led to a Special Commission of Inquiry into the Greyhound Racing Industry in NSW. On 7 July 2016, New South Wales Premier Mike Baird announced that greyhound racing was to be banned in the state from 1 July 2017 after the inquiry found evidence of systemic animal cruelty, including mass greyhound killings and live baiting, however the plan for the ban was later reversed before it was implemented. After the NSW announcement to ban racing, Australian Capital Territory (ACT) Chief Minister Andrew Barr stated that greyhound racing would be banned in the ACT.

Aside from professional racing, many Greyhounds enjoy success on the amateur race track. Organizations like the Large Gazehound Racing Association (LGRA) and the National Oval Track Racing Association (NOTRA) provide opportunities for Greyhounds and other sighthound breeds to compete in amateur racing events all over the United States.

Historically, the Greyhound has, since its first appearance as a hunting type and breed, enjoyed a specific degree of fame and definition in Western literature, heraldry and art as the most elegant or noble companion and hunter of the canine world. In modern times, the professional racing industry, with its large numbers of track-bred Greyhounds, as well as international adoption programs aimed at rescuing and re-homing dogs that were at a surplus to the industry. They have redefined the breed in their almost mutually dependent pursuit of its welfare- as a sporting dog that will supply friendly companionship in its retirement. Outside the racing industry and coursing community, the Kennel Clubs' registered breed still enjoys a modest following as a show dog and pet. There is an emerging pattern visible in recent years (2009–2010) of a significant decline in track betting and multiple track closures in the US, which will have consequences for the origin of future companion Greyhounds and the re-homing of current ex-racers.

Greyhounds are typically a healthy and long-lived breed, and hereditary illness is rare. Some Greyhounds have been known to develop esophageal achalasia, gastric dilatation volvulus (also known as bloat), and osteosarcoma. If exposed to "E. coli", they may develop Alabama rot. Because the Greyhound's lean physique makes it ill-suited to sleeping on hard surfaces, owners of both racing and companion Greyhounds generally provide soft bedding; without bedding, Greyhounds are prone to develop painful skin sores. The average lifespan of a Greyhound is 9 to 11 years.

Due to the Greyhound's unique physiology and anatomy, a veterinarian who understands the issues relevant to the breed is generally needed when the dogs need treatment, particularly when anesthesia is required. Greyhounds cannot metabolize barbiturate-based anesthesia in the same way that other breeds can because their livers have lower amounts of oxidative enzymes. Greyhounds demonstrate unusual blood chemistry , which can be misread by veterinarians not familiar with the breed and can result in an incorrect diagnosis.

Greyhounds are very sensitive to insecticides. Many vets do not recommend the use of flea collars or flea spray on Greyhounds if the product is a pyrethrin-based. Products like Advantage, Frontline, Lufenuron, and Amitraz are safe for use on Greyhounds, however, and are very effective in controlling fleas and ticks.

Greyhounds have higher levels of red blood cells than other breeds. Since red blood cells carry oxygen to the muscles, this higher level allows the hound to move larger quantities of oxygen faster from the lungs to the muscles. Conversely, Greyhounds have lower levels of platelets than other breeds. Veterinary blood services often use Greyhounds as universal blood donors.

Greyhounds do not have undercoats and thus are less likely to trigger dog allergies in humans (they are sometimes incorrectly referred to as "hypoallergenic"). The lack of an undercoat, coupled with a general lack of body fat, also makes Greyhounds more susceptible to extreme temperatures (both hot and cold); because of this, they must be housed inside.

The key to the speed of a Greyhound can be found in its light but muscular build, large heart, highest percentage of fast-twitch muscle of any breed, double suspension gallop, and extreme flexibility of its spine. "Double suspension rotary gallop" describes the fastest running gait of the Greyhound in which all four feet are free from the ground in two phases, contracted and extended, during each full stride.

The breed's origin has in popular literature often been romantically connected to Ancient Egypt, in which it is believed "that the breed dates back about 4,000 years;" a belief for which there is no scientific evidence. While similar in appearance to Saluki (Persian Greyhound) or Sloughi (tombs at Beni Hassan c. 2000 BC), analyses of DNA reported in 2004 suggest that the Greyhound may not be closely related to these breeds, but is a close relative of herding dogs. Historical literature on the first sighthound in Europe (Arrian), the "vertragus", probable antecedent of the Greyhound, suggests that its origin lies with the ancient Celts from Eastern Europe or Eurasia. Greyhound-type dogs of small, medium, and large size, appear to have been bred across Europe since that time. All modern, pure-bred pedigree Greyhounds derive from the Greyhound stock recorded and registered first in private studbooks in the 18th century, then in public studbooks in the 19th century, which ultimately were registered with coursing, racing, and kennel club authorities of the United Kingdom.

Historically, these sighthounds were used primarily for hunting in the open where their keen eyesight was valuable. It is believed that they (or at least similarly named dogs) were introduced to the British Isles in the 5th and 6th century BC from Celtic mainland Europe, although the Picts and other peoples of the northern British Isles (modern Scotland) were believed to have had large hounds similar to that of the deerhound before the 6th century BC.

The name "Greyhound" is generally believed to come from the Old English "grighund". "Hund" is the antecedent of the modern "hound", but the meaning of "grig" is undetermined, other than in reference to dogs in Old English and Old Norse. Its origin does not appear to have any common root with the modern word "grey" for color, and indeed the Greyhound is seen with a wide variety of coat colors. The lighter colors, patch-like markings and white appeared in the breed that was once ordinarily grey in color. The Greyhound is the only dog mentioned by name in the Bible; many versions, including the King James version, name the Greyhound as one of the "four things stately" in the Proverbs. However, some newer biblical translations, including The New International Version, have changed this to "strutting rooster", which appears to be an alternative translation of the Hebrew term "mothen zarzir". However, the Douay–Rheims Bible translation from the late 4th-century Latin Vulgate into English translates this term as "a cock."

According to Pokorny the English name "Greyhound" does not mean "grey dog/hound", but simply "fair dog". Subsequent words have been derived from the Proto-Indo-European root *g'her- "shine, twinkle": English "grey", Old High German "gris" "grey, old," Old Icelandic "griss" "piglet, pig," Old Icelandic "gryja" "to dawn," "gryjandi" "morning twilight," Old Irish "grian" "sun," Old Church Slavonic "zorja" "morning twilight, brightness." The common sense of these words is "to shine; bright."

In 1928, the first winner of Best in Show at Crufts was Primley Sceptre, a Greyhound owned by H. Whitley.







</doc>
<doc id="12939" url="https://en.wikipedia.org/wiki?curid=12939" title="Geometric algebra">
Geometric algebra

The geometric algebra (GA) of a vector space is an algebra over a field, noted for its multiplication operation called the geometric product on a space of elements called multivectors, which is a superset of both the scalars formula_1 and the vector space formula_2. Mathematically, a geometric algebra may be defined as the Clifford algebra of a vector space with a quadratic form. Clifford's contribution was to define a new product, the geometric product, that united the Grassmann and Hamilton algebras into a single structure. Adding the dual of the Grassmann exterior product (the "meet") allows the use of the Grassmann–Cayley algebra and a conformal version of the latter together with a conformal Clifford algebra yields a conformal geometric algebra (CGA) providing a framework for classical geometries. In practice, these and several derived operations allow a correspondence of elements, subspaces and operations of the algebra with geometric interpretations.

The scalars and vectors have their usual interpretation, and make up distinct subspaces of a GA. Bivectors provide a more natural representation of pseudovector quantities in vector algebra such as oriented area, oriented angle of rotation, torque, angular momentum, electromagnetic field and the Poynting vector. A trivector can represent an oriented volume, and so on. An element called a blade may be used to represent a subspace of formula_2 and orthogonal projections onto that subspace. Rotations and reflections are represented as elements. Unlike vector algebra, a GA naturally accommodates any number of dimensions and any quadratic form such as in relativity.

Specific examples of geometric algebras applied in physics include the spacetime algebra (or the less common alternative formulation, the algebra of physical space) and the conformal geometric algebra. Geometric calculus, an extension of GA that incorporates differentiation and integration, can be used to formulate other theories such as complex analysis, differential geometry, e.g. by using the Clifford algebra instead of differential forms. Geometric algebra has been advocated, most notably by David Hestenes and Chris Doran, as the preferred mathematical framework for physics. Proponents claim that it provides compact and intuitive descriptions in many areas including classical and quantum mechanics, electromagnetic theory and relativity. GA has also found use as a computational tool in computer graphics and robotics.

The geometric product was first briefly mentioned by Hermann Grassmann, who was chiefly interested in developing the closely related exterior algebra. In 1878, William Kingdon Clifford greatly expanded on Grassmann's work to form what are now usually called Clifford algebras in his honor (although Clifford himself chose to call them "geometric algebras"). For several decades, geometric algebras went somewhat ignored, greatly eclipsed by the vector calculus then newly developed to describe electromagnetism. The term "geometric algebra" was repopularized in the 1960s by Hestenes, who advocated its importance to relativistic physics.

There are a number of different ways to define a geometric or Clifford algebra. Hestenes's original approach was axiomatic, "full of geometric significance" and equivalent to the universal Clifford algebra.
Given a finite-dimensional quadratic space formula_2 over a field formula_1 with a symmetric bilinear form (the "inner product", e.g. the Euclidean or Lorentzian metric) formula_6, the geometric algebra for this quadratic space is the geometric algebra formula_7, together with the exterior algebra formula_8. Since it is usual in GA, for the remainder of this article, only the real case, formula_9, will be considered. The notation formula_10 or formula_11 will be used to describe the geometric algebra.

The essential product in the algebra is called the "geometric product", and the product in the contained exterior algebra is called the "exterior product" (frequently called the "outer product" and less often the "wedge"). It is standard to denote these respectively by juxtaposition (i.e., suppressing any explicit multiplication symbol) and the symbol formula_12. The above definition of the geometric algebra is abstract, so we summarize the properties of the geometric product by the following set of axioms. The geometric product of formula_11 has the following properties for multivectors formula_14:

The exterior product has the same properties, except that formula_33 for formula_34.

Note that in the final property above, the real number formula_35 need not be nonnegative if formula_36 is not positive-definite. An important property of the geometric product is the existence of elements having a multiplicative inverse. If formula_37 for some vector formula_29, then formula_39 exists and is equal to formula_40. A nonzero element of the algebra does not necessarily have a multiplicative inverse. For example, if formula_41 is a vector in formula_2 such that formula_43, the element formula_44 is both a nontrivial idempotent element and a nonzero zero divisor, and thus has no inverse.

It is usual to identify formula_45 with formula_46, with associated natural embeddings formula_47 and formula_48. In this article, this identification is assumed. Throughout, the term "vector" refers to an element of formula_2 (and its image under this embedding).

 
For vectors formula_29 and formula_51, we may write the geometric product of any two vectors formula_29 and formula_51 as the sum of a symmetric product and an antisymmetric product:

Thus we can define the "inner product" of vectors as

so that the symmetric product can be written as

Conversely, formula_36 is completely determined by the algebra. The antisymmetric part is the exterior product of the two vectors, the product of the contained exterior algebra:

Then by simple addition:

The inner and exterior products are associated with familiar concepts from standard vector algebra. Geometrically, formula_29 and formula_51 are parallel if their geometric product is equal to their inner product, whereas formula_29 and formula_51 are perpendicular if their geometric product is equal to their exterior product. In a geometric algebra for which the square of any nonzero vector is positive, the inner product of two vectors can be identified with the dot product of standard vector algebra. The exterior product of two vectors can be identified with the signed area enclosed by a parallelogram the sides of which are the vectors. The cross product of two vectors in formula_64 dimensions with positive-definite quadratic form is closely related to their exterior product.

Most instances of geometric algebras of interest have a nondegenerate quadratic form. If the quadratic form is fully degenerate, the inner product of any two vectors is always zero, and the geometric algebra is then simply an exterior algebra. Unless otherwise stated, this article will treat only nondegenerate geometric algebras.

The exterior product is naturally extended as an associative bilinear binary operator between any two elements of the algebra, satisfying the identities

where the sum is over all permutations of the indices, with formula_67 the sign of the permutation, and formula_68 are vectors (not general elements of the algebra). Since every element of the algebra can be expressed as the sum of products of this form, this defines the exterior product for every pair of elements of the algebra. It follows from the definition that the exterior product forms an alternating algebra.

A multivector that is the exterior product of formula_69 linearly independent vectors is called a "blade", and the blade is said to be a multivector of grade formula_69. From the axioms, with closure, every multivector of the geometric algebra is a sum of blades.

Consider a set of formula_69 linearly independent vectors formula_72 spanning an formula_69-dimensional subspace of the vector space. With these, we can define a real symmetric matrix

By the spectral theorem, formula_75 can be diagonalized to diagonal matrix formula_76 by an orthogonal matrix formula_77 via

Define a new set of vectors formula_79, known as orthogonal basis vectors, to be those transformed by the orthogonal matrix:

Since orthogonal transformations preserve inner products, it follows that formula_81 and thus the formula_79 are perpendicular. In other words, the geometric product of two distinct vectors formula_83 is completely specified by their exterior product, or more generally

Therefore, every blade of grade formula_69 can be written as a geometric product of formula_69 vectors. More generally, if a degenerate geometric algebra is allowed, then the orthogonal matrix is replaced by a block matrix that is orthogonal in the nondegenerate block, and the diagonal matrix has zero-valued entries along the degenerate dimensions. If the new vectors of the nondegenerate subspace are normalized according to

then these normalized vectors must square to formula_88 or formula_89. By Sylvester's law of inertia, the total number of formula_88s and the total number of formula_89s along the diagonal matrix is invariant. By extension, the total number formula_92 of these vectors that square to formula_88 and the total number formula_94 that square to formula_89 is invariant. (The total number of basis vectors that square to zero is also invariant, and may be nonzero if the degenerate case is allowed.) We denote this algebra formula_96. For example, formula_97 models formula_64-dimensional Euclidean space, formula_99 relativistic spacetime and formula_100 a conformal geometric algebra of a formula_64-dimensional space.

The set of all possible products of formula_102 orthogonal basis vectors with indices in increasing order, including formula_17 as the empty product, forms a basis for the entire geometric algebra (an analogue of the PBW theorem). For example, the following is a basis for the geometric algebra formula_104:
A basis formed this way is called a canonical basis for the geometric algebra, and any other orthogonal basis for formula_2 will produce another canonical basis. Each canonical basis consists of formula_107 elements. Every multivector of the geometric algebra can be expressed as a linear combination of the canonical basis elements. If the canonical basis elements are formula_108 with formula_109 being an index set, then the geometric product of any two multivectors is

The terminology "formula_111-vector" is often encountered to describe multivectors containing elements of only one grade. In higher dimensional space, some such multivectors are not blades (cannot be factored into the outer product of formula_111 vectors). By way of example, formula_113 in formula_114 cannot be factored; typically, however, such elements of the algebra do not yield to any geometric interpretation. Only formula_115 and formula_102-vectors are always blades in formula_102-space.

Using an orthogonal basis, a graded vector space structure can be established. Elements of the geometric algebra that are scalar multiples of formula_17 are grade-formula_119 blades and are called "scalars". Multivectors that are in the span of formula_120 are grade-formula_17 blades and are the ordinary vectors. Multivectors in the span of formula_122 are grade-formula_123 blades and are the bivectors. This terminology continues through to the last grade of formula_102-vectors. Alternatively, grade-formula_102 blades are called pseudoscalars, grade-formula_126 blades pseudovectors, etc. Many of the elements of the algebra are not graded by this scheme since they are sums of elements of differing grade. Such elements are said to be of "mixed grade". The grading of multivectors is independent of the basis chosen originally.

A multivector formula_18 may be decomposed with the grade-projection operator formula_128, which outputs the grade-formula_69 portion of formula_18. As a result:

As an example, the geometric product of two vectors formula_132 since formula_133 and formula_134 and formula_135, for formula_136 other than formula_119 and formula_123.

The decomposition of a multivector formula_18 may also be split into those components that are even and those that are odd:

This makes the algebra a formula_142-graded algebra or superalgebra with the geometric product. Since the geometric product of two even multivectors is an even multivector, they define an "even subalgebra". The even subalgebra of an formula_102-dimensional geometric algebra is isomorphic to a full geometric algebra of formula_126 dimensions. Examples include formula_145 and formula_146.

Geometric algebra represents subspaces of formula_2 as blades, and so they coexist in the same algebra with vectors from formula_2. A formula_111-dimensional subspace formula_150 of formula_2 is represented by taking an orthogonal basis formula_152 and using the geometric product to form the blade formula_153. There are multiple blades representing formula_150; all those representing formula_150 are scalar multiples of formula_156. These blades can be separated into two sets: positive multiples of formula_156 and negative multiples of formula_156. The positive multiples of formula_156 are said to have "the same orientation" as formula_156, and the negative multiples the "opposite orientation".

Blades are important since geometric operations such as projections, rotations and reflections depend on the factorability via the exterior product that (the restricted class of) formula_102-blades provide but that (the generalized class of) grade-formula_102 multivectors do not when formula_163.

Unit pseudoscalars are blades that play important roles in GA. A unit pseudoscalar for a non-degenerate subspace formula_150 of formula_2 is a blade that is the product of the members of an orthonormal basis for formula_150. It can be shown that if formula_167 and formula_168 are both unit pseudoscalars for formula_150, then formula_170 and formula_171.

Suppose the geometric algebra formula_172 with the familiar positive definite inner product on formula_173 is formed. Given a plane (formula_123-dimensional subspace) of formula_173, one can find an orthonormal basis formula_176 spanning the plane, and thus find a unit pseudoscalar formula_177 representing this plane. The geometric product of any two vectors in the span of formula_178 and formula_179 lies in formula_180, that is, it is the sum of a formula_119-vector and a formula_123-vector.

By the properties of the geometric product, formula_183. The resemblance to the imaginary unit is not incidental: the subspace formula_184 is formula_185-algebra isomorphic to the complex numbers. In this way, a copy of the complex numbers is embedded in the geometric algebra for each 2-dimensional subspace of formula_2 on which the quadratic form is definite.

It is sometimes possible to identify the presence of an imaginary unit in a physical equation. Such units arise from one of the many quantities in the real algebra that square to formula_89, and these have geometric significance because of the properties of the algebra and the interaction of its various subspaces.

In formula_104, a further familiar case occurs. Given a canonical basis consisting of orthonormal vectors formula_189 of formula_2, the set of "all" formula_123-vectors is spanned by
Labelling these formula_136, formula_194 and formula_111 (momentarily deviating from our uppercase convention), the subspace generated by formula_119-vectors and formula_123-vectors is exactly formula_198. This set is seen to be a subalgebra of formula_104, and furthermore is isomorphic as an formula_185-algebra to the quaternions, another important algebraic system.

Let formula_201 be a basis of formula_2, i.e. a set of formula_102 linearly independent vectors that span the formula_102-dimensional vector space formula_2. The basis that is dual to formula_201 is the set of elements of the dual vector space formula_207 that forms a biorthogonal system with this basis, thus being the elements denoted formula_208 satisfying
where formula_210 is the Kronecker delta.

Given a nondegenerate quadratic form on formula_2, formula_207 becomes naturally identified with formula_2, and the dual basis may be regarded as elements of formula_2, but are not in general the same set as the original basis.

Given further a GA of formula_2, let 
be the pseudoscalar (which does not necessarily square to formula_217) formed from the basis formula_201. The dual basis vectors may be constructed as
where the formula_220 denotes that the formula_136th basis vector is omitted from the product.

It is common practice to extend the exterior product on vectors to the entire algebra. This may be done through the use of the grade projection operator:

This generalization is consistent with the above definition involving antisymmetrization. Another generalization related to the exterior product is the commutator product:

The regressive product (usually referred to as the "meet") is the dual of the exterior product (or "join" in this context). The dual specification of elements permits, for blades formula_18 and formula_21, the intersection (or meet) where the duality is to be taken relative to the smallest grade blade containing both formula_18 and formula_21 (the join).
with formula_167 the unit pseudoscalar of the algebra. The regressive product, like the exterior product, is associative.

The inner product on vectors can also be generalized, but in more than one non-equivalent way. The paper gives a full treatment of several different inner products developed for geometric algebras and their interrelationships, and the notation is taken from there. Many authors use the same symbol as for the inner product of vectors for their chosen extension (e.g. Hestenes and Perwass). No consistent notation has emerged.

Among these several different generalizations of the inner product on vectors are:

A number of identities incorporating the contractions are valid without restriction of their inputs.
For example,

Benefits of using the left contraction as an extension of the inner product on vectors include that the identity formula_240 is extended to formula_241 for any vector formula_29 and multivector formula_21, and that the projection operation formula_244 is extended to formula_245 for any blade formula_21 and any multivector formula_18 (with a minor modification to accommodate null formula_21, given below).

Although versors are inherently more useful, they are a subgroup of linear functions on multivectors and these can still be used when necessary. The geometric algebra of an formula_102-dimensional vector space is spanned by a basis of formula_107 elements. If a multivector is represented by a formula_251 real column matrix in terms of a basis, then all linear transformations of the multivector can be expressed as the matrix multiplication by a formula_252 real matrix. However, such a general linear transformation allows arbitrary exchanges among grades, such as a "rotation" of a scalar into a vector, which has no evident geometric interpretation.

A general linear transformation from vectors to vectors is of interest. With the natural restriction to preserving the induced exterior algebra, the "outermorphism" of the linear transformation is its unique extension. If formula_253 is a linear function that maps vectors to vectors, then its outermorphism is the function that obeys the rule
for a blade, extended to the whole algebra through linearity.

Although a lot of attention has been placed on CGA in recent years, it is to be noted that GA is not just one algebra, it is one of a family of algebras with the same essential structure.

formula_97 may be considered as an extension or completion of vector algebra. "From Vectors to Geometric Algebra" covers basic analytic geometry and gives an introduction to stereographic projection.

The even subalgebra of formula_256 is isomorphic to the complex numbers, as may be seen by writing a vector formula_257 in terms of its components in an orthonormal basis and left multiplying by the basis vector formula_258, yielding

where we identify formula_260 since

Similarly, the even subalgebra of formula_97 with basis formula_263 is isomorphic to the quaternions as may be seen by identifying formula_264, formula_265 and formula_266.

Every associative algebra has a matrix representation; replacing the three Cartesian basis vectors by the Pauli matrices gives a representation of formula_97:

Dotting the "Pauli vector" (a dyad):

In physics, the main applications are the geometric algebra of Minkowski 3+1 spacetime, , called spacetime algebra (STA), or less commonly, , called the algebra of physical space (APS), where is isomorphic to the "even" subalgebra of the 3+1 algebra, .

While in STA points of spacetime are represented simply by vectors, in APS, points of formula_276-dimensional spacetime are instead represented by paravectors: a formula_64-dimensional vector (space) plus a formula_17-dimensional scalar (time).

In spacetime algebra the electromagnetic field tensor has a bivector representation formula_279. Here, the formula_280 is the unit pseudoscalar (or four-dimensional volume element), formula_281 is the unit vector in time direction, and formula_282 and formula_21 are the classic electric and magnetic field vectors (with a zero time component). Using the four-current formula_284, Maxwell's equations then become

In geometric calculus, juxtapositioning of vectors such as in formula_285 indicate the geometric product and can be decomposed into parts as formula_286. Here formula_156 is the covector derivative in any spacetime and reduces to formula_288 in flat spacetime. Where formula_289 plays a role in Minkowski formula_290-spacetime which is synonymous to the role of formula_288 in Euclidean formula_64-space and is related to the d'Alembertian by formula_293. Indeed, given an observer represented by a future pointing timelike vector formula_281 we have

Boosts in this Lorentzian metric space have the same expression formula_297 as rotation in Euclidean space, where formula_298 is the bivector generated by the time and the space directions involved, whereas in the Euclidean case it is the bivector generated by the two space directions, strengthening the "analogy" to almost identity.

The Dirac matrices are a representation of formula_99, showing the equivalence with matrix representations used by physicists.

The first model here is formula_300, the GA version of homogeneous coordinates used in projective geometry. Here a vector represents a point and an outer product of vectors an oriented length yet we may work with the algebra in just the same way as in formula_97. However, a useful inner product cannot be defined in the space and so there is no geometric product either leaving only outer product and non-metric uses of duality such as meet and join.

Nevertheless, there has been investigation of 4-dimensional alternatives to the full 5-dimensional CGA for limited geometries such as rigid body movements. A selection of these can be found in Part IV of Guide to Geometric Algebra in Practice. Note that the algebra formula_302 appears as a subalgebra of CGA by selecting just one null vector and dropping the other and further that the “motor algebra” (isomorphic to dual quaternions) is the even subalgebra of formula_302.

In a recent paper, Dorst seeks a solution for expressing projective transformations in GA (usually handled via 4-by-4 coordinate matrices). In a postscript to that paper, reference is made to a further paper that Dorst describes as resolving most of the weaknesses in this (research) area.

A compact description of the current state of the art is provided by , which also includes further references, in particular to . Other useful references are and .
Working within GA, Euclidean space formula_304 (along with a conformal point at infinity) is embedded projectively in the CGA formula_305 via the identification of Euclidean points with formula_17-d subspaces in the formula_290-d null cone of the formula_308-d CGA vector subspace. This allows all conformal transformations to be done as rotations and reflections and is covariant, extending incidence relations of projective geometry to circles and spheres.

Specifically, we add orthogonal basis vectors formula_309 and formula_310 such that formula_311 and formula_312 to the basis of formula_313 and identify null vectors

This procedure has some similarities to the procedure for working with homogeneous coordinates in projective geometry and in this case allows the modeling of Euclidean transformations as orthogonal transformations.

A fast changing and fluid area of GA, CGA is also being investigated for applications to relativistic physics.

For any vector formula_29 and any invertible vector formula_318,
where the projection of formula_29 onto formula_318 (or the parallel part) is
and the rejection of formula_29 from formula_318 (or the orthogonal part) is

Using the concept of a formula_111-blade formula_21 as representing a subspace of formula_2 and every multivector ultimately being expressed in terms of vectors, this generalizes to projection of a general multivector onto any invertible formula_111-blade formula_21 as
with the rejection being defined as

The projection and rejection generalize to null blades formula_21 by replacing the inverse formula_334 with the pseudoinverse formula_335 with respect to the contractive product. The outcome of the projection coincides in both cases for non-null blades. For null blades formula_21, the definition of the projection given here with the first contraction rather than the second being onto the pseudoinverse should be used, as only then is the result necessarily in the subspace represented by formula_21.
The projection generalizes through linearity to general multivectors formula_18. The projection is not linear in formula_21 and does not generalize to objects formula_21 that are not blades.

Simple reflections in a hyperplane are readily expressed in the algebra through conjugation with a single vector. These serve to generate the group of general rotoreflections and rotations.

The reflection formula_341 of a vector formula_342 along a vector formula_318, or equivalently in the hyperplane orthogonal to formula_318, is the same as negating the component of a vector parallel to formula_318. The result of the reflection will be 

This is not the most general operation that may be regarded as a reflection when the dimension formula_163. A general reflection may be expressed as the composite of any odd number of single-axis reflections. Thus, a general reflection formula_348 of a vector formula_29 may be written
where

If we define the reflection along a non-null vector formula_318 of the product of vectors as the reflection of every vector in the product along the same vector, we get for any product of an odd number of vectors that, by way of example,
and for the product of an even number of vectors that

Using the concept of every multivector ultimately being expressed in terms of vectors, the reflection of a general multivector formula_18 using any reflection versor formula_357 may be written
where formula_359 is the automorphism of reflection through the origin of the vector space (formula_360) extended through linearity to the whole algebra.

If we have a product of vectors formula_361 then we denote the reverse as

As an example, assume that formula_363 we get

Scaling formula_365 so that formula_366 then

so formula_368 leaves the length of formula_369 unchanged. We can also show that

so the transformation formula_368 preserves both length and angle. It therefore can be identified as a rotation or rotoreflection; formula_365 is called a rotor if it is a proper rotation (as it is if it can be expressed as a product of an even number of vectors) and is an instance of what is known in GA as a "versor".

There is a general method for rotating a vector involving the formation of a multivector of the form formula_373 that produces a rotation formula_374 in the plane and with the orientation defined by a formula_123-blade formula_376.

Rotors are a generalization of quaternions to formula_102-dimensional spaces.

A formula_111-versor is a multivector represented by the geometric product of formula_111 invertible vectors. Unit quaternions (originally called versors by Hamilton) may be identified with rotors in 3D space in much the same way as real 2D rotors subsume complex numbers; for the details refer to Dorst.

Some authors use the term “versor product” to refer to the frequently occurring case where an operand is "sandwiched" between operators. The descriptions for rotations and reflections, including their outermorphisms, are examples of such sandwiching. The outermorphisms have a particularly simple algebraic form. Specifically, a mapping of vectors of the form

Since both operators and operand are versors there is potential for alternative examples such as rotating a rotor or reflecting a spinor always provided that some geometrical or physical significance can be attached to such operations.

By the Cartan–Dieudonné theorem we have that every isometry can be given as reflections in hyperplanes and since composed reflections provide rotations then we have that orthogonal transformations are versors.

In group terms, for a real, non-degenerate formula_10, having identified the group formula_383 as the group of all invertible elements of formula_384, Lundholm gives a proof that the "versor group" formula_385 (the set of invertible versors) is equal to the Lipschitz group formula_386 ( Clifford group, although Lundholm deprecates this usage).

Lundholm defines the formula_387, formula_388, and formula_389 subgroups, generated by unit vectors, and in the case of formula_388 and formula_389, only an even number of such vector factors can be present.
Spinors are defined as elements of the even subalgebra of a real GA; an analysis of the GA approach to spinors is given by Francis and Kosowsky.

For vectors formula_270 and formula_271 spanning a parallelogram we have
with the result that formula_395 is linear in the product of the "altitude" and the "base" of the parallelogram, that is, its area.

Similar interpretations are true for any number of vectors spanning an formula_102-dimensional parallelotope; the exterior product of vectors formula_397, that is formula_398, has a magnitude equal to the volume of the formula_102-parallelotope. An formula_102-vector does not necessarily have a shape of a parallelotope – this is a convenient visualization. It could be any shape, although the volume equals that of the parallelotope.

We may define the line parametrically by formula_401 where formula_92 and formula_403 are position vectors for points T and P and formula_369 is the direction vector for the line.

Then
so
and

The mathematical description of rotational forces such as torque and angular momentum often makes use of the cross product of vector calculus in three dimensions with a convention of orientation (handedness).
The cross product can be viewed in terms of the exterior product allowing a more natural geometric interpretation of the cross product as a bivector using the dual relationship

For example, torque is generally defined as the magnitude of the perpendicular force component times distance, or work per unit angle.

Suppose a circular path in an arbitrary plane containing orthonormal vectors formula_410 and formula_411 is parameterized by angle.

By designating the unit bivector of this plane as the imaginary number

this path vector can be conveniently written in complex exponential form

and the derivative with respect to angle is

So the torque, the rate of change of work formula_150, due to a force formula_1, is

Unlike the cross product description of torque, formula_420, the geometric algebra description does not introduce a vector in the normal direction; a vector that does not exist in two and that is not unique in greater than three dimensions. The unit bivector describes the plane and the orientation of the rotation, and the sense of the rotation is relative to the angle between the vectors formula_421 and formula_422.

Geometric calculus extends the formalism to include differentiation and integration including differential geometry and differential forms.

Essentially, the vector derivative is defined so that the GA version of Green's theorem is true,
and then one can write
as a geometric product, effectively generalizing Stokes' theorem (including the differential form version of it).

In formula_425 when formula_18 is a curve with endpoints formula_29 and formula_51, then
reduces to
or the fundamental theorem of integral calculus.

Also developed are the concept of vector manifold and geometric integration theory (which generalizes Cartan's differential forms).


Although the connection of geometry with algebra dates as far back at least to Euclid's "Elements" in the third century B.C. (see Greek geometric algebra), GA in the sense used in this article was not developed until 1844, when it was used in a "systematic way" to describe the geometrical properties and "transformations" of a space. In that year, Hermann Grassmann introduced the idea of a geometrical algebra in full generality as a certain calculus (analogous to the propositional calculus) that encoded all of the geometrical information of a space. Grassmann's algebraic system could be applied to a number of different kinds of spaces, the chief among them being Euclidean space, affine space, and projective space. Following Grassmann, in 1878 William Kingdon Clifford examined Grassmann's algebraic system alongside the quaternions of William Rowan Hamilton in . From his point of view, the quaternions described certain "transformations" (which he called "rotors"), whereas Grassmann's algebra described certain "properties" (or "Strecken" such as length, area, and volume). His contribution was to define a new product — the "geometric product" – on an existing Grassmann algebra, which realized the quaternions as living within that algebra. Subsequently, Rudolf Lipschitz in 1886 generalized Clifford's interpretation of the quaternions and applied them to the geometry of rotations in formula_102 dimensions. Later these developments would lead other 20th-century mathematicians to formalize and explore the properties of the Clifford algebra.

Nevertheless, another revolutionary development of the 19th-century would completely overshadow the geometric algebras: that of vector analysis, developed independently by Josiah Willard Gibbs and Oliver Heaviside. Vector analysis was motivated by James Clerk Maxwell's studies of electromagnetism, and specifically the need to express and manipulate conveniently certain differential equations. Vector analysis had a certain intuitive appeal compared to the rigors of the new algebras. Physicists and mathematicians alike readily adopted it as their geometrical toolkit of choice, particularly following the influential 1901 textbook "Vector Analysis" by Edwin Bidwell Wilson, following lectures of Gibbs.

In more detail, there have been three approaches to geometric algebra: quaternionic analysis, initiated by Hamilton in 1843 and geometrized as rotors by Clifford in 1878; geometric algebra, initiated by Grassmann in 1844; and vector analysis, developed out of quaternionic analysis in the late 19th century by Gibbs and Heaviside. The legacy of quaternionic analysis in vector analysis can be seen in the use of formula_136, formula_194, formula_111 to indicate the basis vectors of formula_435: it is being thought of as the purely imaginary quaternions. From the perspective of geometric algebra, quaternions can be identified as, the even part of the Clifford algebra on Euclidean 3-space, which unifies the three approaches.


Progress on the study of Clifford algebras quietly advanced through the twentieth century, although largely due to the work of abstract algebraists such as Hermann Weyl and Claude Chevalley. The "geometrical" approach to geometric algebras has seen a number of 20th-century revivals. In mathematics, Emil Artin's "Geometric Algebra" discusses the algebra associated with each of a number of geometries, including affine geometry, projective geometry, symplectic geometry, and orthogonal geometry. In physics, geometric algebras have been revived as a "new" way to do classical mechanics and electromagnetism, together with more advanced topics such as quantum mechanics and gauge theory. David Hestenes reinterpreted the Pauli and Dirac matrices as vectors in ordinary space and spacetime, respectively, and has been a primary contemporary advocate for the use of geometric algebra.

In computer graphics and robotics, geometric algebras have been revived in order to efficiently represent rotations and other transformations. For applications of GA in robotics (screw theory, kinematics and dynamics using versors), computer vision, control and neural computing (geometric learning) see Bayro (2010).

GA is a very application-oriented subject. There is a reasonably steep initial learning curve associated with it, but this can be eased somewhat by the use of applicable software.

The following is a list of freely available software that does not require ownership of commercial software or purchase of any commercial products for this purpose:
The link provides a manual, introduction to GA and sample material as well as the software.
Software allowing script creation and including sample visualizations, manual and GA introduction.
For programmers, this is a code generator with support for C, C++, C# and Java.




English translations of early books and papers

Research groups


</doc>
<doc id="12942" url="https://en.wikipedia.org/wiki?curid=12942" title="Genetic">
Genetic

Genetic may refer to:


</doc>
<doc id="12945" url="https://en.wikipedia.org/wiki?curid=12945" title="George Benson">
George Benson

George Benson (born March 22, 1943) is an American guitarist, singer, and songwriter. He began his professional career at the age of 21 as a jazz guitarist. Benson uses a rest-stroke picking technique similar to that of gypsy jazz players such as Django Reinhardt.

A former child prodigy, Benson first came to prominence in the 1960s, playing soul jazz with Jack McDuff and others. He then launched a successful solo career, alternating between jazz, pop, R&B singing, and scat singing. His album "Breezin'" was certified triple-platinum, hitting no. 1 on the "Billboard" album chart in 1976. His concerts were well attended through the 1980s, and he still has a large following. Benson has been honored with a star on the Hollywood Walk of Fame.

Benson was born and raised in the Hill District in Pittsburgh, Pennsylvania. At the age of seven, he first played the ukulele in a corner drug store, for which he was paid a few dollars. At the age of eight, he played guitar in an unlicensed nightclub on Friday and Saturday nights, but the police soon closed the club down. At the age of 9, he started to record. Out of the four sides he cut, two were released: "She Makes Me Mad" backed with "It Should Have Been Me", with RCA-Victor in New York; although one source indicates this record was released under the name "Little Georgie", the 45rpm label is printed with the name George Benson. The single was produced by Leroy Kirkland for RCA's rhythm and blues label, Groove Records. As he has stated in an interview, Benson's introduction to showbusiness had an effect on his schooling. When this was discovered (tied with the failure of his single) his guitar was impounded. Luckily, after he spent time in a juvenile detention centre his stepfather made him a new guitar.* 

Benson attended and graduated from Schenley High School. As a youth he learned how to play straight-ahead instrumental jazz during a relationship performing for several years with organist Jack McDuff. One of his many early guitar heroes was country-jazz guitarist Hank Garland. At the age of 21, he recorded his first album as leader, "The New Boss Guitar", featuring McDuff. Benson's next recording was "It's Uptown with the George Benson Quartet", including Lonnie Smith on organ and Ronnie Cuber on baritone saxophone. Benson followed it up with "The George Benson Cookbook", also with Lonnie Smith and Ronnie Cuber on baritone and drummer Marion Booker. Miles Davis employed Benson in the mid-1960s, featuring his guitar on "Paraphernalia" on his 1968 Columbia release, "Miles in the Sky" before going to Verve Records.

Benson then signed with Creed Taylor's jazz label CTI Records, where he recorded several albums, with jazz heavyweights guesting, to some success, mainly in the jazz field. His 1974 release, "Bad Benson", climbed to the top spot in the "Billboard" jazz chart, while the follow-ups, "Good King Bad" (#51 Pop album) and "Benson and Farrell" (with Joe Farrell), both reached the jazz top-three sellers. Benson also did a version of The Beatles's 1969 album "Abbey Road" called "The Other Side of Abbey Road", also released in 1969, and a version of "White Rabbit", originally written and recorded by San Francisco rock group Great Society, and made famous by Jefferson Airplane. Benson played on numerous sessions for other CTI artists during this time, including Freddie Hubbard and Stanley Turrentine, notably on the latter's acclaimed album "Sugar".

By the mid-to-late 1970s, as he recorded for Warner Bros. Records, a whole new audience began to discover Benson. With the 1976 release "Breezin'", Benson sang a lead vocal on the track "This Masquerade" (notable also for the lush, romantic piano intro and solo by Jorge Dalto), which became a huge pop hit and won a Grammy Award for Record of the Year. (He had sung vocals infrequently on albums earlier in his career, notably his rendition of "Here Comes the Sun" on the "Other Side of Abbey Road" album.) The rest of the album is instrumental, including his rendition of the 1975 Jose Feliciano composition "Affirmation".

In 1976, Benson toured with soul singer Minnie Riperton, who had been diagnosed with terminal breast cancer earlier that year and, in addition, appeared as a guitarist and backup vocalist on Stevie Wonder's song "Another Star" from Wonder's album "Songs in the Key of Life". 

During the same year, 1976, the top selling album 'Breezin' was released on the Warner Brothers label featuring the Bobby Womack penned title track and the Leon Russell penned This Masquerade which is now a jazz standard. Both tracks won Grammy awards that year and the LP put Benson into the musical limelight both in the USA and in Europe. Ironically, Benson had been discouraged up until this time, from using his singing skills, mainly as the company decision makers felt he wasn't competent enough vocally, and he should stick to playing the guitar. It was here that he clearly proved them wrong. 

He also recorded the original version of "The Greatest Love of All" for the 1977 Muhammad Ali bio-pic, "The Greatest", which was later covered by Whitney Houston as "Greatest Love of All". During this time Benson recorded with the German conductor Claus Ogerman. The live take of "On Broadway", recorded a few months later from the 1978 release "Weekend in L.A.", also won a Grammy. He has worked with Freddie Hubbard on a number of his albums throughout the 1960s, 1970s and 1980s.

The Qwest record label (a subsidiary of Warner Bros., run by Quincy Jones) released Benson's breakthrough pop album "Give Me The Night", produced by Jones. Benson made it into the pop and R&B top ten with the song "Give Me the Night" (written by former Heatwave keyboardist Rod Temperton). He had many hit singles such as "Love All the Hurt Away", "Turn Your Love Around", "Inside Love", "Lady Love Me", "20/20", "Shiver", "Kisses in the Moonlight". More importantly, Quincy Jones encouraged Benson to search his roots for further vocal inspiration, and he rediscovered his love for Nat Cole, Ray Charles and Donny Hathaway in the process, influencing a string of further vocal albums into the 1990s. Despite returning to his jazz and guitar playing most recently, this theme was reflected again much later in Benson's 2000 release "Absolute Benson", featuring a cover of one of Hathaway's most notable songs, "The Ghetto". Benson accumulated three other platinum LPs and two gold albums.

In 1990, Benson was awarded an Honorary Doctorate of Music from the Berklee College of Music. 

In 1985, Benson and guitarist Chet Atkins went on the smooth jazz charts with their collaboration "Sunrise", one of two songs from the duo released on Atkins' "Stay Tuned" album. In 1992, Benson appeared on Jack McDuff's "Colour Me Blue" album, his first appearance on a Concord album. Benson signed with Concord Records in 2005 and toured with Al Jarreau in America, South Africa, Australia and New Zealand to promote their 2006 multiple Grammy-winning album "Givin' It Up".

To commemorate the long-term relationship between Benson and Ibanez and to celebrate 30 years of collaboration on the GB Signature Models, Ibanez created the GB30TH, a very limited-edition model featuring a gold-foil finish inspired by the traditional Japanese Garahaku art form. In 2009, Benson was recognized by the National Endowment of the Arts as a Jazz Master, the nation's highest honor in jazz. Benson performed at the 49th issue of the Ohrid Summer Festival in Macedonia on July 25, 2009, and his tribute show to Nat King Cole "An Unforgettable Tribute to Nat King Cole" as part of the Istanbul International Jazz Festival in Turkey on July 27. In the fall of 2009, Benson finished recording a new album entitled "Songs and Stories", with Marcus Miller, producer John Burk, and session musicians David Paich and Steve Lukather. As a part of the promotion for his recent Concord Music Group/Monster Music release Songs and Stories, Benson has appeared and/or performed on "The Tavis Smiley Show", "Jimmy Kimmel Live!" and "Late Night with Jimmy Fallon".

Benson toured throughout 2010 in North America, Europe and the Pacific Rim, including an appearance at the Singapore Sun Festival.
He performed at the Java Jazz Festival March 4–6, 2011. In 2011, Benson released the album "Guitar Man"—revisiting his 1960s/early-1970s guitar-playing roots with a 12-song collection of covers of both jazz and pop standards overseen by producer John Burk.

In June 2013, Benson released his fourth album for Concord Records, "", which featured Wynton Marsalis, Idina Menzel, Till Brönner, and Judith Hill. In September, he returned to perform at Rock in Rio festival, in Rio de Janeiro, 35 years after his first performance at this festival, which was then the inaugural one.

In July 2016, Benson participated as a mentor in the Sky Arts programme "Guitar Star" in the search for the UK and Republic of Ireland’s most talented guitarist.

In May 2018, Benson was featured on the Gorillaz single "Humility".

On July 12, 2018, it was announced that Benson had signed to Mascot Label Group, with a new album tenatively scheduled for the Spring of 2019.

Benson has been married to Johnnie Lee since 1965 and has seven children. Benson describes his music as focusing more on love and romance, and eschewing overt sexuality, due to his commitment to his family and religious practices, with Benson serving as a missionary for Jehovah's Witnesses. Benson stated that he donated considerable funds to the denomination's Watchtower Society.

Benson has been a resident of Englewood, New Jersey.

List of Grammy Awards received by George Benson




</doc>
<doc id="12946" url="https://en.wikipedia.org/wiki?curid=12946" title="Grigory Barenblatt">
Grigory Barenblatt

Grigory Isaakovich Barenblatt (; 10 July 1927 – 21 June 2018) was a Russian mathematician.

Barenblatt graduated in 1950 from Moscow State University, Department of Mechanics and Mathematics. He received his Ph.D. in 1953 from Moscow State University under the supervision of A. N. Kolmogorov.

Barenblatt also received a D.Sc. from Moscow State University in 1957. He was an emeritus Professor in Residence at the Department of Mathematics of the University of California, Berkeley and Mathematician at Department of Mathematics, Lawrence Berkeley National Laboratory. He was G. I. Taylor Professor of Fluid Mechanics at the University of Cambridge from 1992 to 1994 and he was Emeritus G. I. Taylor Professor of Fluid Mechanics. His areas of research were:




</doc>
<doc id="12947" url="https://en.wikipedia.org/wiki?curid=12947" title="Grammatical tense">
Grammatical tense

In grammar, tense is a category that expresses time reference with reference to the moment of speaking. Tenses are usually manifested by the use of specific forms of verbs, particularly in their conjugation patterns. 

Basic tenses found in many languages include the past, present, and future. Some languages have only two distinct tenses, such as past and nonpast, or future and nonfuture. There are also tenseless languages, like most of the Chinese languages, though it can possess a future and nonfuture system, which is typical of Sino-Tibetan languages. On the other hand, some languages make finer tense distinctions, such as remote vs. recent past, or near vs. remote future.

Tenses generally express time relative to the moment of speaking. In some contexts, however, their meaning may be relativized to a point in the past or future which is established in the discourse (the moment being spoken about). This is called "relative" (as opposed to "absolute") tense. Some languages have different verb forms or constructions which manifest relative tense, such as pluperfect ("past-in-the-past") and "future-in-the-past".

Expressions of tense are often closely connected with expressions of the category of aspect; sometimes what are traditionally called tenses (in languages such as Latin) may in modern analysis be regarded as combinations of tense with aspect. Verbs are also often conjugated for mood, and since in many cases the three categories are not manifested separately, some languages may be described in terms of a combined tense–aspect–mood (TAM) system.

The English noun "tense" comes from Old French "tens" "time" (spelled "temps" in modern French through deliberate archaisation), from Latin "time". It is not related to the adjective "tense", which comes from Latin "tensus", the perfect passive participle of "tendere" "stretch".

In modern linguistic theory, tense is understood as a category that expresses (grammaticalizes) time reference; namely one which, using grammatical means, places a state or action in time. Nonetheless, in many descriptions of languages, particularly in traditional European grammar, the term "tense" is applied to series of verb forms or constructions that express not merely position in time, but also additional properties of the state or action – particularly aspectual or modal properties.

The category of aspect expresses how a state or action relates to time – whether it is seen as a complete event, an ongoing or repeated situation, etc. Many languages make a distinction between perfective aspect (denoting complete events) and imperfective aspect (denoting ongoing or repeated situations); some also have other aspects, such as a perfect aspect, denoting a state following a prior event. Some of the traditional "tenses" express time reference together with aspectual information. In Latin and French, for example, the imperfect denotes past time in combination with imperfective aspect, while other verb forms (the Latin perfect, and the French "passé composé" or "passé simple") are used for past time reference with perfective aspect.

The category of mood is used to express modality, which includes such properties as uncertainty, evidentiality, and obligation. Commonly encountered moods include the indicative, subjunctive, and conditional. Mood can be bound up with tense, aspect, or both, in particular verb forms. Hence certain languages are sometimes analysed as having a single tense–aspect–mood (TAM) system, without separate manifestation of the three categories.
The term "tense", then, particularly in less formal contexts, is sometimes used to denote any combination of tense proper, aspect, and mood. As regards English, there are many verb forms and constructions which combine time reference with continuous and/or perfect aspect, and with indicative, subjunctive or conditional mood. Particularly in some English language teaching materials, some or all of these forms can be referred to simply as tenses (see below).

Particular tense forms need not always carry their basic time-referential meaning in every case. A present tense form may sometimes refer to the past (as in the historical present), a past tense form may sometimes refer to the non-past (as in some English conditional sentences), and so on.

Not all languages have tense: tenseless languages include Burmese, Chinese and Dyirbal. Some languages have all three basic tenses (the past, present, and future), while others have only two: some have past and nonpast tenses, the latter covering both present and future times (as in Japanese, and in English in some analyses), whereas others such as Greenlandic and Quechua have future and nonfuture. Some languages have four or more tenses, making finer distinctions either in the past (e.g. remote vs. recent past) or in the future (e.g. near vs. remote future). The six-tense language Kalaw Lagaw Ya of Australia has the remote past, the recent past, the today past, the present, the today/near future and the remote future.

A historical past tense, used for events perceived as historical, is found in, for example, the Amazonian Cubeo language.

Tenses that refer specifically to "today" are called hodiernal tenses; these can be either past or future. Apart from Kalaw Lagaw Ya, another language which features such tenses is Mwera, a Bantu language of Tanzania. It is also suggested that in 17th-century French, the "passé composé" served as a hodiernal past. Tenses which contrast with hodiernals, by referring to the past before today or the future after today, are called pre-hodiernal and post-hodiernal respectively. Some languages also have a crastinal tense, a future tense referring specifically to tomorrow (found in some Bantu languages); or a hesternal tense, a past tense referring specifically to yesterday (although this name is also sometimes used to mean pre-hodiernal). A tense for after tomorrow is thus called post-crastinal, and one for before yesterday is called pre-hesternal.

Another tense found in some languages, including Luganda, is the persistive tense, used to indicate that a state or ongoing action is still the case (or, in the negative, is no longer the case). Luganda also has tenses meaning "so far" and "not yet".

Some languages have special tense forms that are used to express relative tense. Tenses that refer to the past relative to the time under consideration are called "anterior"; these include the pluperfect (for the past relative to a past time) and the future perfect (for the past relative to a future time). Similarly, "posterior" tenses refer to the future relative to the time under consideration, as with the English "future-in-the-past": "(he said that) he would go." Relative tense forms are also sometimes analysed as combinations of tense with aspect: the perfect aspect in the anterior case, or the prospective aspect in the posterior case.

Tense is normally indicated by the use of a particular verb form – either an inflected form of the main verb, or a multi-word construction, or both in combination. Inflection may involve the use of affixes, such as the "-ed" ending that marks the past tense of English regular verbs, but can also entail stem modifications, such as ablaut, as found as in the strong verbs in English and other Germanic languages, or reduplication. Multi-word tense constructions often involve auxiliary verbs or clitics. Examples which combine both types of tense marking include the French "passé composé", which has an auxiliary verb together with the inflected past participle form of the main verb; and the Irish past tense, where the proclitic "do" (in various surface forms) appears in conjunction with the affixed or ablaut-modified past tense form of the main verb.

As has already been mentioned, indications of tense are often bound up with indications of other verbal categories, such as aspect and mood. The conjugation patterns of verbs often also reflect agreement with categories pertaining to the subject, such as person, number and gender. It is consequently not always possible to identify elements that mark any specific category, such as tense, separately from the others.

A few languages have been shown to mark tense information (as well as aspect and mood) on nouns. This may be called nominal TAM.

Languages that do not have grammatical tense, such as Chinese, express time reference chiefly by lexical means – through adverbials, time phrases, and so on. (The same is done in tensed languages, to supplement or reinforce the time information conveyed by the choice of tense.) Time information is also sometimes conveyed as a secondary feature by markers of other categories, as with the Chinese aspect markers "le" and "guo", which in most cases place an action in past time. However, much time information is conveyed implicitly by context – it is therefore not always necessary, when translating from a tensed to a tenseless language, say, to express explicitly in the target language all of the information conveyed by the tenses in the source.

Latin is traditionally described as having six tenses (the Latin for "tense" being "tempus", plural "tempora"):
Of these, the imperfect and perfect can be considered to represent a past tense combined with imperfective and perfective aspect respectively (the first is used for habitual or ongoing past actions or states, and the second for completed actions). The pluperfect and future perfect are relative tenses, referring to the past relative to a past time or relative to a future time.

Latin verbs are conjugated for tense (and aspect) together with mood (indicative, subjunctive, and sometimes imperative) and voice (active or passive). Most forms are produced by inflecting the verb stem, with endings that also depend on the person and number of the subject. Some of the passive forms are produced using a participle together with a conjugated auxiliary verb. For details of the forms, see Latin conjugation.

The tenses of Ancient Greek are similar, but with a three-way aspect contrast in the past: the aorist, the perfect and the imperfect. The aorist was the "simple past", while the imperfective denoted uncompleted action in the past, and the perfect was used for past events having relevance to the present.

The study of modern languages has been greatly influenced by the grammar of the Classical languages, since early grammarians, often monks, had no other reference point to describe their language. Latin terminology is often used to describe modern languages, sometimes with a change of meaning, as with the application of "perfect" to forms in English that do not necessarily have perfective meaning, or the words "Imperfekt" and "Perfekt" to German past tense forms that mostly lack any relationship to the aspects implied by those terms.

English has only two morphological tenses: the present or non-past, as in "he goes, and the past or preterite, as in "he went. The non-past usually references the present, but sometimes references the future (as in "the bus leaves tomorrow"). (It also sometimes references the past, however, in what is called the historical present.)

Constructions with the modal auxiliary verbs "will" and "shall" also frequently reference the future (although they have other uses as well); these are often described as the English future tense. Less commonly, forms with the auxiliaries "would" and (rarely) "should" are described as a relative tense, the future-in-the-past. (The same forms are used for the conditional mood, and for various other meanings.)

The present and past are distinguished by verb form, using either ablaut ("sing(s)" ~ "sang") or suffix ("walk(s)" ~ "walked"). For details, see English verbs.

English also has continuous (progressive) aspect and perfect aspect; these together produce four aspectual types: simple, continuous, perfect, and perfect continuous. Each of these can combine with the tenses to produce a large set of different constructions, mostly involving one or more auxiliary verbs together with a participle or infinitive:

In some contexts, particularly in English language teaching, the tense–aspect combinations in the above table may be referred to simply as tenses. For details of the uses of these constructions, as well as additional verb forms representing different grammatical moods, see Uses of English verb forms.

Proto-Indo-European verbs had present, perfect (stative), imperfect and aorist forms – these can be considered as representing two tenses (present and past) with different aspects. Most languages in the Indo-European family have developed systems either with two morphological tenses (present or "non-past", and past) or with three (present, past and future). The tenses often form part of entangled tense–aspect–mood conjugation systems. Additional tenses, tense–aspect combinations, etc. can be provided by compound constructions containing auxiliary verbs.

The Germanic languages (which include English) have present (non-past) and past tenses formed morphologically, with future and other additional forms made using auxiliaries. In standard German, the compound past "(Perfekt)" has replaced the simple morphological past in most contexts.

The Romance languages (descendants of Latin) have past, present and future morphological tenses, with additional aspectual distinction in the past. French is an example of a language where, as in German, the simple morphological perfective past "(passé simple)" has mostly given way to a compound form "(passé composé)".

Irish, a Celtic language, has past, present and future tenses (see Irish conjugation). The past contrasts perfective and imperfective aspect, and some verbs retain such a contrast in the present. Classical Irish had a three-way aspectual contrast of simple–perfective–imperfective in the past and present tenses.

Persian, an Indo-Iranian language, has past and non-past forms, with additional aspectual distinctions. Future can be expressed using an auxiliary, but almost never in non-formal context.

In the Slavic languages, verbs are intrinsically perfective or imperfective. In Russian and some other languages in the group, perfective verbs have past and future tenses, while imperfective verbs have past, present and future, the imperfective future being a compound tense in most cases. The future tense of perfective verbs is formed in the same way as the present tense of imperfective verbs. However, in South Slavic languages, there may be a greater variety of forms – Bulgarian, for example, has present, past (both "imperfect" and "aorist") and future tenses, for both perfective and imperfective verbs, as well as perfect forms made with an auxiliary (see Bulgarian verbs).

Finnish and Hungarian, both members of the Uralic language family, have morphological present (non-past) and past tenses. The Hungarian verb "van" ("to be") also has a future form.

Turkish verbs conjugate for past, present and future, with a variety of aspects and moods.

Arabic verbs have past and non-past; future can be indicated by a prefix.

Korean verbs have a variety of affixed forms which can be described as representing present, past and future tenses, although they can alternatively be considered to be aspectual. Similarly, Japanese verbs are described as having present and past tenses, although they may be analysed as aspects. Chinese and many other East Asian languages generally lack inflection and are considered to be tenseless languages, although they may have aspect markers which convey certain information about time reference.

For examples of languages with a greater variety of tenses, see the section on possible tenses, above. Fuller information on tense formation and usage in particular languages can be found in the articles on those languages and their grammars.

Rapa is the French Polynesian language of the island of Rapa Iti. Verbs in the indigenous Old Rapa occur with a marker known as TAM which stands for tense, aspect, or mood which can be followed by directional particles or deictic particles. Of the markers there are three tense markers called: Imperfective, Progressive, and Perfective. Which simply mean, Before, Currently, and After. However, specific TAM markers and the type of deictic or directional particle that follows determine and denote different types of meanings in terms of tenses.

Imperfective: denotes actions that have not occurred yet but will occur and expressed by TAM e.

Example:

Progressive: Also expressed by TAM e and denotes actions that are currently happening when used with deictic na, and denotes actions that was just witnessed but still currently happening when used with deictic ra.

Example: 

Perfective: denotes actions that have already occurred or have finished and is marked by TAM ka.

Example:

In Old Rapa there are also other types of tense markers known as Past, Imperative, and Subjunctive.

Past

TAM i marks past action. It is rarely used as a matrix TAM and is more frequently observed in past embedded clauses

Imperative

The imperative is marked in Old Rapa by TAM a. A second person subject is implied by the direct command of the imperative.

For a more polite form rather than a straightforward command imperative TAM a is used with adverbial kānei. Kānei is only shown to be used in imperative structures and was translated by the french as “please”.

It is also used in a more impersonal form. For example, how you would speak toward a pesky neighbor.

Subjunctive

The subjunctive in Old Rapa is marked by kia and can also be used in expressions of desire
The Tokelauan language is a tenseless language. The language uses the same words for all three tenses; the phrase E liliu mai au i te Aho Tōnai literally translates to Come back / me / on Saturday, but the translation becomes ‘I am coming back on Saturday’.

Wuvulu-Aua does not have an explicit tense, but rather tense is conveyed by mood, aspect markers, and time phrases. Wuvulu speakers use a realis mood to convey past tense as speakers can be certain about events that have occurred. In some cases, realis mood is used to convey present tense — often to indicate a state of being. Wuvulu speakers use an irrealis mood to convey future tense.

Tense in Wuvulu-Aua may also be implied by using time adverbials and aspectual markings. Wuvulu contains three verbal markers to indicate sequence of events. The preverbal adverbial "loʔo" 'first' indicates the verb occurs before any other. The postverbal morpheme "liai" and "linia" are the respective intransitive and transitive suffixes indicating a repeated action. The postverbal morpheme "li" and "liria" are the respective intransitive and transitive suffixes indicating a completed action.

Mortlockese uses tense markers such as "mii" and to denote the present tense state of a subject, "aa" to denote a present tense state that an object has changed to from a different, past state, "kɞ" to describe something that has already been completed, "pɞ" and "lɛ" to denote future tense, "pʷapʷ" to denote a possible action or state in future tense, and "sæn/mwo" for something that has not happened yet. Each of these markers is used in conjunction with the subject proclitics except for the markers "aa" and "mii". Additionally, the marker "mii" can be used with any type of intransitive verb.




</doc>
<doc id="12948" url="https://en.wikipedia.org/wiki?curid=12948" title="Grammatical aspect">
Grammatical aspect

Aspect is a grammatical category that expresses how an action, event, or state, denoted by a verb, extends over time. Perfective aspect is used in referring to an event conceived as bounded and unitary, without reference to any flow of time during ("I helped him"). Imperfective aspect is used for situations conceived as existing continuously or repetitively as time flows ("I was helping him"; "I used to help people").

Further distinctions can be made, for example, to distinguish states and ongoing actions (continuous and progressive aspects) from repetitive actions (habitual aspect).

Certain aspectual distinctions express a relation in time between the event and the time of reference. This is the case with the perfect aspect, which indicates that an event occurred prior to (but has continuing relevance at) the time of reference: "I have eaten"; "I had eaten"; "I will have eaten".

Different languages make different grammatical aspectual distinctions; some (such as Standard German; see below) do not make any. The marking of aspect is often conflated with the marking of tense and mood (see tense–aspect–mood). Aspectual distinctions may be restricted to certain tenses: in Latin and the Romance languages, for example, the perfective–imperfective distinction is marked in the past tense, by the division between preterites and imperfects. Explicit consideration of aspect as a category first arose out of study of the Slavic languages; here verbs often occur in pairs, with two related verbs being used respectively for imperfective and perfective meanings.

The concept of grammatical aspect should not be confused with perfect and imperfect "verb forms"; the meanings of the latter terms are somewhat different, and in some languages, the common names used for verb forms may not follow the actual aspects precisely.

The Indian linguist Yaska (ca. 7th century BCE) dealt with grammatical aspect, distinguishing actions that are processes ("bhāva"), from those where the action is considered as a completed whole ("mūrta"). This is the key distinction between the imperfective and perfective. Yaska also applied this distinction to a verb versus an action nominal.

Grammarians of the Greek and Latin languages also showed an interest in aspect, but the idea did not enter into the modern Western grammatical tradition until the 19th century via the study of the grammar of the Slavic languages. The earliest use of the term recorded in the Oxford English Dictionary dates from 1853.

Aspect is often confused with the closely related concept of tense, because they both convey information about time. While tense relates the time of referent to some other time, commonly the speech event, aspect conveys other temporal information, such as duration, completion, or frequency, as it relates to the time of action. Thus tense refers to "temporally when" while aspect refers to "temporally how". Aspect can be said to describe the texture of the time in which a situation occurs, such as a single point of time, a continuous range of time, a sequence of discrete points in time, etc., whereas tense indicates its location in time.

For example, consider the following sentences: "I eat", "I am eating", "I have eaten", and "I have been eating". All are in the present tense, indicated by the present-tense verb of each sentence ("eat", "am", and "have"). Yet since they differ in aspect each conveys different information or points of view as to how the action pertains to the present.

Grammatical aspect is a "formal" property of a language, distinguished through overt inflection, derivational affixes, or independent words that serve as grammatically required markers of those aspects. For example, the K'iche' language spoken in Guatemala has the inflectional prefixes "k"- and "x"- to mark incompletive and completive aspect; Mandarin Chinese has the aspect markers -"le" 了, -"zhe" 着, "zài"- 在, and -"guò" 过 to mark the perfective, durative stative, durative progressive, and experiential aspects, and also marks aspect with adverbs; and English marks the continuous aspect with the verb "to be" coupled with present participle and the perfect with the verb "to have" coupled with past participle. Even languages that do not mark aspect morphologically or through auxiliary verbs, however, can convey such distinctions by the use of adverbs or other syntactic constructions.

Grammatical aspect is distinguished from lexical aspect or "aktionsart", which is an inherent feature of verbs or verb phrases and is determined by the nature of the situation that the verb describes.

The most fundamental aspectual distinction, represented in many languages, is between perfective aspect and imperfective aspect. This is the basic aspectual distinction in the Slavic languages. It semantically corresponds to the distinction between the morphological forms known respectively as the aorist and imperfect in Greek, the preterite and imperfect in Spanish, the simple past (passé simple) and imperfect in French, and the perfect and imperfect in Latin (from the Latin "perfectus", meaning "completed").

Essentially, the perfective aspect looks at an event as a complete action, while the imperfective aspect views an event as the process of unfolding or a repeated or habitual event (thus corresponding to the progressive/continuous aspect for events of short-term duration and to habitual aspect for longer terms). For events of short durations in the past, the distinction often coincides with the distinction in the English language between the simple past "X-ed," as compared to the progressive "was X-ing" (compare "I wrote the letters this morning" (i.e. finished writing the letters: an action completed and "I was writing letters this morning"). In describing longer time periods, English needs context to maintain the distinction between the habitual ("I called him often in the past" – a habit that has no point of completion) and perfective ("I called him once" – an action completed), although the construct "used to" marks both habitual aspect and past tense and can be used if the aspectual distinction otherwise is not clear.

Sometimes, English has a lexical distinction where other languages may use the distinction in grammatical aspect. For example, the English verbs "to know" (the state of knowing) and "to find out" (knowing viewed as a "completed action") correspond to the imperfect and perfect forms of the equivalent verbs in French and Spanish, "savoir" and "saber". This is also true when the sense of verb "to know" is "to know somebody", in this case opposed in aspect to the verb "to meet" (or even to the construction "to get to know"). These correspond to imperfect and perfect forms of "conocer" in Spanish. In German, on the other hand, the distinction is also lexical (as in English) through verbs "kennen" and "kennenlernen", although the semantic relation between both forms is much more straightforward since "kennen" means "to know" and "lernen" means "to learn".

The Germanic languages combine the concept of aspect with the concept of tense. Although English largely separates tense and aspect formally, its aspects (neutral, progressive, perfect, progressive perfect, and [in the past tense] habitual) do not correspond very closely to the distinction of perfective vs. imperfective that is found in most languages with aspect. Furthermore, the separation of tense and aspect in English is not maintained rigidly. One instance of this is the alternation, in some forms of English, between sentences such as "Have you eaten?" and "Did you eat?". Another is in the pluperfect ("I had eaten"), which sometimes represents the combination of past tense and perfect ("I was full because I had already eaten"), but sometimes simply represents a past action that is anterior to another past action ("A little while after I had eaten, my friend arrived"). (The latter situation is often represented in other languages by a simple perfective tense. Formal Spanish and French use a past anterior tense in cases such as this.)

Like tense, aspect is a way that verbs represent time. However, rather than locating an event or state in time, the way tense does, aspect describes "the internal temporal constituency of a situation", or in other words, aspect is a way "of conceiving the flow of the process itself". English aspectual distinctions in the past tense include "I went, I used to go, I was going, I had gone"; in the present tense "I lose, I am losing, I have lost, I have been losing, I am going to lose"; and with the future modal "I will see, I will be seeing, I will have seen, I am going to see". What distinguishes these aspects within each tense is not (necessarily) when the event occurs, but how the time in which it occurs is viewed: as complete, ongoing, consequential, planned, etc.

In most dialects of Ancient Greek, aspect is indicated uniquely by verbal morphology. For example, the very frequently used aorist, though a functional preterite in the indicative mood, conveys historic or 'immediate' aspect in the subjunctive and optative. The perfect in all moods is used as an aspectual marker, conveying the sense of a resultant state. E.g. – I see (present); – I saw (aorist); – I am in a state of having seen = I know (perfect).

Many Sino-Tibetan languages, like Mandarin, lack grammatical tense but are rich in aspect.

There is a distinction between grammatical aspect, as described here, and lexical aspect. Lexical aspect is an inherent property of a verb or verb-complement phrase, and is not marked formally. The distinctions made as part of lexical aspect are different from those of grammatical aspect. Typical distinctions are between states ("I owned"), activities ("I shopped"), accomplishments ("I painted a picture"), achievements ("I bought"), and punctual, or semelfactive, events ("I sneezed"). These distinctions are often relevant syntactically. For example, states and activities, but not usually achievements, can be used in English with a prepositional "for"-phrase describing a time duration: "I had a car for five hours", "I shopped for five hours", but not "*I bought a car for five hours". Lexical aspect is sometimes called "Aktionsart", especially by German and Slavic linguists. Lexical or situation aspect is marked in Athabaskan languages.

One of the factors in situation aspect is telicity. Telicity might be considered a kind of lexical aspect, except that it is typically not a property of a verb in isolation, but rather a property of an entire verb "phrase". Achievements, accomplishments and semelfactives have telic situation aspect, while states and activities have atelic situation aspect.

The other factor in situation aspect is duration, which is also a property of a verb phrase. Accomplishments, states, and activities have duration, while achievements and semelfactives do not.

In some languages, aspect and time are very clearly separated, making them much more distinct to their speakers. There are a number of languages that mark aspect much more saliently than time. Prominent in this category are Chinese and American Sign Language, which both differentiate many aspects but rely exclusively on optional time-indicating terms to pinpoint an action with respect to time. In other language groups, for example in most modern Indo-European languages (except Slavic languages), aspect has become almost entirely conflated, in the verbal morphological system, with time.

In Russian, aspect is more salient than tense in narrative. Russian, like other Slavic languages, uses different lexical entries for the different aspects, whereas other languages mark them morphologically, and still others with auxiliaries (e.g., English).

In literary Arabic (, "al-fuṣḥā") the verb has two aspect-tenses: perfective (past), and imperfective (non-past). There is some disagreement among grammarians whether to view the distinction as a distinction in aspect, or tense, or both. The past verb (, "al-fiʿl al-māḍī") denotes an event (, "ḥadaṯ") completed in the past, but it says nothing about the relation of this past event to present status. For example, , "waṣala", "arrived", indicates that arrival occurred in the past without saying anything about the present status of the arriver – maybe they stuck around, maybe they turned around and left, etc. – nor about the aspect of the past event except insofar as completeness can be considered aspectual. This past verb is clearly similar if not identical to the Greek aorist, which is considered a tense but is more of an aspect marker. In the Arabic, aorist aspect is the logical consequence of past tense. By contrast, the "Verb of Similarity" (, "al-fiʿl al-muḍāriʿ"), so called because of its resemblance to the active participial noun, is considered to denote an event in the present or future without committing to a specific aspectual sense beyond the incompleteness implied by the tense: ("yaḍribu", he strikes/is striking/will strike/etc.). Those are the only two "tenses" in Arabic (not counting "amr", command, which the tradition counts as denoting future events.) At least that's the way the tradition sees it. To explicitly mark aspect, Arabic uses a variety of lexical and syntactic devices.

Contemporary Arabic dialects are another matter. One major change from al-fuṣḥā is the use of a prefix particle ( "bi" in most dialects) to explicitly mark progressive, continuous, or habitual aspect: , "bi-yiktib", he is now writing, writes all the time, etc.

Aspect can mark the stage of an action. The prospective aspect is a combination of tense and aspect that indicates the action is in preparation to take place. The inceptive aspect identifies the beginning stage of an action (e.g. Esperanto uses "ek-", e.g. "Mi ekmanĝas", "I am beginning to eat".) and inchoative and ingressive aspects identify a change of state ("The flowers started blooming") or the start of an action ("He started running"). Aspects of stage continue through progressive, pausative, resumptive, cessive, and terminative.

Important qualifications:


The English tense–aspect system has two morphologically distinct tenses, present and past. No marker of a future tense exists on the verb in English; the futurity of an event may be expressed through the use of the auxiliary verbs "will" and "shall", by a present form plus an adverb, as in "tomorrow we go to New York City", or by some other means. Past is distinguished from present–future, in contrast, with internal modifications of the verb. These two tenses may be modified further for progressive aspect (also called "continuous" aspect), for the perfect, or for both. These two aspectual forms are also referred to as BE +ING and HAVE +EN, respectively, which avoids what may be unfamiliar terminology.

Aspects of the present tense:
(While many elementary discussions of English grammar classify the present perfect as a past tense, it relates the action to the present time. One cannot say of someone now deceased that he "has eaten" or "has been eating". The present auxiliary implies that he is in some way "present" (alive), even if the action denoted is completed (perfect) or partially completed (progressive perfect).)

Aspects of the past tense:

Aspects can also be marked on non-finite forms of the verb: "(to) be eating" (infinitive with progressive aspect), "(to) have eaten" (infinitive with perfect aspect), "having eaten" (present participle or gerund with perfect aspect), etc. The perfect infinitive can further be governed by modal verbs to express various meanings, mostly combining modality with past reference: "I should have eaten" etc. In particular, the modals "will" and "shall" and their subjunctive forms "would" and "should" are used to combine future or hypothetical reference with aspectual meaning: 

The uses of the progressive and perfect aspects are quite complex. They may refer to the viewpoint of the speaker:

But they can have other illocutionary forces or additional modal components:

For further discussion of the uses of the various tense–aspect combinations, see Uses of English verb forms.

English expresses some other aspectual distinctions with other constructions. "Used to" + VERB is a past habitual, as in "I used to go to school," and "going to / gonna" + VERB is a prospective, a future situation highlighting current intention or expectation, as in "I'm going to go to school next year."

Note that the aspectual systems of certain dialects of English, such as African-American Vernacular English (see for example habitual be), and of creoles based on English vocabulary, such as Hawaiian Creole English, are quite different from those of standard English, and often distinguish aspect at the expense of tense.

Although Standard German does not have aspects, many Upper German languages, all West Central German languages, and some more vernacular German languages do make one aspectual distinction, and so do the colloquial languages of many regions, the so-called German regiolects. While officially discouraged in schools and seen as 'bad language', local English teachers like the distinction, because it corresponds well with the English continuous form. It is formed by the conjugated auxiliary verb "sein" ("to be") followed by the preposition "am" and the infinitive, or the nominalized verb. The latter two are phonetically indistinguishable; in writing, capitalization differs: "Ich war am essen" vs. "Ich war am Essen" (I was eating, compared to the Standard German approximation: "Ich war beim Essen"); yet these forms are not standardized and thus are relatively infrequently written down or printed, even in quotations or direct speech.

In the Tyrolean and other Bavarian regiolect the prefix *da can be found, which form perfective aspects. "I hu's gleant" (Ich habe es gelernt = I learnt it) vs. "I hu's daleant" (*Ich habe es DAlernt = I succeeded in learning).

In Dutch (a West Germanic language), two types of continuous form are used. Both types are considered Standard Dutch.

The first type is very similar to the non-standard German type. It is formed by the conjugated auxiliary verb "zijn" ("to be"), followed by "aan het" and the gerund (which in Dutch matches the infinitive). For example:


The second type is formed by one of the conjugated auxiliary verbs "liggen" ("to lie"), "zitten" ("to sit"), "hangen" ("to hang"), "staan" ("to stand") or "lopen" ("to walk"), followed by the preposition "te" and the infinitive. The conjugated verbs indicate the stance of the subject performing or undergoing the action.


Sometimes the meaning of the auxiliary verb is diminished to 'being engaged in'. Take for instance these examples:


In these cases, there is generally an undertone of irritation.

The Slavic languages make a clear distinction between perfective and imperfective aspects; it was in relation to these languages that the modern concept of aspect originally developed.

In Slavic languages, a given verb is, in itself, either perfective or imperfective. Consequently each language contains many pairs of verbs, corresponding to each other in meaning, except that one expresses perfective aspect and the other imperfective. (This may be considered a form of lexical aspect.) Perfective verbs are commonly formed from imperfective ones by the addition of a prefix, or else the imperfective verb is formed from the perfective one by modification of the stem or ending. Suppletion also plays a small role. Perfective verbs cannot generally be used with the meaning of a present tense – their present-tense forms in fact have future reference. An example of such a pair of verbs, from Polish, is given below:

In at least the East Slavic and West Slavic languages, there is a three-way aspect differentiation for verbs of motion, with two forms of imperfective, determinate and indeterminate, and one form of perfective. The two forms of imperfective can be used in all three tenses (past, present, and future), but the perfective can only be used with past and future. The indeterminate imperfective expresses habitual aspect (or motion in no single direction), while the determinate imperfective expresses progressive aspect. The difference corresponds closely to that between the English "I (regularly) go to school" and "I am going to school (now)". The three-way difference is given below for the Russian basic (unprefixed) verbs of motion. When prefixes are attached to Russian verbs of motion, they become more or less normal imperfective/perfective pairs, although the prefixes are generally attached to the indeterminate imperfective to form the prefixed imperfective and to the determinate imperfective to form the prefixed perfective. For example, prefix "при-" + indeterminate "ходи́ть" = "приходи́ть"; and prefix "при-" + determinate "идти́" = "прийти" (to arrive (on foot)).

Modern Romance languages merge the concepts of aspect and tense but consistently distinguish perfective and imperfective aspects in the past tense. This derives directly from the way the Latin language used to render both aspects and "consecutio temporum".

Italian language example using the verb "mangiare" ("to eat"):

Mood: "indicativo" (indicative)


The "imperfetto"/"trapassato prossimo" contrasts with the "passato remoto"/"trapassato remoto" in that "imperfetto" renders an imperfective (continuous) past while "passato remoto" expresses an aorist (punctual/historical) past.

Other aspects in Italian are rendered with other periphrases, like prospective ("io sto per mangiare" "I'm about to eat", "io starò per mangiare" "I shall be about to eat"), or continuous/progressive ("io sto mangiando" "I'm eating", "io starò mangiando" "I shall be eating").

Finnish and Estonian, among others, have a grammatical aspect contrast of telicity between telic and atelic. Telic sentences signal that the intended goal of an action is achieved. Atelic sentences do not signal whether any such goal has been achieved. The aspect is indicated by the case of the object: accusative is telic and partitive is atelic. For example, the (implicit) purpose of shooting is to kill, such that:
In rare cases corresponding telic and atelic forms can be unrelated by meaning.

Derivational suffixes exist for various aspects. Examples:

There are derivational suffixes for verbs, which carry frequentative, momentane, causative, and inchoative aspect meanings. Also, pairs of verbs differing only in transitivity exist.

The language we know as Reo Rapa was not created by the combination of 2 languages but through the introduction of Tahitian to the Rapa monolingual community. Old Rapa words are still used for the grammar and structure of the sentence of phrase but most common context words were replaced be Tahitian. Reo Rapa is similar to the English Language as they both have specific tense words such as "did" or "do".


The Hawaiian language conveys aspect as follows:


Wuvulu language is a minority language in Pacific. The Wuvulu verbal aspect is hard to organize because of its number of morpheme combination and interaction of semantics between morpheme. Perfective, imperfective negation, simultaneous and habitual are four aspects markers in Wuvulu language.


There are three types of aspects one must consider when analyzing the Tokelauan language: inherent aspect, situation aspect, and viewpoint aspect.

The inherent aspect describes the purpose of a verb and what separates verbs from one another. According to Vendler, inherent aspect can be categorized into four different types: activities, achievements, accomplishments, and states. Simple activities include verbs such as pull, jump, and punch. Some achievements are continue and win. Drive-a-car is an accomplishment while hate is an example of a state. Another way to recognize a state inherent aspect is to note whether or not it changes. For example, if someone were to hate vegetables because they are allergic, this state of hate is unchanging and thus, a state inherent aspect. On the other hand, an achievement, unlike a state, only lasts for a short amount of time. Achievement is the highpoint of an action.

Another type of aspect is situation aspect. Situation aspect is described to be what one is experiencing in his or her life through that circumstance. Therefore, it is his or her understanding of the situation. Situation aspect are abstract terms that are not physically tangible. They are also used based upon one's point of view. For example, a professor may say that a student who comes a minute before each class starts is a punctual student. Based upon the professor's judgment of what punctuality is, he or she may make that assumption of the situation with the student. Situation aspect is firstly divided into states and occurrences, then later subdivided under occurrences into processes and events, and lastly, under events, there are accomplishments and achievements.

The third type of aspect is viewpoint aspect. Viewpoint aspect can be likened to situation aspect such that they both take into consideration one's inferences. However, viewpoint aspect diverges from situation aspect because it is where one decides to view or see such event. A perfect example is the glass metaphor: Is the glass half full or is it half empty. The choice of being half full represents an optimistic viewpoint while the choice of being half empty represents a pessimistic viewpoint. Not only does viewpoint aspect separate into negative and positive, but rather different point of views. Having two people describe a painting can bring about two different viewpoints. One may describe a situation aspect as a perfect or imperfect. A perfect situation aspect entails an event with no reference to time, while an imperfect situation aspect makes a reference to time with the observation.

Aspect in Torau is marked with post-verbal particles or clitics. While the system for marking the imperfective aspect is complex and highly developed, it is unclear if Torau marks the perfective and neutral viewpoints. The imperfective clitics index one of the core arguments, usually the nominative subject, and follow the rightmost element in a syntactic structure larger than the word. The two distinct forms for marking the imperfective aspect are "(i)sa-" and "e-". While more work needs to be done on this language, the preliminary hypothesis is that "(i)sa-" encodes the stative imperfective and "e-" encodes the active imperfective. It is also important to note that reduplication always cooccurs with "e-", but it usually does not with "(i)sa-." This example below shows these two imperfective aspect markers giving different meanings to similar sentences.

            "Pita    ma-to                 mate=sa-la."

"           " Peter   RL.3SGS-PST   be.dead=IPFV-3SGS

            ‘Peter was dead.’

            "Pita    ma-to               maa≈mate=e-la."

            Peter   RF.3SG-PST  RD≈be.dead=IPFV-3SGS

            ‘Peter was dying.’

In Torau, the suffix -"to", which must attach to a preverbal particle, may indicate similar meaning to the perfective aspect. In realis clauses, this suffix conveys an event that is entirely in the past and no longer occurring. When "-to" is used in irrealis clauses, the speaker conveys that the event will definitely occur (Palmer, 2007). Although this suffix isn’t explicitly stated as a perfective viewpoint marker, the meaning that it contributes is very similar to the perfective viewpoint.

Like many Austronesian languages, the verbs of the Malay language follow a system of affixes to express changes in meaning. To express the aspects, Malay uses a number of auxiliary verbs:

Like many Austronesian languages, the verbs of the Philippine languages follow a complex system of affixes to express subtle changes in meaning. However, the verbs in this family of languages are conjugated to express the aspects and not the tenses. Though many of the Philippine languages do not have a fully codified grammar, most of them follow the verb aspects that are demonstrated by Filipino or Tagalog.

Creole languages typically use the unmarked verb for timeless habitual aspect, or for stative aspect, or for perfective aspect in the past. Invariant pre-verbal markers are often used. Non-stative verbs typically can optionally be marked for the progressive, habitual, completive, or irrealis aspect. The progressive in English-based Atlantic Creoles often uses "de" (from English "be"). Jamaican Creole uses "a" (from English "are") or "de" for the present progressive and a combination of the past time marker ("did" , "behn" , "ehn" or "wehn") and the progressive marker ("a" or "de") for the past progressive (e.g. "did a" or "wehn de"). Haitian Creole uses the progressive marker "ap". Some Atlantic Creoles use one marker for both the habitual and progressive aspects. In Tok Pisin, the optional progressive marker follows the verb. Completive markers tend to come from superstrate words like "done" or "finish", and some creoles model the future/irrealis marker on the superstrate word for "go".
<li role="note">

American Sign Language (ASL) is similar to many other sign languages in that it has no grammatical tense but many verbal aspects produced by modifying the base verb sign.

An example is illustrated with the verb TELL. The basic form of this sign is produced with the initial posture of the index finger on the chin, followed by a movement of the hand and finger tip toward the indirect object (the recipient of the telling). Inflected into the unrealized inceptive aspect ("to be just about to tell"), the sign begins with the hand moving from in front of the trunk in an arc to the initial posture of the base sign (i.e., index finger touching the chin) while inhaling through the mouth, dropping the jaw, and directing eye gaze toward the verb's object. The posture is then held rather than moved toward the indirect object. During the hold, the signer also stops the breath by closing the glottis. Other verbs (such as "look at", "wash the dishes", "yell", "flirt") are inflected into the unrealized inceptive aspect similarly: The hands used in the base sign move in an arc from in front of the trunk to the initial posture of the underlying verb sign while inhaling, dropping the jaw, and directing eye gaze toward the verb's object (if any), but subsequent movements and postures are dropped as the posture and breath are held.

Other aspects in ASL include the following: stative, inchoative ("to begin to..."), predispositional ("to tend to..."), susceptative ("to... easily"), frequentative ("to... often"), protractive ("to... continuously"), incessant ("to... incessantly"), durative ("to... for a long time"), iterative ("to... over and over again"), intensive ("to... very much"), resultative ("to... completely"), approximative ("to... somewhat"), semblitive ("to appear to..."), increasing ("to... more and more"). Some aspects combine with others to create yet finer distinctions.

Aspect is unusual in ASL in that transitive verbs derived for aspect lose their grammatical transitivity. They remain semantically transitive, typically assuming an object made prominent using a topic marker or mentioned in a previous sentence. See Syntax in ASL for details.

The following aspectual terms are found in the literature. Approximate English equivalents are given.






</doc>
<doc id="12950" url="https://en.wikipedia.org/wiki?curid=12950" title="Glucose">
Glucose

Glucose is a simple sugar with the molecular formula CHO. Glucose circulates in the blood of animals as blood sugar. It is made during photosynthesis from water and carbon dioxide, using energy from sunlight. It is the most important source of energy for cellular respiration. Glucose is stored as a polymer, in plants as starch and in animals as glycogen.

With six carbon atoms, it is classed as a hexose, a subcategory of the monosaccharides. -Glucose is one of the sixteen aldohexose stereoisomers. The -isomer, -glucose, also known as dextrose, occurs widely in nature, but the -isomer, -glucose, does not. Glucose can be obtained by hydrolysis of carbohydrates such as milk sugar (lactose), cane sugar (sucrose), maltose, cellulose, glycogen, etc. It is commonly commercially manufactured from cornstarch by hydrolysis via pressurized steaming at controlled pH in a jet followed by further enzymatic depolymerization. Unbonded glucose is one of the main ingredients of honey.
In 1747, Andreas Marggraf was the first to isolate glucose. Glucose is on the World Health Organization's List of Essential Medicines, the most important medications needed in a basic health system. The name glucose derives through the French from the Greek γλυκός, which means "sweet," in reference to must, the sweet, first press of grapes in the making of wine. The suffix "-ose" is a chemical classifier, denoting a carbohydrate.

Glucose is the most widely used aldohexose in most living organisms. One possible explanation for this is that glucose has a lower tendency than other aldohexoses to react nonspecifically with the amine groups of proteins. This reaction—glycation—impairs or destroys the function of many proteins. Glucose's low rate of glycation can be attributed to its having a more stable cyclic form compared to other aldohexoses, which means it spends less time than they do in its reactive open-chain form. The reason for glucose having the most stable cyclic form of all the aldohexoses is that its hydroxy groups (with the exception of the hydroxy group on the anomeric carbon of -glucose) are in the equatorial position. Many of the long-term complications of diabetes (e.g., blindness, kidney failure, and peripheral neuropathy) are probably due to the glycation of proteins or lipids. In contrast, enzyme-regulated addition of sugars to protein is called glycosylation and is essential for the function of many proteins.

Glucose is a ubiquitous fuel in biology. It is used as an energy source in most organisms, from bacteria to humans, through either aerobic respiration, anaerobic respiration, or fermentation. Glucose is the human body's key source of energy, through aerobic respiration, providing about 3.75 kilocalories (16 kilojoules) of food energy per gram. Breakdown of carbohydrates (e.g., starch) yields mono- and disaccharides, most of which is glucose. Through glycolysis and later in the reactions of the citric acid cycle and oxidative phosphorylation, glucose is oxidized to eventually form carbon dioxide and water, yielding energy mostly in the form of ATP. The insulin reaction, and other mechanisms, regulate the concentration of glucose in the blood.

Glucose supplies almost all the energy for the brain, so its availability influences psychological processes. When glucose is low, psychological processes requiring mental effort (e.g., self-control, effortful decision-making) are impaired.
As a result of its importance in human health, glucose is an analyte in common medical blood tests. Eating or fasting prior to taking a blood sample has an effect on analyses for glucose in the blood; a high fasting glucose blood sugar level may be a sign of prediabetes or diabetes mellitus.

Use of glucose as an energy source in cells is by either aerobic respiration, anaerobic respiration, or fermentation. All of these processes follow from an earlier metabolic pathway known as glycolysis. The first step of glycolysis is the phosphorylation of glucose by a hexokinase to form glucose 6-phosphate. The main reason for the immediate phosphorylation of glucose is to prevent its diffusion out of the cell as the charged phosphate group prevents glucose 6-phosphate from easily crossing the cell membrane. Furthermore, addition of the high-energy phosphate group activates glucose for subsequent breakdown in later steps of glycolysis. At physiological conditions, this initial reaction is irreversible.

In anaerobic respiration, one glucose molecule produces a net gain of two ATP molecules (four ATP molecules are produced during glycolysis through substrate-level phosphorylation, but two are required by enzymes used during the process). In aerobic respiration, a molecule of glucose is much more profitable in that a maximum net production of 30 or 32 ATP molecules (depending on the organism) through oxidative phosphorylation is generated.

Organisms use glucose as a precursor for the synthesis of several important substances. Starch, cellulose, and glycogen ("animal starch") are common glucose polymers (polysaccharides). Some of these polymers (starch or glycogen) serve as energy stores, while others (cellulose and chitin, which is made from a derivative of glucose) have structural roles. Oligosaccharides of glucose combined with other sugars serve as important energy stores. These include lactose, the predominant sugar in milk, which is a glucose-galactose disaccharide, and sucrose, another disaccharide which is composed of glucose and fructose. Glucose is also added onto certain proteins and lipids in a process called glycosylation. This is often critical for their functioning. The enzymes that join glucose to other molecules usually use phosphorylated glucose to power the formation of the new bond by coupling it with the breaking of the glucose-phosphate bond.

Other than its direct use as a monomer, glucose can be broken down to synthesize a wide variety of other biomolecules. This is important, as glucose serves both as a primary store of energy and as a source of organic carbon. Glucose can be broken down and converted into lipids. It is also a precursor for the synthesis of other important molecules such as vitamin C (ascorbic acid).

Diabetes is a metabolic disorder where the body is unable to regulate levels of glucose in the blood either because of a lack of insulin in the body or the failure, by cells in the body, to respond properly to insulin. Each of these situations can be caused by persistently high elevations of blood glucose levels, through pancreatic burnout and insulin resistance. The pancreas is the organ responsible for the secretion of insulin. Insulin is a hormone that regulates glucose levels, allowing the body's cells to absorb and use glucose. Without it, glucose cannot enter the cell and therefore cannot be used as fuel for the body's functions. If the pancreas is exposed to persistently high elevations of blood glucose levels, the insulin-producing cells in the pancreas could be damaged, causing a lack of insulin in the body. Insulin resistance occurs when the pancreas tries to produce more and more insulin in response to persistently elevated blood glucose levels. Eventually, the rest of the body becomes resistant to the insulin that the pancreas is producing, thereby requiring more insulin to achieve the same blood glucose-lowering effect, and forcing the pancreas to produce even more insulin to compete with the resistance. This negative spiral contributes to pancreatic burnout, and the disease progression of diabetes.

To monitor the body's response to blood glucose-lowering therapy, glucose levels can be measured. Blood glucose monitoring can be performed by multiple methods, such as the fasting glucose test which measures the level of glucose in the blood after 8 hours of fasting. Another test is the 2-hour glucose tolerance test (GTT) – for this test, the person has a fasting glucose test done, then drinks a 75-gram glucose drink and is retested. This test measures the ability of the person's body to process glucose. Over time the blood glucose levels should decrease as insulin allows it to be taken up by cells and exit the blood stream.

Individuals with diabetes or other conditions that result in low blood sugar often carry small amounts of sugar in various forms. One sugar commonly used is glucose, often in the form of glucose tablets (glucose pressed into a tablet shape sometimes with one or more other ingredients as a binder), hard candy, or sugar packet.

Glucose is a monosaccharide with formula CHO or H-(C=O)-(CHOH)-H, whose five hydroxyl (OH) groups are arranged in a specific way along its six-carbon back.

In its fleeting open-chain form, the glucose molecule has an open (as opposed to cyclic) and unbranched backbone of six carbon atoms, C-1 through C-6; where C-1 is part of an aldehyde group H(C=O)-, and each of the other five carbons bears one hydroxyl group -OH. The remaining bonds of the backbone carbons are satisfied by hydrogen atoms -H. Therefore, glucose is both a hexose and an aldose, or an aldohexose. The aldehyde group makes glucose a reducing sugar giving a positive reaction with the Fehling test.

Each of the four carbons C-2 through C-5 is a stereocenter, meaning that its four bonds connect to four different substituents. (Carbon C-2, for example, connects to -(C=O)H, -OH, -H, and -(CHOH)H.) In -glucose, these four parts must be in a specific three-dimensional arrangement. Namely, when the molecule is drawn in the Fischer projection, the hydroxyls on C-2, C-4, and C-5 must be on the right side, while that on C-3 must be on the left side.

The positions of those four hydroxyls are exactly reversed in the Fischer diagram of -glucose. - and -glucose are two of the 16 possible aldohexoses; the other 14 are allose, altrose, galactose, gulose, idose, mannose, and talose, each with two enantiomers, “-” and “-”.

It is important to note that the linear form of glucose makes up less than 3% of the glucose molecules in a water solution. The rest is one of two cyclic forms of glucose that are formed when the hydroxyl group on carbon 5 (C5) bonds to the aldehyde carbon 1 (C1).

In solutions, the open-chain form of glucose (either "-" or "-") exists in equilibrium with several cyclic isomers, each containing a ring of carbons closed by one oxygen atom. In aqueous solution however, more than 99% of glucose molecules, at any given time, exist as pyranose forms. The open-chain form is limited to about 0.25% and furanose forms exists in negligible amounts. The terms "glucose" and "-glucose" are generally used for these cyclic forms as well. The ring arises from the open-chain form by an intramolecular nucleophilic addition reaction between the aldehyde group (at C-1) and either the C-4 or C-5 hydroxyl group, forming a hemiacetal linkage, -C(OH)H-O-.

The reaction between C-1 and C-5 yields a six-membered heterocyclic system called a pyranose, which is a monosaccharide sugar (hence "–ose") containing a derivatised pyran skeleton. The (much rarer) reaction between C-1 and C-4 yields a five-membered furanose ring, named after the cyclic ether furan. In either case, each carbon in the ring has one hydrogen and one hydroxyl attached, except for the last carbon (C-4 or C-5) where the hydroxyl is replaced by the remainder of the open molecule (which is -(C(CHOH)HOH)-H or -(CHOH)-H, respectively).

The ring-closing reaction makes carbon C-1 chiral, too, since its four bonds lead to -H, to -OH, to carbon C-2, and to the ring oxygen. These four parts of the molecule may be arranged around C-1 (the anomeric carbon) in two distinct ways, designated by the prefixes "α-" and "β-". When a glucopyranose molecule is drawn in the Haworth projection, the designation "α-" means that the hydroxyl group attached to C-1 and the -CHOH group at C-5 lies on opposite sides of the ring's plane (a "trans" arrangement), while "β-" means that they are on the same side of the plane (a "cis" arrangement). Therefore, the open-chain isomer -glucose gives rise to four distinct cyclic isomers: α--glucopyranose, β--glucopyranose, α--glucofuranose, and β--glucofuranose. These five structures exist in equilibrium and interconvert, and the interconversion is much more rapid with acid catalysis.

The other open-chain isomer -glucose similarly gives rise to four distinct cyclic forms of -glucose, each the mirror image of the corresponding -glucose.

The rings are not planar, but are twisted in three dimensions. The glucopyranose ring (α or β) can assume several non-planar shapes, analogous to the "chair" and "boat" conformations of cyclohexane. Similarly, the glucofuranose ring may assume several shapes, analogous to the "envelope" conformations of cyclopentane.

In the solid state, only the glucopyranose forms are observed, forming colorless crystalline solids that are highly soluble in water and acetic acid but poorly soluble in methanol and ethanol. They melt at ("α") and ("β"), and decompose at higher temperatures into carbon and water.

Each glucose isomer is subject to rotational isomerism. Within the cyclic form of glucose, rotation may occur around the O6-C6-C5-O5 torsion angle, termed the "ω"-angle, to form three staggered rotamer conformations called "gauche"-"gauche" (gg), "gauche"-"trans" (gt) and "trans"-"gauche" (tg). There is a tendency for the "ω"-angle to adopt a "gauche" conformation, a tendency that is attributed to the gauche effect.

All forms of glucose are colorless and easily soluble in water, acetic acid, and several other solvents. They are only sparingly soluble in methanol and ethanol.

The open-chain form is thermodynamically unstable, and it spontaneously isomerizes to the cyclic forms. (Although the ring closure reaction could in theory create four- or three-atom rings, these would be highly strained, and are not observed in practice.) In solutions at room temperature, the four cyclic isomers interconvert over a time scale of hours, in a process called mutarotation. Starting from any proportions, the mixture converges to a stable ratio of α:β 36:64. The ratio would be α:β 11:89 if it were not for the influence of the anomeric effect. Mutarotation is considerably slower at temperatures close to .

Mutarotation consists of a temporary reversal of the ring-forming reaction, resulting in the open-chain form, followed by a reforming of the ring. The ring closure step may use a different -OH group than the one recreated by the opening step (thus switching between pyranose and furanose forms), or the new hemiacetal group created on C-1 may have the same or opposite handedness as the original one (thus switching between the α and β forms). Thus, though the open-chain form is barely detectable in solution, it is an essential component of the equilibrium.

Depending on conditions, three major solid forms of glucose can be crystallised from water solutions: α-glucopyranose, β-glucopyranose, and β-glucopyranose hydrate.

Whether in water or in the solid form, - (+) glucose is dextrorotatory, meaning it will rotate the direction of polarized light clockwise as seen looking toward the light source. The effect is due to the chirality of the molecules, and indeed the mirror-image isomer, - (-)glucose, is levorotatory (rotates polarized light counterclockwise) by the same amount. The strength of the effect is different for each of the five tautomers.

Note that the - prefix does not refer directly to the optical properties of the compound. It indicates that the C-5 chiral center has the same handedness as that of -glyceraldehyde (which was so labeled because it is dextrorotatory). The fact that -glucose is dextrorotatory is a combined effect of its four chiral centers, not just of C-5; and indeed some of the other -aldohexoses are levorotatory.

In plants and some prokaryotes, glucose is a product of photosynthesis. Photosynthesis is when plants use sunlight to convert six carbon dioxide molecules and six water molecules, into one glucose molecule and six oxygen molecules. Glucose is also formed by the breakdown of polymeric forms of glucose—glycogen (in animals and fungi) or starch (in plants); the cleavage of glycogen is termed glycogenolysis, of starch, starch degradation. In animals, glucose is synthesized in the liver and kidneys from non-carbohydrate intermediates, such as pyruvate, lactate and glycerol, in the process of gluconeogenesis. In some deep-sea bacteria, glucose is produced by chemosynthesis.

Glucose is produced commercially via the enzymatic hydrolysis of starch. Many crops can be used as the source of starch. Maize, rice, wheat, cassava, corn husk and sago are all used in various parts of the world. In the United States, corn starch (from maize) is used almost exclusively. Most commercial glucose occurs as a component of invert sugar, a roughly 1:1 mixture of glucose and fructose. In principle, cellulose could be hydrolysed to glucose, but this process is not yet commercially practical. Glucose syrup, also known as corn syrup, is essentially a purified aqueous solution of saccharides obtained from edible starch that has a dextrose equivalency (DE) of 20 or more. Dried corn syrup is glucose syrup with the water removed. Glucose has a DE of 100; dried maltodextrin has a DE of less than 20. Corn syrup has a DE between 20 and 95.

Most dietary carbohydrates contain glucose, either as their only building block (as in the polysaccharides starch and glycogen), or together with another monosaccharide (as in the hetero-polysaccharides sucrose and lactose). Unbounded glucose is one of the main ingredients of honey.

In the lumen of the duodenum and small intestine, the glucose oligo- and polysaccharides are broken down to monosaccharides by the pancreatic and intestinal glycosidases. Other polysaccharides cannot be processed by the human intestine and require assistance by intestinal flora if they are to be broken down; the most notable exceptions are sucrose (fructose-glucose) and lactose (galactose-glucose). Glucose is then transported across the apical membrane of the enterocytes by SLC5A1 (SGLT1), and later across their basal membrane by SLC2A2 (GLUT2).

Some glucose is converted to lactic acid by astrocytes, which is then utilized as an energy source by brain cells; some glucose is used by intestinal cells and red blood cells, while the rest reaches the liver, adipose tissue and muscle cells, where it is absorbed and stored as glycogen (under the influence of insulin). Liver cell glycogen can be converted to glucose and returned to the blood when insulin is low or absent; muscle cell glycogen is not returned to the blood because of a lack of enzymes. In fat cells, glucose is used to power reactions that synthesize some fat types and have other purposes. Glycogen is the body's "glucose energy storage" mechanism, because it is much more "space efficient" and less reactive than glucose itself.

Glucose was first isolated from raisins in 1747 by the German chemist Andreas Marggraf. Since glucose is a basic necessity of many organisms, a correct understanding of its chemical makeup and structure contributed greatly to a general advancement in organic chemistry. This understanding occurred largely as a result of the investigations of Emil Fischer, a German chemist who received the 1902 Nobel Prize in Chemistry for his findings. The synthesis of glucose established the structure of organic material and consequently formed the first definitive validation of Jacobus Henricus van 't Hoff's theories of chemical kinetics and the arrangements of chemical bonds in carbon-bearing molecules. Between 1891 and 1894, Fischer established the stereochemical configuration of all the known sugars and correctly predicted the possible isomers, applying van 't Hoff's theory of asymmetrical carbon atoms.




</doc>
<doc id="12955" url="https://en.wikipedia.org/wiki?curid=12955" title="George Pólya">
George Pólya

George Pólya (; ; December 13, 1887 – September 7, 1985) was a Hungarian mathematician. He was a professor of mathematics from 1914 to 1940 at ETH Zürich and from 1940 to 1953 at Stanford University. He made fundamental contributions to combinatorics, number theory, numerical analysis and probability theory. He is also noted for his work in heuristics and mathematics education. According to György Marx he was one of The Martians.

Pólya was born in Budapest, Austria-Hungary to Anna Deutsch and Jakab Pólya, Hungarian Jews who had converted to the Roman Catholic faith in 1886. Although his parents were religious and he was baptized into the Roman Catholic Church, George Pólya grew up to be an agnostic. He was a professor of mathematics from 1914 to 1940 at ETH Zürich in Switzerland and from 1940 to 1953 at Stanford University. He remained Stanford Professor Emeritus for the rest of his life and career. He worked on a range of mathematical topics, including series, number theory, mathematical analysis, geometry, algebra, combinatorics, and probability. He was an Invited Speaker of the ICM in 1928 at Bologna, in 1936 at Oslo, and in 1950 at Cambridge, Massachusetts.

He died in Palo Alto, California, United States.

Early in his career, Pólya wrote with Gábor Szegő two influential problem books "Problems and Theorems in Analysis" ("I: Series, Integral Calculus, Theory of Functions" and "II: Theory of Functions. Zeros. Polynomials. Determinants. Number Theory. Geometry"). Later in his career, he spent considerable effort to identify systematic methods of problem-solving to further discovery and invention in mathematics for students, teachers, and researchers. He wrote five books on the subject: "How to Solve It", "Mathematics and Plausible Reasoning" ("Volume I: Induction and Analogy in Mathematics", and "Volume II: Patterns of Plausible Inference"), and "Mathematical Discovery: On Understanding, Learning, and Teaching Problem Solving" (volumes 1 and 2).

In "How to Solve It", Pólya provides general heuristics for solving a gamut of problems, including both mathematical and non-mathematical problems. The book includes advice for teaching students of mathematics and a mini-encyclopedia of heuristic terms. It was translated into several languages and has sold over a million copies. Russian physicist Zhores I. Alfyorov (Nobel laureate in 2000) praised it, noting that he was a fan. The American mathematician Terence Tao used the book to prepare for the International Mathematical Olympiad. The book is still used in mathematical education. Douglas Lenat's Automated Mathematician and Eurisko artificial intelligence programs were inspired by Pólya's work.

In addition to his works directly addressing problem solving, Pólya wrote another short book called "Mathematical Methods in Science", based on a 1963 work supported by the National Science Foundation, edited by Leon Bowden, and published by the Mathematical Association of America (MAA) in 1977. As Pólya notes in the preface, Professor Bowden carefully followed a tape recording of a course Pólya gave several times at Stanford in order to put the book together. Pólya notes in the preface "that the following pages will be useful, yet they should not be regarded as a finished expression."

There are three prizes named after Pólya, causing occasional confusion of one for another. In 1969 the Society for Industrial and Applied Mathematics (SIAM) established the George Pólya Prize, given alternately in two categories for "a notable application of combinatorial theory" and for "a notable contribution in another area of interest to George Pólya." In 1976 the Mathematical Association of America (MAA) established the George Pólya Award "for articles of expository excellence" published in the "College Mathematics Journal". In 1987 the London Mathematical Society (LMS) established the Pólya Prize for "outstanding creativity in, imaginative exposition of, or distinguished contribution to, mathematics within the United Kingdom."

A mathematics center has been named in Pólya's honor at the University of Idaho in Moscow, Idaho. The mathematics center focuses mainly on tutoring students in the subjects of algebra and calculus.






</doc>
<doc id="12956" url="https://en.wikipedia.org/wiki?curid=12956" title="OpenGL Utility Toolkit">
OpenGL Utility Toolkit

The OpenGL Utility Toolkit (GLUT) is a library of utilities for OpenGL programs, which primarily perform system-level I/O with the host operating system. Functions performed include window definition, window control, and monitoring of keyboard and mouse input. Routines for drawing a number of geometric primitives (both in solid and wireframe mode) are also provided, including cubes, spheres and the Utah teapot. GLUT also has some limited support for creating pop-up menus.

GLUT was written by Mark J. Kilgard, author of "OpenGL Programming for the X Window System" and "The Cg Tutorial: The Definitive Guide to Programmable Real-Time Graphics", while he was working for Silicon Graphics Inc.

The two aims of GLUT are to allow the creation of rather portable code between operating systems (GLUT is cross-platform) and to make learning OpenGL easier. Getting started with OpenGL programming while using GLUT often takes only a few lines of code and does not require knowledge of operating system–specific windowing APIs.

All GLUT functions start with the codice_1 prefix (for example, codice_2 marks the current window as needing to be redrawn).

The original GLUT library by Mark Kilgard supports the X Window System (GLX) and was ported to Microsoft Windows (WGL) by Nate Robins. Additionally, macOS ships with a GLUT framework that supports its own NSGL/CGL.

Kilgard's GLUT library is no longer maintained, and its license did not permit the redistribution of modified versions of the library. This spurred the need for free software or open source reimplementations of the API from scratch. The first such library was FreeGLUT, which aims to be a reasonably close reproduction, though introducing a small number of new functions to deal with GLUT's limitations. OpenGLUT, a fork of FreeGLUT, adds a number of new features to the original API, but work on it ceased in May 2005.

Mark Kilgard has a GitHub repository for GLUT. The glut.h header file contains the following license:

Some of GLUT's original design decisions made it hard for programmers to perform desired tasks. This led many to create non-canon patches and extensions to GLUT. Some free software or open source reimplementations also include fixes.

Some of the more notable limitations of the original GLUT library include:

Since it is no longer maintained (essentially replaced by the open source FreeGLUT) the above design issues are still not resolved in the original GLUT.




</doc>
<doc id="12957" url="https://en.wikipedia.org/wiki?curid=12957" title="Giovanni Boccaccio">
Giovanni Boccaccio

Giovanni Boccaccio (; ; 16 June 1313 – 21 December 1375) was an Italian writer, poet, correspondent of Petrarch, and an important Renaissance humanist. Boccaccio wrote a number of notable works, including "The Decameron" and "On Famous Women". He wrote his imaginative literature mostly in the Italian vernacular, as well as other works in Latin, and is particularly noted for his realistic dialogue which differed from that of his contemporaries, medieval writers who usually followed formulaic models for character and plot.

The details of Boccaccio's birth are uncertain. He was born in Florence or in a village near Certaldo where his family was from. He was the son of Florentine merchant Boccaccino di Chellino and an unknown woman; he was likely born out of wedlock. Boccaccio's stepmother was called Margherita de' Mardoli.

Boccaccio grew up in Florence. His father worked for the Compagnia dei Bardi and, in the 1320s, married Margherita dei Mardoli, who was of a well-to-do family. Boccaccio may have been tutored by Giovanni Mazzuoli and received from him an early introduction to the works of Dante. In 1326, his father was appointed head of a bank and moved with his family to Naples. Boccaccio was an apprentice at the bank but disliked the banking profession. He persuaded his father to let him study law at the "Studium" (the present-day University of Naples), where he studied canon law for the next six years. He also pursued his interest in scientific and literary studies.

His father introduced him to the Neapolitan nobility and the French-influenced court of Robert the Wise (the king of Naples) in the 1330s. At this time, he fell in love with a married daughter of the king, who is portrayed as "Fiammetta" in many of Boccaccio's prose romances, including "Il Filocolo" (1338). Boccaccio became a friend of fellow Florentine Niccolò Acciaioli, and benefited from his influence as the administrator, and perhaps the lover, of Catherine of Valois-Courtenay, widow of Philip I of Taranto. Acciaioli later became counselor to Queen Joanna I of Naples and, eventually, her "Grand Seneschal".

It seems that Boccaccio enjoyed law no more than banking, but his studies allowed him the opportunity to study widely and make good contacts with fellow scholars. His early influences included Paolo da Perugia (a curator and author of a collection of myths called the "Collectiones"), humanists Barbato da Sulmona and Giovanni Barrili, and theologian Dionigi di Borgo San Sepolcro.

In Naples, Boccaccio began what he considered his true vocation of poetry. Works produced in this period include "Il Filostrato" and "Teseida" (the sources for Chaucer's "Troilus and Criseyde" and "The Knight's Tale", respectively), "The Filocolo" (a prose version of an existing French romance), and "La caccia di Diana" (a poem in "terza rima" listing Neapolitan women). The period featured considerable formal innovation, including possibly the introduction of the Sicilian octave, where it influenced Petrarch.

Boccaccio returned to Florence in early 1341, avoiding the plague of 1340 in that city, but also missing the visit of Petrarch to Naples in 1341. He had left Naples due to tensions between the Angevin king and Florence. His father had returned to Florence in 1338, where he had gone bankrupt. His mother died shortly afterward (possibly, as she was unknown — see above). Boccaccio continued to work, although dissatisfied with his return to Florence, producing "Comedia delle ninfe fiorentine" in 1341 (also known as "Ameto"), a mix of prose and poems, completing the fifty-canto allegorical poem "Amorosa visione" in 1342, and "Fiammetta" in 1343. The pastoral piece "Ninfale fiesolano" probably dates from this time, also. In 1343, Boccaccio's father remarried to Bice del Bostichi. His children by his first marriage had all died, but he had another son named Iacopo in 1344.

In Florence, the overthrow of Walter of Brienne brought about the government of "popolo minuto" ("small people", workers). It diminished the influence of the nobility and the wealthier merchant classes and assisted in the relative decline of Florence. The city was hurt further in 1348 by the Black Death, which killed some three-quarters of the city's population, later represented in the "Decameron".

From 1347, Boccaccio spent much time in Ravenna, seeking new patronage and, despite his claims, it is not certain whether he was present in plague-ravaged Florence. His stepmother died during the epidemic and his father was closely associated with the government efforts as Minister of Supply in the city. His father died in 1349 and Boccaccio was forced into a more active role as head of the family.

Boccaccio began work on "The Decameron" around 1349. It is probable that the structures of many of the tales date from earlier in his career, but the choice of a hundred tales and the frame-story "lieta brigata" of three men and seven women dates from this time. The work was largely complete by 1352. It was Boccaccio's final effort in literature and one of his last works in Italian; the only other substantial work was "Corbaccio" (dated to either 1355 or 1365). Boccaccio revised and rewrote "The Decameron" in 1370–1371. This manuscript has survived to the present day.

From 1350, Boccaccio became closely involved with Italian humanism (although less of a scholar) and also with the Florentine government. His first official mission was to Romagna in late 1350. He revisited that city-state twice and also was sent to Brandenburg, Milan and Avignon. He also pushed for the study of Greek, housing Barlaam of Calabria, and encouraging his tentative translations of works by Homer, Euripides, and Aristotle. In these years, he also took minor orders.

In October 1350, he was delegated to greet Francesco Petrarch as he entered Florence and also to have Petrarch as a guest at Boccaccio's home, during his stay. The meeting between the two was extremely fruitful and they were friends from then on, Boccaccio calling Petrarch his teacher and "magister". Petrarch at that time encouraged Boccaccio to study classical Greek and Latin literature. They met again in Padua in 1351, Boccaccio on an official mission to invite Petrarch to take a chair at the university in Florence. Although unsuccessful, the discussions between the two were instrumental in Boccaccio writing the "Genealogia deorum gentilium"; the first edition was completed in 1360 and this remained one of the key reference works on classical mythology for over 400 years. It served as an extended defense for the studies of ancient literature and thought. Despite the Pagan beliefs at its core, Boccaccio believed that much could be learned from antiquity. Thus, he challenged the arguments of clerical intellectuals who wanted to limit access to classical sources to prevent any moral harm to Christian readers. The revival of classical antiquity became a foundation of the Renaissance, and his defense of the importance of ancient literature was an essential requirement for its development. The discussions also formalized Boccaccio's poetic ideas. Certain sources also see a conversion of Boccaccio by Petrarch from the open humanist of the "Decameron" to a more ascetic style, closer to the dominant fourteenth century ethos. For example, he followed Petrarch (and Dante) in the unsuccessful championing of an archaic and deeply allusive form of Latin poetry. In 1359, following a meeting with Pope Innocent VI and further meetings with Petrarch, it is probable that Boccaccio took some kind of religious mantle. There is a persistent (but unsupported) tale that he repudiated his earlier works as profane in 1362, including "The Decameron".
In 1360, Boccaccio began work on "De mulieribus claris", a book offering biographies of one hundred and six famous women, that he completed in 1374.

A number of Boccaccio's close friends and other acquaintances were executed or exiled in the purge following the failed coup of 1361. It was in this year that Boccaccio left Florence to reside in Certaldo, although not directly linked to the conspiracy, where he became less involved in government affairs. He did not undertake further missions for Florence until 1365, and traveled to Naples and then on to Padua and Venice, where he met up with Petrarch in grand style at Palazzo Molina, Petrarch's residence as well as the place of Petrarch's library. He later returned to Certaldo. He met Petrarch only once again in Padua in 1368. Upon hearing of the death of Petrarch (19 July 1374), Boccaccio wrote a commemorative poem, including it in his collection of lyric poems, the "Rime".

He returned to work for the Florentine government in 1365, undertaking a mission to Pope Urban V. The papacy returned to Rome from Avignon in 1367, and Boccaccio was again sent to Urban, offering congratulations. He also undertook diplomatic missions to Venice and Naples.

Of his later works, the moralistic biographies gathered as "De casibus virorum illustrium" (1355–74) and "De mulieribus claris" (1361–1375) were most significant. Other works include a dictionary of geographical allusions in classical literature, "De montibus, silvis, fontibus, lacubus, fluminibus, stagnis seu paludibus, et de nominibus maris liber". He gave a series of lectures on Dante at the Santo Stefano church in 1373 and these resulted in his final major work, the detailed "Esposizioni sopra la Commedia di Dante". Boccaccio and Petrarch were also two of the most educated people in early Renaissance in the field of archaeology.

Boccaccio's change in writing style in the 1350s was due in part to meeting with Petrarch, but it was mostly due to poor health and a premature weakening of his physical strength. It also was due to disappointments in love. Some such disappointment could explain why Boccaccio came suddenly to write in a bitter "Corbaccio" style, having previously written always in praise of women and love. Petrarch describes how Pietro Petrone (a Carthusian monk) on his death bed in 1362 sent another Carthusian (Gioacchino Ciani) to urge him to renounce his worldly studies. Petrarch then dissuaded Boccaccio from burning his own works and selling off his personal library, letters, books, and manuscripts. Petrarch even offered to purchase Boccaccio's library, so that it would become part of Petrarch's library. However, upon Boccaccio's death, his entire collection was given to the monastery of Santo Spirito, in Florence, where it still resides.

His final years were troubled by illnesses, some relating to obesity and what often is described as dropsy, severe edema that would be described today as congestive heart failure. He died on 21 December 1375 in Certaldo, where he is buried.



See Consoli's bibliography for an exhaustive listing.






</doc>
<doc id="12958" url="https://en.wikipedia.org/wiki?curid=12958" title="Giuseppe Verdi">
Giuseppe Verdi

Giuseppe Fortunino Francesco Verdi (; 9 or 10 October 1813 – 27 January 1901) was an Italian opera composer. He was born near Busseto to a provincial family of moderate means, and developed a musical education with the help of a local patron. Verdi came to dominate the Italian opera scene after the era of Vincenzo Bellini, Gaetano Donizetti, and Gioachino Rossini, whose works significantly influenced him. By his 30s, he had become one of the pre-eminent opera composers in history.

In his early operas, Verdi demonstrated a sympathy with the Risorgimento movement which sought the unification of Italy. He also participated briefly as an elected politician. The chorus "Va, pensiero" from his early opera "Nabucco" (1842), and similar choruses in later operas, were much in the spirit of the unification movement, and the composer himself became esteemed as a representative of these ideals. An intensely private person, Verdi, however, did not seek to ingratiate himself with popular movements and as he became professionally successful was able to reduce his operatic workload and sought to establish himself as a landowner in his native region. He surprised the musical world by returning, after his success with the opera "Aida" (1871), with three late masterpieces: his Requiem (1874), and the operas "Otello" (1887) and "Falstaff" (1893).

His operas remain extremely popular, especially the three peaks of his 'middle period': "Rigoletto, Il trovatore" and "La traviata", and the 2013 bicentenary of his birth was widely celebrated in broadcasts and performances.

Verdi, the first child of Carlo Giuseppe Verdi (1785–1867) and Luigia Uttini (1787–1851), was born at their home in Le Roncole, a village near Busseto, then in the Département Taro and within the borders of the First French Empire following the annexation of the Duchy of Parma and Piacenza in 1808. The baptismal register, prepared on 11 October 1813, lists his parents Carlo and Luigia as "innkeeper" and "spinner" respectively. Additionally, it lists Verdi as being "born yesterday", but since days were often considered to begin at sunset, this could have meant either 9 or 10 October. Following his mother, Verdi always celebrated his birthday on 9 October, the day he himself believed he was born.

Verdi had a younger sister, Giuseppa, who died aged 17 in 1833. From the age of four, Verdi was given private lessons in Latin and Italian by the village schoolmaster, Baistrocchi, and at six he attended the local school. After learning to play the organ, he showed so much interest in music that his parents finally provided him with a spinet. Verdi's gift for music was already apparent by 1820–21 when he began his association with the local church, serving in the choir, acting as an altar boy for a while, and taking organ lessons. After Baistrocchi's death, Verdi, at the age of eight, became the official paid organist.

The music historian Roger Parker points out that both of Verdi's parents "belonged to families of small landowners and traders, certainly not the illiterate peasants from which Verdi later liked to present himself as having emerged... Carlo Verdi was energetic in furthering his son's education...something which Verdi tended to hide in later life... [T]he picture emerges of youthful precocity eagerly nurtured by an ambitious father and of a sustained, sophisticated and elaborate formal education."

In 1823, when he was 10, Verdi's parents arranged for the boy to attend school in Busseto, enrolling him in a "Ginnasio"—an upper school for boys—run by Don Pietro Seletti, while they continued to run their inn at Le Roncole. Verdi returned to Busseto regularly to play the organ on Sundays, covering the distance of several kilometres on foot. At age 11, Verdi received schooling in Italian, Latin, the humanities, and rhetoric. By the time he was 12, he began lessons with Ferdinando Provesi, "maestro di cappella" at San Bartolomeo, director of the municipal music school and co-director of the local "Società Filarmonica" (Philharmonic Society). Verdi later stated: "From the ages of 13 to 18 I wrote a motley assortment of pieces: marches for band by the hundred, perhaps as many little "sinfonie" that were used in church, in the theatre and at concerts, five or six concertos and sets of variations for pianoforte, which I played myself at concerts, many serenades, cantatas (arias, duets, very many trios) and various pieces of church music, of which I remember only a "Stabat Mater"." This information comes from the "Autobiographical Sketch" which Verdi dictated to the publisher Giulio Ricordi late in life, in 1879, and remains the leading source for his early life and career. Written, understandably, with the benefit of hindsight, it is not always reliable when dealing with issues more contentious than those of his childhood.

The other director of the Philharmonic Society was , a wholesale grocer and distiller, who was described by a contemporary as a "manic dilettante" of music. The young Verdi did not immediately become involved with the Philharmonic. By June 1827, he had graduated with honours from the "Ginnasio" and was able to focus solely on music under Provesi. By chance, when he was 13, Verdi was asked to step in as a replacement to play in what became his first public event in his home town; he was an immediate success mostly playing his own music to the surprise of many and receiving strong local recognition.

By 1829–30, Verdi had established himself as a leader of the Philharmonic: "none of us could rival him" reported the secretary of the organisation, Giuseppe Demaldè. An eight-movement cantata, "I deliri di Saul", based on a drama by Vittorio Alfieri, was written by Verdi when he was 15 and performed in Bergamo. It was acclaimed by both Demaldè and Barezzi, who commented: "He shows a vivid imagination, a philosophical outlook, and sound judgment in the arrangement of instrumental parts." In late 1829, Verdi had completed his studies with Provesi, who declared that he had no more to teach him. At the time, Verdi had been giving singing and piano lessons to Barezzi's daughter Margherita; by 1831, they were unofficially engaged.

Verdi set his sights on Milan, then the cultural capital of northern Italy, where he applied unsuccessfully to study at the Conservatory. Barezzi made arrangements for him to become a private pupil of , who had been "maestro concertatore" at La Scala, and who described Verdi's compositions as "very promising." Lavigna encouraged Verdi to take out a subscription to La Scala, where he heard Maria Malibran in operas by Gioachino Rossini and Vincenzo Bellini. Verdi began making connections in the Milanese world of music that were to stand him in good stead. These included an introduction by Lavigna to an amateur choral group, the "Società Filarmonica", led by Pietro Massini. Attending the "Società" frequently in 1834, Verdi soon found himself functioning as rehearsal director (for Rossini's "La cenerentola") and continuo player. It was Massini who encouraged him to write his first opera, originally titled "Rocester", to a libretto by the journalist Antonio Piazza.

 List of compositions by Giuseppe Verdi
In mid-1834, Verdi sought to acquire Provesi's former post in Busseto but without success. But with Barezzi's help he did obtain the secular post of "maestro di musica". He taught, gave lessons, and conducted the Philharmonic for several months before returning to Milan in early 1835. By the following July, he obtained his certification from Lavigna. Eventually in 1835 Verdi became director of the Busseto school with a three-year contract. He married Margherita in May 1836, and by March 1837, she had given birth to their first child, Virginia Maria Luigia on 26 March 1837. Icilio Romano followed on 11 July 1838. Both the children died young, Virginia on 12 August 1838, Ilicio on 22 October 1839.

In 1837, the young composer asked for Massini's assistance to stage his opera in Milan. The La Scala impresario, Bartolomeo Merelli, agreed to put on "Oberto" (as the reworked opera was now called, with a libretto rewritten by Temistocle Solera) in November 1839. It achieved a respectable 13 additional performances, following which Merelli offered Verdi a contract for three more works.

While Verdi was working on his second opera "Un giorno di regno", Margherita died of encephalitis at the age of 26. Verdi adored his wife and children and was devastated by their deaths. "Un giorno", a comedy, was premiered only a few months later. It was a flop and only given the one performance. Following its failure, it is claimed Verdi vowed never to compose again, but in his "Sketch" he recounts how Merelli persuaded him to write a new opera.

Verdi was to claim that he gradually began to work on the music for "Nabucco", the libretto of which had originally been rejected by the composer Otto Nicolai: "This verse today, tomorrow that, here a note, there a whole phrase, and little by little the opera was written", he later recalled. By the autumn of 1841 it was complete, originally under the title "Nabucodonosor". Well received at its first performance on 9 March 1842, "Nabucco" underpinned Verdi's success until his retirement from the theatre, twenty-nine operas (including some revised and updated versions) later. At its revival in La Scala for the 1842 autumn season it was given an unprecedented (and later unequalled) total of 57 performances; within three years it had reached (among other venues) Vienna, Lisbon, Barcelona, Berlin, Paris and Hamburg; in 1848 it was heard in New York, in 1850 in Buenos Aires. Porter comments that "similar accounts...could be provided to show how widely and rapidly all [Verdi's] other successful operas were disseminated."

A period of hard work for Verdi—with the creation of twenty operas (excluding revisions and translations)—followed over the next sixteen years, culminating in "Un ballo in maschera". This period was not without its frustrations and setbacks for the young composer, and he was frequently demoralised. In April 1845, in connection with "I due Foscari", he wrote: "I am happy, no matter what reception it gets, and I am utterly indifferent to everything. I cannot wait for these next three years to pass. I have to write six operas, then "addio" to everything." In 1858 Verdi complained: "Since "Nabucco", you may say, I have never had one hour of peace. Sixteen years in the galleys."

After the initial success of "Nabucco", Verdi settled in Milan, making a number of influential acquaintances. He attended the "Salotto Maffei", Countess Clara Maffei's salons in Milan, becoming her lifelong friend and correspondent. A revival of "Nabucco" followed in 1842 at La Scala where it received a run of fifty-seven performances, and this led to a commission from Merelli for a new opera for the 1843 season. "I Lombardi alla prima crociata" was based on a libretto by Solera and premiered in February 1843. Inevitably, comparisons were made with "Nabucco"; but one contemporary writer noted: "If ["Nabucco"] created this young man's reputation, "I Lombardi" served to confirm it."

Verdi paid close attention to his financial contracts, making sure he was appropriately remunerated as his popularity increased. For "I Lombardi" and "Ernani" (1844) in Venice he was paid 12,000 lire (including supervision of the productions); "Attila" and "Macbeth" (1847), each brought him 18,000 lire. His contracts with the publishers Ricordi in 1847 were very specific about the amounts he was to receive for new works, first productions, musical arrangements, and so on. He began to use his growing prosperity to invest in land near his birthplace. In 1844 he purchased Il Pulgaro, 62 acres (23 hectares) of farmland with a farmhouse and outbuildings, providing a home for his parents from May 1844. Later that year, he also bought the Palazzo Cavalli (now known as the Palazzo Orlandi) on the via Roma, Busseto's main street. In May 1848, Verdi signed a contract for land and houses at Sant'Agata in Busseto, which had once belonged to his family. It was here he built his own house, completed in 1880, now known as the Villa Verdi, where he lived from 1851 until his death.

In March 1843, Verdi visited Vienna (where Gaetano Donizetti was musical director) to oversee a production of "Nabucco". The older composer, recognising Verdi's talent, noted in a letter of January 1844: "I am very, very happy to give way to people of talent like Verdi... Nothing will prevent the good Verdi from soon reaching one of the most honourable positions in the cohort of composers." Verdi travelled on to Parma, where the Teatro Regio di Parma was producing "Nabucco" with Strepponi in the cast. For Verdi the performances were a personal triumph in his native region, especially as his father, Carlo, attended the first performance. Verdi remained in Parma for some weeks beyond his intended departure date. This fuelled speculation that the delay was due to Verdi's interest in Giuseppina Strepponi (who stated that their relationship began in 1843). Strepponi was in fact known for her amorous relationships (and many illegitimate children) and her history was an awkward factor in their relationship until they eventually agreed on marriage.

After successful stagings of "Nabucco" in Venice (with twenty-five performances in the 1842/43 season), Verdi began negotiations with the impresario of La Fenice to stage "I Lombardi", and to write a new opera. Eventually, Victor Hugo's "Hernani" was chosen, with Francesco Maria Piave as librettist. "Ernani" was successfully premiered in 1844, and within six months had been performed at twenty other theatres in Italy, and also in Vienna. The writer Andrew Porter notes that for the next ten years, Verdi's life "reads like a travel diary—a timetable of visits...to bring new operas to the stage or to supervise local premieres." La Scala premiered none of these new works, except for "Giovanna d'Arco". Verdi "never forgave the Milanese for their reception of "Un giorno di regno"."

During this period, Verdi began to work more consistently with his librettists. He relied on Piave again for "I due Foscari", performed in Rome in November 1844, then on Solera once more for "Giovanna d'Arco", at La Scala in February 1845, while in August that year he was able to work with Salvadore Cammarano on "Alzira" for the Teatro di San Carlo in Naples. Solera and Piave worked together on "Attila" for La Fenice (March 1846).

In April 1844, Verdi took on Emanuele Muzio, eight years his junior, as a pupil and amanuensis. He had known him since about 1828 as another of Barezzi's protégés. Muzio, who in fact was Verdi's only pupil, became indispensable to the composer. He reported to Barezzi that Verdi "has a breadth of spirit, of generosity, a wisdom." In November 1846, Muzio wrote of Verdi: "If you could see us, I seem more like a friend, rather than his pupil. We are always together at dinner, in the cafes, when we play cards...; all in all, he doesn't go anywhere without me at his side; in the house we have a big table and we both write there together, and so I always have his advice." Muzio was to remain associated with Verdi, assisting in the preparation of scores and transcriptions, and later conducting many of his works in their premiere performances in the US and elsewhere outside Italy. He was chosen by Verdi as one of the executors of his will, but predeceased the composer in 1890.

After a period of illness Verdi began work on "Macbeth" in September 1846. He dedicated the opera to Barezzi: "I have long intended to dedicate an opera to you, as you have been a father, a benefactor and a friend for me. It was a duty I should have fulfilled sooner if imperious circumstances had not prevented me. Now, I send you "Macbeth", which I prize above all my other operas, and therefore deem worthier to present to you." In 1997 Martin Chusid wrote that "Macbeth" was the only one of Verdi's operas of his "early period" to remain regularly in the international repertoire, although in the 21st century "Nabucco" has also entered the lists.

Strepponi's voice declined and her engagements dried up in the 1845 to 1846 period, and she returned to live in Milan whilst retaining contact with Verdi as his "supporter, promoter, unofficial adviser, and occasional secretary" until she decided to move to Paris in October 1846. Before she left Verdi gave her a letter that pledged his love. On the envelope, Strepponi wrote: "5 or 6 October 1846. They shall lay this letter on my heart when they bury me."

Verdi had completed "I masnadieri" for London by May 1847 except for the orchestration. This he left until the opera was in rehearsal, since he wanted to hear "la [Jenny] Lind and modify her role to suit her more exactly." Verdi agreed to conduct the premiere on 22 July 1847 at Her Majesty's Theatre, as well as the second performance. Queen Victoria and Prince Albert attended the first performance, and for the most part, the press was generous in its praise.

For the next two years, except for two visits to Italy during periods of political unrest, Verdi was based in Paris. Within a week of returning to Paris in July 1847, he received his first commission from the Paris Opéra. Verdi agreed to adapt "I Lombardi" to a new French libretto; the result was "Jérusalem", which contained significant changes to the music and structure of the work (including an extensive ballet scene) to meet Parisian expectations. Verdi was awarded the Order of Chevalier of the Legion of Honour. To satisfy his contracts with the publisher , Verdi dashed off "Il Corsaro". Budden comments "In no other opera of his does Verdi appear to have taken so little interest "before" it was staged."

On hearing the news of the "Cinque Giornate", the "Five Days" of street fighting that took place between 18 and 22 March 1848 and temporarily drove the Austrians out of Milan, Verdi travelled there, arriving on 5 April. He discovered that Piave was now "Citizen Piave" of the newly proclaimed Republic of San Marco. Writing a patriotic letter to him in Venice, Verdi concluded "Banish every petty municipal idea! We must all extend a fraternal hand, and Italy will yet become the first nation of the world...I am drunk with joy! Imagine that there are no more Germans here!!"

Verdi had been admonished by the poet Giuseppe Giusti for turning away from patriotic subjects, the poet pleading with him to "do what you can to nourish the [sorrow of the Italian people], to strengthen it, and direct it to its goal." Cammarano suggested adapting Joseph Méry's 1828 play "La Bataille de Toulouse", which he described as a story "that should stir every man with an Italian soul in his breast". The premiere was set for late January 1849. Verdi travelled to Rome before the end of 1848. He found that city on the verge of becoming a (short-lived) republic, which commenced within days of "La battaglia di Legnano"'s enthusiastically received premiere. In the spirit of the time were the tenor hero's final words, "Whoever dies for the fatherland cannot be evil-minded".

Verdi had intended to return to Italy in early 1848, but was prevented by work and illness, as well as, most probably, by his increasing attachment to Strepponi. Verdi and Strepponi left Paris in July 1849, the immediate cause being an outbreak of cholera, and Verdi went directly to Busseto to continue work on completing his latest opera, "Luisa Miller", for a production in Naples later in the year.

Verdi was committed to the publisher Giovanni Ricordi for an opera—which became "Stiffelio"—for Trieste in the Spring of 1850; and, subsequently, following negotiations with La Fenice, developed a libretto with Piave and wrote the music for "Rigoletto" (based on Victor Hugo's "Le roi s'amuse") for Venice in March 1851. This was the first of a sequence of three operas (followed by "Il trovatore" and "La traviata") which were to cement his fame as a master of opera.

The failure of "Stiffelio" (attributable not least to the censors of the time taking offence at the taboo subject of the supposed adultery of a clergyman's wife and interfering with the text and roles) incited Verdi to take pains to rework it, although even in the completely recycled version of "Aroldo" (1857) it still failed to please. "Rigoletto", with its intended murder of royalty, and its sordid attributes, also upset the censors. Verdi would not compromise: What does the sack matter to the police? Are they worried about the effect it will produce?...Do they think they know better than I?...I see the hero has been made no longer ugly and hunchbacked!! Why? A singing hunchback...why not?...I think it splendid to show this character as outwardly deformed and ridiculous, and inwardly passionate and full of love. I chose the subject for these very qualities...if they are removed I can no longer set it to music.
Verdi substituted a Duke for the King, and the public response and subsequent success of the opera all over Italy and Europe fully vindicated the composer. Aware that the melody of the Duke's song "La donna è mobile" ("Woman is fickle") would become a popular hit, Verdi excluded it from orchestral rehearsals for the opera, and rehearsed the tenor separately.

For several months Verdi was preoccupied with family matters. These stemmed from the way in which the citizens of Busseto were treating Giuseppina Strepponi, with whom he was living openly in an unmarried relationship. She was shunned in the town and at church, and while Verdi appeared indifferent, she was certainly not. Furthermore, Verdi was concerned about the administration of his newly acquired property at Sant'Agata. A growing estrangement between Verdi and his parents was perhaps also attributable to Strepponi (the suggestion that this situation was sparked by the birth of a child to Verdi and Strepponi which was given away as a foundling lacks any firm evidence). In January 1851, Verdi broke off relations with his parents, and in April they were ordered to leave Sant'Agata; Verdi found new premises for them and helped them financially to settle into their new home. It may not be coincidental that all six Verdi operas written in the period 1849–53 ("La battaglia, Luisa Miller, Stiffelio, Rigoletto, Il trovatore" and "La traviata"), have, uniquely in his oeuvre, heroines who are, in the opera critic Joseph Kerman's words, "women who come to grief because of sexual transgression, actual or perceived". Kerman, like the psychologist Gerald Mendelssohn, sees this choice of subjects as being influenced by Verdi's uneasy passion for Strepponi.

Verdi and Strepponi moved into Sant'Agata on 1 May 1851. May also brought an offer for a new opera from La Fenice, which Verdi eventually realised as "La traviata". That was followed by an agreement with the Rome Opera company to present "Il trovatore" for January 1853. Verdi now had sufficient earnings to retire, should he have wished to do so. He had reached a stage where he could develop his operas as he wished, rather than be dependent on commissions from third parties. "Il trovatore" was in fact the first opera he wrote without a specific commission (apart from "Oberto"). At around the same time he began to consider creating an opera from Shakespeare's "King Lear". After first (1850) seeking a libretto from Cammarano (which never appeared), Verdi later (1857) commissioned one from Antonio Somma, but this proved intractable, and no music was ever written. Verdi began work on "Il trovatore" after the death of his mother in June 1851. The fact that this is "the one opera of Verdi's which focuses on a mother rather than a father" is perhaps related to her death.

In the winter of 1851–52 Verdi decided to go to Paris with Strepponi, where he concluded an agreement with the Opéra to write what became "Les vêpres siciliennes", his first original work in the style of grand opera. In February 1852, the couple attended a performance of Alexander Dumas "fils"'s play, "The Lady of the Camellias"; Verdi immediately began to compose music for what would later become "La traviata".

After his visit to Rome for "Il trovatore" in January 1853, Verdi worked on completing "La traviata", but with little hope of its success, due to his lack of confidence in any of the singers engaged for the season. Furthermore, the management insisted that the opera be given a historical, not a contemporary setting. The premiere in March 1853 was indeed a failure: Verdi wrote: "Was the fault mine or the singers'? Time will tell." Subsequent productions (following some rewriting) throughout Europe over the following two years fully vindicated the composer; Roger Parker has written ""Il trovatore" consistently remains one of the three or four most popular operas in the Verdian repertoire: but it has never pleased the critics".

In the eleven years up to and including "Traviata", Verdi had written sixteen operas. Over the next eighteen years (up to "Aida"), he wrote only six new works for the stage. Verdi was happy to return to Sant'Agata and, in February 1856, was reporting a "total abandonment of music; a little reading; some light occupation with agriculture and horses; that's all". A couple of months later, writing in the same vein to Countess Maffei he stated: "I'm not doing anything. I don't read. I don't write. I walk in the fields from morning to evening, trying to recover, so far without success, from the stomach trouble caused me by "I vespri siciliani". Cursed operas!" An 1858 letter by Strepponi to the publisher Léon Escudier describes the kind of lifestyle that increasingly appealed to the composer: "His love for the country has become a mania, madness, rage, and fury—anything you like that is exaggerated. He gets up almost with the dawn, to go and examine the wheat, the maize, the vines, etc...Fortunately our tastes for this sort of life coincide, except in the matter of sunrise, which he likes to see up and dressed, and I from my bed."

Nonetheless on 15 May, Verdi signed a contract with La Fenice for an opera for the following spring. This was to be "Simon Boccanegra". The couple stayed in Paris until January 1857 to deal with these proposals, and also the offer to stage the translated version of "Il trovatore" as a grand opera. Verdi and Strepponi travelled to Venice in March for the premiere of "Simon Boccanegra", which turned out to be "a fiasco" (as Verdi reported, although on the second and third nights, the reception improved considerably).

With Strepponi, Verdi went to Naples early in January 1858 to work with Somma on the libretto of the opera "Gustave III", which over a year later would become "Un ballo in maschera". By this time, Verdi had begun to write about Strepponi as "my wife" and she was signing her letters as "Giuseppina Verdi". Verdi raged against the stringent requirements of the Neapolitan censor stating: "I'm drowning in a sea of troubles. It's almost certain that the censors will forbid our libretto." With no hope of seeing his "Gustavo III" staged as written, he broke his contract. This resulted in litigation and counter-litigation; with the legal issues resolved, Verdi was free to present the libretto and musical outline of "Gustave III" to the Rome Opera. There, the censors demanded further changes; at this point, the opera took the title "Un ballo in maschera".

Arriving in Sant'Agata in March 1859 Verdi and Strepponi found the nearby city of Piacenza occupied by about 6,000 Austrian troops who had made it their base, to combat the rise of Italian interest in unification in the Piedmont region. In the ensuing Second Italian War of Independence the Austrians abandoned the region and began to leave Lombardy, although they remained in control of the Venice region under the terms of the armistice signed at Villafranca. Verdi was disgusted at this outcome: "[W]here then is the independence of Italy, so long hoped for and promised?...Venice is not Italian? After so many victories, what an outcome... It is enough to drive one mad" he wrote to Clara Maffei.

Verdi and Strepponi now decided on marriage; they travelled to Collonges-sous-Salève, a village then part of Piedmont. On 29 August 1859 the couple were married there, with only the coachman who had driven them there and the church bell-ringer as witnesses. At the end of 1859, Verdi wrote to his friend Cesare De Sanctis "[Since completing "Ballo"] I have not made any more music, I have not seen any more music, I have not thought anymore about music. I don't even know what colour my last opera is, and I almost don't remember it." He began to remodel Sant'Agata, which took most of 1860 to complete and on which he continued to work for the next twenty years. This included major work on a square room that became his workroom, his bedroom, and his office.
Having achieved some fame and prosperity, Verdi began in 1859 to take an active interest in Italian politics. His early commitment to the Risorgimento movement is difficult to estimate accurately; in the words of the music historian Philip Gossett "myths intensifying and exaggerating [such] sentiment began circulating" during the nineteenth century. An example is the claim that when the "Va, pensiero" chorus in "Nabucco" was first sung in Milan, the audience, responding with nationalistic fervour, demanded an encore. As encores were expressly forbidden by the government at the time, such a gesture would have been extremely significant. But in fact the piece encored was not "Va, pensiero" but the hymn "Immenso Jehova".

The growth of the "identification of Verdi's music with Italian nationalist politics" perhaps began in the 1840s. In 1848, the nationalist leader Giuseppe Mazzini, (whom Verdi had met in London the previous year) requested Verdi (who complied) to write a patriotic hymn. The opera historian Charles Osborne describes the 1849 La battaglia di Legnano as "an opera with a purpose" and maintains that "while parts of Verdi's earlier operas had frequently been taken up by the fighters of the Risorgimento...this time the composer had given the movement its own opera" It was not until 1859 in Naples, and only then spreading throughout Italy, that the slogan "Viva Verdi" was used as an acronym for Viva Vittorio Emanuele Re D<nowiki>'</nowiki>Italia" (Viva Victor Emmanuel King of Italy)", (who was then king of Piedmont). After Italy was unified in 1861, many of Verdi's early operas were increasingly re-interpreted as Risorgimento works with hidden Revolutionary messages that perhaps had not been originally intended by either the composer or his librettists.

In 1859, Verdi was elected as a member of the new provincial council, and was appointed to head a group of five who would meet with King Vittorio Emanuele II in Turin. They were enthusiastically greeted along the way and in Turin Verdi himself received much of the publicity. On 17 October Verdi met with Cavour, the architect of the initial stages of Italian unification. Later that year the government of Emilia was subsumed under the United Provinces of Central Italy, and Verdi's political life temporarily came to an end. Whilst still maintaining nationalist feelings, he declined in 1860 the office of provincial council member to which he had been elected "in absentia". Cavour however was anxious to convince a man of Verdi's stature that running for political office was essential to strengthening and securing Italy's future. The composer confided to Piave some years later that "I accepted on the condition that after a few months I would resign." Verdi was elected on 3 February 1861 for the town of Borgo San Donnino (Fidenza) to the Parliament of Piedmont-Sardinia in Turin (which from March 1861 became the Parliament of the Kingdom of Italy), but following the death of Cavour in 1861, which deeply distressed him, he scarcely attended. Later, in 1874, Verdi was appointed a member of the Italian Senate, but did not participate in its activities.

In the months following the staging of "Ballo", Verdi was approached by several opera companies seeking a new work or making offers to stage one of his existing ones, but refused them all. But when, in December 1860, an approach was made from Saint Petersburg's Imperial Theatre, the offer of 60,000 francs plus all expenses was doubtless a strong incentive. Verdi came up with the idea of adapting the 1835 Spanish play "Don Alvaro o la fuerza del sino" by Angel Saavedra, which became "La forza del destino", with Piave writing the libretto. The Verdis arrived in St. Petersburg in December 1861 for the premiere, but casting problems meant that it had to be postponed.

Returning via Paris from Russia on 24 February 1862, Verdi met two young Italian writers, the twenty-year-old Arrigo Boito and Franco Faccio. Verdi had been invited to write a piece of music for the 1862 International Exhibition in London, and charged Boito with writing a text, which became the "Inno delle nazioni". Boito, as a supporter of the grand opera of Giacomo Meyerbeer and an opera composer in his own right, was later in the 1860s critical of Verdi's "reliance on formula rather than form", incurring the composer's wrath. Nevertheless, he was to become Verdi's close collaborator in his final operas. The St. Petersburg premiere of "La forza" finally took place in September 1862, and Verdi received the Order of St. Stanislaus.

A revival of "Macbeth" in Paris in 1865 was not a success, but he obtained a commission for a new work, "Don Carlos", based on the drama, "Don Carlos" by Friedrich Schiller. He and Giuseppina spent late 1866 and much of 1867 in Paris, where they heard, and did not warm to, Giacomo Meyerbeer's last opera "L'Africaine", and Richard Wagner's overture to "Tannhäuser." The opera's premiere in 1867 drew mixed comments. While the critic Théophile Gautier praised the work, the composer Georges Bizet was disappointed at Verdi's changing style: "Verdi is no longer Italian. He is following Wagner."

During the 1860s and 1870s Verdi paid great attention to his estate around Busseto, purchasing additional land, dealing with unsatisfactory (in one case, embezzling) stewards, installing irrigation, and coping with variable harvests and economic slumps. In 1867, both Verdi's father Carlo, with whom he had restored good relations, and his early patron and father-in-law Antonio Barezzi, died. Verdi and Giuseppina decided to adopt Carlo's great-niece Filomena Maria Verdi, then seven years old, as their own child. She was to marry in 1878 the son of Verdi's friend and lawyer Angelo Carrara and her family became eventually the heirs of Verdi's estate.

"Aida" was commissioned by the Egyptian government for the opera house built by the Khedive Isma'il Pasha to celebrate the opening of the Suez Canal in 1869. The opera house actually opened with a production of "Rigoletto". The prose libretto in French by Camille du Locle, based on a scenario by the Egyptologist Auguste Mariette, was transformed to Italian verse by Antonio Ghislanzoni. Verdi was offered the enormous sum of 150,000 francs for the opera (even though he confessed that Ancient Egypt was "a civilization I have never been able to admire"), and it was first performed in Cairo in 1871. Verdi spent much of 1872 and 1873 supervising the Italian productions of "Aida" at Milan, Parma and Naples, effectively acting as producer and demanding high standards and adequate rehearsal time. During the rehearsals for the Naples production he wrote his string quartet, the only chamber music by him to survive, and the only major work in the form by an Italian of the 19th century.

In 1869, Verdi had been asked to compose a section for a requiem mass in memory of Gioachino Rossini. He compiled and completed the requiem, but its performance was abandoned (and its premiere did not take place until 1988). Five years later, Verdi reworked his "Libera Me" section of the Rossini Requiem and made it a part of his Requiem honouring Alessandro Manzoni, who had died in 1873. The complete Requiem was first performed at the cathedral in Milan on the anniversary of Manzoni's death on 22 May 1874. The "spinto" soprano Teresa Stolz (1834–1902), who had sung in La Scala productions from 1865 onwards, was the soloist in the first and many later performances of the Requiem; in February 1872, she had created Aida in its European premiere in Milan. She became closely associated personally with Verdi (exactly how closely remains conjectural), to Giuseppina Verdi's initial disquiet; but the women were reconciled and Stolz remained a companion of Verdi after Giuseppina's death in 1897 until his own death.

Verdi conducted his Requiem in Paris, London and Vienna in 1875 and in Cologne in 1876. It seemed that it would be his last work. In the words of his biographer John Rosselli, it "confirmed him as the unique presiding genius of Italian music. No fellow composer...came near him in popularity or reputation". Verdi, now in his sixties, initially seemed to withdraw into retirement. He deliberately shied away from opportunities to publicise himself or to become involved with new productions of his works, but secretly he began work on "Otello", which Boito (to whom the composer had been reconciled by Ricordi) had proposed to him privately in 1879. The composition was delayed by a revision of "Simon Boccanegra" which Verdi undertook with Boito, produced in 1881, and a revision of "Don Carlos". Even when "Otello" was virtually completed, Verdi teased "Shall I finish it? Shall I have it performed? Hard to tell, even for me." As news leaked out, Verdi was pressed by opera houses across Europe with enquiries; eventually the opera was triumphantly premiered at La Scala in February 1887.

Following the success of "Otello" Verdi commented, "After having relentlessly massacred so many heroes and heroines, I have at last the right to laugh a little." He had considered a variety of comic subjects but had found none of them wholly suitable and confided his ambition to Boito. The librettist said nothing at the time but secretly began work on a libretto based on "The Merry Wives of Windsor" with additional material taken from "Henry IV, Part 1" and "Part 2". Verdi received the draft libretto probably in early July 1889 after he had just read Shakespeare's play: "Benissimo! Benissimo!... No one could have done better than you", he wrote back to Boito. But he still had doubts: his age, his health (which he admits to being good) and his ability to complete the project: "If I were not to finish the music?". If the project failed, it would have been a waste of Boito's time, and have distracted him from completing his own new opera. Finally on 10 July 1889 he wrote again: "So be it! So let's do "Falstaff"! For now, let's not think of obstacles, of age, of illnesses!" Verdi emphasised the need for secrecy, but continued "If you are in the mood, then start to write." Later he wrote to Boito (capitals and exclamation marks are Verdi's own): "What joy to be able to say to the public: HERE WE ARE AGAIN!!! COME AND SEE US!"

The first performance of "Falstaff" took place at La Scala on 9 February 1893. For the first night, official ticket prices were thirty times higher than usual. Royalty, aristocracy, critics and leading figures from the arts all over Europe were present. The performance was a huge success; numbers were encored, and at the end the applause for Verdi and the cast lasted an hour. That was followed by a tumultuous welcome when the composer, his wife and Boito arrived at the Grand Hotel de Milan. Even more hectic scenes ensued when he went to Rome in May for the opera's premiere at the Teatro Costanzi, when crowds of well-wishers at the railway station initially forced Verdi to take refuge in a tool-shed. He witnessed the performance from the Royal Box at the side of King Umberto and the Queen.

In his last years Verdi undertook a number of philanthropic ventures, publishing in 1894 a song for the benefit of earthquake victims in Sicily, and from 1895 onwards planning, building and endowing a rest-home for retired musicians in Milan, the Casa di Riposo per Musicisti, and building a hospital at Villanova sull'Arda, close to Busseto. His last major composition, the choral set of "Four sacred pieces", was published in 1898. In 1900 he was deeply upset at the assassination of King Umberto and sketched a setting of a poem in his memory but was unable to complete it. While staying at the Grand Hotel, Verdi suffered a stroke on 21 January 1901. He gradually grew more feeble over the next week, during which Stolz cared for him, and died on 27 January at the age of 87.

Verdi was initially buried in a private ceremony at Milan's Cimitero Monumentale. A month later, his body was moved to the crypt of the Casa di Riposo. On this occasion, "Va, pensiero" from "Nabucco" was conducted by Arturo Toscanini with a chorus of 820 singers. A huge crowd was in attendance, estimated at 300,000. Boito wrote to a friend, in words which recall the mysterious final scene of "Don Carlos", "[Verdi] sleeps like a King of Spain in his Escurial, under a bronze slab that completely covers him."
Not all of Verdi's personal qualities were amiable. John Rosselli concluded after writing his biography that "I do not very much like the man Verdi, in particular the autocratic rentier-cum-estate owner, part-time composer, and seemingly full-time grumbler and reactionary critic of the later years", yet admits that like other writers, he must "admire him, warts and all...a deep integrity runs beneath his life, and can be felt even when he is being unreasonable or wrong."

Budden suggests that "With Verdi...the man and the artist on many ways developed side by side." Ungainly and awkward in society in his early years, "as he became a man of property and underwent the civilizing influence of Giuseppina...[he] acquired assurance and authority." He also learnt to keep himself to himself, never discussing his private life and maintaining when it suited his convenience legends about his supposed 'peasant' origins, his materialism and his indifference to criticism. Mendelsohn describes the composer as "an intensely private man who deeply resented efforts to inquire into his personal affairs. He regarded journalists and would-be biographers, as well as his neighbors in Busseto and the operatic public at large, as an intrusive lot, against whose prying attentions he needed constantly to defend himself."

Verdi was similarly never explicit about his religious beliefs. Anti-clerical by nature in his early years, he nonetheless built a chapel at Sant'Agata, but is rarely recorded as going to church. Strepponi wrote in 1871 "I won't say [Verdi] is an atheist, but he is not much of a believer." Rosselli comments that in the Requiem "The prospect of Hell appears to rule...[the Requiem] is troubled to the end," and offers little consolation.

"See also List of compositions by Giuseppe Verdi and individual articles on the works."

The writer Friedrich Schiller (four of whose plays were adapted as operas by Verdi) distinguished two types of artist in his 1795 essay "On Naïve and Sentimental Poetry". The philosopher Isaiah Berlin ranked Verdi in the 'naïve' category—"They are not...self-conscious. They do not...stand aside to contemplate their creations and express their own feelings...They are able...if they have genius, to embody their vision fully." (The 'sentimentals' seek to recreate nature and natural feelings on their own terms—Berlin instances Wagner—"offering not peace, but a sword.") Verdi's operas are not written according to an aesthetic theory, or with a purpose to change the tastes of their audiences. In conversation with a German visitor in 1887 he is recorded as saying that, whilst "there was much to be admired in [Wagner's operas] "Tannhäuser" and "Lohengrin"...in his recent operas [Wagner] seemed to be overstepping the bounds of what can be expressed in music. For him "philosophical" music was incomprehensible." Although Verdi's works belong, as Rosselli admits "to the most artificial of genres...[they] ring emotionally true: truth and directness make them exciting, often hugely so."

That is not to say his operas did not come as great innovations. What sounds to a modern listener as derivative of the bel canto, his first major success, "Nabucco", came as a something entirely new. Never before had opera been so harmonically complex and direct. No longer was there the empty vocal display of the bel canto period composers. Granted, there is a significant amount of vocal fireworks, but they exist for the purpose of drama, not to show off singers. Aside from this, his use of the chorus was entirely new. Before "Nabucco", an opera's chorus was limited to be only a background voice, another instrument. In Nabucco, this is abolished; he uses the chorus as character, to show the suffering and consensus of the people. The famous "Va, pensiero" is an example of this.

The first of his "big three" operas, "Rigoletto", followed by "La Traviata", and ending with "Il Trovatore", also was revolutionary. In a letter to "Rigoletto"'s librettist, Francesco Maria Piave, he says, "I conceived "Rigoletto" almost without arias, without finales but only an unending string of duets." And that it is. Rigoletto is one of, if not the earliest operas to abandon the traditional distinction between the sung aria, and the more speech-like recitative.

After these three operas, his works took an increasing amount of time to finish, were significantly longer, and more masterfully orchestrated.

The earliest study of Verdi's music, published in 1859 by the Italian critic Abraham Basevi, already distinguished four periods in Verdi's music. The early, 'grandiose' period, ended according to Basevi with "La battaglia di Legnano" (1849), and a 'personal' style began with the next opera "Luisa Miller." These two operas are generally agreed today by critics to mark the division between Verdi's 'early' and 'middle' periods. The 'middle' period is felt to end with "La traviata" (1853) and "Les vêpres siciliennes" (1855), with a 'late' period commencing with "Simon Boccanegra" (1857) running through to "Aida" (1871). The last two operas, "Otello" and "Falstaff", together with the Requiem and the "Four Sacred Pieces," then represent a 'final' period.

Verdi was to claim in his "Sketch" that during his early training with Lavigna "I did nothing but canons and fugues...No-one taught me orchestration or how to handle dramatic music." He is known to have written a variety of music for the Busseto Philharmonic society, including vocal music, band music and chamber works, (and including an alternative overture to Rossini's "Barber of Seville") but few of these works survive. (He may have given instructions before his death to destroy his early works).

Verdi uses in his early operas (and, in his own stylized versions, throughout his later work) the standard elements of Italian opera content of the period, referred to by the opera writer Julian Budden as the 'Code Rossini', after the composer who established through his work and popularity the accepted templates of these forms; they were also used by the composers dominant during Verdi's early career, Bellini, Donizetti and Saverio Mercadante. Amongst the essential elements are the aria, the duet, the ensemble, and the finale sequence of an act. The aria format, centred on a soloist, typically involved three sections; a slow introduction, marked typically cantabile or adagio, a "tempo di mezzo" which might involve chorus or other characters, and a cabaletta, an opportunity for bravura singing for the soloist. The duet was similarly formatted. Finales, covering climactic sequences of action, used the various forces of soloists, ensemble and chorus, usually culminating with an exciting stretto section. Verdi was to develop these and the other formulae of the generation preceding him with increasing sophistication during his career. 

The operas of the early period show Verdi learning by doing and gradually establishing mastery over the different elements of opera. "Oberto" is poorly structured, and the orchestration of the first operas is generally simple, sometimes even basic. The musicologist Richard Taruskin suggests "the most striking effect in the early Verdi operas, and the one most obviously allied to the mood of the Risorgimento, was the big choral number sung—crudely or sublimely, according to the ear of the beholder—in unison. The success of "Va, pensiero" in "Nabucco" (which Rossini approvingly denoted as "a grand aria sung by sopranos, contraltos, tenors and basses"), was replicated in the similar "O Signor, dal tetto natio" in "I lombardi" and in 1844 in the chorus "Si ridesti il Leon di Castiglia" in "Ernani", the battle hymn of the conspirators seeking freedom In "I due Foscari" Verdi first uses recurring themes identified with main characters; here and in future operas the accent moves away from the 'oratorio' characteristics of the first operas towards individual action and intrigue.

From this period onwards Verdi also develops his instinct for "tinta" (literally 'colour'), a term which he used for characterising elements of an individual opera score—Parker gives as an example "the rising 6th that begins so many lyric pieces in "Ernani"". "Macbeth", even in its original 1847 version, shows many original touches; characterization by key (the Macbeths themselves generally singing in sharp keys, the witches in flat keys), a preponderance of minor key music, and highly original orchestration. In the 'dagger scene' and the duet following the murder of Duncan, the forms transcend the 'Code Rossini' and propel the drama in a compelling fashion. Verdi was to comment in 1868 that Rossini and his followers missed "the golden thread that binds all the parts together and, rather than a set of numbers without coherence, makes an opera." "Tinta" was for Verdi this "golden thread", an essential unifying factor in his works.

The writer David Kimbell states that in "Luisa Miller" and "Stiffelio" (the earliest operas of this period) there appears to be a "growing freedom in the large scale structure...and an acute attention to fine detail." Others echo those feelings. Julian Budden expresses the impact of "Rigoletto" and its place in Verdi's output as follows: "Just after 1850 at the age of 38, Verdi closed the door on a period of Italian opera with "Rigoletto". The so-called "ottocento" in music is finished. Verdi will continue to draw on certain of its forms for the next few operas, but in a totally new spirit." One example of Verdi's wish to move away from "standard forms" appears in his feelings about the structure of "Il trovatore". To his librettist, Cammarano, Verdi plainly states in a letter of April 1851 that if there were no standard forms—"cavatinas, duets, trios, choruses, finales, etc. ... and if you could avoid beginning with an opening chorus...", he would be quite happy. 

Two external factors had their impacts on Verdi's compositions of this period. One is that with increasing reputation and financial security he no longer needed to commit himself to the productive treadmill, had more freedom to choose his own subjects, and had more time to develop them according to his own ideas. In the years 1849 to 1859 he wrote eight new operas, compared with fourteen in the previous ten years.

Another factor was the changed political situation; the failure of the 1848 revolutions led both to some diminution of the Risorgimento ethos (at least initially) and a significant increase in theatre censorship. This is reflected both in Verdi's choices of plots dealing more with personal relationships than political conflict, and in a (partly consequent) dramatic reduction in the operas of this period in the number of choruses (of the type which had first made him famous)—not only are there on average 40% fewer choruses in the 'middle' period operas compared to the 'early' period', but whereas virtually all the 'early' operas commence with a chorus, only one ("Luisa Miller") of the 'middle' period operas begin this way. Instead, Verdi experiments with a variety of means, e.g. a stage band ("Rigoletto"), an aria for bass ("Stiffelio"), a party scene ("La traviata"). Chusid also notes Verdi's increasing tendency to replace full-scale overtures with shorter orchestral introductions. Parker comments that "La traviata", the last opera of the 'middle' period, is "again a new adventure. It gestures towards a level of 'realism'...the contemporary world of waltzes pervades the score, and the heroine's death from disease is graphically depicted in the music." Verdi's increasing command of musical highlighting of changing moods and relationships is exemplified in Act III of "Rigoletto", where Duke's flippant song "La donna è mobile" is followed immediately by the quartet "Bella figlia dell'amore", contrasting the rapacious Duke and his inamorata with the (concealed) indignant Rigoletto and his grieving daughter. Taruskin asserts this is "the most famous ensemble Verdi ever composed."

 Chusid notes Strepponi's description of the operas of the 1860s and 1870s as being "modern" whereas Verdi described the pre-1849 works as "the cavatina operas", as further indication that "Verdi became increasingly dissatisfied with the older, familiar conventions of his predecessors that he had adopted at the outset of his career," Parker sees a physical differentiation of the operas from "Les vêpres siciliennes" (1855) to "Aida" (1871) is that they are significantly longer, and with larger cast-lists, than previous works. They also reflect a shift towards the French genre of grand opera, notable in more colorful orchestration, counterpointing of serious and comic scenes, and greater spectacle. The opportunities of transforming Italian opera by utilising such resources appealed to him. For a commission from the Paris Opéra he expressly demanded a libretto from Eugène Scribe, the favorite librettist of Meyerbeer, telling him: "I want—in fact, I must have—a grandiose, impassioned and original subject." The result was "Les vêpres siciliennes", and the scenarios of "Simon Boccanegra" (1857), "Un ballo in maschera" (1859), "La forza del destino" (1862), "Don Carlos" (1865) and "Aida" (1872) all meet the same criteria. Porter notes that "Un ballo" marks an almost complete synthesis of Verdi's style with the grand opera hallmarks, such that "huge spectacle is not mere decoration but essential to the drama...musical and theatrical lines remain taut [and] the characters still sing as warmly, passionately and personally as in "Il trovatore"."

When the composer Ferdinand Hiller asked Verdi whether he preferred "Aida" or "Don Carlos", Verdi replied that "Aida" had "more bite and (if you'll forgive the word), more "theatricality"." During the rehearsals for the Naples production of "Aida" Verdi amused himself by writing his only string quartet, a sprightly work which shows in its last movement that he had not lost the skill for fugue-writing that he had learned with Lavigna.

Verdi's three last major works continued to show new development in conveying drama and emotion. The first to appear, in 1874 was his Requiem, scored for operatic forces but by no means an "opera in ecclesiastical dress" (the words in which Hans von Bülow condemned it before even hearing it). Although in the Requiem Verdi puts to use many of the techniques he learned in opera, its musical forms and emotions are not those of the stage. Verdi's tone painting at the opening of the Requiem is vividly described by the Italian composer Ildebrando Pizzetti, writing in 1941: "in [the words] murmured by an invisible crowd over the slow swaying of a few simple chords, you straightaway sense the fear and sadness of a vast multitude before the mystery of death. In the [following] "Et lux perpetuum" the melody spreads it wings...before falling back on itself...you hear a sigh for consolation and eternal peace."

By the time "Otello" premièred in 1887, more than 15 years after "Aida", the operas of Verdi's (predeceased) contemporary Richard Wagner had begun their ascendancy in popular taste, and many sought or identified Wagnerian aspects in Verdi's latest composition. Budden points out that there is little in the music of "Otello" that relates either to the "verismo" opera of the younger Italian composers, and little if anything which can be construed as a homage to the New German School. Nonetheless there is still much originality, building on the strengths which Verdi had already demonstrated; the powerful storm which opens the opera "in medias res", the recollection of the love duet of Act I in Otello's dying words (more an aspect of "tinta" than "leitmotif"), imaginative touches of harmony in Iago's "Era la notte" (Act II).

Finally, six years later, appeared "Falstaff", Verdi's only comedy apart from the early, ill-fated "Un giorno di regno". In this work Roger Parker writes that:

Although Verdi's operas brought him a popular following, not all contemporary critics approved of his work. The English critic Henry Chorley allowed in 1846 that "he is the only modern man...having a style—for better or worse", but found all his output unacceptable. "[His] faults [are] grave ones, calculated to destroy and degrade taste beyond those of any Italian composer in the long list" wrote Chorley, whilst conceding that "howsoever incomplete may have been his training, howsoever mistaken his aspirations may have proved...he "has" aspired." But by the time of Verdi's death, 55 years later, his reputation was assured, and the 1910 edition of Grove's Dictionary pronounced him "one of the greatest and most popular opera composers of the nineteenth century."

Verdi had no pupils apart from Muzio and no school of composers sought to follow his style which, however much it reflected his own musical direction, was rooted in the period of his own youth. By the time of his death, "verismo" was the accepted style of young Italian composers. The New York Metropolitan Opera frequently staged "Rigoletto, Trovatore" and "Traviata" during this period and featured "Aida" in every season from 1898 to 1945. Interest in the operas reawakened in mid-1920s Germany and this sparked a revival in England and elsewhere. From the 1930s onward there began to appear scholarly biographies and publications of documentation and correspondence.

In 1959 the Instituto di Studi Verdiani (from 1989 the Istituto Nazionale di Studi Verdiani) was founded in Parma and became a leading centre for research and publication of Verdi studies, and in the 1970s the American Institute for Verdi Studies was founded at New York University.

Three Italian conservatories, the Milan Conservatory and those in Turin and Como, are named after Verdi, as are many Italian theatres.

Verdi’s hometown of Busseto displays Luigi Secchi's statue of a seated Verdi in 1913, next to the Teatro Verdi built in his honour in the 1850s. It is one of many statues to the composer in Italy. The Giuseppe Verdi Monument, a 1906 marble memorial, sculpted by Pasquale Civiletti, is located in Verdi Square in Manhattan, New York City. The monument includes a statue of Verdi himself and life-sized statues of four characters from his operas, (Aida, Otello, and Falstaff from the operas of the same names, and Leonora from "Il trovatore").

Verdi has been the subject of a number of film and stage works. These include the 1938 film directed by Carmine Gallone, "Giuseppe Verdi", starring Fosco Giachetti; the 1982 miniseries, "The Life of Verdi", directed by Renato Castellani, where Verdi was played by Ronald Pickup, with narration by Burt Lancaster in the English version; and the 1985 play "After Aida", by Julian Mitchell (1985). He is a character in the 2011 opera "Risorgimento!" by Italian composer Lorenzo Ferrero, written to commemorate the 150th anniversary of Italian unification of 1861.

Verdi appeared on the Italian 1000 lire notes printed from 1962–76. Beginning in 1969 his house was shown on the reverse. He has also appeared on various postage stamps.

Verdi's operas are frequently staged around the world. All of his operas are available in recordings in a number of versions, and on DVD – Naxos Records offers a complete boxed set.

Modern productions may differ substantially from those originally envisaged by the composer. Jonathan Miller's 1982 version of "Rigoletto" for English National Opera, set in the world of modern American mafiosi, received critical plaudits. But the same company's staging in 2002 of "Un ballo in maschera" as "A Masked Ball", directed by Calixto Bieito, including "satanic sex rituals, homosexual rape, [and] a demonic dwarf", got a general critical thumbs down.

Meanwhile, the music of Verdi can still evoke a range of cultural and political resonances. Excerpts from the Requiem were featured at the funeral of Diana, Princess of Wales in 1997. On 12 March 2011 during a performance of "Nabucco" at the Opera di Roma celebrating 150 years of Italian unification, the conductor Riccardo Muti paused after "Va pensiero" and turned to address the audience (which included the then Italian Prime Minister, Silvio Berlusconi) to complain about cuts in state funding of culture; the audience then joined in a repeat of the chorus. In 2014, the pop singer Katy Perry appeared at the Grammy Award wearing a dress designed by Valentino, embroidered with the music of "Dell'invito trascorsa e gia l'ora" from the start of "La traviata". The bicentenary of Verdi's birth in 2013 was celebrated in numerous events around the world, both in performances and broadcasts.

In recent years historians have vigorously debated how political Verdi's operas were. In particular, the "Chorus of the Hebrew Slaves" (known as "Va, pensiero") from the third act of the opera "Nabucco" was intended to be an anthem for Italian patriots, who were seeking to unify their country and free it from foreign control in the years up to 1861 (the chorus's theme of exiles singing about their homeland, and its lines such as "O mia patria, si bella e perduta" / "O my country, so lovely and so lost" were thought to have resonated with many Italians). Beginning in Naples in 1859 and spreading throughout Italy, the slogan "Viva VERDI" was used as an acronym for "Viva Vittorio Emanuele Re D<nowiki>'</nowiki>Italia" ("Long live Victor Emmanuel King of Italy"), referring to Victor Emmanuel II.

George Martin says Verdi was "the greatest artist" of the Risorgimento. "Throughout his work its values, its issues recur constantly, and he expressed them with great power". Franco DellaPeruta agrees that the operas and the Risorgimento are linked, emphasizing Verdi's patriotic intent and links to the values of the Risorgimento. Verdi started as a republican, became a strong supporter of Cavour, and entered the Italian parliament on Cavour's suggestion. His politics caused him to be frequently in trouble with the Austrian censors. Verdi's main works of 1842–49 were especially relevant to the struggle for independence, including 'Nabucco' (1842), 'I Lombardi alla Prima Crociata' (1843), 'Ernani' (1844), 'Attila' (1846), 'Macbeth' (1847), and 'La Battaglia di Legnano' (1848). However, starting in the 1850s, his operas showed few patriotic themes because of the heavy censorship by the absolutist regime in power. Verdi later became disillusioned by politics, but he was personally active part in the political world of events of the Risorgimento and was elected to the first Italian parliament in 1861. Likewise, Marco Pizzo argues that after 1815, music became a political tool, and many songwriters expressed ideals of freedom and equality. Pizzo claims that Verdi was part of this movement, for his operas were inspired by the love of country, the struggle for Italian independence, and speak to the sacrifice of patriots and exiles.

On the other side of the debate, Mary Ann Smart argues that music critics at the time seldom mentioned any political themes. Likewise, Roger Parker argues that the political dimension of Verdi's operas was exaggerated by nationalistic historians looking for a hero in the late 19th century.

Notes
Sources
General

Libretti and scores

Modern performances

Recordings


</doc>
<doc id="12960" url="https://en.wikipedia.org/wiki?curid=12960" title="German Navy">
German Navy

The German Navy ( or simply —) is the navy of Germany and part of the unified "Bundeswehr" ("Federal Defense"), the German Armed Forces. The German Navy was originally known as the "Bundesmarine" ("Federal Navy") from 1956 until 1995 when "Deutsche Marine" ("German Navy") became the official name with respect to the 1990 incorporation of the East German "Volksmarine" ("People's Navy"). It is deeply integrated into the NATO alliance. Its primary mission is protection of Germany's territorial waters and maritime infrastructure as well as sea lines of communication. Apart from this, the German Navy participates in peacekeeping operations, and renders humanitarian assistance and disaster relief. They also participate in Anti-Piracy operations.

The German Navy traces its roots back to the "Reichsflotte" (Imperial Fleet) of the revolutionary era of 1848–52. The Reichsflotte was the first German navy to sail under the black-red-gold flag. Founded on 14 June 1848 by the orders of the democratically elected Frankfurt Parliament, the Reichsflotte's brief existence ended with the failure of the revolution and it was disbanded on 2 April 1852; thus, the modern day navy celebrates its birthday on 14 June.

Between May 1945 and 1956, the German Mine Sweeping Administration and its successor organizations, made up of former members of Nazi Germany's "Kriegsmarine" ("War Navy"), became something of a transition stage for the navy, allowing the future "Marine" to draw on recently experienced personnel upon its formation. Also, from 1949-52 the US Navy had maintained the Naval Historical Team in Bremerhaven. This group of former Kriegsmarine officers acting as historical and tactical consultants to the Americans, was significant in establishing a German element in the NATO senior naval staff. In 1956, with West Germany's accession to NATO, the "Bundesmarine" ("Federal Navy"), as the navy was known colloquially, was formally established. In the same year the East German "Volkspolizei See" (literally "People's Police Sea") became the "Volksmarine" ("People's Navy"). During the Cold War all of the German Navy's combat vessels were assigned to NATO's Allied Forces Baltic Approaches's naval command NAVBALTAP.

With the accession of East Germany to the Federal Republic of Germany in 1990 the Volksmarine along with the whole National People's Army ("Nationale Volksarmee", NVA) became part of the Bundeswehr. Since 1995 the name "German Navy" is used in international context, while the official name since 1956 remains "Marine" without any additions. As of 16 December 2016, the strength of the navy is 16,137 men and women.

A number of naval forces have operated in different periods. See

German warships permanently participate in all four "NATO Maritime Groups". The German Navy is also engaged in operations against international terrorism such as Operation Enduring Freedom and NATO Operation Active Endeavour.

Presently the largest operation the German Navy is participating in is UNIFIL off the coast of Lebanon. The German contribution to this operation is two frigates, four fast attack craft, and two auxiliary vessels. The naval component of UNIFIL has been under German command.

The navy is operating a number of development and testing installations as part of an inter-service and international network. Among these is the Centre of Excellence for Operations in Confined and Shallow Waters (COE CSW), an affiliated centre of Allied Command Transformation. The COE CSW was established in April 2007 and officially accredited by NATO on 26 May 2009. It is co-located with the staff of the German Flotilla 1 in Kiel whose Commander is double-hatted as Director, COE CSW.

In total, there are about 65 commissioned ships in the German Navy, including; 10 frigates, 5 corvettes, 3 minesweepers, 10 minehunters, 6 submarines, 11 replenishment ships and 20 miscellaneous auxiliary vessels. The displacement of the navy is 220,000 tonnes. In addition, the German Navy and the Royal Danish Navy are in cooperation in the "Ark Project". This agreement made the Ark Project responsible for the strategic sealift of German armed forces where the full-time charter of three roll-on-roll-off cargo and troop ships are ready for deployments. In addition, these ships are also kept available for the use of the other European NATO countries.

The three vessels have a combined displacement of 60,000 tonnes.
Including these ships, the total ships' displacement available to the Deutsche Marine is 280,000 tonnes.

A total of five Joint Support Ships, two JSS800 and three JSS400, were planned during the 1995–2010 period but the programme appears now to have been abandoned, not having been mentioned in two recent defence reviews. The larger ships would have been tasked for strategic troop transport and amphibious operations, and were to displace 27,000 to 30,000 tons for 800 soldiers. The German Navy will use the Joint Support Ship HNLMS Karel Doorman (A833) of the Royal Netherlands Navy as part of the integration of the German Navy Marines (Seebatallion) in the Royal Netherlands Marine Corps as of 2016.

The naval air arm of the German Navy is called the "Marinefliegerkommando". The Marinefliegerkommando operate 55 aircraft.

The German Navy is commanded by the Inspector of the Navy ("Inspekteur der Marine") supported by the Navy Command ("Marinekommando") in Rostock.










</doc>
<doc id="12961" url="https://en.wikipedia.org/wiki?curid=12961" title="GÉANT">
GÉANT

GÉANT is the pan-European data network for the research and education community. It interconnects national research and education networks (NRENs) across Europe, enabling collaboration on projects ranging from biological science, to earth observation, to arts and culture. The GÉANT project combines a high-bandwidth, high-capacity 50,000 km network with a growing range of services. These allow researchers to collaborate, working together wherever they are located. Services include identity and trust, multi-domain monitoring perfSONAR MDM, dynamic circuits and roaming via the eduroam service.

Together with European NRENs, GÉANT connects 50 million users in over 10,000 institutions. Through links to research networks in other regions (such as Internet2 and ESnet in the USA, AfricaConnect in Africa, TEIN in Asia-Pacific and RedCLARA in Latin America), GÉANT enables collaboration between researchers in over half the world’s countries.

Co-funded by the European Commission and Europe’s NRENs, the GÉANT network was built and is operated by the GÉANT Association. The GÉANT project is a collaboration between 41 partners: 38 European NRENs, and NORDUnet (representing the five Nordic countries).

The GÉANT project began in November 2000, entered full production operation in December 2001 (fully replacing a network called TEN-155). Originally due to finish in October 2004, it was subsequently extended until April 2005. 

The second generation network, named GÉANT2, began in September 2004 and continued through 2009, growing the network to 30 national networks in 34 countries. 

The next GÉANT project (GN3) began on 1 April 2009 and continued until April 2013. This was then superseded by the GN3plus project which was scheduled to run for two years. It is funded under the EC’s seventh research and development Research Framework Programme (often referred to as FP7). 

The Project is now in its fourth iteration (GN4).

As well as providing the high-bandwidth links across Europe, the GÉANT network also acts as a testbed for new technology.

It was the first "hybrid" network deployed on an international scale, combining routed IP and switched infrastructure. This enables the network to offer general traffic alongside virtual "private" network paths for projects, such as the Large Hadron Collider, which have particular requirements involving dedicated bandwidth, security and flexibility.

GÉANT supported native IPv6 since 2002 and multicast IPv6 since 2004. It is involved in network research, in areas such as carrier class network technologies, photonic switching, federated network architectures and virtualisation.

In 2013 a substantial network migration program was completed, meaning users could be offered multiple 100 Gbit/s links, with the core network supporting 500 Gbit/s and a network design that will support up to 8Tbit/s. 

Already, over 1 Petabyte of data are transferred every day via the GÉANT backbone network.

The GÉANT project is a collaboration between 41 partners: 38 European NRENs and NORDUnet (representing the five Nordic countries).

The full list of NREN project partners are available on the website.

GÉANT links to research networks in other world regions, including:

These links not only help international research collaboration but also aid with projects that deliver societal benefit, such as e-health, telemedicine and weather forecasting/disaster warning systems. Allowing researchers to work within their own countries also stems migration from less developed countries, helping bridge the digital divide.

GÉANT is used by research communities, such as:


</doc>
<doc id="12962" url="https://en.wikipedia.org/wiki?curid=12962" title="Gamma-Hydroxybutyric acid">
Gamma-Hydroxybutyric acid

γ-Hydroxybutyric acid (GHB), also known as 4-hydroxybutanoic acid, is a naturally occurring neurotransmitter and a psychoactive drug. It is a precursor to GABA, glutamate, and glycine in certain brain areas, and it acts on the GHB receptor and it is a weak agonist at the GABA receptor.

GHB has been used in a medical setting as a general anesthetic and as a treatment for cataplexy, narcolepsy, and alcoholism. It is also used illegally as an intoxicant, to try to increase athletic performance, and as a date rape drug and as a recreational drug. It is commonly used in the form of a salt, such as sodium γ-hydroxybutyrate (Na.GHB, sodium oxybate, or Xyrem) or potassium γ-hydroxybutyrate (K.GHB, potassium oxybate).

GHB is also produced as a result of fermentation, and is found in small quantities in some beers and wines, beef and small citrus fruits.

Succinic semialdehyde dehydrogenase deficiency is a disease that causes GHB to accumulate in the blood.

The only common medical use for GHB today are in the treatment of narcolepsy and more rarely alcoholism. It is sometimes used off-label for the treatment of fibromyalgia.

GHB is the active ingredient in the prescription medication sodium oxybate (Xyrem). Sodium oxybate is approved by the U.S. Food and Drug Administration (FDA) for the treatment of cataplexy associated with narcolepsy and excessive daytime sleepiness (EDS) associated with narcolepsy.

GHB has been shown to reliably increase slow-wave sleep and decrease the tendency for REM sleep in modified multiple sleep latency tests

GHB is a central nervous system depressant used as an intoxicant. It has many street names. Its effects have been described anecdotally as comparable with ethanol (alcohol) and MDMA use, such as euphoria, disinhibition, enhanced libido and empathogenic states. At higher doses, GHB may induce nausea, dizziness, drowsiness, agitation, visual disturbances, depressed breathing, amnesia, unconsciousness, and death. When death is associated with GHB, it is sometimes in conjunction with other drugs, such as alcohol or benzodiazepine which influence the same neurotransmitter (gamma-aminobutyric acid, GABA). The effects of GHB can last from 1.5 to 4 hours, or longer if large doses have been consumed. Consuming GHB with alcohol can cause respiratory arrest and vomiting in combination with unrouseable sleep, which is a potentially lethal combination.

Recreational doses of 1-2 g generally provide a feeling of euphoria, and larger doses create deleterious effects such as reduced motor function and drowsiness. The sodium salt of GHB has a salty taste. Other salt forms such as calcium GHB and magnesium GHB have also been reported, but the sodium salt is by far the most common.

Some prodrugs convert to GHB in the stomach and blood stream, such as γ-butyrolactone (GBL). Other prodrugs, such as 1,4-butanediol (1,4-B), also have their own toxicity concerns. GBL and 1,4-B are normally found as pure liquids, but they may be mixed with other more harmful solvents when intended for industrial use, e.g., as paint stripper or varnish thinner.

GHB can be manufactured with little knowledge of chemistry, as it involves the mixing of its two precursors, GBL and an alkali hydroxide such as sodium hydroxide, to form the GHB salt. Due to the ease of manufacture and the availability of its precursors, it is not usually produced in illicit laboratories like other synthetic drugs, but in private homes by low level producers. While available as a prescription for the rare and severe forms of sleep disorder narcolepsy in most of Europe, GHB was banned in the U.S. by the FDA in 1990. However, on 17 July 2002, GHB was approved for treatment of cataplexy, often associated with narcolepsy. GHB is "colourless and odorless".

GHB has been used as a club drug, apparently starting in the 1990s, as small doses of GHB can act as a euphoriant and are believed to be aphrodisiac. GHB slang terms are "liquid ecstasy", "lollipops", "liquid X" or "liquid E" due to its tendency to produce euphoria and sociability and its use in the dance party scene.

By 2009 this use had diminished, possibly due to efforts to control distribution of GHB and its analogs, or to the narrow range of dosing and adverse effects of confusion, dizziness, blurred vision, hot/cold flushes, profuse sweating, vomiting, and loss of consciousness when overdosed. The downward trend was still apparent in 2012.

Some athletes have used GHB or analogs because they have been marketed as being anabolic agents, although there is no evidence that it builds muscle or improves performance in athletes.

GHB became known to the general public as a date rape drug by the late 1990s. GHB is colourless and odorless and has been described as "very easy to add to drinks". When unobtrusively administered in a drink the victim will quickly feel groggy and sleepy, and upon recovery may have an impaired ability to recall memories of events that occurred during the period of intoxication. Consequently, the evidence and the identification of the perpetrator of rape is often difficult.

It is difficult to establish how often GHB is used to facilitate rape as it is difficult to detect in a urine sample after a day, and many victims may only recall the rape some time after this,

However a 2006 study suggested that there was "no evidence to suggest widespread date rape drug use" in the UK and that less than 2% of cases involved GHB while 17% involved cocaine, and a survey in the Netherlands published in 2010 found that the proportion of drug-related rape where GHB was used appeared to be greatly overestimated by the media.

There have been several high-profile cases of GHB as a date rape drug that received national attention in the United States. In early 1999 a 15-year-old girl, Samantha Reid of Rockwood, Michigan, died from GHB poisoning. Reid’s death inspired the legislation titled the "Hillory J. Farias and Samantha Reid Date-Rape Drug Prohibition Act of 2000." This is the law that made GHB a schedule 1 controlled substance.

GHB can be detected in hair. Hair testing can be a useful tool in court cases or for the victim's own information. Over-the-counter urine test kits only test for date rape drugs that are benzodiazepines, and GHB is not a benzodiazepine. To detect GHB in urine, the sample must be taken within four hours of GHB ingestion, and cannot be tested at home.

In humans, GHB has been shown to reduce the elimination rate of alcohol. This may explain the respiratory arrest that has been reported after ingestion of both drugs. A review of the details of 194 deaths attributed to or related to GHB over a ten-year period found that most were from respiratory depression caused by interaction with alcohol or other drugs.

One publication has investigated 226 deaths attributed to GHB. Of 226 deaths included, 213 had a cardiorespiratory arrest and 13 had fatal accidents. Seventy-one deaths (34%) had no co-intoxicants. Postmortem blood GHB was 18–4400 mg/L (median=347) in deaths negative for co-intoxicants.

One report has suggested that sodium oxybate overdose might be fatal, based on deaths of three patients who had been prescribed the drug. However, for two of the three cases, post-mortem GHB concentrations were 141 and 110 mg/L, which is within the expected range of concentrations for GHB after death, and the third case was a patient with a history of intentional drug overdose. The toxicity of GHB has been an issue in criminal trials, as in the death of Felicia Tang, where the defense argued that death was due to GHB, not murder.

GHB is produced in the body in very small amounts, and blood levels may climb after death to levels in the range of 30–50 mg/L. Levels higher than this are found in GHB deaths. Levels lower than this may be due to GHB or to postmortem endogenous elevations.

A UK parliamentary committee commissioned report found the use of GHB to be less dangerous than tobacco and alcohol in social harms, physical harm and addiction.

In multiple studies, GHB has been found to impair spatial memory, working memory, learning and memory in rats with chronic administration. These effects are associated with decreased NMDA receptor expression in the cerebral cortex and possibly other areas as well. In addition, the neurotoxicity appears to be caused by oxidative stress.

Although there have been reported fatalities due to GHB withdrawal, reports are inconclusive and further research is needed. A common problem is that GHB does not leave traces in the body after a short period of time, complicating diagnosis and research. Addiction occurs when repeated drug use disrupts the normal balance of brain circuits that control rewards, memory and cognition, ultimately leading to compulsive drug taking.

Rats forced to consume massive doses of GHB will intermittently prefer GHB solution to water but, after experiments on rats, it was noted that "no rat showed any sign of withdrawal when GHB was finally removed at the end of the 20-week period" or during periods of voluntary abstinence.

GHB has also been associated with a withdrawal syndrome of insomnia, anxiety, and tremor that usually resolves within three to twenty-one days. The withdrawal syndrome can be severe producing acute delirium and may require hospitalization in an intensive care unit for management. Management of GHB dependence involves considering the person's age, comorbidity and the pharmacological pathways of GHB. The mainstay of treatment for severe withdrawal is supportive care and benzodiazepines for control of acute delirium, but larger doses are often required compared to acute delirium of other causes (e.g. > 100 mg/d of diazepam). Baclofen has been suggested as an alternative or adjunct to benzodiazepines based on anecdotal evidence and some animal data. However, there is less experience with the use of baclofen for GHB withdrawal, and additional research in humans is needed. Baclofen was first suggested as an adjunct because benzodiazepines do not affect GABA receptors and thus have no cross-tolerance with GHB while baclofen, which works via GABA receptors, is cross-tolerant with GHB and may be more effective in alleviating withdrawal effects of GHB.

GHB withdrawal is not widely discussed in textbooks and some psychiatrists, general practitioners, and even hospital emergency physicians may not be familiar with this withdrawal syndrome.

Overdose of GHB can sometimes be difficult to treat because of its multiple effects on the body. GHB tends to cause rapid unconsciousness at doses above 3500 mg, with single doses over 7000 mg often causing life-threatening respiratory depression, and higher doses still inducing bradycardia and cardiac arrest. Other side-effects include convulsions (especially when combined with stimulants), and nausea/vomiting (especially when combined with alcohol).

The greatest life threat due to GHB overdose (with or without other substances) is respiratory arrest. Other relatively common causes of death due to GHB ingestion include aspiration of vomitus, positional asphyxia, and trauma sustained while intoxicated (e.g., motor vehicle accidents while driving under the influence of GHB). The risk of aspiration pneumonia and positional asphyxia risk can be reduced by laying the patient down in the recovery position. People are most likely to vomit as they become unconscious, and as they wake up. It is important to keep the victim awake and moving, who must not be left alone due to the risk of death through vomiting. Frequently they will be in a good mood but this does not mean they are not in danger. GHB overdose is a medical emergency and immediate assessment in an emergency department is needed.

Convulsions from GHB can be treated with the benzodiazepines diazepam or lorazepam. Even though these benzodiazepines are also CNS depressants, they primarily modulate GABA receptors whereas GHB is primarily a GABA receptor agonist, and so do not worsen CNS depression as much as might be expected.

Because of the faster and more complete absorption of GBL relative to GHB, its dose-response curve is steeper, and overdoses of GBL tend to be more dangerous and problematic than overdoses involving only GHB or 1,4-B. Any GHB/GBL overdose is a medical emergency and should be cared for by appropriately trained personnel.

A newer synthetic drug SCH-50911, which acts as a selective GABA antagonist, quickly reverses GHB overdose in mice. However, this treatment has yet to be tried in humans, and it is unlikely that it will be researched for this purpose in humans due to the illegal nature of clinical trials of GHB, and the lack of medical indemnity coverage inherent in using an untested treatment for a life-threatening overdose.

GHB may be quantitated in blood or plasma to confirm a diagnosis of poisoning in hospitalized patients, provide evidence in an impaired driving arrest or to assist in a medicolegal death investigation. Blood or plasma GHB concentrations are usually in a range of 50–250 mg/L in persons receiving the drug therapeutically (during general anesthesia), 30–100 mg/L in those arrested for impaired driving, 50–500 mg/L in acutely intoxicated patients and 100–1000 mg/L in victims of fatal overdosage. Urine is often the preferred specimen for routine drug abuse monitoring purposes. Both γ-butyrolactone (GBL) and 1,4-butanediol are converted to GHB in the body.

In January 2016, it was announced scientists had developed a way to detect GHB, among other things, in saliva.

Cells produce GHB by reduction of succinic semialdehyde via succinic semialdehyde reductase (SSR). This enzyme appears to be induced by cAMP levels, meaning substances that elevate cAMP, such as forskolin and vinpocetine, may increase GHB synthesis and release. Conversely, endogeneous GHB production in those taking valproic acid will be inhibited via inhibition of the conversion from succinic acid semialdehyde to GHB. It is important to note, however, that direct administration of GHB or endogenous GHB already present in the body will not be affected by valproic acid. People with the disorder known as succinic semialdehyde dehydrogenase deficiency, also known as γ-hydroxybutyric aciduria, have elevated levels of GHB in their urine, blood plasma and cerebrospinal fluid.

The precise function of GHB in the body is not clear. It is known, however, that the brain expresses a large amount of receptors that are activated by GHB. These receptors are excitatory and not responsible for the sedative effects of GHB – they have been shown to elevate the principal excitatory neurotransmitter—glutamate. The benzamide antipsychotics—amisulpride, sulpiride—have been shown to bind to this receptor in vivo. Other antipsychotics were tested and were not found to have an affinity for this receptor.

It is a precursor to GABA, glutamate, and glycine in certain brain areas.

GHB has neuroprotective properties and has been found to protect cells from hypoxia.

GHB is also produced as a result of fermentation and so is found in small quantities in some beers and wines, in particular fruit wines. The amount found in wine is pharmacologically insignificant and not sufficient to produce psychoactive effects.

GHB has at least two distinct binding sites in the central nervous system. GHB is an agonist at the newly characterized GHB receptor, which is excitatory, and it is a weak agonist at the GABA receptor, which is inhibitory. GHB is a naturally occurring substance that acts in a similar fashion to some neurotransmitters in the mammalian brain. GHB is probably synthesized from GABA in GABAergic neurons, and released when the neurons fire.

GHB has been found to activate oxytocinergic neurons in the supraoptic nucleus.

If taken orally, GABA itself does not effectively cross the blood–brain barrier.

GHB induces the accumulation of either a derivative of tryptophan or tryptophan itself in the extracellular space, possibly by increasing tryptophan transport across the blood–brain barrier. The blood content of certain neutral amino-acids, including tryptophan, is also increased by peripheral GHB administration. GHB-induced stimulation of tissue serotonin turnover may be due to an increase in tryptophan transport to the brain and in its uptake by serotonergic cells. As the serotonergic system may be involved in the regulation of sleep, mood, and anxiety, the stimulation of this system by high doses of GHB may be involved in certain neuropharmacological events induced by GHB administration.

However, at therapeutic doses, GHB reaches much higher concentrations in the brain and activates GABA receptors, which are primarily responsible for its sedative effects. GHB's sedative effects are blocked by GABA antagonists.

The role of the GHB receptor in the behavioural effects induced by GHB is more complex. GHB receptors are densely expressed in many areas of the brain, including the cortex and hippocampus, and these are the receptors that GHB displays the highest affinity for. There has been somewhat limited research into the GHB receptor; however, there is evidence that activation of the GHB receptor in some brain areas results in the release of glutamate, the principal excitatory neurotransmitter. Drugs that selectively activate the GHB receptor cause absence seizures in high doses, as do GHB and GABA(B) agonists.

Activation of both the GHB receptor and GABA(B) is responsible for the addictive profile of GHB. GHB's effect on dopamine release is biphasic. Low concentrations stimulate dopamine release via the GHB receptor. Higher concentrations inhibit dopamine release via GABA(B) receptors as do other GABA(B) agonists such as baclofen and phenibut. After an initial phase of inhibition, dopamine release is then increased via the GHB receptor. Both the inhibition and increase of dopamine release by GHB are inhibited by opioid antagonists such as naloxone and naltrexone. Dynorphin may play a role in the inhibition of dopamine release via kappa opioid receptors.

This explains the paradoxical mix of sedative and stimulatory properties of GHB, as well as the so-called "rebound" effect, experienced by individuals using GHB as a sleeping agent, wherein they awake suddenly after several hours of GHB-induced deep sleep. That is to say that, over time, the concentration of GHB in the system decreases below the threshold for significant GABA receptor activation and activates predominantly the GHB receptor, leading to wakefulness.

Recently, analogs of GHB, such as 4-hydroxy-4-methylpentanoic acid (UMB68) have been synthesised and tested on animals, in order to gain a better understanding of GHB's mode of action. Analogues of GHB such as 3-methyl-GHB, 4-methyl-GHB, and 4-phenyl-GHB have been shown to produce similar effects to GHB in some animal studies, but these compounds are even less well researched than GHB itself. Of these analogues, only 4-methyl-GHB (γ-hydroxyvaleric acid, GHV) and a prodrug form γ-valerolactone (GVL) have been reported as drugs of abuse in humans, and on the available evidence seem to be less potent but more toxic than GHB, with a particular tendency to cause nausea and vomiting.

Other prodrug ester forms of GHB have also rarely been encountered by law enforcement, including 1,4-butanediol diacetate (BDDA/DABD), methyl-4-acetoxybutanoate (MAB), and ethyl-4-acetoxybutanoate (EAB), but these are, in general, covered by analogue laws in jurisdictions where GHB is illegal, and little is known about them beyond their delayed onset and longer duration of action. The intermediate compound γ-hydroxybutyraldehyde (GHBAL) is also a prodrug for GHB; however, as with all aliphatic aldehydes this compound is caustic and is strong-smelling and foul-tasting; actual use of this compound as an intoxicant is likely to be unpleasant and result in severe nausea and vomiting.

Both of the metabolic breakdown pathways shown for GHB can run in either direction, depending on the concentrations of the substances involved, so the body can make its own GHB either from GABA or from succinic semialdehyde. Under normal physiological conditions, the concentration of GHB in the body is rather low, and the pathways would run in the reverse direction to what is shown here to produce endogenous GHB. However, when GHB is consumed for recreational or health promotion purposes, its concentration in the body is much higher than normal, which changes the enzyme kinetics so that these pathways operate to metabolise GHB rather than producing it.

Alexander Zaytsev worked on this chemical family and published work on it in 1874. The first extended research into GHB and its use in humans was conducted in the early 1960s by Dr. Henri Laborit to use in studying the neurotransmitter GABA. It was studied in a range of uses including obstetric surgery and during childbirth and as an anxiolytic; there were anecdotal reports of it having antidepressant and aphrodisiac effects as well. It was also studied as an intraveuous anesthetic agent and was marketed for that purpose starting in 1964 in Europe but it was not widely adopted as it caused seizures; as of 2006 that use was still authorized in France and Italy but not widely used. It was also studied to treat alcohol addiction; while the evidence for this use is weak, however sodium oxybate is marketed for this use in Italy.

GHB and sodium oxybate were also studied for use in narcolepsy from the 1960s onwards.

In May 1990 GHB was introduced as a dietary supplement and was marketed to body builders, for help with weight control and as a sleep aid, and as a "replacement" for l-tryptophan, which was removed from the market in November 1989 when batches of it were found to cause eosinophilia-myalgia syndrome. By November of that year 57 cases of illness caused by the GHB supplements had been reported to the Centers for Disease Control and Prevention, with people having taken up to three teaspoons of GHB; there were no deaths but nine people needed care in an intensive care unit. The FDA issued a warning in November 1990 that sale of GHB was illegal. GHB continued to be manufactured and sold illegally and it and analogs were adopted as a club drug and came to be used as a date rape drug, and the DEA made seizures and the FDA reissued warnings several times throughout the 1990s.

At the same time, research on the use of GHB in the form of sodium oxybate had formalized, as a company called Orphan Medical had filed an investigational new drug application and was running clinical trials with the intention of gaining regulatory approval for use to treat narcolepsy.

A popular children's toy, Bindeez (also known as Aqua Dots, in the United States), produced by Melbourne company Moose, was banned in Australia in early November 2007 when it was discovered that 1,4-butanediol (1,4-B), which is metabolized into GHB, had been substituted for the non-toxic plasticiser 1,5-pentanediol in the bead manufacturing process. Three young children were hospitalized as a result of ingesting a large number of the beads, and the toy was recalled.

In the United States, it was placed on Schedule I of the Controlled Substances Act in March 2000. However, used in sodium oxybate under an IND or NDA from the US FDA, it is considered a Schedule III substance but with Schedule I trafficking penalties, one of several drugs that are listed in multiple schedules.

On 20 March 2001, the UN Commission on Narcotic Drugs placed GHB in Schedule IV of the 1971 Convention on Psychotropic Substances.

In the UK GHB was made a class C drug in June 2003. In October 2013 the ACMD recommended upgrading it from schedule IV to schedule II in line with UN recommendations. Their report concluded that the minimal use of Xyrem in the UK meant that prescribers would be minimally inconvenienced by the rescheduling. This advice was followed and GHB was moved to schedule 2 on 7 January 2015.

In Hong Kong, GHB is regulated under Schedule 1 of Hong Kong's Chapter 134 "Dangerous Drugs Ordinance". It can only be used legally by health professionals and for university research purposes. The substance can be given by pharmacists under a prescription. Anyone who supplies the substance without prescription can be fined HK$10,000. The penalty for trafficking or manufacturing the substance is a HK$150,000 fine and life imprisonment. Possession of the substance for consumption without license from the Department of Health is illegal with a HK$100,000 fine or 5 years of jail time.

In New Zealand and Australia, GHB, 1,4-B and GBL are all Class B illegal drugs, along with any possible esters, ethers and aldehydes. GABA itself is also listed as an illegal drug in these jurisdictions, which seems unusual given its failure to cross the blood–brain barrier, but there was a perception among legislators that all known analogues should be covered as far as this was possible. Attempts to circumvent the illegal status of GHB have led to the sale of derivatives such as 4-methyl-GHB (γ-hydroxyvaleric acid, GHV) and its prodrug form γ-valerolactone (GVL), but these are also covered under the law by virtue of their being "substantially similar" to GHB or GBL and; so importation, sale, possession and use of these compounds is also considered to be illegal.

In Chile, GHB is a controlled drug under the law (psychotropic substances and narcotics).

In Norway and in Switzerland, GHB is considered a narcotic and is only available by prescription under the trade name Xyrem (Union Chimique Belge S.A.).

Sodium oxybate is also used therapeutically in Italy under the brand name Alcover for treatment of alcohol withdrawal and dependence.




</doc>
<doc id="12963" url="https://en.wikipedia.org/wiki?curid=12963" title="Giordano Bruno">
Giordano Bruno

Giordano Bruno (; ; ; 1548 – 17 February 1600), born Filippo Bruno, was an Italian Dominican friar, philosopher, mathematician, poet, and cosmological theorist. He is known for his cosmological theories, which conceptually extended the then-novel Copernican model. He proposed that the stars were just distant suns surrounded by their own exoplanets and raised the possibility that these planets could even foster life of their own (a philosophical position known as cosmic pluralism). He also insisted that the universe is in fact infinite and could have no celestial body at its "center".

Starting in 1593, Bruno was tried for heresy by the Roman Inquisition on charges of denial of several core Catholic doctrines, including eternal damnation, the Trinity, the divinity of Christ, the virginity of Mary, and transubstantiation. Bruno's pantheism was also a matter of grave concern, as was his teaching of the "transmigration of the soul". The Inquisition found him guilty, and he was burned at the stake in Rome's Campo de' Fiori in 1600. After his death, he gained considerable fame, being particularly celebrated by 19th- and early 20th-century commentators who regarded him as a martyr for science, although historians have debated the extent to which his heresy trial was a response to his astronomical views or to other aspects of his philosophy and theology.
Bruno's case is still considered a landmark in the history of free thought and the emerging sciences.

In addition to cosmology, Bruno also wrote extensively on the art of memory, a loosely organized group of mnemonic techniques and principles. Historian Frances Yates argues that Bruno was deeply influenced by Arab astrology (particularly the philosophy of Averroes), Neoplatonism, Renaissance Hermeticism, and legends surrounding the Egyptian god Thoth. Other studies of Bruno have focused on his qualitative approach to mathematics and his application of the spatial concepts of geometry to language.

Born Filippo Bruno in Nola (in Campania, then part of the Kingdom of Naples) in 1548, he was the son of Giovanni Bruno, a soldier, and Fraulissa Savolino. In his youth he was sent to Naples to be educated. He was tutored privately at the Augustinian monastery there, and attended public lectures at the Studium Generale. At the age of 17, he entered the Dominican Order at the monastery of San Domenico Maggiore in Naples, taking the name Giordano, after Giordano Crispo, his metaphysics tutor. He continued his studies there, completing his novitiate, and became an ordained priest in 1572 at age 24. During his time in Naples he became known for his skill with the art of memory and on one occasion traveled to Rome to demonstrate his mnemonic system before Pope Pius V and Cardinal Rebiba. In his later years Bruno claimed that the Pope accepted his dedication to him of the lost work "On The Ark of Noah" at this time.

While Bruno was distinguished for outstanding ability, his taste for free thinking and forbidden books soon caused him difficulties. Given the controversy he caused in later life it is surprising that he was able to remain within the monastic system for eleven years. In his testimony to Venetian inquisitors during his trial, many years later, he says that proceedings were twice taken against him for having cast away images of the saints, retaining only a crucifix, and for having recommended controversial texts to a novice. Such behavior could perhaps be overlooked, but Bruno's situation became much more serious when he was reported to have defended the Arian heresy, and when a copy of the banned writings of Erasmus, annotated by him, was discovered hidden in the convent privy. When he learned that an indictment was being prepared against him in Naples he fled, shedding his religious habit, at least for a time.

Bruno first went to the Genoese port of Noli, then to Savona, Turin and finally to Venice, where he published his lost work "On the Signs of the Times" with the permission (so he claimed at his trial) of the Dominican Remigio Nannini Fiorentino. From Venice he went to Padua, where he met fellow Dominicans who convinced him to wear his religious habit again. From Padua he went to Bergamo and then across the Alps to Chambéry and Lyon. His movements after this time are obscure.

In 1579 he arrived in Geneva. As D.W. Singer, a Bruno biographer, notes, "The question has sometimes been raised as to whether Bruno became a Protestant, but it is intrinsically most unlikely that he accepted membership in Calvin's communion" During his Venetian trial he told inquisitors that while in Geneva he told the Marchese de Vico of Naples, who was notable for helping Italian refugees in Geneva, "I did not intend to adopt the religion of the city. I desired to stay there only that I might live at liberty and in security." Bruno had a pair of breeches made for himself, and the Marchese and others apparently made Bruno a gift of a sword, hat, cape and other necessities for dressing himself; in such clothing Bruno could no longer be recognized as a priest. Things apparently went well for Bruno for a time, as he entered his name in the Rector's Book of the University of Geneva in May 1579. But in keeping with his personality he could not long remain silent. In August he published an attack on the work of Antoine de la Faye, a distinguished professor. He and the printer were promptly arrested. Rather than apologizing, Bruno insisted on continuing to defend his publication. He was refused the right to take sacrament. Though this right was eventually restored, he left Geneva.

He went to France, arriving first in Lyon, and thereafter settling for a time (1580–1581) in Toulouse, where he took his doctorate in theology and was elected by students to lecture in philosophy. It seems he also attempted at this time to return to Catholicism, but was denied absolution by the Jesuit priest he approached. When religious strife broke out in the summer of 1581, he moved to Paris. There he held a cycle of thirty lectures on theological topics and also began to gain fame for his prodigious memory. Bruno's feats of memory were based, at least in part, on his elaborate system of mnemonics, but some of his contemporaries found it easier to attribute them to magical powers. His talents attracted the benevolent attention of the king Henry III. The king summoned him to the court. Bruno subsequently reported "I got me such a name that King Henry III summoned me one day to discover from me if the memory which I possessed was natural or acquired by magic art. I satisfied him that it did not come from sorcery but from organised knowledge; and, following this, I got a book on memory printed, entitled "The Shadows of Ideas", which I dedicated to His Majesty. Forthwith he gave me an Extraordinary Lectureship with a salary."

In Paris Bruno enjoyed the protection of his powerful French patrons. During this period, he published several works on mnemonics, including "De umbris idearum" ("On the Shadows of Ideas", 1582), "Ars Memoriae" ("The Art of Memory", 1582), and "Cantus Circaeus" ("Circe's Song", 1582). All of these were based on his mnemonic models of organised knowledge and experience, as opposed to the simplistic logic-based mnemonic techniques of Petrus Ramus then becoming popular. Bruno also published a comedy summarizing some of his philosophical positions, titled "Il Candelaio" ("The Torchbearer", 1582). In the 16th century dedications were, as a rule, approved beforehand, and hence were a way of placing a work under the protection of an individual. Given that Bruno dedicated various works to the likes of King Henry III, Sir Philip Sidney, Michel de Castelnau (French Ambassador to England), and possibly Pope Pius V, it is apparent that this wanderer had risen sharply in status and moved in powerful circles.

In April 1583, Bruno went to England with letters of recommendation from Henry III as a guest of the French ambassador, Michel de Castelnau. There he became acquainted with the poet Philip Sidney (to whom he dedicated two books) and other members of the Hermetic circle around John Dee, though there is no evidence that Bruno ever met Dee himself. He also lectured at Oxford, and unsuccessfully sought a teaching position there. His views were controversial, notably with John Underhill, Rector of Lincoln College and subsequently bishop of Oxford, and George Abbot, who later became Archbishop of Canterbury. Abbot mocked Bruno for supporting "the opinion of Copernicus that the Earth did go round, and the heavens did stand still; whereas in truth it was his own head which rather did run round, and his brains did not stand still", and reports accusations that Bruno plagiarized Ficino's work.

Nevertheless, his stay in England was fruitful. During that time Bruno completed and published some of his most important works, the six "Italian Dialogues," including the cosmological tracts "La Cena de le Ceneri" ("The Ash Wednesday Supper", 1584), "De la Causa, Principio et Uno" ("On Cause, Principle and Unity", 1584), "De l'Infinito, Universo e Mondi" ("On the Infinite, Universe and Worlds", 1584) as well as "Lo Spaccio de la Bestia Trionfante" ("The Expulsion of the Triumphant Beast", 1584) and "De gl' Heroici Furori" ("On the Heroic Frenzies", 1585). Some of these were printed by John Charlewood. Some of the works that Bruno published in London, notably "The Ash Wednesday Supper", appear to have given offense. Once again, Bruno's controversial views and tactless language lost him the support of his friends. John Bossy has advanced the theory that, while staying in the French Embassy in London, Bruno was also spying on Catholic conspirators, under the pseudonym "Henry Fagot', for Sir Francis Walsingham, Queen Elizabeth's Secretary of State.

Bruno is sometimes cited as being the first to propose that the universe is infinite, which he did during his time in England, but an English scientist, Thomas Digges, put forth this idea in a published work in 1576, some eight years earlier than Bruno.

In October 1585, after the French embassy in London was attacked by a mob, Bruno returned to Paris with Castelnau, finding a tense political situation. Moreover, his 120 theses against Aristotelian natural science and his pamphlets against the mathematician Fabrizio Mordente soon put him in ill favor. In 1586, following a violent quarrel about Mordente's invention, the differential compass, he left France for (Germany).

In Germany he failed to obtain a teaching position at Marburg, but was granted permission to teach at Wittenberg, where he lectured on Aristotle for two years. However, with a change of intellectual climate there, he was no longer welcome, and went in 1588 to Prague, where he obtained 300 taler from Rudolf II, but no teaching position. He went on to serve briefly as a professor in Helmstedt, but had to flee again when he was excommunicated by the Lutherans.

During this period he produced several Latin works, dictated to his friend and secretary Girolamo Besler, including "De Magia" ("On Magic"), "Theses De Magia" ("Theses on Magic") and "De Vinculis in Genere" ("A General Account of Bonding"). All these were apparently transcribed or recorded by Besler (or Bisler) between 1589 and 1590. He also published "De Imaginum, Signorum, Et Idearum Compositione" ("On the Composition of Images, Signs and Ideas", 1591).

In 1591 he was in Frankfurt. Apparently, during the Frankfurt Book Fair, he received an invitation to Venice from the patrician Giovanni Mocenigo, who wished to be instructed in the art of memory, and also heard of a vacant chair in mathematics at the University of Padua. At the time the Inquisition seemed to be losing some of its strictness, and because Venice was the most liberal state in Italy, Bruno was lulled into making the fatal mistake of returning to Italy.

He went first to Padua, where he taught briefly, and applied unsuccessfully for the chair of mathematics, which was given instead to Galileo Galilei one year later. Bruno accepted Mocenigo's invitation and moved to Venice in March 1592. For about two months he served as an in-house tutor to Mocenigo. When Bruno announced his plan to leave Venice to his host, the latter, who was unhappy with the teachings he had received and had apparently come to dislike Bruno, denounced him to the Venetian Inquisition, which had Bruno arrested on 22 May 1592. Among the numerous charges of blasphemy and heresy brought against him in Venice, based on Mocenigo's denunciation, was his belief in the plurality of worlds, as well as accusations of personal misconduct. Bruno defended himself skillfully, stressing the philosophical character of some of his positions, denying others and admitting that he had had doubts on some matters of dogma. The Roman Inquisition, however, asked for his transfer to Rome. After several months of argument, the Venetian authorities reluctantly consented and Bruno was sent to Rome in February 1593.

During the seven years of his trial in Rome, Bruno was held in confinement, lastly in the Tower of Nona. Some important documents about the trial are lost, but others have been preserved, among them a summary of the proceedings that was rediscovered in 1940. The numerous charges against Bruno, based on some of his books as well as on witness accounts, included blasphemy, immoral conduct, and heresy in matters of dogmatic theology, and involved some of the basic doctrines of his philosophy and cosmology. Luigi Firpo speculates the charges made against Bruno by the Roman Inquisition were:

Bruno defended himself as he had in Venice, insisting that he accepted the Church's dogmatic teachings, but trying to preserve the basis of his philosophy. In particular, he held firm to his belief in the plurality of worlds, although he was admonished to abandon it. His trial was overseen by the Inquisitor Cardinal Bellarmine, who demanded a full recantation, which Bruno eventually refused. On 20 January 1600, Pope Clement VIII declared Bruno a heretic and the Inquisition issued a sentence of death. According to the correspondence of Gaspar Schopp of Breslau, he is said to have made a threatening gesture towards his judges and to have replied: "Maiori forsan cum timore sententiam in me fertis quam ego accipiam" ("Perhaps you pronounce this sentence against me with greater fear than I receive it").

He was turned over to the secular authorities. On Ash Wednesday, 17 February 1600, in the Campo de' Fiori (a central Roman market square), with his "tongue imprisoned because of his wicked words". He was hung upside down naked before he was finally burned at the stake. His ashes were thrown into the Tiber river. All of Bruno's works were placed on the "Index Librorum Prohibitorum" in 1603.
The inquisition cardinals who judged Giordano Bruno were Cardinal Bellarmino (Bellarmine), Cardinal Madruzzo (Madruzzi), Cardinal Camillo Borghese (later Pope Paul V), Domenico Cardinal Pinelli, Pompeio Cardinal Arrigoni, Cardinal Sfondrati, Pedro Cardinal De Deza Manuel and Cardinal Santorio (Archbishop of Santa Severina, Cardinal-Bishop of Palestrina).

The earliest likeness of Bruno is an engraving published in 1715 and cited by Salvestrini as "the only known portrait of Bruno". Salvestrini suggests that it is a re-engraving made from a now lost original. This engraving has provided the source for later images.

The records of Bruno's imprisonment by the Venetian inquisition in May 1592 describe him as a man "of average height, with a hazel-coloured beard and the appearance of being about forty years of age".
Alternately, a passage in a work by George Abbot indicates that Bruno was of diminutive stature: "When that Italian Didapper, who intituled himselfe Philotheus Iordanus Brunus Nolanus, magis elaboratae Theologiae Doctor, &c. with a name longer than his body...". The word "didapper" used by Abbot is the derisive term which at the time meant "a small diving waterfowl".

In the first half of the 15th century, Nicholas of Cusa challenged the then widely accepted philosophies of Aristotelianism, envisioning instead an infinite universe whose center was everywhere and circumference nowhere, and moreover teeming with countless stars. He also predicted that neither were the rotational orbits circular nor were their movements uniform.

In the second half of the 16th century, the theories of Copernicus (1473–1543) began diffusing through Europe. Copernicus conserved the idea of planets fixed to solid spheres, but considered the apparent motion of the stars to be an illusion caused by the rotation of the Earth on its axis; he also preserved the notion of an immobile center, but it was the Sun rather than the Earth. Copernicus also argued the Earth was a planet orbiting the Sun once every year. However he maintained the Ptolemaic hypothesis that the orbits of the planets were composed of perfect circles—deferents and epicycles—and that the stars were fixed on a stationary outer sphere.

Despite the widespread publication of Copernicus' work "De revolutionibus orbium coelestium", during Bruno's time most educated Catholics subscribed to the Aristotelian geocentric view that the earth was the center of the universe, and that all heavenly bodies revolved around it. The ultimate limit of the universe was the "primum mobile", whose diurnal rotation was conferred upon it by a transcendental God, not part of the universe (although, as the kingdom of heaven, adjacent to it), a motionless prime mover and first cause. The fixed stars were part of this celestial sphere, all at the same fixed distance from the immobile earth at the center of the sphere. Ptolemy had numbered these at 1,022, grouped into 48 constellations. The planets were each fixed to a transparent sphere.

Few astronomers of Bruno's time accepted Copernicus's heliocentric model. Among those who did were the Germans Michael Maestlin (1550–1631), Christoph Rothmann, Johannes Kepler (1571–1630), the Englishman Thomas Digges, author of "A Perfit Description of the Caelestial Orbes," and the Italian Galileo Galilei (1564–1642).

In 1584, Bruno published two important philosophical dialogues (La Cena de le Ceneri and De l'infinito universo et mondi) in which he argued against the planetary spheres (Christoph Rothmann did the same in 1586 as did Tycho Brahe in 1587) and affirmed the Copernican principle.

In particular, to support the Copernican view and oppose the objection according to which the motion of the Earth would be perceived by means of the motion of winds, clouds etc., in La Cena de le Ceneri Bruno anticipates some of the arguments of Galilei on the relativity principle. Note that he also uses the example now known as Galileo's ship.

Theophilus – [...] air through which the clouds and winds move are parts of the Earth, [...] to mean under the name of Earth the whole machinery and the entire animated part, which consists of dissimilar parts; so that the rivers, the rocks, the seas, the whole vaporous and turbulent air, which is enclosed within the highest mountains, should belong to the Earth as its members, just as the air [does] in the lungs and in other cavities of animals by which they breathe, widen their arteries, and other similar effects necessary for life are performed. The clouds, too, move through accidents in the body of the Earth and are in its bowels as are the waters. [...]

With the Earth move [...] all things that are on the Earth. If, therefore, from a point outside the Earth something were thrown upon the Earth, it would lose, because of the latter's motion, its straightness as would be seen on the ship [...] moving along a river, if someone on point C of the riverbank were to throw a stone along a straight line, and would see the stone miss its target by the amount of the velocity of the ship's motion. But if someone were placed high on the mast of that ship, move as it may however fast, he would not miss his target at all, so that the stone or some other heavy thing thrown downward would not come along a straight line from the point E which is at the top of the mast, or cage, to the point D which is at the bottom of the mast, or at some point in the bowels and body of the ship. Thus, if from the point D to the point E someone who is inside the ship would throw a stone straight up, it would return to the bottom along the same line however far the ship moved, provided it was not subject to any pitch and roll."

Bruno's infinite universe was filled with a substance—a "pure air," aether, or "spiritus"—that offered no resistance to the heavenly bodies which, in Bruno's view, rather than being fixed, moved under their own impetus (momentum). Most dramatically, he completely abandoned the idea of a hierarchical universe.

The universe is then one, infinite, immobile... It is not capable of comprehension and therefore is endless and limitless, and to that extent infinite and indeterminable, and consequently immobile.

Bruno's cosmology distinguishes between "suns" which produce their own light and heat, and have other bodies moving around them; and "earths" which move around suns and receive light and heat from them. Bruno suggested that some, if not all, of the objects classically known as fixed stars are in fact suns. According to astrophysicist Steven Soter, he was the first person to grasp that "stars are other suns with their own planets."

Bruno wrote that other worlds "have no less virtue nor a nature different from that of our Earth" and, like Earth, "contain animals and inhabitants".

During the late 16th century, and throughout the 17th century, Bruno's ideas were held up for ridicule, debate, or inspiration. Margaret Cavendish, for example, wrote an entire series of poems against "atoms" and "infinite worlds" in "Poems and Fancies" in 1664. Bruno's true, if partial, vindication would have to wait for the implications and impact of Newtonian cosmology.

Bruno's overall contribution to the birth of modern science is still controversial. Some scholars follow Frances Yates stressing the importance of Bruno's ideas about the universe being infinite and lacking geocentric structure as a crucial crosspoint between the old and the new. Others see in Bruno's idea of multiple worlds instantiating the infinite possibilities of a pristine, indivisible One, a forerunner of Everett's many-worlds interpretation of quantum mechanics.

While most academics note Bruno's theological position as pantheism, physicist and philosopher Max Bernhard Weinstein in his "Welt- und Lebensanschauungen, Hervorgegangen aus Religion, Philosophie und Naturerkenntnis" ("World and Life Views, Emerging From Religion, Philosophy and Nature"), wrote that the theological model of pandeism was strongly expressed in the teachings of Bruno, especially with respect to the vision of a deity which had no particular relation to one part of the infinite universe more than any other, and was immanent, as present on Earth as in the Heavens, subsuming in itself the multiplicity of existence.

The Vatican has published few official statements about Bruno's trial and execution. In 1942, Cardinal Giovanni Mercati, who discovered a number of lost documents relating to Bruno's trial, stated that the Church was perfectly justified in condemning him. On the 400th anniversary of Bruno's death, in 2000, Cardinal Angelo Sodano declared Bruno's death to be a "sad episode" but, despite his regret, he defended Bruno's prosecutors, maintaining that the Inquisitors "had the desire to serve freedom and promote the common good and did everything possible to save his life". In the same year, Pope John Paul II made a general apology for "the use of violence that some have committed in the service of truth".

Some authors have characterized Bruno as a "martyr of science," suggesting parallels with the Galileo affair which began around 1610. They assert that, even though Bruno's theological beliefs, or perceptions of them by others, were an important factor in his heresy trial, his Copernicanism and cosmological beliefs played a significant role in the outcome.

"It should not be supposed", writes A. M. Paterson of Bruno and his "heliocentric solar system," that he "reached his conclusions via some mystical revelation...His work is an essential part of the scientific and philosophical developments that he initiated." Paterson echoes Hegel in writing that Bruno "ushers in a modern theory of knowledge that understands all natural things in the universe to be known by the human mind through the mind's dialectical structure".

Ingegno writes that Bruno embraced the philosophy of Lucretius, "aimed at liberating man from the fear of death and the gods." Characters in Bruno's "Cause, Principle and Unity" desire "to improve speculative science and knowledge of natural things," and to achieve a philosophy "which brings about the perfection of the human intellect most easily and eminently, and most closely corresponds to the truth of nature."

Other scholars oppose such views, and claim Bruno's martyrdom to science to be exaggerated, or outright false. For Yates, while "nineteenth century liberals" were thrown "into ecstasies" over Bruno's Copernicanism, "Bruno pushes Copernicus' scientific work back into a prescientific stage, back into Hermetism, interpreting the Copernican diagram as a hieroglyph of divine mysteries."

According to historian Mordechai Feingold, "Both admirers and critics of Giordano Bruno basically agree that he was pompous and arrogant, highly valuing his opinions and showing little patience with anyone who even mildly disagreed with him." Discussing Bruno's experience of rejection when he visited Oxford University, Feingold suggests that "it might have been Bruno's manner, his language and his self-assertiveness, rather than his ideas" that caused offence.

In his "Lectures on the History of Philosophy" Hegel writes that Bruno's life represented "a bold rejection of all Catholic beliefs resting on mere authority."

Alfonso Ingegno states that Bruno's philosophy "challenges the developments of the Reformation, calls into question the truth-value of the whole of Christianity, and claims that Christ perpetrated a deceit on mankind... Bruno suggests that we can now recognize the universal law which controls the perpetual becoming of all things in an infinite universe." A. M. Paterson says that, while we no longer have a copy of the official papal condemnation of Bruno, his heresies included "the doctrine of the infinite universe and the innumerable worlds" and his beliefs "on the movement of the earth".

Michael White notes that the Inquisition may have pursued Bruno early in his life on the basis of his opposition to Aristotle, interest in Arianism, reading of Erasmus, and possession of banned texts. White considers that Bruno's later heresy was "multifaceted" and may have rested on his conception of infinite worlds. "This was perhaps the most dangerous notion of all... If other worlds existed with intelligent beings living there, did they too have their visitations? The idea was quite unthinkable."

Frances Yates rejects what she describes as the "legend that Bruno was prosecuted as a philosophical thinker, was burned for his daring views on innumerable worlds or on the movement of the earth." Yates however writes that "the Church was... perfectly within its rights if it included philosophical points in its condemnation of Bruno's heresies" because "the philosophical points were quite inseparable from the heresies."

According to the "Stanford Encyclopedia of Philosophy", "in 1600 there was no official Catholic position on the Copernican system, and it was certainly not a heresy. When [...] Bruno [...] was burned at the stake as a heretic, it had nothing to do with his writings in support of Copernican cosmology."

The website of the Vatican Secret Archives, discussing a summary of legal proceedings against Bruno in Rome, states: "In the same rooms where Giordano Bruno was questioned, for the same important reasons of the relationship between science and faith, at the dawning of the new astronomy and at the decline of Aristotle's philosophy, sixteen years later, Cardinal Bellarmino, who then contested Bruno's heretical theses, summoned Galileo Galilei, who also faced a famous inquisitorial trial, which, luckily for him, ended with a simple abjuration."

Following the 1870 Capture of Rome by the newly created Kingdom of Italy and the end of the Church's temporal power over the city, the erection of a monument to Bruno on the site of his execution became feasible. The monument was sharply opposed by the clerical party, but was finally erected by the Rome Municipality and inaugurated in 1889.

A statue of a stretched human figure standing on its head, designed by Alexander Polzin and depicting Bruno's death at the stake, was placed in Potsdamer Platz station in Berlin on 2 March 2008.
Retrospective iconography of Bruno shows him with a Dominican cowl but not tonsured. Edward Gosselin has suggested that it is likely Bruno kept his tonsure at least until 1579, and it is possible that he wore it again thereafter.

An idealized animated version of Bruno appears in the first episode of the 2014 television series "". In this depiction, Bruno is shown with a more modern look, without tonsure and wearing clerical robes and without his hood. "Cosmos" presents Bruno as an impoverished philosopher who was ultimately executed due to his refusal to recant his belief in other worlds, a portrayal that was criticized by some as simplistic or historically inaccurate.

The 2016 song "Roman Sky" by hard rock band Avenged Sevenfold focuses on the death of Bruno.

Also the song ""Anima Mundi"" by Massimiliano Larocca and the album "Numen Lumen" by neofolk group Hautville, which tracks Bruno's lyrics, were dedicated to the philosopher.

Algernon Charles Swinburne wrote a poem honouring Giordano Bruno in 1889, when the statue of Bruno was constructed in Rome.

Czeslaw Milosz evokes the story and image of Giordano Bruno in his poem "Campo Dei Fiori" (Warsaw 1943).

Heather McHugh depicted Bruno as the principal of a story told (at dinner, by an "underestimated" travel guide) to a group of contemporary American poets in Rome. The poem (originally published in McHugh's collection of poems "Hinge & Sign", nominee for the National Book Award, and subsequently reprinted widely) channels the very question of ars poetica, or meta-meaning itself, through the embedded narrative of the suppression of Bruno's words, silenced towards the end of his life both literally and literarily.

Bruno and his theory of "the coincidence of contraries" ("coincidentia oppositorum") play an important role in James Joyce's novel "Finnegans Wake". Joyce wrote in a letter to his patroness, Harriet Shaw Weaver, "His philosophy is a kind of dualism – every power in nature must evolve an opposite in order to realise itself and opposition brings reunion". Amongst his numerous allusions to Bruno in his novel, including his trial and torture, Joyce plays upon Bruno's notion of "coincidentia oppositorum" through applying his name to word puns such as "Browne and Nolan" (the name of Dublin printers) and '"brownesberrow in nolandsland".

Giordano Bruno features as the hero in a series of historical crime novels by S.J. Parris (a pseudonym of Stephanie Merritt). In order these are "Heresy", "Prophecy", "Sacrilege", "Treachery" and "Conspiracy".

Bertold Brecht wrote one of his "Calendar Stories" (Kalendergeschichten) on Bruno Giordano. In "The heretic's coat" ("Der Mantel des Ketzers"), Brecht extols Bruno's unwavering honesty and selfless concern for justice.

"The Last Confession" by Morris West (posthumously published) is a fictional autobiography of Bruno, ostensibly written shortly before his execution.

In 1973 the biographical drama "Giordano Bruno" was released, an Italian/French movie directed by Giuliano Montaldo, starring Gian Maria Volontè as Bruno.

The computer game "In Memoriam" features a lead character who claims to be Bruno, returned from the dead to seek vengeance.

Bruno features as a main character in the historical segments of John Crowley's mystical Ægypt tetralogy of novels. The story covers his education as a Dominican and his investigation for heresy, and presents multiple versions of his execution on the Campo de' Fiori.

Bruno plays a small but significant role in Martin Seay's 2016 novel "The Mirror Thief".

Bruno is referenced in Natasha Mostert's "Season of the Witch."

His name appears and he is recognized in several novels, including
He is cited and quoted in Pauline Hunter Blair's last adult novel, "Jacob's Ladder" (Church Farmhouse Books, Bottisham, 2003).

The Giordano Bruno Foundation (German: Giordano-Bruno-Stiftung) is a non-profit foundation based in Germany that pursues the "Support of Evolutionary Humanism". It was founded by entrepreneur Herbert Steffen in 2004. The Giordano Bruno Foundation is considered critical of religion, which it characterizes as detrimental to cultural evolution.

The SETI League makes an annual award honoring the memory of Giordano Bruno to a deserving person or persons who have made a significant contribution to the practice of SETI (the search for extraterrestrial intelligence). The award was proposed by sociologist Donald Tarter in 1995 on the 395th anniversary of Bruno's death. The trophy presented is called a Bruno.

The 22 km impact crater Giordano Bruno on the far side of the Moon is named in his honor, as are the main belt Asteroids 5148 Giordano and 13223 Cenaceneri; the latter is named after his philosophical dialogue "La Cena de le Ceneri" ("The Ash Wednesday Supper") (see above).

Broadcasting station 2GB in Sydney, Australia is named for Bruno. The two letters "GB" in the call sign were chosen to honour Bruno, who was much admired by Theosophists who were the original holders of the station's licence.






</doc>
<doc id="12964" url="https://en.wikipedia.org/wiki?curid=12964" title="Geddy Lee">
Geddy Lee

Geddy Lee Weinrib, (born Gary Lee Weinrib; July 29, 1953), known professionally as Geddy Lee, is a Canadian musician, singer, and songwriter best known as the lead vocalist, bassist, and keyboardist for the Canadian rock group Rush. Lee joined what would become Rush in September 1968, at the request of his childhood friend Alex Lifeson, replacing original bassist and frontman Jeff Jones. Lee's first solo effort, "My Favourite Headache", was released in 2000.

An award-winning musician, Lee's style, technique, and skill on the bass guitar have inspired many rock musicians such as Cliff Burton of Metallica, Steve Harris of Iron Maiden, John Myung of Dream Theater, and Les Claypool of Primus. Along with his Rush bandmates – guitarist Alex Lifeson and drummer Neil Peart – Lee was made an Officer of the Order of Canada on May 9, 1996. The trio was the first rock band to be so honoured, as a group. In 2013, the group was inducted into the Rock & Roll Hall of Fame after 14 years of eligibility; they were nominated overwhelmingly in the Hall's first selection via fan ballot. Lee is ranked 13th by "Hit Parader" on their list of the 100 Greatest Heavy Metal Vocalists of All Time.

Lee was born on July 29, 1953 in Willowdale, (North York) Toronto, Ontario, to Morris and Mary Weinrib (née Manya Rubenstein). His parents were Jewish Holocaust survivors from Poland who had survived the ghetto in their hometown Starachowice, followed by their imprisonments at Dachau and Bergen-Belsen concentration camps, during the Holocaust and World War II. They were about 13 years old when they were initially imprisoned at Auschwitz concentration camp, close to the same age as Anne Frank at that time. "It was kind of surreal pre-teen shit," says Lee, describing how his father bribed guards to bring his mother shoes. After a period, his mother was transferred to Bergen-Belsen and his father to Dachau. When the war ended four years later and the Allies liberated the camps, his father set out in search of his mother and found her at a displaced persons camp. They married there and eventually emigrated to Canada.

In Canada, Lee's parents gave him a Jewish education, with a bar mitzvah at age 13. His father was a skilled musician, but died the year before from medical problems resulting from his imprisonment. This forced his mother to find outside work to support three children. Lee feels that not having parents at home during those years was probably a factor in his becoming a musician: "It was a terrible blow that I lost him, but the course of my life changed because my mother couldn't control us." He said that losing his father at such an early age made him aware of how "quickly life can disappear", which inspired him from then on to get the most out of his life and music.

He turned his basement into practice space for a band he formed with high-school friends. After the band began earning income from small performances at high-school shows or other events, he decided to drop out of high school and play rock and roll professionally. His mother was devastated when he told her, and he still feels that he owes her for the disappointments in her life. "All the shit I put her through," he says, "on top of the fact that she just lost her husband. I felt like I had to make sure that it was worth it. I wanted to show her that I was a professional, that I was working hard, and wasn't just a fuckin' lunatic."

Today, Lee considers himself a cultural Jew. "Jweekly" featured Lee's reflections on his mother's experiences as a refugee, and of his own Jewish heritage. Lee's name, "Geddy", was derived from his mother's heavily accented pronunciation of his given first name, "Gary". This was picked up by his friends in school, leading Lee to adopt it as his stage name and later his legal name.

After Rush had become a widely recognized rock group, Lee told the story about his mother's early life to the group's drummer and lyricist, Neil Peart, who then wrote the lyrics to "Red Sector A", inspired by her ordeal. The song, for which Lee wrote the music, was released on the band's 1984 album "Grace Under Pressure". The lyrics include the following verse:

Lee began playing music in school when he was 10 or 11, and got his first acoustic guitar at 14. In school, he first played drums, trumpet and clarinet. However, learning to play instruments in school wasn't satisfying to Lee, and he took basic piano lessons on his own. His interest increased dramatically after listening to some of the popular rock groups at the time. His early influences included Jack Bruce of Cream, John Entwistle of The Who, Jeff Beck, and Procol Harum. "I was mainly interested in early British progressive rock," said Lee. "That's how I learned to play bass, emulating Jack Bruce and people like that." Bruce's style of music was also noticed by Lee, who liked that "his sound was distinctive - it wasn't boring." Lee has also been influenced by Paul McCartney, Chris Squire, and James Jamerson.

Beginning in 1969, Rush began playing professionally in coffeehouses, high school dances and at various outdoor recreational events. By 1971, they were now playing mostly original songs in small clubs and bars, including Toronto's Gasworks and Abbey Road Pub. Lee describes the group during these early years as being "weekend warriors", holding down jobs during the weekdays and playing music on weekends: "We longed to break out of the boring surrounding of the suburbs and the endless similarities . . . the shopping plazas and all that stuff. . . the music was a vehicle for us to speak out." He claims that in the beginning they were simply "a straightforward rock band."

Short of money, they began opening concerts at venues such as Toronto's Victory Burlesque Theatre for the punk band, New York Dolls. By 1972 Rush began performing full-length concerts, consisting mostly of original songs, in cities including Toronto and Detroit. As they gained more recognition, they began performing as an opening act for groups such as Aerosmith, Kiss, and Blue Öyster Cult.

Like Cream, Rush followed the model of a "power trio", with Lee both playing bass and singing. Lee's vocals produced a distinctive, "countertenor" falsetto, and resonant sound. Lee possessed a three-octave vocal range, from baritone through tenor, alto, and mezzo-soprano pitch ranges, although it has significantly decreased with age. Lee's playing style is widely regarded for his use of high treble and very hard playing of the strings, and for utilizing the bass as a lead instrument, often contrapuntal to Lifeson's guitar. In the 1970s and early 1980s, Lee mostly used a Rickenbacker 4001 bass, with a very noticeable grit in his tone. During the band's "synth era" in the mid-1980s, Lee used Steinberger and later Wal basses, with the latter having more of a "jazzy" tone, according to Lee. From 1993's "Counterparts" onward, Lee began using the Fender Jazz Bass almost exclusively, returning to his trademark high treble sound. Lee had first used the Jazz Bass during the recording of " Moving Pictures", on songs such as "Tom Sawyer."

After a number of early albums and increasing popularity, Rush's status as a rock group soared over the following five years as they consistently toured worldwide and produced successful albums, including "2112" (1976), "A Farewell to Kings" (1977), "Hemispheres (1978), "Permanent Waves" (1980), and ""Moving Pictures" (1981). The group's distinctiveness was enhanced when Lee began adding synthesizers in 1977, with the release of "A Farewell to Kings". The additional sounds expanded the group's "textual capabilities", states keyboard critic Greg Armbruster, and allowed the trio to produce an orchestrated and more complex progressive rock music style. It also gave Lee the ability to play bass at the same time, as he could control the synthesizer with foot pedals. In 1981, he won "Keyboard" magazine's poll as "Best New Talent." By the 1984 album "Grace Under Pressure", Lee was surrounding himself with stacks of keyboards on stage.

By the 1980s, Rush had become one of the "biggest rock bands on the planet", selling out arena seats when touring. Lee was considered the most prominent member of the group, being the lead vocalist and known for his dynamic stage movements. According to music critic Tom Mulhern, writing in 1980, "it's dazzling to see so much sheer energy expended without a nervous breakdown." By 1996, with their Test for Echo Tour, they began performing without an opening act, their shows lasting nearly three hours.

Music industry writer Christopher Buttner, who interviewed Lee in 1996, described him as a prodigy and "role model" for what every musician wants to be, noting his proficiency on stage. Buttner cited Lee's ability to vary time signatures, play multiple keyboards, use bass pedal controllers and control sequencers, all while singing lead vocals into as many as three microphones. Buttner adds that few musicians of any instrument "can juggle half of what Geddy can do without literally falling on their ass." As a result, notes Mulhern, Lee's instrumentation was the "pulse" of the group and created a "one-man rhythm section", which complemented guitarist Alex Lifeson and percussionist Neil Peart. Bass instructor Allan Slutsky, or "Dr Licks", credits Lee's "biting, high-end bass lines and creative synthesizer work" for helping the group become "one of the most innovative" of all the supergroups that play arena rock. By 1989, "Guitar Player" magazine had already designated Lee the "Best Rock Bass" player from their reader's poll for the previous five years.

Bass players who have cited Lee as an influence include Cliff Burton of Metallica, Steve Harris of Iron Maiden, John Myung of Dream Theater, and Les Claypool of Primus.

"My Favourite Headache", Lee's first and to date only solo album, was released on November 14, 2000 while Rush was on a hiatus following the deaths of Peart's wife and daughter. Musicians associated with the project includes sometime Rush contributor Ben Mink, "Soundgarden" and "Pearl Jam" drummer Matt Cameron, and others.

The bulk of Lee's work in music has been with Rush (see Rush discography). However, Lee has also contributed to a body of work outside of his involvement with the band through guest appearances and album production. In 1981, Lee was the featured guest for the hit song "Take Off" and its included comedic commentary with Bob and Doug McKenzie (played by Rick Moranis and Dave Thomas, respectively) for the McKenzie Brothers' comedy album "Great White North". While Rush has had great success selling albums, "Take Off" is the highest charting single on the "Billboard" Hot 100 of Lee's career.

In 1982, Lee produced the first (and only) album from Toronto new wave band Boys Brigade. On the 1985 album "We Are the World", by humanitarian consortium USA for Africa, Lee recorded guest vocals for the song "Tears Are Not Enough". Lee sang "O Canada", the Canadian national anthem, at Baltimore's Camden Yards for the 1993 Major League Baseball All-Star Game.

Another version of "O Canada", with a rock arrangement, was recorded by Lee and Lifeson for the soundtrack of the 1999 film "".

Lee also plays bass on Canadian rock band I Mother Earth's track "Good For Sule", which is featured on the group's 1999 album "Blue Green Orange".

Lee was an interview subject in the documentary films "" and "", and has appeared in multiple episodes of the VH1 Classic series "Metal Evolution".

Along with his bandmates, Lee was a guest musician on the Max Webster song "Battle Scar", from the 1980 album "Universal Juveniles".

Lee appeared in Broken Social Scene's music video for their 2006 single "Fire Eye'd Boy", judging the band while they perform various musical tasks, and in 2006, Lee joined Lifeson's supergroup, the Big Dirty Band, to provide songs accompanying "".

In 2013, Lee made a brief cameo appearance as himself in the "How I Met Your Mother" season eight episode "P.S. I Love You".

In 2017, Lee performed with Yes during the band's Rock and Roll Hall of Fame induction, playing bass for the song "Roundabout."

Lee married Nancy Young in 1976. They have a son, Julian, and a daughter, Kyla. He is an avid wine collector, with a collection of 5,000 bottles. He takes annual trips to France, where he indulges in cheese and fine wine. In 2011, a charitable foundation he supports, Grapes for Humanity, created the Geddy Lee Scholarship for students of winemaking at Niagara College.

He is also a longtime fan of baseball, with favourite teams including the Detroit Tigers, the Chicago Cubs, and the Toronto Blue Jays. In the 1980s, Lee began reading the works of Bill James, particularly "The Bill James Baseball Abstracts", which led to an interest in sabermetrics and participation in a fantasy baseball keeper league. He collects baseball memorabilia, once donating part of his collection to the Negro Leagues Baseball Museum, and threw the ceremonial first pitch to inaugurate the 2013 Toronto Blue Jays season. Lee sang the Canadian national anthem before the 1993 MLB All-Star Game. In 2016, Lee planned to produce an independent film about baseball in Italy.

Lee has described himself as a Jewish atheist, explaining to an interviewer, "I consider myself a Jew as a race, but not so much as a religion. I’m not down with religion at all. I’m a Jewish atheist, if that’s possible."

Lee has varied his equipment list continually throughout his career.

In 1998, Fender released the Geddy Lee Jazz Bass, available in Black and 3-Color Sunburst (as of 2009). This signature model is a recreation of Lee's favourite bass, a 1972 Fender Jazz that he bought in a pawn shop in Kalamazoo, Michigan. In 2015, Fender released a revised USA model of his signature bass.

During the 1970s, Lee's main instrument was a modified Rickenbacker 4001. He has also used a Fender Precision Bass, as well as Steinberger and Wal basses.

For Rush's 2010 tour, Lee used two Orange AD200 bass heads together with two OBC410 4x10 bass cabinets.

Over the years, Lee has used synthesizers from Oberheim (Eight-voice, OB-1, OB-X, OB-Xa), PPG (Wave 2.2 and 2.3), Roland (Jupiter 8, D-50, XV-5080, and Fantom X7), Moog (Minimoog, Taurus pedals, Little Phatty), and Yamaha (DX7, KX76). Lee used sequencers early in their development and has continued to use similar innovations as they have developed over the years. Lee has also made use of digital samplers. Combined, these electronic devices have supplied many memorable keyboard sounds, such as the "growl" in "Tom Sawyer" and the percussive melody in the chorus of "The Spirit of Radio."

Beginning with the 1993 album "Counterparts", Rush reduced most keyboard- and synthesizer-derived sounds in their compositions. This reached a peak on the 2002 album "Vapor Trails", Rush's first since 1975's "Caress of Steel" to not feature any keyboards or synthesizers. On the 2007 album "Snakes & Arrows", Lee sparingly adds a Mellotron and bass pedals. However, it does not mark a return to a keyboard-heavy sound for the band. Much like "Vapor Trails", the music is primarily recorded with multiple layers of guitars, bass, drums and percussion.

Newer advances in synthesizer and sampler technology have allowed Lee to store familiar sounds from his old synthesizers alongside new ones in combination synthesizer/samplers, such as the Roland XV-5080. For live shows in 2002 and 2004, Lee and his keyboard technician used the playback capabilities of the XV-5080 to generate virtually all of Rush's keyboard sounds to date, as well as additional complex sound passages that previously required several machines at once to produce.

When playing live, Lee and his bandmates recreate their songs as accurately as possible with digital samplers. Using these samplers, the band members are able to recreate, in real-time, the sounds of non-traditional instruments, accompaniments, vocal harmonies, and other sound "events" that are familiar to those who have heard Rush songs from their albums.

To trigger these sounds in real-time, Lee uses MIDI controllers, placed at the locations on the stage where he has a microphone stand. Lee uses two types of MIDI controllers: one type resembles a traditional synthesizer keyboard on a stand (Yamaha KX76). The second type is a large foot-pedal keyboard, placed on the stage floor (Korg MPK-130, Roland PK-5). Combined, they enable Lee to use his free hands and feet to trigger sounds in electronic equipment that has been placed off-stage. It is with this technology that Lee and his bandmates are able to present their arrangements in a live setting with the level of complexity and fidelity that fans have come to expect, and without the need to resort to the use of backing tracks or employing an additional band member. A notable exception of this was during the "Clockwork Angels Tour", when a string ensemble played string parts, which were originally arranged and conducted by David Campbell on "Clockwork Angels".

Lee's (and his bandmates') use of MIDI controllers to trigger sampled instruments and audio events is visible throughout the "" concert DVD (2005).

From the "Snakes and Arrows" tour onwards, Lee used a Roland Fantom X7 and a Moog Little Phatty synthesizer.

Since 1996, Lee no longer uses traditional bass amplifiers on stage, opting to have the bass guitar signals input directly to the touring front-of-house console, to improve control and balance of sound reinforcement. Faced with the dilemma of what to do with the empty space left behind by the lack of large amplifier cabinets, Lee chose to decorate his side of the stage with unusual items.

For the 1996–1997 "Test for Echo Tour", Lee's side sported a fully stocked old-fashioned household refrigerator. For the 2002 "Vapor Trails" tour, Lee lined his side of the stage with three coin-operated Maytag dryers. Other large appliances appeared later in the same space. For visual effect they were "miked" by the sound crew, just as a real amplifier would be. Rush's crew loaded the dryers with specially-designed Rush-themed T-shirts, different from the shirts on sale to the general public. At the close of each show, Lee and Lifeson tossed these T-shirts into the audience. The dryers can be seen while watching the "Rush in Rio" DVD, the "R40" DVD, and the "R30: 30th Anniversary World Tour" DVD. For the band's "" tour, one of the three dryers was replaced with a rotating shelf-style vending machine. It too was fully stocked and operational during shows. For the "R40" tour, there were four dryers, as opposed to the usual three dryers.

The "Snakes & Arrows Tour" prominently featured three Henhouse brand rotisserie chicken ovens on stage complete with an attendant in a chef's hat and apron to "tend" the chickens during shows. For the 2010–2011 Time Machine Tour, Lee's side of the stage featured a steampunk-inspired combination Time Machine and Sausage Maker, with an attendant occasionally throwing material into its feed hopper during the show. During the 2012-2013 Clockwork Angels Tour, Lee used a different steampunk device called a "Geddison" as a backdrop. This was composed of a giant old-style phonograph horn, an oversized model brain in a jar, a set of brass horns, and a working popcorn popper. The 2015 R40 tour combined several of these elements together, with the exception of the chicken ovens used on the "Snakes and Arrows" tour.




</doc>
<doc id="12967" url="https://en.wikipedia.org/wiki?curid=12967" title="Geologic time scale">
Geologic time scale

The geologic time scale (GTS) is a system of chronological dating that relates geological strata (stratigraphy) to time. It is used by geologists, paleontologists, and other Earth scientists to describe the timing and relationships of events that have occurred during Earth's history. The table of geologic time spans, presented here, agree with the nomenclature, dates and standard color codes set forth by the International Commission on Stratigraphy (ICS).

The primary defined divisions of time are eons, in sequence the Hadean, the Archean, the Proterozoic and the Phanerozoic.
The first three of these can be referred to collectively as the Precambrian supereon. 
Eons are divided into eras, which are in turn divided into periods, epochs and ages. 

Corresponding to eons, eras, periods, epochs and ages, the terms "eonothem", "erathem", "system", "series", "stage" are used to refer to the layers of rock that belong to these stretches of geologic time in Earth's history.

Geologists qualify these units as "early", "mid", and "late" when referring to time, and "lower", "middle", and "upper" when referring to the corresponding rocks. For example, the lower Jurassic Series in chronostratigraphy corresponds to the early Jurassic Epoch in geochronology. The adjectives are capitalized when the subdivision is formally recognized, and lower case when not; thus "early Miocene" but "Early Jurassic."

Evidence from radiometric dating indicates that Earth is about 4.54 billion years old. The geology or "deep time" of Earth's past has been organized into various units according to events which took place. Different spans of time on the GTS are usually marked by corresponding changes in the composition of strata which indicate major geological or paleontological events, such as mass extinctions. For example, the boundary between the Cretaceous period and the Paleogene period is defined by the Cretaceous–Paleogene extinction event, which marked the demise of the non-avian dinosaurs and many other groups of life. Older time spans, which predate the reliable fossil record (before the Proterozoic eon), are defined by their absolute age.

Geologic units from the same time but different parts of the world often look different and contain different fossils, so the same time-span was historically given different names in different locales. For example, in North America, the Lower Cambrian is called the Waucoban series that is then subdivided into zones based on succession of trilobites. In East Asia and Siberia, the same unit is split into Alexian, Atdabanian, and Botomian stages. A key aspect of the work of the International Commission on Stratigraphy is to reconcile this conflicting terminology and define universal horizons that can be used around the world.

Some other planets and moons in the Solar System have sufficiently rigid structures to have preserved records of their own histories, for example, Venus, Mars and the Earth's Moon. Dominantly fluid planets, such as the gas giants, do not preserve their history in a comparable manner. Apart from the Late Heavy Bombardment, events on other planets probably had little direct influence on the Earth, and events on Earth had correspondingly little effect on those planets. Construction of a time scale that links the planets is, therefore, of only limited relevance to the Earth's time scale, except in a Solar System context. The existence, timing, and terrestrial effects of the Late Heavy Bombardment are still debated.

In Ancient Greece, Aristotle (384-322 BCE) observed that fossils of seashells in rocks resembled those found on beaches – he inferred that the fossils in rocks were formed by living animals, and he reasoned that the positions of land and sea had changed over long periods of time. Leonardo da Vinci (1452–1519) concurred with Aristotle's interpretation that fossils represented the remains of ancient life.

The 11th-century Persian geologist Avicenna (Ibn Sina, died 1037) and the 13th-century Dominican bishop Albertus Magnus (died 1280) extended Aristotle's explanation into a theory of a petrifying fluid. Avicenna also first proposed one of the principles underlying geologic time scales, the law of superposition of strata, while discussing the origins of mountains in "The Book of Healing" (1027). The Chinese naturalist Shen Kuo (1031–1095) also recognized the concept of "deep time".

In the late 17th century Nicholas Steno (1638–1686) pronounced the principles underlying geologic (geological) time scales. Steno argued that rock layers (or strata) were laid down in succession, and that each represents a "slice" of time. He also formulated the law of superposition, which states that any given stratum is probably older than those above it and younger than those below it. While Steno's principles were simple, applying them proved challenging. Over the course of the 18th century geologists realized that:

The Neptunist theories popular at this time (expounded by Abraham Werner (1749–1817) in the late 18th century) proposed that all rocks had precipitated out of a single enormous flood. A major shift in thinking came when James Hutton presented his "Theory of the Earth; or, an Investigation of the Laws Observable in the Composition, Dissolution, and Restoration of Land Upon the Globe"
before the Royal Society of Edinburgh in March and April 1785. John McPhee asserts that "as things appear from the perspective of the 20th century, James Hutton in those readings became the founder of modern geology".
Hutton proposed that the interior of Earth was hot, and that this heat was the engine which drove the creation of new rock: land was eroded by air and water and deposited as layers in the sea; heat then consolidated the sediment into stone, and uplifted it into new lands. This theory, known as "Plutonism", stood in contrast to the "Neptunist" flood-oriented theory.

The first serious attempts to formulate a geologic time scale that could be applied anywhere on Earth were made in the late 18th century. The most influential of those early attempts (championed by Werner, among others) divided the rocks of Earth's crust into four types: Primary, Secondary, Tertiary, and Quaternary. Each type of rock, according to the theory, formed during a specific period in Earth history. It was thus possible to speak of a "Tertiary Period" as well as of "Tertiary Rocks." Indeed, "Tertiary" (now Paleogene and Neogene) remained in use as the name of a geological period well into the 20th century and "Quaternary" remains in formal use as the name of the current period.

The identification of strata by the fossils they contained, pioneered by William Smith, Georges Cuvier, Jean d'Omalius d'Halloy, and Alexandre Brongniart in the early 19th century, enabled geologists to divide Earth history more precisely. It also enabled them to correlate strata across national (or even continental) boundaries. If two strata (however distant in space or different in composition) contained the same fossils, chances were good that they had been laid down at the same time. Detailed studies between 1820 and 1850 of the strata and fossils of Europe produced the sequence of geologic periods still used today.

Early work on developing the geologic time scale was dominated by British geologists, and the names of the geologic periods reflect that dominance. The "Cambrian", (the classical name for Wales) and the "Ordovician", and "Silurian", named after ancient Welsh tribes, were periods defined using stratigraphic sequences from Wales. The "Devonian" was named for the English county of Devon, and the name "Carboniferous" was an adaptation of "the Coal Measures", the old British geologists’ term for the same set of strata. The "Permian" was named after Perm, Russia, because it was defined using strata in that region by Scottish geologist Roderick Murchison. However, some periods were defined by geologists from other countries. The "Triassic" was named in 1834 by a German geologist Friedrich Von Alberti from the three distinct layers (Latin meaning triad)—red beds, capped by chalk, followed by black shales—that are found throughout Germany and Northwest Europe, called the ‘Trias’. The "Jurassic" was named by a French geologist Alexandre Brongniart for the extensive marine limestone exposures of the Jura Mountains. The "Cretaceous" (from Latin "creta" meaning ‘chalk’) as a separate period was first defined by Belgian geologist Jean d'Omalius d'Halloy in 1822, using strata in the Paris basin and named for the extensive beds of chalk (calcium carbonate deposited by the shells of marine invertebrates) found in Western Europe.

British geologists were also responsible for the grouping of periods into eras and the subdivision of the Tertiary and Quaternary periods into epochs. In 1841 John Phillips published the first global geologic time scale based on the types of fossils found in each era. Phillips’ scale helped standardize the use of terms like "Paleozoic" ("old life") which he extended to cover a larger period than it had in previous usage, and "Mesozoic" ("middle life") which he invented.

When William Smith and Sir Charles Lyell first recognized that rock strata represented successive time periods, time scales could be estimated only very imprecisely since estimates of rates of change were uncertain. While creationists had been proposing dates of around six or seven thousand years for the age of Earth based on the Bible, early geologists were suggesting millions of years for geologic periods, and some were even suggesting a virtually infinite age for Earth. Geologists and paleontologists constructed the geologic table based on the relative positions of different strata and fossils, and estimated the time scales based on studying rates of various kinds of weathering, erosion, sedimentation, and lithification. Until the discovery of radioactivity in 1896 and the development of its geological applications through radiometric dating during the first half of the 20th century, the ages of various rock strata and the age of Earth were the subject of considerable debate.

The first geologic time scale that included absolute dates was published in 1913 by the British geologist Arthur Holmes. He greatly furthered the newly created discipline of geochronology and published the world-renowned book "The Age of the Earth" in which he estimated Earth's age to be at least 1.6 billion years.

In 1977, the "Global Commission on Stratigraphy" (now the International Commission on Stratigraphy) began to define global references known as GSSP (Global Boundary Stratotype Sections and Points) for geologic periods and faunal stages. The commission's most recent work is described in the 2004 geologic time scale of Gradstein et al. A UML model for how the timescale is structured, relating it to the GSSP, is also available.

Popular culture and a growing number of scientists use the term "Anthropocene" informally to label the current epoch in which we are living. The term was coined by Paul Crutzen and Eugene Stoermer in 2000 to describe the current time, in which humans have had an enormous impact on the environment. It has evolved to describe an "epoch" starting some time in the past and on the whole defined by anthropogenic carbon emissions and production and consumption of plastic goods that are left in the ground.

Critics of this term say that the term should not be used because it is difficult, if not nearly impossible, to define a specific time when humans started influencing the rock strata—defining the start of an epoch. Others say that humans have not even started to leave their biggest impact on Earth, and therefore the Anthropocene has not even started yet.

The ICS has not officially approved the term . The Anthropocene Working Group met in Oslo in April 2016 to consolidate evidence supporting the argument for the Anthropocene as a true geologic epoch. Evidence was evaluated and the group voted to recommend "Anthropocene" as the new geological age in August 2016.
Should the International Commission on Stratigraphy approve the recommendation, the proposal to adopt the term will have to be ratified by the International Union of Geological Sciences before its formal adoption as part of the geologic time scale.

The following table summarizes the major events and characteristics of the periods of time making up the geologic time scale. This table is arranged with the most recent geologic periods at the top, and the most ancient at the bottom. The height of each table entry does not correspond to the duration of each subdivision of time.

The content of the table is based on the current official geologic time scale of the International Commission on Stratigraphy, with the epoch names altered to the early/late format from lower/upper as recommended by the ICS when dealing with chronostratigraphy.

A service providing a Resource Description Framework/Web Ontology Language representation of the timescale is available through the Commission for the Management and Application of Geoscience Information GeoSciML project as a service and at a SPARQL end-point.

The ICS's "Geologic Time Scale 2012" book which includes the new approved time scale also displays a proposal to substantially revise the Precambrian time scale to reflect important events such as the formation of the Earth or the Great Oxidation Event, among others, while at the same time maintaining most of the previous chronostratigraphic nomenclature for the pertinent time span. (See also Period (geology)#Structure.)


Shown to scale:

Compare with the current official timeline, not shown to scale:





</doc>
<doc id="12969" url="https://en.wikipedia.org/wiki?curid=12969" title="Giovanni Arduino">
Giovanni Arduino

Giovanni Arduino is the name of:


</doc>
<doc id="12970" url="https://en.wikipedia.org/wiki?curid=12970" title="Gambler's fallacy">
Gambler's fallacy

The gambler's fallacy, also known as the Monte Carlo fallacy or the fallacy of the maturity of chances, is the mistaken belief that, if something happens more frequently than normal during a given period, it will happen less frequently in the future. It may also be stated as the belief that, if something happens less frequently than normal during a given period, it will happen more frequently in the future. In situations where the outcome being observed is truly random and consists of independent trials of a random process, this belief is false. The fallacy can arise in many situations, but is most strongly associated with gambling, where it is common among players.

The term "Monte Carlo fallacy" originates from the best known example of the phenomenon, which occurred in the Monte Carlo Casino in 1913.

The gambler's fallacy can be illustrated by considering the repeated toss of a fair coin. The outcomes in different tosses are statistically independent and the probability of getting heads on a single toss is (one in two). The probability of getting two heads in two tosses is (one in four) and the probability of getting three heads in three tosses is (one in eight). In general, if "A" is the event where toss "i" of a fair coin comes up heads, then:

If after tossing four heads in a row, the next coin toss also came up heads, it would complete a run of five successive heads. Since the probability of a run of five successive heads is (one in thirty-two), a person might believe that the next flip would be more likely to come up tails rather than heads again. This is incorrect and is an example of the gambler's fallacy. The event "5 heads in a row" and the event "first 4 heads, then a tails" are equally likely, each having probability . Since the first four tosses turn up heads, the probability that the next toss is a head is:

While a run of five heads has a probability of = 0.03125 (a little over 3%), the misunderstanding lies in not realizing that this is the case only before the first coin is tossed. After the first four tosses, the results are no longer unknown, so their probabilities are at that point equal to 1 (100%). The reasoning that it is more likely that a fifth toss is more likely to be tails because the previous four tosses were heads, with a run of luck in the past influencing the odds in the future, forms the basis of the fallacy.

If a fair coin is flipped 21 times, the probability of 21 heads is 1 in 2,097,152. The probability of flipping a head after having already flipped 20 heads in a row is . This is an application of Bayes' theorem.

This can also be shown without knowing that 20 heads have occurred, and without applying Bayes' theorem. Assuming a fair coin:

The probability of getting 20 heads then 1 tail, and the probability of getting 20 heads then another head are both 1 in 2,097,152. When flipping a fair coin 21 times, the outcome is equally likely to be 21 heads as 20 heads and then 1 tail. These two outcomes are equally as likely as any of the other combinations that can be obtained from 21 flips of a coin. All of the 21-flip combinations will have probabilities equal to 0.5, or 1 in 2,097,152. Assuming that a change in the probability will occur as a result of the outcome of prior flips is incorrect because every outcome of a 21-flip sequence is as likely as the other outcomes. In accordance with Bayes' theorem, the likely outcome of each flip is the probability of the fair coin, which is .

The fallacy leads to the incorrect notion that previous failures will create an increased probability of success on subsequent attempts. For a fair 16-sided die, the probability of each outcome occurring is (6.25%). If a win is defined as rolling a 1, the probability a 1 occurring at least once in 16 rolls is:

The probability of a loss on the first roll is (93.75%). According to the fallacy, the player should have a higher chance of winning after one loss has occurred. The probability of at least one win is now:

By losing one toss, the player's probability of winning drops by two percentage points. With 5 losses and 11 rolls remaining, the probability of winning drops to around 50%. The probability of at least one win does not increase after a series of losses. Instead, the probability of success decreases because there are fewer trials left in which to win. The probability of winning will eventually equal the probability of winning a single toss, which is (6.25%) and occurs when only one toss is left.

After a consistent tendency towards tails, a gambler may also decide that tails has become a more likely outcome. Believing the odds to favor tails, the gambler sees no reason to change to heads. The fallacy is the belief that a sequence of trials carries a memory of past results which tend to favor or disfavor future outcomes. It is not necessarily a fallacy as a consistent observed tendency towards one outcome may be taken as evidence that the coin is not fair.

The inverse gambler's fallacy described by Ian Hacking is a situation where a gambler entering a room and seeing a person rolling a double six on a pair of dice may erroneously conclude that the person must have been rolling the dice for quite a while, as they would be unlikely to get a double six on their first attempt.

Researchers have examined whether a similar bias exists for inferences about unknown past events based upon known subsequent events, calling this the "retrospective gambler's fallacy".

An example of a retrospective gambler's fallacy would be to observe multiple successive "heads" on a coin toss and conclude from this that the previously unknown flip was "tails". Real world examples of retrospective gambler's fallacy have been argued to exist in events such as the origin of the Universe. In his book "Universes", John Leslie argues that "the presence of vastly many universes very different in their characters might be our best explanation for why at least one universe has a life-permitting character". Daniel M. Oppenheimer and Benoît Monin argue that "In other words, the 'best explanation' for a low-probability event is that it is only one in a multiple of trials, which is the core intuition of the reverse gambler's fallacy." Philosophical arguments are ongoing about whether such arguments are or are not a fallacy, arguing that the occurrence of our universe says nothing about the existence of other universes or trials of universes. Three studies involving Stanford University students tested the existence of a retrospective gamblers' fallacy. All three studies concluded that people have a gamblers' fallacy retrospectively as well as to future events. The authors of all three studies concluded their findings have significant "methodological implications" but may also have "important theoretical implications" that need investigation and research, saying "[a] thorough understanding of such reasoning processes requires that we not only examine how they influence our predictions of the future, but also our perceptions of the past."

In 1796, Pierre-Simon Laplace described in "A Philosophical Essay on Probabilities" the ways in which men calculated their probability of having sons: "I have seen men, ardently desirous of having a son, who could learn only with anxiety of the births of boys in the month when they expected to become fathers. Imagining that the ratio of these births to those of girls ought to be the same at the end of each month, they judged that the boys already born would render more probable the births next of girls." The expectant fathers feared that if more sons were born in the surrounding community, then they themselves would be more likely to have a daughter. This essay by Laplace is regarded as one of the earliest descriptions of the fallacy.

After having multiple children of the same sex, some parents may believe that they are due to have a child of the opposite sex. While the Trivers–Willard hypothesis predicts that birth sex is dependent on living conditions, stating that more male children are born in good living conditions, while more female children are born in poorer living conditions, the probability of having a child of either sex is still regarded as near 50%.

Perhaps the most famous example of the gambler’s fallacy occurred in a game of roulette at the Monte Carlo Casino on August 18, 1913, when the ball fell in black 26 times in a row. This was an extremely uncommon occurrence, with a probability of around 1 in 136.8 million. Gamblers lost millions of francs betting against black, reasoning incorrectly that the streak was causing an imbalance in the randomness of the wheel, and that it had to be followed by a long streak of red.

The gambler's fallacy does not apply in situations where the probability of different events is not independent. Where this occurs, the probability of future events can change based on the outcome of past events, such as the statistical permutation of events. An example is when cards are drawn from a deck without replacement. If an ace is drawn from a deck and not reinserted, the next draw is less likely to be an ace and more likely to be of another rank. The probability of drawing another ace, assuming that it was the first card drawn and that there are no jokers, has decreased from (7.69%) to (5.88%), while the probability for each other rank has increased from (7.69%) to (7.84%). This effect allows card counting systems to work in games such as blackjack.

In most illustrations of the gambler's fallacy and the reverse gambler's fallacy, the trial (e.g. flipping a coin) is assumed to be fair. In practice, this assumption may not hold. For example, if a fair coin is flipped 21 times, the probability of 21 heads is 1 in 2,097,152. Since the probability of flipping 21 heads in a row is so small, it may well be that the coin is somehow biased towards landing on heads, or that it is being controlled by hidden magnets, or similar. In this case, the smart bet is "heads" because Bayesian inference from the empirical evidence — 21 heads in a row — suggests that the coin is likely to be biased toward heads. Bayesian inference can be used to show that when the long-run proportion of different outcomes are unknown but exchangeable (meaning that the random process from which they are generated may be biased but is equally likely to be biased in any direction) and that previous observations demonstrate the likely direction of the bias, the outcome which has occurred the most in the observed data is the most likely to occur again.

The opening scene of the play "Rosencrantz and Guildenstern Are Dead" by Tom Stoppard discusses these issues as one man continually flips heads and the other considers various possible explanations.

If external factors are allowed to change the probability of the events, the gambler's fallacy may not hold. For example, a change in the game rules might favour one player over the other, improving his or her win percentage. Similarly, an inexperienced player's success may decrease after opposing teams learn about and play against the weaknesses.

When statistics are quoted, they are usually made to sound as impressive as possible. If a politician says that unemployment has gone down for the past six years, it is a safe bet that seven years ago, it went up.

The gambler's fallacy arises out of a belief in a law of small numbers, leading to the erroneous belief that small samples must be representative of the larger population. According to the fallacy, streaks must eventually even out in order to be representative. Amos Tversky and Daniel Kahneman first proposed that the gambler's fallacy is a cognitive bias produced by a psychological heuristic called the representativeness heuristic, which states that people evaluate the probability of a certain event by assessing how similar it is to events they have experienced before, and how similar the events surrounding those two processes are. According to this view, "after observing a long run of red on the roulette wheel, for example, most people erroneously believe that black will result in a more representative sequence than the occurrence of an additional red", so people expect that a short run of random outcomes should share properties of a longer run, specifically in that deviations from average should balance out. When people are asked to make up a random-looking sequence of coin tosses, they tend to make sequences where the proportion of heads to tails stays closer to 0.5 in any short segment than would be predicted by chance, a phenomenon known as insensitivity to sample size. Kahneman and Tversky interpret this to mean that people believe short sequences of random events should be representative of longer ones. The representativeness heuristic is also cited behind the related phenomenon of the clustering illusion, according to which people see streaks of random events as being non-random when such streaks are actually much more likely to occur in small samples than people expect.

The gambler's fallacy can also be attributed to the mistaken belief that gambling, or even chance itself, is a fair process that can correct itself in the event of streaks, known as the just-world hypothesis. Other researchers believe that belief in the fallacy may be the result of a mistaken belief in an internal locus of control. When a person believes that gambling outcomes are the result of their own skill, they may be more susceptible to the gambler's fallacy because they reject the idea that chance could overcome skill or talent.

Some researchers believe that it is possible to define two types of gambler's fallacy: type one and type two. Type one is the classic gambler's fallacy, where individuals believe that a particular outcome is due after a long streak of another outcome. Type two gambler's fallacy, as defined by Gideon Keren and Charles Lewis, occurs when a gambler underestimates how many observations are needed to detect a favorable outcome, such as watching a roulette wheel for a length of time and then betting on the numbers that appear most often. For events with a high degree of randomness, detecting a bias that will lead to a favorable outcome takes an impractically large amount of time and is very difficult, if not impossible, to do. The two types differ in that type one wrongly assumes that gambling conditions are fair and perfect, while type two assumes that the conditions are biased, and that this bias can be detected after a certain amount of time.

Another variety, known as the retrospective gambler's fallacy, occurs when individuals judge that a seemingly rare event must come from a longer sequence than a more common event does. The belief that an imaginary sequence of die rolls is more than three times as long when a set of three sixes is observed as opposed to when there are only two sixes. This effect can be observed in isolated instances, or even sequentially. Another example would involve hearing that a teenager has unprotected sex and becomes pregnant on a given night, and that she has been engaging in unprotected sex for longer than if we hear she had unprotected sex but did not become pregnant, when the probability of becoming pregnant as a result of each intercourse is independent of the amount of prior intercourse.

Another psychological perspective states that gambler's fallacy can be seen as the counterpart to basketball's hot-hand fallacy, in which people tend to predict the same outcome as the previous event - known as positive recency - resulting in a belief that a high scorer will continue to score. In the gambler's fallacy, people predict the opposite outcome of the previous event - negative recency - believing that since the roulette wheel has landed on black on the previous six occasions, it is due to land on red the next. Ayton and Fischer have theorized that people display positive recency for the hot-hand fallacy because the fallacy deals with human performance, and that people do not believe that an inanimate object can become "hot." Human performance is not perceived as random, and people are more likely to continue streaks when they believe that the process generating the results is nonrandom. When a person exhibits the gambler's fallacy, they are more likely to exhibit the hot-hand fallacy as well, suggesting that one construct is responsible for the two fallacies.

The difference between the two fallacies is also found in economic decision-making. A study by Huber, Kirchler, and Stockl in 2010 examined how the hot hand and the gambler's fallacy are exhibited in the financial market. The researchers gave their participants a choice: they could either bet on the outcome of a series of coin tosses, use an expert opinion to sway their decision, or choose a risk-free alternative instead for a smaller financial reward. Participants turned to the expert opinion to make their decision 24% of the time based on their past experience of success, which exemplifies the hot-hand. If the expert was correct, 78% of the participants chose the expert's opinion again, as opposed to 57% doing so when the expert was wrong. The participants also exhibited the gambler's fallacy, with their selection of either heads or tails decreasing after noticing a streak of either outcome. This experiment helped bolster Ayton and Fischer's theory that people put more faith in human performance than they do in seemingly random processes.

While the representativeness heuristic and other cognitive biases are the most commonly cited cause of the gambler's fallacy, research suggests that there may also be a neurological component. Functional magnetic resonance imaging has shown that after losing a bet or gamble, known as riskloss, the frontoparietal network of the brain is activated, resulting in more risk-taking behavior. In contrast, there is decreased activity in the amygdala, caudate, and ventral striatum after a riskloss. Activation in the amygdala is negatively correlated with gambler's fallacy, so that the more activity exhibited in the amygdala, the less likely an individual is to fall prey to the gambler's fallacy. These results suggest that gambler's fallacy relies more on the prefrontal cortex, which is responsible for executive, goal-directed processes, and less on the brain areas that control affective decision-making.

The desire to continue gambling or betting is controlled by the striatum, which supports a choice-outcome contingency learning method. The striatum processes the errors in prediction and the behavior changes accordingly. After a win, the positive behavior is reinforced and after a loss, the behavior is conditioned to be avoided. In individuals exhibiting the gambler's fallacy, this choice-outcome contingency method is impaired, and they continue to make risks after a series of losses.

The gambler's fallacy is a deep-seated cognitive bias and can be very hard to overcome. Educating individuals about the nature of randomness has not always proven effective in reducing or eliminating any manifestation of the fallacy. Participants in a study by Beach and Swensson in 1967 were shown a shuffled deck of index cards with shapes on them, and were instructed to guess which shape would come next in a sequence. The experimental group of participants was informed about the nature and existence of the gambler's fallacy, and were explicitly instructed not to rely on run dependency to make their guesses. The control group was not given this information. The response styles of the two groups were similar, indicating that the experimental group still based their choices on the length of the run sequence. This led to the conclusion that instructing individuals about randomness is not sufficient in lessening the gambler's fallacy.

An individual's susceptibility to the gambler's fallacy may decrease with age. A study by Fischbein and Schnarch in 1997 administered a questionnaire to five groups: students in grades 5, 7, 9, 11, and college students specializing in teaching mathematics. None of the participants had received any prior education regarding probability. The question asked was: "Ronni flipped a coin three times and in all cases heads came up. Ronni intends to flip the coin again. What is the chance of getting heads the fourth time?" The results indicated that as the students got older, the less likely they were to answer with "smaller than the chance of getting tails", which would indicate a negative recency effect. 35% of the 5th graders, 35% of the 7th graders, and 20% of the 9th graders exhibited the negative recency effect. Only 10% of the 11th graders answered this way, and none of the college students did. Fischbein and Schnarch theorized that an individual's tendency to rely on the representativeness heuristic and other cognitive biases can be overcome with age.

Another possible solution comes from Roney and Trick, Gestalt psychologists who suggest that the fallacy may be eliminated as a result of grouping. When a future event such as a coin toss is described as part of a sequence, no matter how arbitrarily, a person will automatically consider the event as it relates to the past events, resulting in the gambler's fallacy. When a person considers every event as independent, the fallacy can be greatly reduced.

Roney and Trick told participants in their experiment that they were betting on either two blocks of six coin tosses, or on two blocks of seven coin tosses. The fourth, fifth, and sixth tosses all had the same outcome, either three heads or three tails. The seventh toss was grouped with either the end of one block, or the beginning of the next block. Participants exhibited the strongest gambler's fallacy when the seventh trial was part of the first block, directly after the sequence of three heads or tails. The researchers pointed out that the participants that did not show the gambler's fallacy showed less confidence in their bets and bet fewer times than the participants who picked with the gambler's fallacy. When the seventh trial was grouped with the second block, and was perceived as not being part of a streak, the gambler's fallacy did not occur.

Roney and Trick argued that instead of teaching individuals about the nature of randomness, the fallacy could be avoided by training people to treat each event as if it is a beginning and not a continuation of previous events. They suggested that this would prevent people from gambling when they are losing, in the mistaken hope that their chances of winning are due to increase based on an interaction with previous events.

Studies have found that asylum judges, loan officers, baseball umpires and lotto players employ the gambler's fallacy consistently in their decision-making.


</doc>
<doc id="12974" url="https://en.wikipedia.org/wiki?curid=12974" title="Gilbert Plains">
Gilbert Plains

Gilbert Plains is an unincorporated urban community in the Gilbert Plains Municipality within the Canadian province of Manitoba that held town status prior to January 1, 2015. It is located on Highway 5 and the CN railway line, between Dauphin and Grandview. It is approximately 250 miles northwest of Winnipeg. Gilbert Plains railway station receives Via Rail service. The local newspaper, The Exponent, services both Gilbert Plains and its neighboring town, Grandview.

Incorporated in 1906, the original townsite was some miles to the south. The community was named for Gilbert Ross, a Métis man who was living in the region when the first European settler, Glenlyon Campbell, arrived.

On January 1, 2015, the Town of Gilbert Plains relinquished its town status when it amalgamated with the Rural Municipality of Gilbert Plains to form the Gilbert Plains Municipality.

Gilbert Plains is situated on the Valley River, in the parkland country between Riding Mountain National Park and Duck Mountain Provincial Park.



</doc>
<doc id="12975" url="https://en.wikipedia.org/wiki?curid=12975" title="Gasparo Contarini">
Gasparo Contarini

Gasparo Contarini (16 October 1483 – 24 August 1542) was an Italian diplomat, cardinal and Bishop of Belluno. He was one of the first proponents of the dialogue with Protestants, after the Reformation.

He was born in Venice, the eldest son of Alvise Contarini, of the ancient noble House of Contarini, and his wife Polissena Malpiero. After a thorough scientific and philosophical training at the University of Padua, he began his career in the service of his native city. From September 1520 to August 25 he was the Republic's ambassador to Charles V, with whom Venice was soon at war, instructed to defend the Republic's alliance with Francis I of France. Though he participated at the Diet of Worms, April 1521, he never saw or spoke with Martin Luther. He accompanied Charles in the Netherlands and Spain.

He participated at the Congress of Ferrara in 1526 as the Republic's representative; at the Congress the League of Cognac was formed against the Emperor, allying France with Venice and several states of Italy. Later, after the Sack of Rome (1527), he assisted in reconciling the emperor with Clement VII, whose release he had obtained, and with the Republic of Bologna. Upon his return to Venice, he was made a senator and a member of the Great Council.

In 1535, Paul III unexpectedly made the secular diplomat a cardinal in order to bind an able man of evangelical disposition to the Roman interests. Contarini accepted, but in his new position did not exhibit his former independence. At the time he was promoted to cardinal, May 21, 1535, he was still a layman. However, already in October 1536 he was appointed Bishop of Belluno One of the fruits of his diplomatic activity is his "De magistratibus et republica Venetorum".

As Cardinal, Contarini figured among the most prominent of the "Spirituali", the leaders of the movement for reform within the Roman church. In April 1536 Paul III appointed a commission to devise ways for a reformation, with Contarini presiding. Paul III received favorably Contarini's "Consilium de Emendanda Ecclesia", which was circulated among the cardinalate, but it remained a dead letter. Contarini in a letter to his friend Cardinal Reginald Pole (dated 11 November 1538) says that his hopes had been wakened anew by the pope's attitude. He and his friends, who formed the Catholic evangelical movement of the Spirituali, thought that all would have been done when the abuses in church life had been put away. What Contarini had to do with it is shown by his letters to the pope in which he complained of the schism in the church, of simony and flattery in the papal court, but above all of papal tyranny, its least grateful passages. Paul's successor Paul IV, once a member on the commission, in 1539 put it on the "Index Librorum Prohibitorum".

In 1541 Cardinal Contarini was papal legate at the Conference of Regensburg, the diet and religious debate marking the culmination of attempts to restore religious unity in Germany by means of conferences. There everything was unfavorable; the Catholic states were bitter, the Evangelicals were distant. Contarini's instructions though apparently free were in fact full of papal reservations. But the papal party had gladly sent him, thinking that through him a union in doctrine could be brought about, while the interest of Rome could be attended to later. Though the princes stood aloof, the theologians and the emperor were for peace, so the main articles were put forth in a formula, Evangelical in thought and Catholic in expression. The papal legate had revised the Catholic proposal and assented to the formula agreed upon. All gave their approval, even Johann Eck, though he later regretted it.

Contarini's theological advisor was Tommaso Badia; his own position is shown in a treatise on justification, composed at Regensburg, which in essential points is Evangelical, differing only in the omission of the negative side and in being interwoven with the teaching of Aquinas. Meanwhile, the papal policy had changed, and Contarini was compelled to follow his leader. He advised the emperor, after the conference had broken up, not to renew it, but to submit everything to the pope.

Ignatius Loyola acknowledged that Cardinal Contarini was largely responsible for the papal approbation of the Society of Jesus, on September 27, 1540. Meanwhile, Rome had drifted further into reaction, and Contarini died while legate at Bologna, at a time when the Inquisition had driven many of his friends and fellows in conviction into exile.

Contarini's book "De magistratibus et republica venetorum" (Paris, 1543) is an important source for the study of sixteenth- and seventeenth-century Venice's unique system of government. It was published in an English translation in 1599. This magisterial work, written during his time as an ambassador to Charles V, extols the various institutions of the Venetian state in a manner designed to emphasize harmony, fairness and serenity. Historians have demonstrated that this text represents Contarini's idealization of Venetian reality. Probably written for a foreign, courtly audience, this work functions as the source for the everlasting propagation of the "myth of Venice" as a stable, unchanging and prosperous society.

His depiction of how members of the council were elected to the senate, for example, aimed to emphasise the way the electoral system prevented factionalism from occurring, instead making sure that “public benefits are largely extended among the citizens” rather than narrowly amongst “one family” . An elaborate lottery is described as giving the maximum amount of chance in appointing patricians to particular offices, and care is taken to point out if two of one family are standing for similar posts. Fairness is further emphasised in Contarini’s constant references to the equality the members of the council enjoyed. They “sit down where it pleases them, for there is no place appointed to any”, and they “with oath promise to do their utmost diligence, that the laws may be observed” . He creates an image of disparate individuals, with factions broken up by the guiding hand of the law, working to ensure those in positions of importance are fairly chosen from their number and without the capacity to serve the interests of a smaller group.

Contarini’s depiction of the Doge lucidly demonstrates the way in which this figure embodies both the conscious illusion of a resplendent monarchical ruler and an equally conscious demonstration of a regime that wishes to portray itself as ruled by many limiting the powers of one. This calculated duality means that Contarini’s doge, which the second book of De magistratibus is almost entirely devoted to discussing, represents the closest point in his text to what actually occurred, because the Doge served as a literal embodiment of the idealisation of the reality of Venetian politics. For Contarini, this duality almost defines the greatness of the Venetian constitution. The Doge is the “heart”, under which “all are comprised” . Contarini places him in the centre of his body metaphor, making him synecdochical for the city and the people that reside within it. This means he is to ensure that the disparate, competing interests of the city beat in time with one another, creating in the process the “perfection of civil agreement”. His job as a conductor, rather than a ruler, means therefore that the role takes on the aspect of representative of the entire city. Contarini’s description of his vestments, privileges and rituals can therefore be compared to Marin Sanudo’s description of the physical spaces of Venice in his essay "In Praise of Venice". Both are designed to extol the virtues of the entire city by describing representative parts. This is apparent in the way both authors treat the chapel of St. Mark. Patron saints were hugely important in terms of civic self-identification in renaissance Italy . Contarini emphasizes this, saying that he is “with exceeding honour solemnized of the Venetians” . His description of the Doge’s close relationship with the saint, through the “solemn pomp” with which he attends mass at the saint’s chapel, attaches him to the aforementioned “exceeding honour”, in a similar fashion to the way in which Sanudo glorifies Venice as a whole by constantly referring to the beauty and worth of St. Mark’s square and chapel as part of his panoramic praise of the city.

At the same time, however, Contarini’s overall purpose is, of course, the glorification of the republican nature of his city. Therefore, he cannot avoid referring to “the other side” of the Doge’s figure when discussing his “royal appearing show” . Things like the “kingly ornaments” which were “always purple garments or cloth of gold”, both very ostentatious assertions of wealth and power, were to ensure he was “recompensed” for his “limitation of authority” . Contarini thus openly concludes that the Doge is a combination of myth and reality, saying that “in everything you may see the show of a king, but his authority is nothing” . Indeed, as Edward Muir points out, “by the sixteenth century virtually every word, gesture and act that the doge made in public was subject to legal and ceremonial regulation” . He could not buy expensive jewels, own property outside Venice or the Veneto, display his insignia outside the Ducal Palace, decorate his apartment as he wanted, receive people in his ducal dress, send official letters, or have close ties with guilds, amongst a great many other restrictions. Legally, therefore, power in Venice came from the numerous councils, not the figurehead. The Doge thus becomes a brazen republican statement. Venice drew attention to a princely, magnificently adorned figurehead, only to direct most executive power to councils of her citizens.



 


</doc>
<doc id="12976" url="https://en.wikipedia.org/wiki?curid=12976" title="Gastroenterology">
Gastroenterology

Gastroenterology (MeSH heading) is the branch of medicine focused on the digestive system and its disorders.

Diseases affecting the gastrointestinal tract, which include the organs from mouth into anus, along the alimentary canal, are the focus of this speciality. Physicians practicing in this field are called gastroenterologists. They have usually completed about eight years of pre-medical and medical education, a year-long internship (if this is not a part of the residency), three years of an internal medicine residency, and two to three years in the gastroenterology fellowship. Gastroenterologists perform a number of diagnostic and therapeutic procedures including colonoscopy, endoscopy, endoscopic retrograde cholangiancreatography (ERCP), endoscopic ultrasound and liver biopsy. Some gastroenterology trainees will complete a "fourth-year" (although this is often their seventh year of graduate medical education) in transplant hepatology, advanced endoscopy, inflammatory bowel disease, motility or other topics.

Hepatology, or hepatobiliary medicine, encompasses the study of the liver, pancreas, and biliary tree, while proctology encompasses the fields of anus and rectum diseases. They are traditionally considered sub-specialties of gastroenterology.

Citing from Egyptian papyri, John F. Nunn identified significant knowledge of gastrointestinal diseases among practicing physicians during the periods of the pharaohs. Irynakhty, of the tenth dynasty, c. 2125 B.C., was a court physician specializing in gastroenterology, sleeping, and proctology.

Among ancient Greeks, Hippocrates attributed digestion to concoction. Galen's concept of the stomach having four "faculties" was widely accepted up to modernity in the seventeenth century.

Eighteenth century:

Nineteenth century:

Twentieth century:

Twenty-first century:

1. International Classification of Disease (ICD 2007)/WHO classification:
2. MeSH subject Heading:
3. National Library of Medicine Catalogue (NLM classification 2006):


In the United States, gastroenterology is an internal medicine subspecialty certified by the American Board of Internal Medicine (ABIM) and the American Osteopathic Board of Internal Medicine (AOBIM).




</doc>
<doc id="12980" url="https://en.wikipedia.org/wiki?curid=12980" title="Gulag">
Gulag

The Gulag (, ; , acronym of , "Main Camps' Administration" or "Chief Administration of [Corrective Labor] Camps") was the government agency in charge of the Soviet forced labor camp system that was created under Vladimir Lenin and reached its peak during Joseph Stalin's rule from the 1930s to the 1950s. The term is also commonly used in the English language to refer to any forced-labor camp in the Soviet Union, including camps which existed in post-Stalin times. The camps housed a wide range of convicts, from petty criminals to political prisoners. Large numbers were convicted by simplified procedures, such as NKVD troikas and other instruments of extrajudicial punishment. The Gulag is recognized as a major instrument of political repression in the Soviet Union.

The agency's full name was the Main Administration of Corrective Labor Camps and Settlements (). It was administered first by the State Political Administration (GPU), later by the NKVD and in the final years by the Ministry of Internal Affairs (MVD). The Solovki prison camp, the first corrective labor camp constructed after the revolution, was established in 1918 and legalized by a decree "On the creation of the forced-labor camps" on April 15, 1919. The internment system grew rapidly, reaching a population of 100,000 in the 1920s. According to Nicolas Werth, author of "The Black Book of Communism", the yearly mortality rate in the Soviet concentration camps strongly varied reaching 5% (1933) and 20% (1942–1943) while dropping considerably in the post-war years at about 1–3% per year at the beginning of the 1950s. The emergent consensus among scholars, based on archival evidence, is that of the 18 million who were sent to the Gulag from 1930 to 1953, roughly 1.5 to 1.7 million perished there or as a result of their detention.

Aleksandr Solzhenitsyn, winner of the Nobel Prize in Literature, who survived eight years of Gulag incarceration, gave the term its international repute with the publication of "The Gulag Archipelago" in 1973. The author likened the scattered camps to "a chain of islands" and as an eyewitness he described the Gulag as a system where people were worked to death. Some scholars support this view, though this claim is controversial, given that the vast majority of people who entered the Gulag came out alive, with the exception of the war years. Although one writer, citing pre-1991 materials, claims that most prisoners in the gulag were killed, Natalya Reshetovskaya, the wife of Aleksandr Solzhenitsyn, said in her memoirs that "The Gulag Archipelago" was based on "campfire folklore" as opposed to objective facts. Similarly, historian Stephen G. Wheatcroft asserts that it is essentially a "literary and political work". Numerous other accounts from survivors state otherwise and the Mitrokhin Archive claimed that these memoirs were part of a KGB campaign, orchestrated by Yuri Andropov in 1974, to discredit Solzhenitsyn. However, this archive itself has its veracity in doubt; among other, more practical issues, by the same token with which Vasili Mitrokhin claimed the Soviet government would obviously be interested in discrediting Solzhenitsyn, Western governments would have as much interest in lending him credence.

In March 1940, there were 53 Gulag camp directorates (colloquially referred to as simply "camps") and 423 labor colonies in the Soviet Union. Today's major industrial cities of the Russian Arctic, such as Norilsk, Vorkuta and Magadan, were originally camps built by prisoners and run by ex-prisoners.

Some suggest that 14 million people were imprisoned in the Gulag labor camps from 1929 to 1953 (the estimates for the period 1918–1929 are even more difficult to calculate). Other calculations by the historian Orlando Fidesa, refer to 25 million prisoners of the Gulag in 1928–1953. A further 6–7 million were deported and exiled to remote areas of the USSR, and 4–5 million passed through labor colonies, plus 3.5 million who were already in, or who had been sent to, labor settlements. According to some estimates, the total population of the camps varied from 510,307 in 1934 to 1,727,970 in 1953. According to other estimates, at the beginning of 1953 the total number of prisoners in prison camps was more than 2.4 million of which more than 465,000 were political prisoners. The institutional analysis of the Soviet concentration system is complicated by the formal distinction between GULAG and GUPVI. GUPVI was the Main Administration for Affairs of Prisoners of War and Internees (, GUPVI), a department of NKVD (later MVD) in charge of handling of foreign civilian internees and POWs in the Soviet Union during and in the aftermath of World War II (1939–1953). (for GUPVI, see Main Administration for Affairs of Prisoners of War and Internees). In many ways the GUPVI system was similar to GULAG. Its major function was the organization of foreign forced labor in the Soviet Union. The top management of GUPVI came from the GULAG system. The major noted distinction from GULAG was the absence of convicted criminals in the GUPVI camps. Otherwise the conditions in both camp systems were similar: hard labor, poor nutrition and living conditions, and high mortality rate.

For the Soviet political prisoners, like Solzhenitsyn, all foreign civilian detainees and foreign POWs (prisoners of war) were imprisoned in the GULAG; the surviving foreign civilians and POWs considered themselves prisoners in the GULAG. According with the estimates, in total, during the whole period of the existence of GUPVI there were over 500 POW camps (within the Soviet Union and abroad), which imprisoned over 4,000,000 POW.

Most Gulag inmates were not political prisoners, although significant numbers of political prisoners could be found in the camps at any one time. Petty crimes and jokes about the Soviet government and officials were punishable by imprisonment. About half of political prisoners in the Gulag camps were imprisoned without trial; official data suggest that there were over 2.6 million sentences to imprisonment on cases investigated by the secret police throughout 1921–53. The GULAG was reduced in size following Stalin's death in 1953, in a period known as the Khrushchev Thaw.

In 1960 the Ministerstvo Vnutrennikh Del (MVD) ceased to function as the Soviet-wide administration of the camps in favor of individual republic MVD branches. The centralized detention facilities temporarily ceased functioning.

Although the term "Gulag" originally referred to a government agency, in English and many other languages the acronym acquired the qualities of a common noun, denoting "the Soviet system of prison-based, unfree labor".

Even more broadly, "Gulag" has come to mean the Soviet repressive system itself, the set of procedures that prisoners once called the "meat-grinder": the arrests, the interrogations, the transport in unheated cattle cars, the forced labor, the destruction of families, the years spent in exile, the early and unnecessary deaths.

Western authors use the term "Gulag" to denote all the prisons and internment camps in the Soviet Union. The term's contemporary usage is at times notably not directly related to the USSR, such as in the expression "North Korea's Gulag" for camps operational today.

The word "Gulag" was not often used in Russian — either officially or colloquially; the predominant terms were "the camps" (лагеря) and "the zone" (зона), usually singular — for the labor camp system and for the individual camps. The official term, "corrective labor camp", was suggested for official use by the politburo of the Communist Party of the Soviet Union in the session of July 27, 1929.

The Russian Empire and the Tsar first invented the exile in Siberia as a punishment within the judicial system: Katorga, a category of punishment within the judicial system of the Russian Empire, had many of the features associated with labor-camp imprisonment: confinement, simplified facilities (as opposed to prisons), and forced labor, usually involving hard, unskilled or semi-skilled work. Katorga camps were established in the 17th century in underpopulated areas of Siberia and the Russian Far East – regions that had few towns or food sources and lacked any organized transportation systems. Despite the isolated conditions, a few prisoners successfully escaped to populated areas. After the change in Russian penal law in 1847, exile and katorga became common punishment for participants in national uprisings within the Russian Empire. This led to increasing numbers of Poles sent to Siberia for katorga. From these times, Siberia gained its fearful connotation of punishment, which was further enhanced by the Soviet GULAG system.

During 1920–50, the leaders of the Communist Party and the Soviet state considered repression to be a tool that was to be used for securing the normal functioning of the Soviet state system, as well as for preserving and strengthening the positions within their social base, the working class (when the Bolsheviks took power, peasants represented 80% of the population). The GULAG system was introduced in order to isolate and eliminate class-alien, socially dangerous, disruptive, suspicious, and other disloyal elements, whose deeds and thoughts were not contributing to the strengthening of the dictatorship of the proletariat. Forced labor as a "method of reeducation" was applied in Solovki prison camp as early as the 1920s, based on Trotsky's experiments with forced labor camps for Czech war prisoners from 1918 and his proposals to introduce "compulsory labor service" voiced in "Terrorism and Communism".

According to journalist Anne Applebaum, approximately 6,000 katorga convicts were serving sentences in 1906 and 28,600 in 1916. From 1918, camp-type detention facilities were set up, as a reformed analogy of the earlier system of penal labor ("katorgas"), operated in Siberia in Imperial Russia. The two main types were "Vechecka Special-purpose Camps" (, ) and forced labor camps (, ). Various categories of prisoners were defined: petty criminals, POWs of the Russian Civil War, officials accused of corruption, sabotage and embezzlement, political enemies, dissidents and other people deemed dangerous for the state. In 1928 there were 30,000 individuals interned; the authorities were opposed to compelled labour. In 1927 the official in charge of prison administration wrote:

The exploitation of prison labor, the system of squeezing "golden sweat" from them, the organization of production in places of confinement, which while profitable from a commercial point of view is fundamentally lacking in corrective significance – these are entirely inadmissible in Soviet places of confinement.

The legal base and the guidance for the creation of the system of "corrective labor camps" (, ), the backbone of what is commonly referred to as the "Gulag", was a secret decree of Sovnarkom of July 11, 1929, about the use of penal labor that duplicated the corresponding appendix to the minutes of Politburo meeting of June 27, 1929.

After having appeared as an instrument and place for isolating counterrevolutionary and criminal elements, the Gulag, because of its principle of "correction by forced labor", quickly became, in fact, an independent branch of the national economy secured on the cheap labor force presented by prisoners. Hence it is followed by one more important reason for the constancy of the repressive policy, namely, the state's interest in unremitting rates of receiving a cheap labor force that was forcibly used, mainly in the extreme conditions of the east and north. The Gulag possessed both punitive and economic functions.

The Gulag was officially established on April 25, 1930 as the ULAG by the OGPU order 130/63 in accordance with the Sovnarkom order 22 p. 248 dated April 7, 1930. It was renamed as the Gulag in November of that year.

The hypothesis that economic considerations were responsible for mass arrests during the period of Stalinism has been refuted on the grounds of former Soviet archives that have become accessible since the 1990s, although some archival sources also tend to support an economic hypothesis. In any case, the development of the camp system followed economic lines. The growth of the camp system coincided with the peak of the Soviet industrialization campaign. Most of the camps established to accommodate the masses of incoming prisoners were assigned distinct economic tasks. These included the exploitation of natural resources and the colonization of remote areas, as well as the realization of enormous infrastructural facilities and industrial construction projects. The plan to achieve these goals with "special settlements" instead of labor camps was dropped after the revealing of the Nazino affair in 1933; subsequently the Gulag system was expanded.

The 1931–32 archives indicate the Gulag had approximately 200,000 prisoners in the camps; while in 1935, approximately 800,000 were in camps and 300,000 in colonies (annual averages).

In the early 1930s, a tightening of Soviet penal policy caused significant growth of the prison camp population. During the Great Purge of 1937–38, mass arrests caused another increase in inmate numbers. Hundreds of thousands of persons were arrested and sentenced to long prison terms on the grounds of one of the multiple passages of the notorious Article 58 of the Criminal Codes of the Union republics, which defined punishment for various forms of "counterrevolutionary activities". Under NKVD Order No. 00447, tens of thousands of Gulag inmates were executed in 1937–38 for "continuing counterrevolutionary activities".

Between 1934 and 1941, the number of prisoners with higher education increased more than eight times, and the number of prisoners with high education increased five times. It resulted in their increased share in the overall composition of the camp prisoners. Among the camp prisoners, the number and share of the intelligentsia was growing at the quickest pace. Distrust, hostility, and even hatred for the intelligentsia was a common characteristic of the Soviet leaders. Information regarding the imprisonment trends and consequences for the intelligentsia derive from the extrapolations of Viktor Zemskov from a collection of prison camp population movements data.

The GULAG was an administration body that watched over the camps; eventually its name would be used for these camps retrospectively. After Lenin's death in 1924, Stalin was able to take control of the government, and began to form the gulag system. On June 27, 1929 the Politburo created a system of self-supporting camps that would eventually replace the existing prisons around the country. These prisons were meant to receive inmates that received a prison sentence that exceeded three years. Prisoners that had a shorter prison sentence than three years were to remain in the prison system that was still under the purview of the NKVD. The purpose of these new camps was to colonize the remote and inhospitable environments throughout the Soviet Union. These changes took place around the same time that Stalin started to institute collectivization and rapid industrial development. Collectivization resulted in a large scale purge of peasants and so-called Kulaks. The Kulaks were supposedly wealthy (comparatively to other Soviet peasants) and were considered to be capitalists by the state, and by extension enemies of socialism. By late 1929 Stalin started a program known as "dekulakization". Stalin demanded that the kulak class be completely wiped out. This resulted in the imprisonment and execution of Soviet peasants. The term "Kulak" would also become associated with anyone who opposed or even seemed unsatisfied with the Soviet government. This resulted in 60,000 people being sent to the camps and another 154,000 exiled in a mere four months. This was only the beginning of the dekulakization process. In 1931 alone 1,803,392 people were exiled. Although these massive relocation processes were successful in getting a large potential free forced labor work force where they needed to be, that is about all it was successful at doing. The "special settlers", as the Soviet government referred to them, all lived on starvation level rations, and many people starved to death in the camps, and anyone who was healthy enough to escape tried to do just that. This resulted in the government having to give rations to a group of people they were getting hardly any use out of, and was just costing the Soviet government money. The Unified State Political Administration (OGPU) quickly realized the problem, and began to reform the dekulakization process. To help prevent the mass escapes the OGPU started to recruit people within the colony to help stop people who attempted to leave, and set up ambushes around known popular escape routes. The OGPU also attempted to raise the living conditions in these camps that would not encourage people to actively try and escape, and Kulaks were promised that they would regain their rights after five years. Even these revisions ultimately failed to resolve the problem, and the dekulakization process was a failure in providing the government with a steady forced labor force. These prisoners were also lucky to be in the gulag in the early 1930s. Prisoners were relatively well off compared to what the prisoners would have to go through in the final years of the gulag.

On the eve of World War II, Soviet archives indicate a combined camp and colony population upwards of 1.6 million in 1939, according to V. P. Kozlov. Anne Applebaum and Steven Rosefielde estimate that 1.2 to 1.5 million people were in Gulag system's prison camps and colonies when the war started.

After the German invasion of Poland that marked the start of World War II in Europe, the Soviet Union invaded and annexed eastern parts of the Second Polish Republic. In 1940 the Soviet Union occupied Estonia, Latvia, Lithuania, Bessarabia (now the Republic of Moldova) and Bukovina. According to some estimates, hundreds of thousands of Polish citizens and inhabitants of the other annexed lands, regardless of their ethnic origin, were arrested and sent to the Gulag camps. However, according to the official data, the total number of sentences for political and antistate (espionage, terrorism) crimes in USSR in 1939–41 was 211,106.

Approximately 300,000 Polish prisoners of war were captured by the USSR during and after the "Polish Defensive War". Almost all of the captured officers and a large number of ordinary soldiers were then murdered (see Katyn massacre) or sent to Gulags. Of the 10,000-12,000 Poles sent to Kolyma in 1940–41, most prisoners of war, only 583 men survived, released in 1942 to join the Polish Armed Forces in the East. Out of General Anders' 80,000 evacuees from Soviet Union gathered in Great Britain only 310 volunteered to return to Soviet-controlled Poland in 1947.

During the Great Patriotic War, Gulag populations declined sharply due to a steep rise in mortality in 1942–43. In the winter of 1941 a quarter of the Gulag's population died of starvation. 516,841 prisoners died in prison camps in 1941–43, from a combination of their harsh working conditions and the famine caused by the German invasion. This period accounts for about half of all gulag deaths, according to Russian statistics.

In 1943, the term "katorga works" () was reintroduced. They were initially intended for Nazi collaborators, but then other categories of political prisoners (for example, members of deported peoples who fled from exile) were also sentenced to "katorga works". Prisoners sentenced to "katorga works" were sent to Gulag prison camps with the most harsh regime and many of them perished.

Up until World War II, the Gulag system expanded dramatically to create a Soviet "camp economy". Right before the war, forced labor provided 46.5% of the nation's nickel, 76% of its tin, 40% of its cobalt, 40.5% of its chrome-iron ore, 60% of its gold, and 25.3% of its timber. And in preparation for war, the NKVD put up many more factories and built highways and railroads.

The Gulag quickly switched to production of arms and supplies for the army after fighting began. At first, transportation remained a priority. In 1940 the NKVD focused most of its energy on railroad construction. This would prove extremely important when the German advance into the Soviet Union started in 1941. In addition, factories converted to produce ammunition, uniforms, and other supplies. Moreover, the NKVD gathered skilled workers and specialists from throughout the Gulag into 380 special colonies which produced tanks, airplanes, armaments, and ammunition.

Despite its low capital costs, the camp economy suffered from serious flaws. For one, actual productivity almost never matched estimates: the estimates proved far too optimistic. In addition, scarcity of machinery and tools plagued the camps, and the tools that the camps did have quickly broke. The Eastern Siberian Trust of the Chief Administration of Camps for Highway Construction destroyed ninety-four trucks in just three years. But the greatest problem was simple – forced labor was less efficient than free labor. In fact, prisoners in the Gulag were, on average, half as productive as free laborers in the USSR at the time, which may be partially explained by malnutrition.

To make up for this disparity, the NKVD worked prisoners harder than ever. To meet rising demand, prisoners worked longer and longer hours, and on lower food-rations than ever before. A camp administrator said in a meeting: "There are cases when a prisoner is given only four or five hours out of twenty-four for rest, which significantly lowers his productivity." In the words of a former Gulag prisoner: "By the spring of 1942, the camp ceased to function. It was difficult to find people who were even able to gather firewood or bury the dead." The scarcity of food stemmed in part from the general strain on the entire Soviet Union, but also lack of central aid to the Gulag during the war. The central government focused all its attention on the military, and left the camps to their own devices. In 1942 the Gulag set up the Supply Administration to find their own food and industrial goods. During this time, not only did food become scarce, but the NKVD limited rations in an attempt to motivate the prisoners to work harder for more food, a policy that lasted until 1948.

In addition to food shortages, the Gulag suffered from labor scarcity at the beginning of the war. The Great Terror of 1936–1938 had provided a large supply of free labor, but by the start of World War II the purges had slowed down. In order to complete all of their projects, camp administrators moved prisoners from project to project. To improve the situation, laws were implemented in mid-1940 that allowed giving short camp sentences (4 months or a year) to those convicted of petty theft, hooliganism, or labor-discipline infractions. By January 1941 the Gulag workforce had increased by approximately 300,000 prisoners. But in 1942 serious food shortages began, and camp populations dropped again. The camps lost still more prisoners to the war effort. (The Soviet Union went into total war footing in June 1941.) Many laborers received early releases so that they could be drafted and sent to the front.

Even as the pool of workers shrank, demand for outputs continued to grow rapidly. As a result, the Soviet government pushed the Gulag to "do more with less". With fewer able-bodied workers and few supplies from outside the camp system, camp administrators had to find a way to maintain production. The solution they found was to push the remaining prisoners still harder. The NKVD employed a system of setting unrealistically high production goals, straining resources in an attempt to encourage higher productivity. As the Axis armies pushed into Soviet territory from June 1941 on, labor resources became further strained, and many of the camps had to evacuate out of Western Russia. From the beginning of the war to halfway through 1944, 40 camps were set up, and 69 were disbanded. During evacuations, machinery received priority, leaving prisoners to reach safety on foot. The speed of Operation Barbarossa's advance prevented the evacuation of all laborers in good time, and the NKVD massacred many to prevent them from falling into German hands. While this practice denied the Germans a source of free labor, it also further restricted the Gulag's capacity to keep up with the Red Army's demands. When the tide of the war turned, however, and the Soviets started pushing the Axis invaders back, fresh batches of laborers replenished the camps. As the Red Army recaptured territories from the Germans, an influx of Soviet ex-POWs greatly increased the Gulag population.

After World War II the number of inmates in prison camps and colonies, again, rose sharply, reaching approximately 2.5 million people by the early 1950s (about 1.7 million of whom were in camps).

When the war in Europe ended in May 1945, as many as two million former Russian citizens were forcefully repatriated into the USSR. On February 11, 1945, at the conclusion of the Yalta Conference, the United States and United Kingdom signed a Repatriation Agreement with the Soviet Union. One interpretation of this agreement resulted in the forcible repatriation of all Soviets. British and U.S. civilian authorities ordered their military forces in Europe to deport to the Soviet Union up to two million former residents of the Soviet Union, including persons who had left the Russian Empire and established different citizenship years before. The forced repatriation operations took place from 1945–47.

Multiple sources state that Soviet POWs, on their return to the Soviet Union, were treated as traitors (see Order No. 270). According to some sources, over 1.5 million surviving Red Army soldiers imprisoned by the Germans were sent to the Gulag. However, that is a confusion with two other types of camps. During and after World War II, freed POWs went to special "filtration" camps. Of these, by 1944, more than 90 percent were cleared, and about 8 percent were arrested or condemned to penal battalions. In 1944, they were sent directly to reserve military formations to be cleared by the NKVD. Further, in 1945, about 100 filtration camps were set for repatriated Ostarbeiter, POWs, and other displaced persons, which processed more than 4,000,000 people. By 1946, the major part of the population of these camps were cleared by NKVD and either sent home or conscripted (see table for details). 226,127 out of 1,539,475 POWs were transferred to the NKVD, i.e. the Gulag.
After Nazi Germany's defeat, ten NKVD-run "special camps" subordinate to the Gulag were set up in the Soviet Occupation Zone of post-war Germany. These "special camps" were former Stalags, prisons, or Nazi concentration camps such as Sachsenhausen (special camp number 7) and Buchenwald (special camp number 2). According to German government estimates "65,000 people died in those Soviet-run camps or in transportation to them." According to German researchers, Sachsenhausen, where 12,500 Soviet era victims have been uncovered, should be seen as an integral part of the Gulag system.
Yet the major reason for the post-war increase in the number of prisoners was the tightening of legislation on property offences in summer 1947 (at this time there was a famine in some parts of the Soviet Union, claiming about 1 million lives), which resulted in hundreds of thousands of convictions to lengthy prison terms, sometimes on the basis of cases of petty theft or embezzlement. At the beginning of 1953 the total number of prisoners in prison camps was more than 2.4 million of which more than 465,000 were political prisoners.

The state continued to maintain the extensive camp system for a while after Stalin's death in March 1953, although the period saw the grip of the camp authorities weaken, and a number of conflicts and uprisings occur ("see" Bitch Wars; Kengir uprising; Vorkuta uprising).

The amnesty in March 1953 was limited to non-political prisoners and for political prisoners sentenced to not more than 5 years, therefore mostly those convicted for common crimes were then freed. The release of political prisoners started in 1954 and became widespread, and also coupled with mass rehabilitations, after Nikita Khrushchev's denunciation of Stalinism in his Secret Speech at the 20th Congress of the CPSU in February 1956.

The "Gulag" institution was closed by the MVD order No 020 of January 25, 1960 but forced labor colonies for political and criminal prisoners continued to exist. Political prisoners continued to be kept in one of the most famous camps Perm-36 until 1987 when it was closed. (See also Foreign forced labor in the Soviet Union.)

The Russian penal system, despite reforms and a reduction in prison population, informally or formally continues many practices endemic to the "Gulag" system, including forced labor, inmates policing inmates, and prisoner intimidation.

In the late 2000s, some human rights activists accused authorities of gradual removal of Gulag remembrance from places such as Perm-36 and Solovki prison camp.

Prior to the dissolution of the Soviet Union, estimates of Gulag victims ranged from 2.3 to 17.6 million (see a History of Gulag population estimates section).
Post-1991 research by historians utilizing archival materials brought this range down considerably. According to a 1993 study of archival Soviet data, a total of 1,053,829 people died in the Gulag from 1934 to 1953. However, taking into account the fact that it was common practice to release prisoners who were either suffering from incurable diseases or near death, a combined statistics on mortality "in the camps" and mortality "caused by the camps" gives a probable figure around 1.6 million. In her recent study, prof. Alexopoulos made an attempt to challenge this consensus figure by encompassing those whose life was shortened due to GULAG conditions. The GULAG mortality estimated in this way yields the figure of 6 million deaths. This estimate is supported by indirect and misinterpreted evidences, and has obvious methodological difficulties. The tentative historical consensus is that of the 18 million people who passed through the gulag from 1930 to 1953, between 1.5 and 1.7 million perished as a result of their detention.

Living and working conditions in the camps varied significantly across time and place, depending, among other things, on the impact of broader events (World War II, countrywide famines and shortages, waves of terror, sudden influx or release of large numbers of prisoners). However, to one degree or another, the large majority of prisoners at most times faced meager food rations, inadequate clothing, overcrowding, poorly insulated housing, poor hygiene, and inadequate health care. Most prisoners were compelled to perform harsh physical labor. In most periods and economic branches, the degree of mechanization of work processes was significantly lower than in the civilian industry: tools were often primitive and machinery, if existent, short in supply. Officially established work hours were in most periods longer and days off were fewer than for civilian workers. Often official work time regulations were extended by local camp administrators.

Andrei Vyshinsky, procurator of the Soviet Union, wrote a memorandum to NKVD chief Nikolai Yezhov in 1938 which stated:

Among the prisoners there are some so ragged and liceridden that they pose a sanitary danger to the rest. These prisoners have deteriorated to the point of losing any resemblance to human beings. Lacking food . . . they collect orts [refuse] and, according to some prisoners, eat rats and dogs.

In general, the central administrative bodies showed a discernible interest in maintaining the labor force of prisoners in a condition allowing the fulfillment of construction and production plans handed down from above. Besides a wide array of punishments for prisoners refusing to work (which, in practice, were sometimes applied to prisoners that were too enfeebled to meet production quota), they instituted a number of positive incentives intended to boost productivity. These included monetary bonuses (since the early 1930s) and wage payments (from 1950 onwards), cuts of individual sentences, general early-release schemes for norm fulfillment and overfulfillment (until 1939, again in selected camps from 1946 onwards), preferential treatment, and privileges for the most productive workers (shock workers or Stakhanovites in Soviet parlance).

A distinctive incentive scheme that included both coercive and motivational elements and was applied universally in all camps consisted in standardized "nourishment scales": the size of the inmates' ration depended on the percentage of the work quota delivered. Naftaly Frenkel is credited for the introduction of this policy. While it was effective in compelling many prisoners to work harder, for many a prisoner it had the adverse effect, accelerating the exhaustion and sometimes causing the death of persons unable to fulfill high production quota.

Immediately after the German attack on the Soviet Union in June 1941 the conditions in camps worsened drastically: quotas were increased, rations cut, and medical supplies came close to none, all of which led to a sharp increase in mortality. The situation slowly improved in the final period and after the end of the war.

Considering the overall conditions and their influence on inmates, it is important to distinguish three major strata of Gulag inmates:


Mortality in Gulag camps in 1934–40 was 4–6 times higher than average in the Soviet Union. The estimated total number of those who died in imprisonment in 1930–53 is at least 1.76 million, about half of which occurred between 1941–43 following the German invasion. If prisoner deaths from labor colonies and special settlements are included, the death toll rises to 2,749,163, although the historian who compiled this estimate (J. Otto Pohl) stresses that it is incomplete, and doesn't cover all prisoner categories for every year.

A severe famine of 1931–1933 swept across many different regions in the Soviet Union. During this time, it is estimated that around six to seven million people starved to death. On 7 August 1932, a new edict drafted by Stalin specified a minimum sentence of ten years or execution for theft from collective farms or of cooperative property. Over the next few months, prosecutions rose fourfold. A large share of cases prosecuted under the law were for the theft of small quantities of grain worth less than fifty rubles. The law was later relaxed on 8 May 1933. Overall, during the first half of 1933, prisons saw more new incoming inmates than the three previous years combined.

Prisoners in the camps faced harsh working conditions. One Soviet report stated that, in early 1933, up to 15% of the prison population in Uzbekistan died monthly. During this time, prisoners were getting around worth of food a day. Many inmates attempted to flee, causing an upsurge in corrosive and violent measures. Camps were directed "not to spare bullets". The bodies of inmates who tried to escape were commonly displayed in the courtyards of the camps, and the administrators would forcibly escort the inmates around the dead bodies as a message. Until 1934, lack of food and the outbreak of diseases started to destabilize the Gulag system. It wasn't until the famine ended that the system started to stabilize.

The convicts in such camps were actively involved in all kinds of labor with one of them being logging ("lesopoval"). The working territory of logging presented by itself a square and was surrounded by forest clearing. Thus, all attempts to exit or escape from it were well observed from the four towers set at each of its corners.

Locals who captured a runaway were given rewards. It is also said that Gulags in colder areas were less concerned with finding escaped prisoners as they would die anyhow from the severely cold winters. In such cases prisoners who did escape without getting shot were often found dead kilometres away from the camp.

In the early days of Gulag, the locations for the camps were chosen primarily for the isolated conditions involved. Remote monasteries in particular were frequently reused as sites for new camps. The site on the Solovetsky Islands in the White Sea is one of the earliest and also most noteworthy, taking root soon after the Revolution in 1918. The colloquial name for the islands, "Solovki", entered the vernacular as a synonym for the labor camp in general. It was presented to the world as an example of the new Soviet method for "re-education of class enemies" and reintegrating them through labor into Soviet society. Initially the inmates, largely Russian intelligentsia, enjoyed relative freedom (within the natural confinement of the islands). Local newspapers and magazines were published and even some scientific research was carried out (e.g., a local botanical garden was maintained but unfortunately later lost completely). Eventually Solovki turned into an ordinary Gulag camp; in fact some historians maintain that it was a pilot camp of this type. In 1929 Maxim Gorky visited the camp and published an apology for it. The report of Gorky's trip to Solovki was included in the cycle of impressions titled "Po Soiuzu Sovetov," Part V, subtitled "Solovki." In the report, Gorky wrote that "camps such as 'Solovki' were absolutely necessary."

With the new emphasis on Gulag as the means of concentrating cheap labor, new camps were then constructed throughout the Soviet sphere of influence, wherever the economic task at hand dictated their existence (or was designed specifically to avail itself of them, such as the White Sea-Baltic Canal or the Baikal Amur Mainline), including facilities in big cities — parts of the famous Moscow Metro and the Moscow State University new campus were built by forced labor. Many more projects during the rapid industrialization of the 1930s, war-time and post-war periods were fulfilled on the backs of convicts. The activity of Gulag camps spanned a wide cross-section of Soviet industry. Gorky organised in 1933 a trip of 120 writers and artists to the White Sea–Baltic Canal, 36 of them wrote a propaganda book about the construction published in 1934 and destroyed in 1937.

The majority of Gulag camps were positioned in extremely remote areas of northeastern Siberia (the best known clusters are "Sevvostlag" ("The North-East Camps") along Kolyma river and "Norillag" near Norilsk) and in the southeastern parts of the Soviet Union, mainly in the steppes of Kazakhstan ("Luglag", "Steplag", "Peschanlag"). A very precise map was made by the Memorial Foundation. These were vast and sparsely inhabited regions with no roads (in fact, the construction of the roads themselves was assigned to the inmates of specialized railroad camps) or sources of food, but rich in minerals and other natural resources (such as timber). However, camps were generally spread throughout the entire Soviet Union, including the European parts of Russia, Belarus, and Ukraine. There were several camps outside the Soviet Union, in Czechoslovakia, Hungary, Poland, and Mongolia, which were under the direct control of the Gulag.

Not all camps were fortified; some in Siberia were marked only by posts. Escape was deterred by the harsh elements, as well as tracking dogs that were assigned to each camp. While during the 1920s and 1930s native tribes often aided escapees, many of the tribes were also victimized by escaped thieves. Tantalized by large rewards as well, they began aiding authorities in the capture of Gulag inmates. Camp guards were given stern incentive to keep their inmates in line at all costs; if a prisoner escaped under a guard's watch, the guard would often be stripped of his uniform and become a Gulag inmate himself. Further, if an escaping prisoner was shot, guards could be fined amounts that were often equivalent to one or two weeks wages.

In some cases, teams of inmates were dropped off in new territory with a limited supply of resources and left to set up a new camp or die. Sometimes it took several waves of colonists before any one group survived to establish the camp.

The area along the Indigirka river was known as "the Gulag inside the Gulag". In 1926, the Oimiakon (Оймякон) village in this region registered the record low temperature of −71.2 °C (−96 °F).

Under the supervision of Lavrenty Beria who headed both NKVD and the Soviet atom bomb program until his demise in 1953, thousands of "zeks" (Gulag inmates) were used to mine uranium ore and prepare test facilities on Novaya Zemlya, Vaygach Island, Semipalatinsk, among other sites.

Throughout the history of the Soviet Union, there were at least 476 separate camp administrations. The Russian researcher Galina Ivanova stated that,

to date, Russian historians have discovered and described 476 camps that existed at different times on the territory of the USSR. It is well known that practically every one of them had several branches, many of which were quite large. In addition to the large numbers of camps, there were no less than 2,000 colonies. It would be virtually impossible to reflect the entire mass of Gulag facilities on a map that would also account for the various times of their existence.

Since many of these existed only for short periods, the number of camp administrations at any given point was lower. It peaked in the early 1950s, when there were more than 100 camp administrations across the Soviet Union. Most camp administrations oversaw several single camp units, some as many as dozens or even hundreds. The infamous complexes were those at Kolyma, Norilsk, and Vorkuta, all in arctic or subarctic regions. However, prisoner mortality in Norilsk in most periods was actually lower than across the camp system as a whole.


According to historian Stephen Barnes, there exist four major ways of looking at the origins and functions of the Gulag. The first approach was championed by Alexander Solzhenitsyn, and is what Barnes terms the 'moral explanation'. According to this view, Soviet ideology eliminated the moral checks on the darker side of human nature – providing convenient justifications for violence and evil-doing on all levels: from political decision-making to personal relations. Another approach is the 'political explanation', according to which the Gulag (along with executions) was primarily a means for eliminating the regime's perceived political enemies (this understanding is favored, among others, by historian Robert Conquest). The 'economic explanation', in turn as set out by historian Anne Applebaum, argues that the Soviet regime instrumentalized the Gulag for its economic development projects. Although never economically profitable, it was perceived as such right up to Stalin's death in 1953. Finally, Barnes advances his own, fourth explanation, which situates the Gulag in the context of modern projects of 'cleansing' the social body of hostile elements, through spatial isolation and physical elimination of individuals defined as harmful.

Hannah Arendt argued that as part of a totalitarian system of government, the camps of the Gulag system were experiments in "total domination." In her view, the goal of a totalitarian system was not merely to establish limits on liberty, but rather to abolish liberty entirely in service of its ideology. She argues that the Gulag system was not merely political repression because the system survived and grew long after Stalin had wiped out all serious political resistance. Although the various camps were initially filled with criminals and political prisoners, eventually they were filled with prisoners who were arrested irrespective of anything relating to them as individuals, but rather only on the basis of their membership in some ever shifting category of imagined threats to the state.

She also argues that the function of the Gulag system was not truly economic. Although the Soviet government deemed them all "forced labor" camps, this in fact highlighted that the work in the camps was deliberately pointless, since all Russian workers could be subject to forced labor. The only real economic purpose they typically served was financing the cost of their own supervision. Otherwise the work performed was generally useless, either by design or made that way through extremely poor planning and execution; some workers even preferred more difficult work if it was actually productive. She differentiated between "authentic" forced-labor camps, concentration camps, and "annihilation camps". In authentic labor camps, inmates worked in "relative freedom and are sentenced for limited periods." Concentration camps had extremely high mortality rates and but were still "essentially organized for labor purposes." Annihilation camps were those where the inmates were "systematically wiped out through starvation and neglect." She criticizes other commentators' conclusion that the purpose of the camps was a supply of cheap labor. According to her, the Soviets were able to liquidate the camp system without serious economic consequences, showing that the camps were not an important source of labor and were overall economically irrelevant.

Arendt argues that together with the systematized, arbitrary cruelty inside the camps, this served the purpose of total domination by eliminating the idea that the arrestees had any political or legal rights. Morality was destroyed by maximizing cruelty and by organizing the camps internally to make the inmates and guards complicit. The terror resulting from operation of the Gulag system caused people outside of the camps to cut all ties with anyone who was arrested or purged and to avoid forming ties with others for fear of being associated with anyone who was targeted. As a result, the camps were essential as the nucleus of a system that destroyed individuality and dissolved all social bonds. Thereby, the system attempted to eliminate any capacity for resistance or self-directed action in the greater population.

Statistical reports made by the OGPU-NKVD-MGB-MVD between the 1930s and 1950s are kept in the State Archive of the Russian Federation formerly called Central State Archive of the October Revolution (CSAOR). These documents were highly classified and inaccessible. Amid glasnost and democratization in the late 1980s, Viktor Zemskov and other Russian researchers managed to gain access to the documents and published the highly classified statistical data collected by the OGPU-NKVD-MGB-MVD and related to the number of the Gulag prisoners, special settlers, etc. In 1995, Zemskov wrote that foreign scientists have begun to be admitted to the restricted-access collection of these documents in the State Archive of the Russian Federation since 1992. However, only one historian, namely Zemskov, was admitted to these archives, and later the archives were again "closed", according to Leonid Lopatnikov.

While considering the issue of reliability of the primary data provided by corrective labor institutions, it is necessary to take into account the following two circumstances. On the one hand, their administration was not interested to understate the number of prisoners in its reports, because it would have automatically led to a decrease in the food supply plan for camps, prisons, and corrective labor colonies. The decrement in food would have been accompanied by an increase in mortality that would have led to wrecking of the vast production program of the Gulag. On the other hand, overstatement of data of the number of prisoners also did not comply with departmental interests, because it was fraught with the same (i.e., impossible) increase in production tasks set by planning bodies. In those days, people were highly responsible for non-fulfilment of plan. It seems that a resultant of these objective departmental interests was a sufficient degree of reliability of the reports.

Between 1990 and 1992, the first precise statistical data on the Gulag based on the Gulag archives were published by Viktor Zemskov. These had been generally accepted by leading Western scholars, despite the fact that a number of inconsistencies were found in this statistics. It is also necessary to note that not all the conclusions drawn by Zemskov based on his data have been generally accepted. Thus, Sergei Maksudov alleged that although literary sources, for example the books of Lev Razgon or Aleksandr Solzhenitsyn, did not envisage the total number of the camps very well and markedly exaggerated their size, on the other hand, Viktor Zemskov, who published many documents by the NKVD and KGB, was far from understanding of the Gulag essence and the nature of socio-political processes in the country. He added that without distinguishing the degree of accuracy and reliability of certain figures, without making a critical analysis of sources, without comparing new data with already known information, Zemskov absolutizes the published materials by presenting them as the ultimate truth. As a result, Maksudov charges that Zemskov attempts to make generalized statements with reference to a particular document, as a rule, do not hold water.
In response, Zemskov wrote that the charge that Zemskov allegedly did not compare new data with already known information could not be called fair. In his words, the trouble with most western writers is that they do not benefit from such comparisons. Zemskov added that when he tried not to overuse the juxtaposition of new information with "old" one, it was only because of a sense of delicacy, not to once again psychologically traumatize the researchers whose works used incorrect figures, as it turned out after the publication of the statistics by the OGPU-NKVD-MGB-MVD.

According to French historian Nicolas Werth, the mountains of the materials of the Gulag archives, which are stored in funds of the State Archive of the Russian Federation and are being constantly exposed during the last fifteen years, represent only a very small part of bureaucratic prose of immense size left over the decades of "creativity" by the "dull and reptile" organization managing the Gulag. In many cases, local camp archives, which had been stored in sheds, barracks, or other rapidly disintegrating buildings, simply disappeared in the same way as most of the camp buildings did.

In 2004 and 2005, some archival documents were published in the edition "Istoriya Stalinskogo Gulaga. Konets 1920-kh — Pervaya Polovina 1950-kh Godov. Sobranie Dokumentov v 7 Tomakh" ("The History of Stalin's Gulag. From the Late 1920s to the First Half of the 1950s. Collection of Documents in Seven Volumes") wherein each of its seven volumes covered a particular issue indicated in the title of the volume: the first volume has the title "Massovye Repressii v SSSR" ("Mass Repression in the USSR"), the second volume has the title "Karatelnaya Sistema. Struktura i Kadry" ("Punitive System. Structure and Cadres"), the third volume has the title "Ekonomika Gulaga" ("Economy of the Gulag"), the fourth volume has the title "Naselenie Gulaga. Chislennost i Usloviya Soderzhaniya" ("The Population of the Gulag. The Number and Conditions of Confinement"), the fifth volume has the title "Specpereselentsy v SSSR" ("Specsettlers in the USSR"), the sixth volume has the title "Vosstaniya, Bunty i Zabastovki Zaklyuchyonnykh" ("Uprisings, Riots, and Strikes of Prisoners"), the seventh volume has the title "Sovetskaya Pepressivno-karatelnaya Politika i Penitentsiarnaya Sistema. Annotirovanniy Ukazatel Del GA RF" ("Soviet Repressive and Punitive Policy. Annotated Index of Cases of the SA RF"). The edition contains the brief introductions by the two "patriarchs of the Gulag science", Robert Conquest and Aleksandr Solzhenitsyn, and 1431 documents, the overwhelming majority of which were obtained from funds of the State Archive of the Russian Federation.

During the decades before the dissolution of the USSR, the debates about the population size of GULAG failed to arrive at generally accepted figures; wide-ranging estimates have been offered, and the bias toward higher or lower side was sometimes ascribed to political views of the particular author. Some of those earlier estimates (both high and low) are shown in the table below.

The glasnost political reforms in the late 1980s and the subsequent dissolution of the USSR led to the release of a large amount of formerly classified archival documents, including new demographic and NKVD data. Analysis of the official GULAG statistics by Western scholars immediately demonstrated that, despite their inconsistency, they do not support previously published higher estimates. Importantly, the released documents made possible to clarify terminology used to describe different categories of forced labour population, because the use of the terms "forced labour", "GULAG", "camps" interchangeably by early researchers led to significant confusion and resulted in significant inconsistencies in the earlier estimates. Archival studies revealed several components of the NKVD penal system in the Stalinist USSR: prisons, labor camps, labor colonies, as well as various "settlements" (exile) and of non-custodial forced labour. Although most of them fit the definition of forced labour, only labour camps, and labour colonies were associated with punitive forced labour in detention. Forced labour camps ("GULAG camps") were hard regime camps, whose inmates were serving more than three-year terms. As a rule, they were situated in remote parts of the USSR, and labour conditions were extremely hard there. They formed a core of the GULAG system. The inmates of "corrective labour colonies" served shorter terms; these colonies were located in less remote parts of the USSR, and they were run by local NKVD administration. Preliminary analysis of the GULAG camps and colonies statistics (see the chart on the right) demonstrated that the population reached the maximum before the World War II, then dropped sharply, partially due to massive releases, partially due to wartime high mortality, and then was gradually increasing until the end of Stalin era, reaching the global maximum in 1953, when the combined population of GULAG camps and labour colonies amounted to 2,625,000.

The results of these archival studies convinced many scholars, including Robert Conquest or Stephen Wheatcroft to reconsider their earlier estimates of the size of the GULAG population, although the 'high numbers' of arrested and deaths are not radically different from earlier estimates. Although such scholars as Rosefielde or Vishnevsky point at several inconsistencies in archival data with Rosefielde pointing out the archival figure of 1,196,369 for the population of the Gulag and labor colonies combined on December 31, 1936 is less than half the 2.75 million labor camp population given to the Census Board by the NKVD for the 1937 census, it is generally believed that these data provide more reliable and detailed information that the indirect data and literary sources available for the scholars during the Cold War era. Although Conquest cited Beria's report to the Politburo of the labor camp numbers at the end of 1938 stating there were almost 7 million prisoners in the labor camps, more than three times the archival figure for 1938 and an official report to Stalin by the Soviet minister of State Security in 1952 stating there were 12 million prisoners in the labor camps.

These data allowed scholars to conclude that during the period of 1928–53, about 14 million prisoners passed through the system of GULAG "labour camps" and 4–5 million passed through the "labour colonies". Thus, these figures reflect the number of convicted persons, and do not take into account the fact that a significant part of Gulag inmates had been convicted more than one time, so the actual number of convicted is somewhat overstated by these statistics. From other hand, during some periods of Gulag history the official figures of GULAG population reflected the camps' capacity, not the actual amount of inmates, so the actual figures were 15% higher in, e.g. 1946.

The Gulag spanned nearly four decades of Soviet and East European history and affected millions of individuals. Its cultural impact was enormous.

The Gulag has become a major influence on contemporary Russian thinking, and an important part of modern Russian folklore. Many songs by the authors-performers known as the "bards", most notably Vladimir Vysotsky and Alexander Galich, neither of whom ever served time in the camps, describe life inside the Gulag and glorified the life of "Zeks". Words and phrases which originated in the labor camps became part of the Russian/Soviet vernacular in the 1960s and 1970s.
The memoirs of Alexander Dolgun, Aleksandr Solzhenitsyn, Varlam Shalamov and Yevgenia Ginzburg, among others, became a symbol of defiance in Soviet society. These writings harshly chastised the Soviet people for their tolerance and apathy regarding the Gulag, but at the same time provided a testament to the courage and resolve of those who were imprisoned.

Another cultural phenomenon in the Soviet Union linked with the Gulag was the forced migration of many artists and other people of culture to Siberia. This resulted in a Renaissance of sorts in places like Magadan, where, for example, the quality of theatre production was comparable to Moscow's and Eddie Rosner played jazz.

Many eyewitness accounts of Gulag prisoners have been published:


Soviet show that the goals of the gulag included colonization of sparsely populated remote areas. To this end, the notion of "free settlement" was introduced.

When well-behaved persons had served the majority of their terms, they could be released for "free settlement" (вольное поселение, "volnoye poseleniye") outside the confinement of the camp. They were known as "free settlers" (вольнопоселенцы, "volnoposelentsy", not to be confused with the term ссыльнопоселенцы,"ssyl'noposelentsy", "exile settlers"). In addition, for persons who served full term, but who were denied the free choice of place of residence, it was recommended to assign them for "free settlement" and give them land in the general vicinity of the place of confinement.

The gulag inherited this approach from the katorga system.

It is estimated that of the 40,000 people collecting state pensions in Vorkuta, 32,000 are trapped former gulag inmates, or their descendants.

Persons who served a term in a camp or in a prison were restricted from taking a wide range of jobs. Concealment of a previous imprisonment was a triable offence. Persons who served terms as "politicals" were nuisances for "First Departments" (, , outlets of the secret police at all enterprises and institutions), because former "politicals" had to be monitored.

Many people released from camps were restricted from settling in larger cities.

Both Moscow and St. Petersburg have memorials to the victims of the Gulag made of boulders from the Solovki camp — the first prison camp in the Gulag system. Moscow's memorial is on Lubyanka Square, the site of the headquarters of the NKVD. People gather at these memorials every year on the Day of Victims of the Repression (October 30).

Moscow has the State Gulag Museum whose first director was Anton Antonov-Ovseyenko. In 2015, another museum dedicated to the Gulag was opened in Moscow.







</doc>
<doc id="12984" url="https://en.wikipedia.org/wiki?curid=12984" title="Geiger counter">
Geiger counter

The Geiger counter is an instrument used for detecting and measuring ionizing radiation used widely in applications such as radiation dosimetry, radiological protection, experimental physics and the nuclear industry.

It detects ionizing radiation such as alpha particles, beta particles and gamma rays using the ionization effect produced in a Geiger–Müller tube; which gives its name to the instrument. In wide and prominent use as a hand-held radiation survey instrument, it is perhaps one of the world's best-known radiation detection instruments.

The original detection principle was discovered in 1908 at the Cavendish laboratory, but it was not until the development of the Geiger-Müller tube in 1928 that the Geiger-Müller counter became a practical instrument. Since then it has been very popular due to its robust sensing element and relatively low cost. However, there are limitations in measuring high radiation rates and the energy of incident radiation.

A Geiger counter consists of a Geiger-Müller tube, the sensing element which detects the radiation, and the processing electronics, which displays the result.

The Geiger-Müller tube is filled with an inert gas such as helium, neon, or argon at low pressure, to which a high voltage is applied. The tube briefly conducts electrical charge when a particle or photon of incident radiation makes the gas conductive by ionization. The ionization is considerably amplified within the tube by the Townsend discharge effect to produce an easily measured detection pulse, which is fed to the processing and display electronics. This large pulse from the tube makes the G-M counter relatively cheap to manufacture, as the subsequent electronics is greatly simplified. The electronics also generates the high voltage, typically 400–900 volts, that has to be applied to the Geiger-Müller tube to enable its operation. To stop the discharge in the geiger tube a little halogen gas or organic material (alcohol) is added to the gas mixture.

To act as a counter the instrument has to have an electronic counter of the pulses from the detector and a timer.
There are two types of radiation readout: counts or radiation dose. The counts display is the simplest and is the number of ionizing events displayed either as a count rate, such as "counts per minute" or "counts per second", or as a total over a set time period (an integrated total). The counts readout is normally used when alpha or beta particles are being detected. More complex to achieve is a display of radiation dose rate, displayed in a unit such as the sievert which is normally used for measuring gamma or X-ray dose rates. A G-M tube can detect the presence of radiation, but not its energy which influences the radiation's ionising effect. Consequently, instruments measuring dose rate require the use of an energy compensated G-M tube, so that the dose displayed relates to the counts detected. The electronics will apply known factors to make this conversion, which is specific to each instrument and is determined by design and calibration.

The readout can be analog or digital, and increasingly, modern instruments are offering serial communications with a host computer or network.

There is usually an option to produce audible representing the number of ionization events detected. This is the distinctive sound normally associated with hand held or portable Geiger counters. The purpose of this is to allow the user to concentrate on manipulation of the instrument whilst retaining auditory feedback on the radiation rate.

There are two main limitations of the Geiger counter. Because the output pulse from a Geiger-Müller tube is always of the same magnitude regardless of the energy of the incident radiation, the tube cannot differentiate between radiation types. A further limitation is the inability to measure high radiation rates due to the "dead time" of the tube. This is an insensitive period after each ionization of the gas during which any further incident radiation will not result in a count, and the indicated rate is, therefore, lower than actual. Typically the dead time will reduce indicated count rates above about 10 to 10 counts per second depending on the characteristic of the tube being used. Whilst some counters have circuitry which can compensate for this, for accurate measurements ion chamber instruments are preferred for high radiation rates.

The intended detection application of a Geiger counter dictates the tube design used. Consequently, there are a great many designs, but they can be generally categorised as "end-window", or windowless "thin-walled", or "thick-walled", and sometimes hybrids of these types.

The first historical uses of the Geiger principle were for the detection of alpha and beta particles, and the instrument is still used for this purpose today. For alpha particles and low energy beta particles, the "end-window" type of G-M tube has to be used as these particles have a limited range even in the free air, and are easily stopped by a solid material. Therefore, the tube requires a window which is thin enough to allow as many as possible of these particles through to the fill gas. The window is usually made of mica with a density of about 1.5 - 2.0 mg/cm.

Alpha particles have the shortest range, and to detect these the window should ideally be within 10 mm of the radiation source due to alpha particle attenuation in free air. However, the G-M tube produces a pulse output which is the same magnitude for all detected radiation, so a Geiger counter with an end window tube cannot distinguish between alpha and beta particles. A skilled operator can use varying distance from radiation source to detector to differentiate between alpha and high energy beta particles, but with the detector in close contact with the radiation source the two types are both detected and are indistinguishable.

The "pancake" Geiger-Muller detector is a variant of the end window probe, but designed with a larger detection area to make checking quicker. However, the pressure of the atmosphere against the low pressure of the fill gas limits the window size due to the limited strength of the window membrane.

Some beta particles can also be detected by a thin-walled "windowless" G-M tube, which has no end window, but allows high energy beta particles to pass through the tube walls. Although the tube walls have a greater stopping power than a thin end window, they still allow these more energetic particles to reach the fill gas.

End-window G-M detectors are still used as a general purpose portable radioactive contamination measurement and detection instrument, owing to their relatively low cost, robustness and their relatively high detection efficiency; particularly with high energy beta particles. However, for discrimination between alpha and beta particles or provision of particle energy information, scintillation counters or proportional counters should be used. Those instrument types are manufactured with much larger detector areas, which means that checking for surface contamination is quicker than with a G-M instrument.

Geiger counters are widely used to detect gamma radiation, and for this the windowless tube is used. However, efficiency is generally low due to the poor interaction of gamma rays compared with alpha and beta particles. For instance, a chrome steel G-M tube is only about 1% efficient over a wide range of energies. Better suited to detect gamma rays is the scintillation detector which has a much greater sensitivity to gamma rays than a geiger detector.

The article on the Geiger-Muller tube carries a more detailed account of the techniques used to detect photon radiation. For high energy gamma it largely relies on interaction of the photon radiation with the tube wall material, usually 1–2 mm of chrome steel on a "thick-walled" tube, to produce electrons within the wall which can enter and ionize the fill gas. This is necessary as the low-pressure gas in the tube has little interaction with high energy gamma photons. However, for low energy photons there is greater gas interaction and the direct gas ionisation effect increases. With decreasing energy the wall effect gives way to a combination of wall effect and direct ionisation, until direct gas ionisation dominates. Due to the variance in response to different photon energies, windowless tubes employ what is known as "energy compensation" which attempts to compensate for these variations over a large energy range.

Low energy photon radiation such as low energy X rays or gamma rays interacts better with the fill gas. Consequently, a typical design for low energy photon detection for these is a long tube with a thin wall or with an end window. The tube has a larger gas volume than a steel walled tube to give an increased chance of particle interaction.

A variation of the Geiger tube is used to measure neutrons, where the gas used is boron trifluoride or helium-3 and a plastic moderator is used to slow the neutrons. This creates an alpha particle inside the detector and thus neutrons can be counted.

The term "Geiger counter" is commonly used to mean a hand-held survey type meter, however the Geiger principle is in wide use in installed "area gamma" alarms for personnel protection, and in process measurement and interlock applications.
A Geiger tube is still the sensing device, but the processing electronics will have a higher degree of sophistication and reliability than that used in a hand held survey meter.

For hand-held units there are two fundamental physical configurations: the "integral" unit with both detector and electronics in the same unit, and the "two-piece" design which has a separate detector probe and an electronics module connected by a short cable.

In the 1930s a mica window was added to the cylindrical design allowing low-penetration radiation to pass through with ease.

The integral unit allows single-handed operation, so the operator can use the other hand for personal security in challenging monitoring positions, but the two piece design allows easier manipulation of the detector, and is commonly used for alpha and beta surface contamination monitoring where careful manipulation of the probe is required or the weight of the electronics module would make operation unwieldy. A number of different sized detectors are available to suit particular situations, such as placing the probe in small apertures or confined spaces.

Gamma and X-Ray detectors generally use an "integral" design so the Geiger–Müller tube is conveniently within the electronics enclosure. This can easily be achieved because the casing usually has little attentuation, and is employed in ambient gamma measurements where distance from the source of radiation is not a significant factor. However, to facilitate more localised measurements such as "surface dose", the position of the tube in the enclosure is sometimes indicated by targets on the enclosure so an accurate measurement can be made with the tube at the correct orientation and a known distance from the surface.

There is a particular type of gamma instrument known as a "hot spot" detector which has the detector tube on the end of a long pole or flexible conduit. These are used to measure high radiation gamma locations whilst protecting the operator by means of distance shielding.

Particle detection of alpha and beta can be used in both integral and two-piece designs. A pancake probe (for alpha/beta) is generally used to increase the area of detection in two-piece instruments whilst being relatively light weight. In integral instruments using an end window tube there is a window in the body of the casing to prevent shielding of particles. There are also hybrid instruments which have a separate probe for particle detection and a gamma detection tube within the electronics module. The detectors are switchable by the operator, depending the radiation type that is being measured.

In the United Kingdom the National Radiological Protection Board issued a user guidance note on selecting the best portable instrument type for the radiation measurement application concerned.. This covers all radiation protection instrument technologies and includes a guide to the use of G-M detectors.

In 1908 Hans Geiger, under the supervision of Ernest Rutherford at the Victoria University of Manchester (now the University of Manchester), developed an experimental technique for detecting alpha particles that would later be used in the Geiger-Müller tube. This early counter was only capable of detecting alpha particles and was part of a larger experimental apparatus. The fundamental ionization mechanism used was discovered by John Sealy Townsend by his work between 1897 and 1901, and is known as the Townsend discharge, which is the ionization of molecules by ion impact.

It was not until 1928 that Geiger and Walther Müller (a PhD student of Geiger) developed the sealed Geiger-Müller tube which developed the basic ionization principles previously used experimentally. This was relatively small and rugged, and could not only detect alpha and beta radiation such as prior models but also gamma radiation. Now a practical radiation instrument could be produced relatively cheaply, and so the Geiger-Muller counter was born. As the tube output required little electronic processing, a distinct advantage in the thermionic valve era due to minimal valve count and low power consumption, the instrument achieved great popularity as a portable radiation detector.

Modern versions of the Geiger counter use the halogen tube invented in 1947 by Sidney H. Liebson. It superseded the earlier Geiger tube because of its much longer life and lower operating voltage, typically 400-900 volts.




</doc>
<doc id="12985" url="https://en.wikipedia.org/wiki?curid=12985" title="General Synod">
General Synod

The General Synod is the title of the governing body of some church organizations. 

In the Church of England, the General Synod, which was established in 1970 (replacing the Church Assembly), is the legislative body of the Church.

In the Episcopal Church in the United States of America, the equivalent is General Convention.

General Synods of other churches within the Anglican Communion

The United Church of Christ in the United States also calls their main governing body a General Synod. It meets every two years and consists of over 600 delegates from various congregations and conferences.

The Associate Reformed Presbyterian Church has as its highest Church court the General Synod. The ARP General Synod meets yearly (in recent years, it has, almost without exception, been held at Bonclarken). The delegates to the General Synod of the ARP Church are the elder representatives elected from each church's Session and all ministers from all presbyteries that comprise the Church (excluding ministers and elders from the independent ARP Synods of Mexico and Pakistan).

The Evangelical Church of Augsburg and Helvetic Confession in Austria and the United Evangelical Lutheran Church of Germany each call their main legislative bodies Generalsynode. In the Evangelical Church in Prussia the legislating body was called Generalsynode between 1846 and 1953.

The governing body of the Reformed Church in America, a Calvinist denomination in the United States and Canada, is known as the General Synod.

"Kirkemøtet", the governing body of the Church of Norway is normally translated to General Synod. It convenes once a year, and consists of 85 representatives, of whom seven or eight are sent from each of the dioceses.

The Batak Christian Protestant Church (BPCP), or "Huria Kristen Batak Protestan" (abbreviated HKBP), held a twice-a-year General Synod (Sinode Godang), to discuss about matters in HKBP, and to elect the new "Ephorus" (or Board) for the HKBP. The first General Synod of HKBP was held in 1922.

In the North American Lutheran tradition, General Synod refers to a church body which existed from 1820–1918. See Evangelical Lutheran General Synod of the United States of North America.




</doc>
