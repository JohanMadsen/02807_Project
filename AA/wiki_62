<doc id="8247" url="https://en.wikipedia.org/wiki?curid=8247" title="Digital synthesizer">
Digital synthesizer

A digital synthesizer is a synthesizer that uses digital signal processing (DSP) techniques to make musical sounds. This in contrast to older analog synthesizers, which produce music using analog electronics, and samplers, which play back digital recordings of acoustic, electric, or electronic instruments. Some digital synthesizers emulate analog synthesizers; others include sampling capability in addition to digital synthesis.

The very earliest digital synthesis experiments were made with computers, as part of academic research into sound generation. In 1973, the Japanese company Yamaha licensed the algorithms for frequency modulation synthesis (FM synthesis) from John Chowning, who had experimented with it at Stanford University since 1971. Yamaha's engineers began adapting Chowning's algorithm for use in a commercial digital synthesizer, adding improvements such as the "key scaling" method to avoid the introduction of distortion that normally occurred in analog systems during frequency modulation, though it would take several years before Yamaha were to release their FM digital synthesizers. In the 1970s, Yamaha were granted a number of patents, under the company's former name "Nippon Gakki Seizo Kabushiki Kaisha", evolving Chowning's early work on FM synthesis technology. Yamaha built the first prototype digital synthesizer in 1974.

Released in 1979, the Casio VL-1 was the first commercial digital synthesizer, selling for $69.95. Yamaha eventually commercialized their FM synthesis technology and released the first FM digital synthesizer in 1980, the Yamaha GS-1, but at an expensive retail price of $16,000.

Early commercial digital synthesizers used simple hard-wired digital circuitry to implement techniques such as additive synthesis and FM synthesis. Other techniques, such as wavetable synthesis and physical modeling, only became possible with the advent of high-speed microprocessor and digital signal processing technology. Two other early commercial digital synthesizers were the Fairlight CMI, introduced in 1979, and the New England Digital Synclavier II, introduced in 1980. The Fairlight CMI was a sampling synthesizer, while the Synclavier originally used FM synthesis technology licensed from Yamaha, before adding sampling synthesis later in the 1980s. The Fairlight CMI and the Synclavier were both expensive systems, retailing for more than $20,000 in the early 1980s. The cost of digital synthesizers began falling rapidly in the early 1980s. E-mu Systems introduced the Emulator sampling synthesizer in 1982 at a retail price of $7,900. Although not as flexible or powerful as either the Fairlight CMI or the Synclavier, its lower cost and portability made it popular.

Introduced in 1983, the Yamaha DX7 was the breakthrough digital synthesizer to have a major impact, both innovative and affordable, and thus spelling the decline of analog synthesizers. It used FM synthesis and, although it was incapable of the sampling synthesis of the Fairlight CMI, its price was around $2,000, putting it within range of a much larger number of musicians. The DX-7 was also known for its "key scaling" method to avoid distortion and for its recognizabley bright tonality that was partly due to its high sampling rate of 57 kHz. It became indispensable to many music artists of the 1980s, and would become one of the best-selling synthesizers of all time.

In 1987, Roland released its own influential synthesizer of the time, the D-50. This popular synth broke new ground in affordably combining short samples and digital oscillators, as well as the innovation of built-in digital effects (reverb., chorus, equalizer). Roland called this Linear Arithmetic (LA) synthesis. This instrument is responsible for some of the very recognisable preset synthesizer sounds of the late 1980s, such as the Pizzagogo sound used on Enya's "Orinoco Flow."

It gradually became feasible to include high quality samples of existing instruments as opposed to synthesizing them. In 1988, Korg introduced the last of the hugely popular trio of digital synthesizers of the 1980s after the DX7 and D50, the M1. This heralded both the increasing popularisation of digital sample-based synthesis, and the rise of 'workstation' synthesizers. After this time, many popular modern digital synthesizers have been described as not being full synthesizers in the most precise sense, as they play back samples stored in their memory. However, they still include options to shape the sounds through use of envelopes, LFOs, filters and effects such as reverb. The Yamaha Motif and Roland Fantom series of keyboards are typical examples of this type, described as 'ROMplers' ; at the same time, they are also examples of "workstation" synthesizers.
With the addition of sophisticated sequencers on board, now added to built-in effects and other features, the 'workstation' synthesizer had been born. These always include a multi-track sequencer, and can often record and playback samples, and in later years full audio tracks, to be used to record an entire song. These are usually also ROMplers, playing back samples, to give a wide variety of realistic instrument and other sounds such as drums, string instruments and wind instruments to sequence and compose songs, along with popular keyboard instrument sounds such as electric pianos and organs.

As there was still interest in analog synthesizers, and with the increase of computing power, over the 1990s another type of synthesizer arose : the analog modeling, or "virtual analog" synthesizer. These use computing power to simulate traditional analog waveforms and circuitry such as envelopes and filters, with the most popular examples of this type of instrument including the Nord Lead and Access Virus.

As the cost of processing power and memory fell, new types of synthesizers emerged, offering a variety of novel sound synthesis options. The Korg Oasys was one such example, packaging multiple digital synthesizers into a single unit.

Digital synthesizers can now be completely emulated in software ("softsynth"), and run on conventional PC hardware. Such soft implementations require careful programming and a fast CPU to get the same latency response as their dedicated equivalents. To reduce latency, some professional sound card manufacturers have developed specialized Digital Signal Processing ([DSP]) hardware. Dedicated digital synthesizers have the advantage of a performance-friendly user interface (physical controls like buttons for selecting features and enabling functionality, and knobs for setting variable parameters). On the other hand, software synthesizers have the advantages afforded by a rich graphical display.

With focus on performance-oriented keyboards and digital computer technology, manufacturers of commercial electronic instruments created some of the earliest digital synthesizers for studio and experimental use with computers being able to handle built-in sound synthesis algorithms.

The main difference is that a digital synthesizer uses digital processors and analog synthesizers use analog circuitry. A digital synthesizer is in essence a computer with (often) a piano-keyboard and an LCD as an interface. An analog synthesizer is made up of sound-generating circuitry and modulators. Because computer technology is rapidly advancing, it is often possible to offer more features in a digital synthesizer than in an analog synthesizer at a given price. However, both technologies have their own merit. Some forms of synthesis, such as, for instance, sampling and additive synthesis are not feasible in analog synthesizers, while on the other hand, many musicians prefer the character of analog synthesizers over their digital equivalent.

The New wave era of the 1980s first brought the digital synthesizer to the public ear. Bands like Talking Heads and Duran Duran used the digitally made sounds on some of their most popular albums. Other more pop-inspired bands like Hall & Oates began incorporating the digital synthesizer into their sound in the 1980s. Through breakthroughs in technology in the 1990s many modern synthesizers use DSP.

Working in more or less the same way, every digital synthesizer appears similar to a computer. At a steady sample rate, digital synthesis produces a stream of numbers. Sound from speakers is then produced by a conversion to analog form. Through signal generation, voice and instrument-level processing, a signal flow is created and controlled either by MIDI capabilities or voice and instrument-level controls.



</doc>
<doc id="8249" url="https://en.wikipedia.org/wiki?curid=8249" title="Definition of music">
Definition of music

A definition of music endeavors to give an accurate and concise explanation of music's basic attributes or essential nature and it involves a process of defining what is meant by the term "music". Many authorities have suggested definitions, but defining music turns out to be more difficult than might first be imagined and there is ongoing debate. A number of explanations start with the notion of music as "organized sound" but they also highlight that this is perhaps too broad a definition and cite examples of organized sound that are not defined as music, such as human speech, and sounds found in both natural and industrial environments . The problem of defining music is further complicated by the influence of culture in music cognition. 

The "Concise Oxford Dictionary" defines music as "the art of combining vocal or instrumental sounds (or both) to produce beauty of form, harmony, and expression of emotion" . However, the music genres known as noise music and musique concrète, for instance, challenge these ideas about what constitutes music's essential attributes by using sounds not widely considered as musical, like randomly produced electronic distortion, feedback, static, cacophony, and compositional processes using indeterminacy (; ).

An oft cited notable example of the dilemma in defining music is a work entitled "4'33"" (1952) by the American composer John Cage (1912–1992). The written score has three movements and directs the performer(s) to appear on stage, indicate by gesture or other means when the piece begins, then make no sound and only mark sections and the end by gesture. What is heard are only whatever ambient sounds may occur in the room. Some argue this is not music because, for example, it contains no sounds that are conventionally considered "musical" and the composer and performer(s) exert no control over the organization of the sounds heard . Others argue it is music because the conventional definitions of musical sounds are unnecessarily and arbitrarily limited, and control over the organization of the sounds is achieved by the composer and performer(s) through their gestures that divide what is heard into specific sections and a comprehensible form .

Because of differing fundamental concepts of music, the languages of many cultures do not contain a word that can be accurately translated as "music," as that word is generally understood by Western cultures . Inuit and most North American Indian languages do not have a general term for music. Among the Aztecs, the ancient Mexican theory of rhetorics, poetry, dance, and instrumental music used the Nahuatl term "In xochitl-in kwikatl" to refer to a complex mix of music and other poetic verbal and non-verbal elements, and reserve the word "Kwikakayotl" (or cuicacayotl) only for the sung expressions . In Africa there is no term for music in Tiv, Yoruba, Igbo, Efik, Birom, Hausa, Idoma, Eggon or Jarawa. Many other languages have terms which only partly cover what Western culture typically means by the term "music" (). The Mapuche of Argentina do not have a word for "music", but they do have words for instrumental versus improvised forms ("kantun"), European and non-Mapuche music ("kantun winka"), ceremonial songs ("öl"), and "tayil" .

While some languages in West Africa have no term for music, some West African languages accept the general concepts of music (). "Musiqi" is the Persian word for the science and art of music, "muzik" being the sound and performance of music (), though some things European-influenced listeners would include, such as Quran chanting, are excluded.

Ben Watson points out that Ludwig van Beethoven's "Grosse Fuge" (1825) "sounded like noise" to his audience at the time. Indeed, Beethoven's publishers persuaded him to remove it from its original setting as the last movement of a string quartet. He did so, replacing it with a sparkling "Allegro". They subsequently published it separately . Musicologist Jean-Jacques Nattiez considers the difference between noise and music nebulous, explaining that "The border between music and noise is always culturally defined—which implies that, even within a single society, this border does not always pass through the same place; in short, there is rarely a consensus ... By all accounts there is no "single" and "intercultural" universal concept defining what music might be" .

An often-cited definition of music is that it is "organized sound", a term originally coined by modernist composer Edgard Varèse in reference to his own musical aesthetic. Varèse's concept of music as "organized sound" fits into his vision of "sound as living matter" and of "musical space as open rather than bounded" . He conceived the elements of his music in terms of "sound-masses", likening their organization to the natural phenomenon of crystallization . Varèse thought that "to stubbornly conditioned ears, anything new in music has always been called noise", and he posed the question, "what is music but organized noises?" .

The fifteenth edition of the "Encyclopædia Britannica" states that "while there are no sounds that can be described as inherently unmusical, musicians in each culture have tended to restrict the range of sounds they will admit." A human organizing element is often felt to be implicit in music (sounds produced by non-human agents, such as waterfalls or birds, are often described as "musical", but perhaps less often as "music"). The composer R. Murray states that the sound of classical music "has decays; it is granular; it has attacks; it fluctuates, swollen with impurities—and all this creates a musicality that comes before any 'cultural' musicality." However, in the view of semiologist Jean-Jacques Nattiez, "just as music is whatever people choose to recognize as such, noise is whatever is recognized as disturbing, unpleasant, or both" . (See "music as social construct" below.)""'

Levi R. Bryant defines music not as a language, but as a marked-based, problem-solving method such as mathematics .

Most definitions of music include a reference to sound and a list of universals of music can be generated by stating the elements (or aspects) of sound: pitch, timbre, loudness, duration, spatial location and texture (). However, in terms more specifically relating to music: following Wittgenstein, cognitive psychologist Eleanor Rosch proposes that categories are not clean cut but that something may be more or less a member of a category . As such the search for musical universals would fail and would not provide one with a valid definition . This is primarily because other cultures have different understandings in relation to the sounds that English language writers refer to as music.

Many people do, however, share a general idea of music. The Websters definition of music is a typical example: "the science or art of ordering tones or sounds in succession, in combination, and in temporal relationships to produce a composition having unity and continuity" ("Webster's Collegiate Dictionary", online edition).

This approach to the definition focuses not on the "construction" but on the "experience" of music. An extreme statement of the position has been articulated by the Italian composer Luciano Berio: “Music is everything that one listens to with the intention of listening to music” . This approach permits the boundary between music and noise to change over time as the conventions of musical interpretation evolve within a culture, to be different in different cultures at any given moment, and to vary from person to person according to their experience and proclivities. It is further consistent with the subjective reality that even what would commonly be considered music is experienced as nonmusic if the mind is concentrating on other matters and thus not perceiving the sound's "essence" "as music" .

In his 1983 book, "Music as Heard", which sets out from the phenomenological position of Husserl, Merleau-Ponty, and Ricœur, Thomas Clifton defines music as "an ordered arrangement of sounds and silences whose meaning is presentative rather than denotative. . . . This definition distinguishes music, as an end in itself, from compositional technique, and from sounds as purely physical objects." More precisely, "music is the actualization of the possibility of any sound whatever to present to some human being a meaning which he experiences with his body—that is to say, with his mind, his feelings, his senses, his will, and his metabolism" . It is therefore "a certain reciprocal relation established between a person, his behavior, and a sounding object" .

Clifton accordingly differentiates music from non-music on the basis of the human behavior involved, rather than on either the nature of compositional technique or of sounds as purely physical objects. Consequently, the distinction becomes a question of what is meant by musical behavior: "a musically behaving person is one whose very being is absorbed in the significance of the sounds being experienced." However, "It is not altogether accurate to say that this person is listening "to" the sounds. First, the person is doing more than listening: he is perceiving, interpreting, judging, and feeling. Second, the preposition 'to' puts too much stress on the sounds as such. Thus, the musically behaving person experiences musical significance by means of, or through, the sounds" .

In this framework, Clifton finds that there are two things that separate music from non-music: (1) musical meaning is presentative, and (2) music and non-music are distinguished in the idea of personal involvement. "It is the notion of personal involvement which lends significance to the word "ordered" in this definition of music" . This is not to be understood, however, as a sanctification of extreme relativism, since "it is precisely the 'subjective' aspect of experience which lured many writers earlier in this century down the path of sheer opinion-mongering. Later on this trend was reversed by a renewed interest in 'objective,' scientific, or otherwise non-introspective musical analysis. But we have good reason to believe that a musical experience is not a purely private thing, like seeing pink elephants, and that reporting about such an experience need not be subjective in the sense of it being a mere matter of opinion" .

Clifton's task, then, is to describe musical experience and the objects of this experience which, together, are called "phenomena," and the activity of describing phenomena is called "phenomenology" . It is important to stress that this definition of music says nothing about aesthetic standards. Music is not a fact or a thing in the world, but a meaning constituted by human beings. . . . To talk about such experience in a meaningful way demands several things. First, we have to be willing to let the composition speak to us, to let it reveal its own order and significance. . . . Second, we have to be willing to question our assumptions about the nature and role of musical materials. . . . Last, and perhaps most important, we have to be ready to admit that describing a meaningful experience is itself meaningful. 

"Music, often an art/entertainment, is a total social fact whose definitions vary according to era and culture," according to Jean . It is often contrasted with noise. According to musicologist Jean-Jacques Nattiez: "The border between music and noise is always culturally defined—which implies that, even within a single society, this border does not always pass through the same place; in short, there is rarely a consensus... By all accounts there is no "single" and "intercultural" universal concept defining what music might be" . Given the above demonstration that "there is no limit to the number or the genre of variables that might intervene in a definition of the musical," an organization of definitions and elements is necessary.

Nattiez (1990, 17) describes definitions according to a tripartite semiological scheme similar to the following:
There are three levels of description, the poietic, the neutral, and the esthesic:

Table describing types of definitions of music :
Because of this range of definitions, the study of music comes in a wide variety of forms. There is the study of sound and vibration or acoustics, the cognitive study of music, the study of music theory and performance practice or music theory and ethnomusicology and the study of the reception and history of music, generally called musicology.

Composer Iannis Xenakis in "Towards a Metamusic" (chapter 7 of "Formalized Music") defined music in the following way :





</doc>
<doc id="8253" url="https://en.wikipedia.org/wiki?curid=8253" title="Dayton, Ohio">
Dayton, Ohio

Dayton () is the sixth-largest city in the state of Ohio and the county seat of Montgomery County. A small part of the city extends into Greene County. The 2017 U.S. census estimate put the city population at 140,371, while Greater Dayton was estimated to be at 803,416 residents. This makes Dayton the fourth-largest metropolitan area in Ohio and 63rd in the United States. Dayton is within Ohio's Miami Valley region, just north of the Greater Cincinnati.

Ohio's borders are within of roughly 60 percent of the country's population and manufacturing infrastructure, making the Dayton area a logistical centroid for manufacturers, suppliers, and shippers. Dayton also hosts significant research and development in fields like industrial, aeronautical, and astronautical engineering that have led to many technological innovations. Much of this innovation is due in part to Wright-Patterson Air Force Base and its place in the community. With the decline of heavy manufacturing, Dayton's businesses have diversified into a service economy that includes insurance and legal sectors as well as healthcare and government sectors.

Along with defense and aerospace, healthcare accounts for much of the Dayton area's economy. Hospitals in the Greater Dayton area have an estimated combined employment of nearly 32,000 and a yearly economic impact of $6.8 billion. It is estimated that Premier Health Partners, a hospital network, contributes more than $2 billion a year to the region through operating, employment, and capital expenditures. In 2011, Dayton was rated the #3 city in the nation by HealthGrades for excellence in healthcare. Many hospitals in the Dayton area are consistently ranked by "Forbes", "U.S. News & World Report", and HealthGrades for clinical excellence.

Dayton is also noted for its association with aviation; the city is home to the National Museum of the United States Air Force and is the birthplace of Orville Wright. Other well-known individuals born in the city include poet Paul Laurence Dunbar and entrepreneur John H. Patterson. Dayton is also known for its many patents, inventions, and inventors, most notably the Wright brothers' invention of powered flight. In 2008, 2009, and 2010, "Site Selection" magazine ranked Dayton the #1 mid-sized metropolitan area in the nation for economic development. Also in 2010, Dayton was named one of the best places in the United States for college graduates to find a job.

Dayton was founded on April 1, 1796, by 12 settlers known as the Thompson Party. They traveled in March from Cincinnati up the Great Miami River by pirogue and landed at what is now St. Clair Street, where they found two small camps of Native Americans. Among the Thompson Party was Benjamin Van Cleve, whose memoirs provide insights into the Ohio Valley's history. Two other groups traveling overland arrived several days later.

In 1797, Daniel C. Cooper laid out Mad River Road, the first overland connection between Cincinnati and Dayton, opening the "Mad River Country" to settlement. Ohio was admitted into the Union in 1803, and the city of Dayton was incorporated in 1805. The city was named after Jonathan Dayton, a captain in the American Revolutionary War who signed the U.S. Constitution and owned a significant amount of land in the area. In 1827, construction on the Dayton-Cincinnati canal began, which would provide a better way to transport goods from Dayton to Cincinnati and contribute significantly to Dayton's economic growth during the 1800s.

Innovation led to business growth in the region. In 1884, John Henry Patterson acquired James Ritty's National Manufacturing Company along with his cash register patents and formed the National Cash Register Company (NCR). The company manufactured the first mechanical cash registers and played a crucial role in the shaping of Dayton's reputation as an epicenter for manufacturing in the early 1900s. In 1906, Charles F. Kettering, a leading engineer at the company, helped develop the first electric cash register, which propelled NCR into the national spotlight. NCR also helped develop the US Navy Bombe, a code-breaking machine that helped crack the Enigma machine cipher during World War II.

Dayton has been the home for many patents and inventions since the 1870s. According to the National Park Service, citing information from the U.S. Patent Office, Dayton had granted more patents per capita than any other U.S. city in 1890 and ranked fifth in the nation as early as 1870. The Wright brothers, inventors of the airplane, and Charles F. Kettering, world-renowned for his numerous inventions, hailed from Dayton. The city was also home to James Ritty's Incorruptible Cashier, the first mechanical cash register, and Arthur E. Morgan's hydraulic jump, a flood prevention mechanism that helped pioneer hydraulic engineering. Paul Laurence Dunbar, an African-American poet and novelist, penned his most famous works in the late 19th century and became an integral part of the city's history.
Powered aviation began in Dayton. Orville and Wilbur Wright were the first to construct and demonstrate powered flight. Although the first flight was in Kitty Hawk, North Carolina, their Wright Flyer was built in Dayton, and was returned to Dayton for improvements and further flights at Huffman Field, a cow pasture eight miles (13 km) northeast of Dayton, near the current Wright Patterson Air Force Base.

When the government tried to move development to Langley field in southern Virginia, six Dayton businessmen including Edward A. Deeds, formed the Dayton-Wright Airplane Company in Moraine and established a flying field. Deeds also opened a field to the north in the flood plain of the Great Miami River between the confluences of that river, the Stillwater River, and the Mad River, near downtown Dayton. Later named McCook Field for Alexander McDowell McCook, an American Civil War general, this became the Army Signal Corps' primary aviation research and training location. Wilbur Wright also purchased land near Huffman prairie to continue their research.

During World War I, the Army purchased 40 acres adjacent to Huffman Prairie for the Fairfield Aviation General Supply Depot. As airplanes developed more capability, they needed more runway space than McCook could offer, and a new location was sought. The Patterson family formed the Dayton Air Service Committee, Inc which held a campaign that raised $425,000 in two days and purchased 4,520.47 acres (18.2937 km2) northeast of Dayton, including Wilbur Wright Field and the Huffman Prairie Flying Field. Wright Field was "formally dedicated" on 12 October 1927. After World War II, Wright Field and the adjacent Patterson Field, Dayton Army Air Field, and Clinton Army Air Field were merged as an the Headquarters, Air Force Technical Base. On 13 January 1948, the facility was renamed Wright-Patterson Air Force Base. 

A catastrophic flood in March 1913, known as the Great Dayton Flood, led to the creation of the Miami Conservancy District, a series of dams and hydraulic jumps installed around Dayton, in 1914. Like other cities across the country, Dayton was heavily involved in the war effort during World War II. Several locations around the city hosted the Dayton Project, a branch of the larger Manhattan Project, to develop polonium triggers used in early atomic bombs. The war efforts led to a manufacturing boom throughout the city, including high demand for housing and other services. At one point, emergency housing was put into place due to a housing shortage in the region, much of which is still in use today.

Between the 1940s and the 1970s, the city saw significant growth in suburban areas from population migration. Veterans were returning from military service in large numbers seeking industrial and manufacturing jobs, a part of the local industry that was expanding rapidly. Advancements in architecture also contributed to the suburban boom. New, modernized shopping centers and the Interstate Highway System allowed workers to commute greater distances and families to live further from the downtown area. More than 127,000 homes were built in Montgomery County during the 1950s.

Since the 1980s, however, Dayton's population has declined, mainly due to the loss of manufacturing jobs and decentralization of metropolitan areas, as well as the national housing crisis that began in 2008. While much of the state has suffered for similar reasons, the impact on Dayton has been greater than most. Dayton had the third-greatest percentage loss of population in the state since the 1980s, behind Cleveland and Youngstown. Despite this, Dayton has begun diversifying its workforce from manufacturing into other growing sectors such as healthcare and education.

Downtown expansion that began in the 2000s has helped revitalize the city and encourage growth. Fifth Third Field, home of the Dayton Dragons, was built in 2000. The highly successful minor league baseball team has been an integral part of Dayton's culture. In 2001, the city's public park system, Five Rivers MetroParks, built RiverScape MetroPark, an outdoor entertainment venue that attracts more than 400,000 visitors each year. A new performance arts theater, the Schuster Center, opened in 2003. A large health network in the region, Premier Health Partners, expanded its Miami Valley Hospital with a 12-story tower addition.

In 2010, the Downtown Dayton Partnership, in cooperation with the City of Dayton and community leaders, introduced the Greater Downtown Dayton Plan. It focuses on job creation and retention, infrastructure improvements, housing, recreation, and collaboration. The plan is to be implemented through the year 2020.

In 1995, the Dayton Agreement, a peace accord between the parties to the hostilities of the conflict in Bosnia-Herzegovina and the former Yugoslavia, was negotiated at Wright-Patterson Air Force Base, near Fairborn, Ohio, from November 1 to 21.

Richard Holbrooke wrote about this event in his memoirs:

Dayton is known as the "Gem City." The nickname's origin is uncertain, but several theories exist. In the early 19th century, a well-known racehorse named Gem hailed from Dayton. In 1845, an article published in the "Cincinnati Daily Chronicle" by an author known as T stated: 
In the late 1840s, Major William D. Bickham of the "Dayton Journal" began a campaign to nickname Dayton the "Gem City." The name was adopted by the city's Board of Trade several years later. Paul Laurence Dunbar referred to the nickname in his poem, "Toast to Dayton," as noted in the following excerpt:
<poem>
She shall ever claim our duty,
For she shines—the brightest gem
That has ever decked with beauty
</poem>

Dayton also plays a role in a nickname given to the state of Ohio, "Birthplace of Aviation." Dayton is the hometown of the Wright brothers, aviation pioneers who are credited with inventing and building the world's first successful airplane. After their first manned flights in Kitty Hawk, North Carolina, which they had chosen due to its ideal weather and climate conditions, the Wrights returned to Dayton and continued testing at nearby Huffman Prairie.

Additionally, Dayton is colloquially referred to as "Little Detroit". This nickname comes from Dayton's prominence as a Midwestern manufacturing center.

According to the United States Census Bureau, the city has a total area of , of which is land and is water.

Dayton's climate features hot, muggy summers and cold, dry winters, and is classified as a humid continental climate (Köppen "Dfa"), using the isotherm. Unless otherwise noted, all normal figures quoted within the text below are from the official climatology station, Dayton International Airport, at an elevation of about to the north of downtown Dayton, which lies within the valley of the Miami River; thus temperatures there are typically cooler than in downtown.

At the airport, monthly mean temperatures range from in January to in July. The highest temperature ever recorded in Dayton was on July 22, 1901, and the coldest was on February 13 during the Great Blizzard of 1899. On average, there are 14 days of + highs and 4.5 nights of sub- lows annually. Snow is moderate, with a normal seasonal accumulation of , usually occurring from November to March, occasionally April, and rarely October. Precipitation averages annually, with total rainfall peaking in May.

Dayton is subject to severe weather typical of the Midwestern United States. Tornadoes are possible from the spring to the fall. Floods, blizzards, and severe thunderstorms can also occur.

Dayton's population declined significantly from a peak of 262,332 residents in 1960 to only 141,759 in 2010. This was in part due to the slowdown of the region's manufacturing and the growth of Dayton's affluent suburbs including Oakwood, Englewood, Beavercreek, Springboro, Miamisburg, Kettering, and Centerville. The city's most populous ethnic group, white, declined from 78.1% in 1960 to 51.7% by 2010. However, recent census estimates show a 1.3% population increase since 2010, the first increase in five decades.

As of the 2000 census, the median income for a household in the city was $27,523, and the median income for a family was $34,978. Males had a median income of $30,816 versus $24,937 for females. The per capita income for the city was $34,724. About 18.2% of families and 23.0% of the population were below the poverty line, including 32.0% of those under age 18 and 15.3% of those age 65 or over.

As of the 2010 census, there were 141,759 people, 58,404 households, and 31,064 families residing in the city. The population density was . There were 74,065 housing units at an average density of . The racial makeup of the city was 51.7% White, 42.9% African American, 0.3% Native American, 0.9% Asian, 1.3% from other races, and 2.9% from two or more races. Hispanic or Latino of any race were 3.0% of the population.

There were 58,404 households, of which 28.3% had children under the age of 18 living with them, 25.9% were married couples living together, 21.4% had a female householder with no husband present, 5.9% had a male householder with no wife present, and 46.8% were non-families. 38.8% of all households were made up of individuals, and 11.2% had someone living alone who was 65 years of age or older. The average household size was 2.26, and the average family size was 3.03.

The median age in the city was 34.4 years. 22.9% of residents were under the age of 18; 14.2% were between the ages of 18 and 24; 25.3% were from 25 to 44; 25.8% were from 45 to 64, and 11.8% were 65 years of age or older. The gender makeup of the city was 48.7% male and 51.3% female.

The 2013 census population estimate showed an increasing city of Dayton population for the first time in five decades, attributed to revitalization efforts downtown and the increasing downtown population. However, the 2014 population estimate indicates a net decrease of 897 individuals from 2013's estimate.

Dayton's economy is relatively diversified and vital to the overall economy of the state of Ohio. In 2008 and 2009, "Site Selection" magazine ranked Dayton the #1 medium-sized metropolitan area in the U.S. for economic development. Dayton is also among the top 100 metropolitan areas in both exports and export-related jobs, ranked 16 and 14 respectively by the Brookings Institution. The 2010 report placed the value of exports at $4.7 billion and the number of export-related jobs at 44,133. The Dayton Metropolitan Statistical Area ranks 4th in Ohio's Gross Domestic Product with a 2008 industry total of $33.78 billion. Additionally, Dayton ranks third among 11 major metropolitan areas in Ohio for exports to foreign countries. The Dayton Development Coalition is attempting to leverage the region's large water capacity, estimated to be 1.5 trillion gallons of renewable water aquifers, to attract new businesses. Moody's Investment Services revised Dayton's bond rating from A1 to the stronger rating of Aa2 as part of its global recalibration process. Standard & Poor's upgraded Dayton's rating from A+ to AA- in the summer of 2009.

"Bloomberg Businessweek" ranked Dayton in 2010 as one of the best places in the U.S. for college graduates looking for a job. Companies such as Reynolds and Reynolds, CareSource, DPL, LexisNexis, Kettering Health Network, Premier Health Partners, and Standard Register have their headquarters in Dayton. It is also the former home of the Speedwell Motor Car Company, MeadWestvaco (formerly known as the Mead Paper Company), and NCR. NCR was headquartered in Dayton for over 125 years and was a major innovator in computer technology.

The Dayton region gave birth to aviation and is known for its high concentration of aerospace and aviation technology. In 2009, Governor Ted Strickland designated Dayton as Ohio's aerospace innovation hub, the state's first such technology hub. Two major United States research and development organizations have leveraged Dayton's historical leadership in aviation and maintain their headquarters in the area: The National Air and Space Intelligence Center (NASIC) and the Air Force Research Laboratory (AFRL). NASIC is the U.S. military's primary producer of intelligence on foreign air and space forces, weapons, and systems, while the AFRL provides leading-edge warfighting capabilities. Both have their headquarters at Wright-Patterson Air Force Base. Wright-Patterson Air Force Base is one of the Air Force's largest air base wings. The installation generated a total economic impact in the Dayton area of $4.67 billion in fiscal year 2011, a decline from $5.1 billion in fiscal year 2009. In addition, state officials are working to make the Dayton region a hub and a leader for UAV research and manufacturing.
Several research organizations support NASIC, AFRL, and the Dayton community. The Advanced Technical Intelligence Center is a confederation of government, academic, and industry partners that leverage advanced technical intelligence expertise. daytaOhio is a non-profit organization based at Wright State University in Dayton, which also hosts five Ohio Centers of Excellence, one of which is the Knowledge Enabled Computing (Kno.e.sis) center, which specializes in making technical advances in computer science areas such as semantics and big data. The University of Dayton Research Institute (UDRI) is led by the University of Dayton. In 2004 and 2005, UDRI was ranked #2 in the nation by the National Science Foundation in federal and industry-funded materials research. The Cognitive Technologies Division (CTD) of Applied Research Associates, Inc., which carries out human-centered research and design, is headquartered in the Dayton suburb of Fairborn. The city of Dayton has started Tech Town, a development project to attract technology-based firms and revitalize the downtown area. Tech Town is home to the world's first RFID business incubator. The University of Dayton-led Institute for Development & Commercialization of Sensor Technologies (IDCAST) at TechTown is a world-class center for excellence in remote sensing and sensing technology. It is one of Dayton's technology business incubators housed in The Entrepreneurs Center building.

The Kettering Health Network and Premier Health Partners have a major role on the Dayton area's economy. Hospitals in the Greater Dayton area have an estimated combined employment of nearly 32,000 and a yearly economic impact of $6.8 billion. In addition, several Dayton area hospitals consistently earn top national ranking and recognition including the "U.S. News & World Report"'s list of "America's Best Hospitals" as well as many of HealthGrades top ratings. The most notable hospitals are Miami Valley Hospital and Kettering Medical Center. In 2011, the Dayton area was rated number three in the nation by HealthGrades for excellence in healthcare. Also in 2011, Dayton was ranked the fourth best in the nation for emergency medicine care. Then in 2013, HealthGrades ranked the Dayton region number one in the nation for the lowest hospital mortality rate.

The Dayton region has several key institutes and centers for health care. The Center for Tissue Regeneration and Engineering at Dayton focuses on the science and development of human tissue regeneration. The National Center for Medical Readiness (NCMR) is also in the Dayton area. The center includes Calamityville, which is a state-of-the-art disaster training facility. Over a period of five years, Calamityville is estimated to have a regional economic impact of $374 million. Also, the Neurological Institute at Miami Valley Hospital is an institute focused on the diagnosis, treatment, and research of neurological disorders.

According to Dayton's 2011 Comprehensive Annual Financial Report, the top employers in the city proper are:

The Dayton City Commission is composed of the mayor and four city commissioners. Each city commission member is elected at-large on a non-partisan basis for four-year, overlapping terms. All policy items are decided by the City Commission, which is empowered by the City Charter to pass ordinances and resolutions, adopt regulations, and appoint the city manager. The city manager is responsible for budgeting and implementing policies and initiatives. Dayton was the first large American city to adopt the city manager form of municipal government, in 1913.

Unlike many Midwestern cities its age, Dayton has very broad and straight downtown streets (generally two or three full lanes in each direction) that improved access to the downtown even after the automobile became popular. The main reason for the broad streets was that Dayton was a marketing and shipping center from its beginning; streets were broad to enable wagons drawn by teams of three to four pairs of oxen to turn around. Also, some of today's streets were once barge canals flanked by draw-paths.
A courthouse building was built in downtown Dayton in 1888 to supplement Dayton's original Neoclassical courthouse, which still stands. This second, "new" courthouse has since been replaced with new facilities as well as a park. The Old Court House has been a favored political campaign stop. On September 17, 1859, Abraham Lincoln delivered an address on its steps. Eight other presidents have visited the courthouse, either as presidents or during presidential campaigns: Andrew Johnson, James Garfield, John F. Kennedy, Lyndon B. Johnson, Richard Nixon, Gerald Ford, Ronald Reagan, and Bill Clinton.

In 2009, the CareSource Management Group finished construction of a $55 million corporate headquarters in downtown Dayton. The , 10-story building was downtown's first new office tower in more than a decade.

Dayton's two tallest buildings are the Kettering Tower at and the KeyBank Tower at . Kettering Tower was originally Winters Tower, the headquarters of Winters Bank. The building was renamed after Virginia Kettering when Winters was merged into BankOne. KeyBank Tower was known as the MeadWestvaco Tower before KeyBank gained naming rights to the building in 2008.

Ted Rall said in 2015 that over the last five decades Dayton has been demolishing some of its architecturally significant buildings to reduce the city's rental vacancy rate and thus increase the occupancy rate.

Dayton's ten historic neighborhoods — Oregon District, Wright Dunbar, Dayton View, Grafton Hill, McPherson Town, Webster Station, Huffman, Kenilworth, St. Anne's Hill, and South Park — feature mostly single-family houses and mansions in the Neoclassical, Jacobethan, Tudor Revival, English Gothic, Chateauesque, Craftsman, Queen Anne, Georgian Revival, Colonial Revival, Renaissance Revival Architecture, Shingle Style Architecture, Prairie, Mission Revival, Eastlake/Italianate, American Foursquare, and Federal styles. Downtown Dayton is also a large area that encompasses several neighborhoods itself and has seen a recent uplift and revival.

Dayton's suburbs with a population of 10,000 or more include Beavercreek, Centerville, Clayton, Englewood, Fairborn, Harrison Township, Huber Heights, Kettering, Miami Township, Miamisburg, Oakwood, Riverside, Springboro (partial), Trotwood, Vandalia, Washington Township, West Carrollton, and Xenia.

Dayton is an outdoor city with an extensive bicycle and jogging trail system, five rivers, lakes and camping areas.

In cooperation with the Miami Conservancy District, Five Rivers MetroParks hosts 340 miles of paved trails, the largest network of paved off-street trails in the United States. In 2010, the city of Troy was named "bike friendly" by the League of American Bicyclists, which gave the city the organization's bronze designation. The honorable mention made Dayton one of two cities in Ohio to receive the award, the other being Columbus, and one of 15 cities nationwide.

The Dayton Region ranked within the top 10% in the nation in arts and culture. In a 2012 readers' poll by "American Style" magazine, Dayton ranked #2 in the country among mid-size cities as an arts destination, ranking higher than larger cities such as Atlanta, St. Louis, and Cincinnati. Dayton is the home of the Dayton Art Institute.

The Benjamin and Marian Schuster Performing Arts Center in downtown Dayton is a world-class performing arts center and the home venue of the Dayton Philharmonic Orchestra, Dayton Opera, and the Dayton Ballet. In addition to philharmonic and opera performances, the Schuster Center hosts concerts, lectures, and traveling Broadway shows, and is a popular spot for weddings and other events. The historic Victoria Theatre in downtown Dayton hosts concerts, traveling Broadway shows, ballet, a summertime classic film series, and more. The Loft Theatre, also downtown, is the home of the Human Race Theatre Company. The Dayton Playhouse, in West Dayton, is the site of numerous plays and theatrical productions. Between 1957 and 1995, the Kenley Players presented live theater productions in Dayton. In 2013, John Kenley was inducted into the Dayton Theatre Hall of Fame.

Dayton is the home to several ballet companies including:

The city's fine dining restaurants include The Pine Club, a nationally known steakhouse. Dayton is home to a variety of pizza chains that have become woven into local culture, the most notable of which are Cassano's and Marion's Piazza. Notable Dayton-based restaurant chains include Hot Head Burritos.

In addition to restaurants, the city is also home to Esther Price Candies, a candy and chocolate company, and Mike-sells, the oldest potato chip company in the United States.

Many major religions are represented in Dayton. Christianity is represented in Dayton by dozens of denominations and their respective churches. Notable Dayton churches include the First Lutheran Church, Sacred Heart Church, and Ginghamsburg Church. Dayton's Muslim community is largely represented by the Islamic Society of Greater Dayton (ISGD), a Muslim community that includes a mosque on Josie Street. Dayton is also home to the United Theological Seminary, one of 13 seminaries affiliated with the United Methodist Church. Judaism is represented by Temple Israel. Hinduism is represented by the Hindu Temple of Dayton.

Tourists visiting Montgomery County accounted for $1.7 billion in business activity in 2007. Tourism also accounts for one out of every 14 private sector jobs in the county. Tourism in the Dayton region is led by the National Museum of the United States Air Force at Wright-Patterson Air Force Base. It is the largest and oldest military aviation museum in the world. The museum draws over 1.3 million visitors per year and is one of the most-visited tourist attractions in Ohio. The museum houses the National Aviation Hall of Fame.

Other museums also play significant roles in the tourism and economy of the Dayton area. The Dayton Art Institute, a museum of fine arts, owns collections containing more than 20,000 objects spanning 5,000 years of art and archaeological history. The Dayton Art Institute was rated one of the top 10 best art museums in the United States for children. The Boonshoft Museum of Discovery is a children's museum of science with numerous exhibits, one of which includes an indoor zoo with nearly 100 different animals.

There are also some notable historical museums in the region. The Dayton Aviation Heritage National Historical Park, operated by the National Park Service, commemorates the lives and achievements of Dayton natives Orville and Wilbur Wright and Paul Laurence Dunbar. The Wright brothers' famous Wright Flyer III aircraft is housed in a museum at Carillon Historical Park. Dayton is also home to America's Packard Museum, which contains many restored historical Packard vehicles. SunWatch Indian Village/Archaeological Park, a partially reconstructed 12th-century prehistoric American Indian village, is on the south end of Dayton; it is organized around a central plaza dominated by wood posts forming an astronomical calendar. The park includes a museum where visitors can learn about the Indian history of the Miami Valley.

The Vectren Dayton Air Show is an annual air show that takes place at the Dayton International Airport. The Vectren Dayton Airshow is one of the largest air shows in the United States.

The Dayton area is served by Five Rivers MetroParks, encompassing over 23 facilities for year-round recreation, education, and conservation. In cooperation with the Miami Conservancy District, the MetroParks maintains over of paved, multi-use scenic trails that connect Montgomery County with Greene, Miami, Warren, and Butler counties. From 1996 to 1998, Dayton hosted the National Folk Festival. Since then, the annual Cityfolk Festival has continued to bring folk, ethnic, and world music and arts to Dayton. The Five Rivers MetroParks also owns and operates the PNC Second Street Market near downtown Dayton. The market has more than 50 vendors selling items such as produce, cooked foods, baked goods, crafts, and flowers.

The Dayton area hosts several arenas and venues. South of Dayton in Kettering is the Fraze Pavilion, which hosts many nationally and internationally known musicians. Several notable performances have included the Backstreet Boys, Boston, and Steve Miller Band. South of downtown, on the banks of the Great Miami River, is the University of Dayton Arena, home venue for the University of Dayton Flyers basketball teams and the location of various other events and concerts. It also hosts the Winter Guard International championships, at which hundreds of percussion and color guard ensembles from around the world compete. North of Dayton is the Hara Arena, which frequently hosts expo events and concerts. In addition, the Dayton Amateur Radio Association hosts the annual Dayton Hamvention, North America's largest hamfest, at the Greene County Fairgrounds in nearby Xenia. Up to 30,000 amateur radio operators attend this convention. The Nutter Center, which is just east of Dayton in the suburb of Fairborn, is the home arena for athletics of Wright State University and the former Dayton Bombers hockey team. This venue is used for many concerts, community events, and various national traveling shows and performances.

The Oregon District is a historic residential and commercial district in southeast downtown Dayton. The district is populated with art galleries, specialty shops, pubs, nightclubs, and coffee houses.

The city of Dayton is also host to yearly festivals, notably the Dayton Celtic Festival and the City Folk Festival. The Dayton Celtic Festival attracts more than 30,000 people yearly and has Irish dancing, food, crafts, and performers such as Gaelic Storm. Other festivals held in the city of Dayton include the Dayton Blues Festival, Dayton Music Fest, Urban Nights, Women in Jazz, the African American and Cultural Festival, and the Dayton Reggae Fest.

The Dayton area is home to several minor league and semi pro teams, as well as NCAA Division I sports programs.
The Dayton Dragons professional baseball team is the minor league affiliate for the Cincinnati Reds. The Dayton Dragons are the first (and only) team in minor league baseball history to sell out an entire season before it began and was voted as one of the top 10 hottest tickets to get in all of professional sports by Sports Illustrated. The Dayton Dragons 815 consecutive sellouts surpassed the NBA's Portland Trail Blazers for the longest sellout streak across all professional sports in the U.S.

The Gem City Rollergirls flat track roller derby league is the first (and only) WFTDA league in Dayton, Ohio. The team was established in 2006 and began a rapid climb in the national ranks in 2015. At present, the league hosts double-header bouts at Hara Arena, playing their A-Team (The Purple Reign) and their B-Team (The Violet Femmes) against visiting teams. The league is skater-owned and skater-run.

The University of Dayton and Wright State University both host NCAA basketball. The University of Dayton Arena has hosted more games in the NCAA men's basketball tournament over its history than any other venue. UD Arena is also the site of the First Round games of the NCAA Tournament. In 2012, eight teams competed for the final four spots in the NCAA Basketball Tournament. Wright State University's NCAA men's basketball is the Wright State Raiders and the University of Dayton's NCAA men's basketball team is the Dayton Flyers.

The Dayton Gems were a minor league ice hockey team in the International Hockey League from 1964 to 1977, 1979–1980, and most recently 2009 to 2012.

The Dayton Bombers were an ECHL ice hockey team from 1991-2009. They most recently played the North Division of the ECHL's American Conference. In June 2009, it was announced the Bombers would turn in their membership back to the league. 

Despite the folding of the Bombers, hockey remained in Dayton as the Dayton Gems of the International Hockey League were formed in the fall of 2009 at Hara Arena. The Gems folded after the 2011–12 season. Shortly after the Gems folded, it was announced a new team, the Dayton Demonz, would begin play in 2012 in the Federal Hockey League (FHL). The Demonz folded in 2015 and were immediately replaced by the Dayton Demolition, also in the FHL. However, the Demolition would cease operations after only one season when Hara Arena decided to close due to financial difficulties.

Dayton hosted the first American Professional Football Association game (precursor to the NFL). The game was played at Triangle Park between the Dayton Triangles and the Columbus Panhandles on October 3, 1920, and is considered one of the first professional football games ever played. Football teams in the Dayton area include the Dayton Flyers and the Dayton Sharks.

The Dayton region is also known for the many golf courses and clubs that it hosts. The Miami Valley Golf Club, Moraine Country Club, NCR Country Club, and the Pipestone Golf Course are some of the more notable courses. Also, several PGA Championships have been held at area golf courses. The Miami Valley Golf Club hosted the 1957 PGA Championship, the Moraine Country Club hosted the 1945 PGA Championship, and the NCR Country club hosted the 1969 PGA Championship.Additionally, NCR CC hosted the 1986 U.S. Women's Open and the 2005 U.S. Senior Open. Other notable courses include the Yankee Trace Golf Club, the Beavercreek Golf Club, Dayton Meadowbrook Country Club, Sycamore Creek Country Club, Heatherwoode Golf Club, Community Golf Course, and Kitty Hawk Golf Course.

The city of Dayton is the home to the Dayton Area Rugby Club. As of 2010, the club fields three squads and play their home games at Eastwood Metropark.

Dayton is served in print by "The Dayton Daily News", the city's sole remaining daily newspaper. The "Dayton Daily News" is owned by Cox Enterprises. The Dayton region's main business newspaper is the "Dayton Business Journal". The "Dayton City Paper " is a community paper focused on music, art and independent thought.

There are numerous magazines produced in and for the Dayton region. "The Dayton Magazine" provides insight to arts, food and events. "Focus on Business" is published by the Chamber of Commerce to provide awareness of companies and initiatives affecting the regional economy

Nielsen Media Research ranked the 11-county Dayton television market as the No. 62 market in the United States. The market is served by stations affiliated with major American networks including: WKEF, Channel 22 – ABC, operated by Sinclair Broadcasting, WHIO-TV, Channel 7 – CBS, operated by Cox Media Group, WPTD, Channel 16 – PBS, operated by ThinkTV, which also operates WPTO, assigned to Oxford, WDTN, Channel 2 – NBC, operated by Media General, WBDT, Channel 26 – The CW, operated by Acme Television, and WRGT-TV, Channel 45 – Fox/My Network TV, operated under a local marketing agreement by Sinclair Broadcasting. The nationally syndicated morning talk show "The Daily Buzz" originated from WBDT-TV, the Acme property in Miamisburg, before moving to its current home in Florida. 

Dayton is also served by 42 AM and FM radio stations directly, and numerous other stations are heard from elsewhere in southwest Ohio, which serve outlying suburbs and adjoining counties.
The city of Dayton was mentioned in the hit song "Elevate" from Hip-Hop rapper Drake in his fifth studio album titled Scorpion. 

The Greater Dayton Regional Transit Authority (RTA) operates public bus routes in the Dayton metro area. In addition to routes covered by traditional diesel-powered buses, RTA has a number of electric trolley bus routes. The Dayton trolleybus system is the second longest-running of the six remaining trolleybus systems in the U.S., having entered service in 1933. It is the present manifestation of an electric transit service that has operated continuously in Dayton since 1888.

Dayton operates a Greyhound Station which provides inter-city bus transportation to and from Dayton. The hub is in the Greater Dayton Regional Transit Authority North-West hub in Trotwood.

Air transportation is available north of Dayton proper, via Dayton International Airport in Vandalia, Ohio. The airport offers service to 21 markets through 10 airlines. In 2008, it served 2.9 million passengers. The Dayton International Airport is also a significant regional air freight hub hosting FedEx Express, UPS Airlines, United States Postal Service, and major commercial freight carriers.

The Dayton area also has several regional airports. The Dayton–Wright Brothers Airport is a general aviation airport owned by the City of Dayton south of the central business district of Dayton on Springboro Pike in Miami Township. It serves as the reliever airport for Dayton International Airport. The airport primarily serves corporate and personal aircraft users. The Dahio Trotwood Airport, also known as Dayton-New Lebanon Airport, is a privately owned, public-use airport west of the central business district of Dayton. The Moraine Airpark is a privately owned, public-use airport southwest of the city of Dayton.

The Dayton region is primarily served by three interstates:

Other major routes for the region include:

From 2010 through 2017, the Ohio Department of Transportation (ODOT) performed a $533 million construction project to modify, reconstruct and widen I-75 through downtown Dayton, from Edwin C Moses Blvd. to Stanley Avenue.

Dayton hosts several inter-modal freight railroad terminals. Two Class I railroads, CSX and Norfolk Southern Railway, operate switching yards in the city.

The Dayton Public Schools operates 34 schools that serve 16,855 students, including:

The city of Dayton has more than 35 private schools within the city, including:

Dayton has 33 charter schools. Three of the top five charter schools named in 2011 are K-8 schools managed by National Heritage Academies. Notable charter schools include:

The Dayton area was ranked tenth for higher education among metropolitan areas in the United States by "Forbes" in 2009. The city is home to two major universities. The University of Dayton is a private, Catholic institution founded in 1850 by the Marianist order. It has the only American Bar Association (ABA)-approved law school in the Dayton area. The University of Dayton is Ohio's largest private university and is also home to the University of Dayton Research Institute, which ranks third in the nation for sponsored materials research, and the Center for Tissue Regeneration and Engineering at Dayton, which focuses on human tissue regeneration.

The public Wright State University became a state university in 1967. Wright State University established the National Center for Medical Readiness, a national training program for disaster preparedness and relief. Wright State's Boonshoft School of Medicine is the Dayton area's only medical school and is a leader in biomedical research.

Dayton is also home to Sinclair Community College, the largest community college at a single location in Ohio and one of the nation's largest community colleges. Sinclair is acclaimed as one of the country's best community colleges. Sinclair was founded as the YMCA college in 1887.

Other schools just outside Dayton that shape the educational landscape are Antioch College and Antioch University, both in Yellow Springs, Central State University in Wilberforce, Kettering College of Medical Arts and School of Advertising Art in Kettering, DeVry University in Beavercreek, and Clark State Community College in Springfield. The Air Force Institute of Technology, which was founded in 1919 and serves as a graduate school for the United States Air Force, is at the nearby Wright-Patterson Air Force Base.


Dayton consistently has had one of the highest crime rates among US cities. Dayton has experienced an improving public safety environment since 2003, with crime declining in key categories according to FBI Uniform Crime Reports and Dayton Police Department data. In 2009, crime continued to fall in the city of Dayton. Crime in the categories of forcible rape, aggravated assault, property crime, motor vehicle theft, robbery, burglary, theft and arson all showed declines for 2009. Overall, crime in Dayton dropped 40% over the previous year. The Dayton Police Department reported a total of 39 murders in 2016, which marked a 39.3% increase in homicides from 2015.

Also notable, John Dillinger, a famous bank robber during the early 1930s, was captured and arrested by Dayton city police while visiting his girlfriend at a high-class boarding house in downtown Dayton.





</doc>
<doc id="8254" url="https://en.wikipedia.org/wiki?curid=8254" title="Diode">
Diode

A diode is a two-terminal electronic component that conducts current primarily in one direction (asymmetric conductance); it has low (ideally zero) resistance in one direction, and high (ideally infinite) resistance in the other. A diode vacuum tube or thermionic diode is a vacuum tube with two electrodes, a heated cathode and a plate, in which electrons can flow in only one direction, from cathode to plate. A semiconductor diode, the most common type today, is a crystalline piece of semiconductor material with a p–n junction connected to two electrical terminals. Semiconductor diodes were the first semiconductor electronic devices. The discovery of asymmetric electrical conduction across the contact between a crystalline mineral and a metal was made by German physicist Ferdinand Braun in 1874. Today, most diodes are made of silicon, but other materials such as gallium arsenide and germanium are used.

The most common function of a diode is to allow an electric current to pass in one direction (called the diode's "forward" direction), while blocking it in the opposite direction (the "reverse" direction). As such, the diode can be viewed as an electronic version of a check valve. This unidirectional behavior is called rectification, and is used to convert alternating current (ac) to direct current (dc). Forms of rectifiers, diodes can be used for such tasks as extracting modulation from radio signals in radio receivers.

However, diodes can have more complicated behavior than this simple on–off action, because of their nonlinear current-voltage characteristics. Semiconductor diodes begin conducting electricity only if a certain threshold voltage or cut-in voltage is present in the forward direction (a state in which the diode is said to be "forward-biased"). The voltage drop across a forward-biased diode varies only a little with the current, and is a function of temperature; this effect can be used as a temperature sensor or as a voltage reference. Also, diodes' high resistance to current flowing in the reverse direction suddenly drops a low resistance when the reverse voltage across the diode reaches a value called the breakdown voltage.

A semiconductor diode's current–voltage characteristic can be tailored by selecting the semiconductor materials and the doping impurities introduced into the materials during manufacture. These techniques are used to create special-purpose diodes that perform many different functions. For example, diodes are used to regulate voltage (Zener diodes), to protect circuits from high voltage surges (avalanche diodes), to electronically tune radio and TV receivers (varactor diodes), to generate radio-frequency oscillations (tunnel diodes, Gunn diodes, IMPATT diodes), and to produce light (light-emitting diodes). Tunnel, Gunn and IMPATT diodes exhibit negative resistance, which is useful in microwave and switching circuits.

Diodes, both vacuum and semiconductor, can be used as shot-noise generators.

Thermionic (vacuum-tube) diodes and solid-state (semiconductor) diodes were developed separately, at approximately the same time, in the early 1900s, as radio receiver detectors. Until the 1950s, vacuum diodes were used more frequently in radios because the early point-contact semiconductor diodes were less stable. In addition, most receiving sets had vacuum tubes for amplification that could easily have the thermionic diodes included in the tube (for example the 12SQ7 double diode triode), and vacuum-tube rectifiers and gas-filled rectifiers were capable of handling some high-voltage/high-current rectification tasks better than the semiconductor diodes (such as selenium rectifiers) that were available at that time.

In 1873, Frederick Guthrie observed that a grounded, white hot metal ball brought in close proximity to an electroscope would discharge a positively charged electroscope, but not a negatively charged electroscope.

In 1880, Thomas Edison observed unidirectional current between heated and unheated elements in a bulb, later called Edison effect, and was granted a patent on application of the phenomenon for use in a dc voltmeter.

About 20 years later, John Ambrose Fleming (scientific adviser to the Marconi Company
and former Edison employee) realized that the Edison effect could be used as a radio detector. Fleming patented the first true thermionic diode, the Fleming valve, in Britain on November 16, 1904 (followed by in November 1905).

Throughout the vacuum tube era, valve diodes were used in almost all electronics such as radios, televisions, sound systems and instrumentation. They slowly lost market share beginning in the late 1940s due to selenium rectifier technology and then to semiconductor diodes during the 1960s. Today they are still used in a few high power applications where their ability to withstand transient voltages and their robustness gives them an advantage over semiconductor devices, and in musical instrument and audiophile applications.

In 1874, German scientist Karl Ferdinand Braun discovered the "unilateral conduction" across a contact between a metal and a mineral. Indian scientist Jagadish Chandra Bose was the first to use a crystal for detecting radio waves in 1894. The crystal detector was developed into a practical device for wireless telegraphy by Greenleaf Whittier Pickard, who invented a silicon crystal detector in 1903 and received a patent for it on November 20, 1906. Other experimenters tried a variety of other minerals as detectors. Semiconductor principles were unknown to the developers of these early rectifiers. During the 1930s understanding of physics advanced and in the mid 1930s researchers at Bell Telephone Laboratories recognized the potential of the crystal detector for application in microwave technology. Researchers at Bell Labs, Western Electric, MIT, Purdue and in the UK intensively developed point-contact diodes ("crystal rectifiers" or "crystal diodes") during World War II for application in radar. After World War II, AT&T used these in their microwave towers that criss-crossed the United States, and many radar sets use them even in the 21st century. In 1946, Sylvania began offering the 1N34 crystal diode. During the early 1950s, junction diodes were developed.

At the time of their invention, asymmetrical conduction devices were known as rectifiers. In 1919, the year tetrodes were invented, William Henry Eccles coined the term "diode" from the Greek roots "di" (from "δί"), meaning 'two', and "ode" (from "ὁδός"), meaning 'path'. The word "diode", however, as well as "triode, tetrode, pentode, hexode", were already in use as terms of multiplex telegraphy.

Although all diodes "rectify", the term "rectifier" is usually applied to diodes intended for power supply application in order to differentiate them from diodes intended for small signal circuits.

A thermionic diode is a thermionic-valve device consisting of a sealed, evacuated glass or metal envelope containing two electrodes: a cathode and a plate. The cathode is either "indirectly heated" or "directly heated". If indirect heating is employed, a heater is included in the envelope.

In operation, the cathode is heated to red heat (800–1000 °C). A directly heated cathode is made of tungsten wire and is heated by current passed through it from an external voltage source. An indirectly heated cathode is heated by infrared radiation from a nearby heater that is formed of Nichrome wire and supplied with current provided by an external voltage source.

The operating temperature of the cathode causes it to release electrons into the vacuum, a process called thermionic emission. The cathode is coated with oxides of alkaline earth metals, such as barium and strontium oxides. These have a low work function, meaning that they more readily emit electrons than would the uncoated cathode.

The plate, not being heated, does not emit electrons; but is able to absorb them.

The alternating voltage to be rectified is applied between the cathode and the plate. When the plate voltage is positive with respect to the cathode, the plate electrostatically attracts the electrons from the cathode, so a current of electrons flows through the tube from cathode to plate. When the plate voltage is negative with respect to the cathode, no electrons are emitted by the plate, so no current can pass from the plate to the cathode.

Point-contact diodes were developed starting in the 1930s, out of the early crystal detector technology, and are now generally used in the 3 to 30 gigahertz range. Point-contact diodes use a small diameter metal wire in contact with a semiconductor crystal, and are of either "non-welded" contact type or "welded contact" type. Non-welded contact construction utilizes the Schottky barrier principle. The metal side is the pointed end of a small diameter wire that is in contact with the semiconductor crystal. In the welded contact type, a small P region is formed in the otherwise N type crystal around the metal point during manufacture by momentarily passing a relatively large current through the device. Point contact diodes generally exhibit lower capacitance, higher forward resistance and greater reverse leakage than junction diodes.

A p–n junction diode is made of a crystal of semiconductor, usually silicon, but germanium and gallium arsenide are also used. Impurities are added to it to create a region on one side that contains negative charge carriers (electrons), called an n-type semiconductor, and a region on the other side that contains positive charge carriers (holes), called a p-type semiconductor. When the n-type and p-type materials are attached together, a momentary flow of electrons occur from the n to the p side resulting in a third region between the two where no charge carriers are present. This region is called the depletion region because there are no charge carriers (neither electrons nor holes) in it. The diode's terminals are attached to the n-type and p-type regions. The boundary between these two regions, called a p–n junction, is where the action of the diode takes place. When a sufficiently higher electrical potential is applied to the P side (the anode) than to the N side (the cathode), it allows electrons to flow through the depletion region from the N-type side to the P-type side. The junction does not allow the flow of electrons in the opposite direction when the potential is applied in reverse, creating, in a sense, an electrical check valve.

Another type of junction diode, the Schottky diode, is formed from a metal–semiconductor junction rather than a p–n junction, which reduces capacitance and increases switching speed.

A semiconductor diode's behavior in a circuit is given by its current–voltage characteristic, or I–V graph (see graph below). The shape of the curve is determined by the transport of charge carriers through the so-called "depletion layer" or "depletion region" that exists at the p–n junction between differing semiconductors. When a p–n junction is first created, conduction-band (mobile) electrons from the N-doped region diffuse into the P-doped region where there is a large population of holes (vacant places for electrons) with which the electrons "recombine". When a mobile electron recombines with a hole, both hole and electron vanish, leaving behind an immobile positively charged donor (dopant) on the N side and negatively charged acceptor (dopant) on the P side. The region around the p–n junction becomes depleted of charge carriers and thus behaves as an insulator.

However, the width of the depletion region (called the depletion width) cannot grow without limit. For each electron–hole pair recombination made, a positively charged dopant ion is left behind in the N-doped region, and a negatively charged dopant ion is created in the P-doped region. As recombination proceeds and more ions are created, an increasing electric field develops through the depletion zone that acts to slow and then finally stop recombination. At this point, there is a "built-in" potential across the depletion zone.

If an external voltage is placed across the diode with the same polarity as the built-in potential, the depletion zone continues to act as an insulator, preventing any significant electric current flow (unless electron–hole pairs are actively being created in the junction by, for instance, light; see photodiode). This is called the "reverse bias" phenomenon.

However, if the polarity of the external voltage opposes the built-in potential, recombination can once again proceed, resulting in a substantial electric current through the p–n junction (i.e. substantial numbers of electrons and holes recombine at the junction). For silicon diodes, the built-in potential is approximately 0.7 V (0.3 V for germanium and 0.2 V for Schottky). Thus, if an external voltage greater than and opposite to the built-in voltage is applied, a current will flow and the diode is said to be "turned on" as it has been given an external "forward bias". The diode is commonly said to have a forward "threshold" voltage, above which it conducts and below which conduction stops. However, this is only an approximation as the forward characteristic is smooth (see I-V graph above).

A diode's I–V characteristic can be approximated by four regions of operation:


In a small silicon diode operating at its rated currents, the voltage drop is about 0.6 to 0.7 volts. The value is different for other diode types—Schottky diodes can be rated as low as 0.2 V, germanium diodes 0.25 to 0.3 V, and red or blue light-emitting diodes (LEDs) can have values of 1.4 V and 4.0 V respectively.

At higher currents the forward voltage drop of the diode increases. A drop of 1 V to 1.5 V is typical at full rated current for power diodes.

The "Shockley ideal diode equation" or the "diode law" (named after the bipolar junction transistor co-inventor William Bradford Shockley) gives the I–V characteristic of an ideal diode in either forward or reverse bias (or no bias). The following equation is called the "Shockley ideal diode equation" when "n", the ideality factor, is set equal to 1 :

where

The thermal voltage "V" is approximately 25.85 mV at 300 K, a temperature close to "room temperature" commonly used in device simulation software. At any temperature it is a known constant defined by:

where "k" is the Boltzmann constant, "T" is the absolute temperature of the p–n junction, and "q" is the magnitude of charge of an electron (the elementary charge).

The reverse saturation current, "I", is not constant for a given device, but varies with temperature; usually more significantly than "V", so that "V" typically decreases as "T" increases.

The "Shockley ideal diode equation" or the "diode law" is derived with the assumption that the only processes giving rise to the current in the diode are drift (due to electrical field), diffusion, and thermal recombination–generation (R–G) (this equation is derived by setting n = 1 above). It also assumes that the R–G current in the depletion region is insignificant. This means that the "Shockley ideal diode equation" doesn't account for the processes involved in reverse breakdown and photon-assisted R–G. Additionally, it doesn't describe the "leveling off" of the I–V curve at high forward bias due to internal resistance. Introducing the ideality factor, n, accounts for recombination and generation of carriers.

Under "reverse bias" voltages the exponential in the diode equation is negligible, and the current is a constant (negative) reverse current value of −"I". The reverse "breakdown region" is not modeled by the Shockley diode equation.

For even rather small "forward bias" voltages the exponential is very large, since the thermal voltage is very small in comparison. The subtracted '1' in the diode equation is then negligible and the forward diode current can be approximated by

The use of the diode equation in circuit problems is illustrated in the article on diode modeling.

At forward voltages less than the saturation voltage, the voltage versus current characteristic curve of most diodes is not a straight line. The current can be approximated by formula_3 as mentioned in the previous section. 

In detector and mixer applications, the current can be estimated by a Taylor's series. The odd terms can be omitted because they produce frequency components that are outside the pass band of the mixer or detector. Even terms beyond the second derivative usually need not be included because they are small compared to the second order term. The desired current component is approximately proportional to the square of the input voltage, so the response is called "square law" in this region.

Following the end of forward conduction in a p–n type diode, a reverse current can flow for a short time. The device does not attain its blocking capability until the mobile charge in the junction is depleted.

The effect can be significant when switching large currents very quickly. A certain amount of "reverse recovery time" t (on the order of tens of nanoseconds to a few microseconds) may be required to remove the reverse recovery charge Q from the diode. During this recovery time, the diode can actually conduct in the reverse direction. This might give rise to a large constant current in the reverse direction for a short time while the diode is reverse biased. The magnitude of such a reverse current is determined by the operating circuit (i.e., the series resistance) and the diode is said to be in the storage-phase. In certain real-world cases it is important to consider the losses that are incurred by this non-ideal diode effect. However, when the slew rate of the current is not so severe (e.g. Line frequency) the effect can be safely ignored. For most applications, the effect is also negligible for Schottky diodes.

The reverse current ceases abruptly when the stored charge is depleted; this abrupt stop is exploited in step recovery diodes for generation of extremely short pulses.

Normal (p–n) diodes, which operate as described above, are usually made of doped silicon or germanium. Before the development of silicon power rectifier diodes, cuprous oxide and later selenium was used. Their low efficiency required a much higher forward voltage to be applied (typically 1.4 to 1.7 V per "cell", with multiple cells stacked so as to increase the peak inverse voltage rating for application in high voltage rectifiers), and required a large heat sink (often an extension of the diode's metal substrate), much larger than the later silicon diode of the same current ratings would require. The vast majority of all diodes are the p–n diodes found in CMOS integrated circuits, which include two diodes per pin and many other internal diodes.


Other uses for semiconductor diodes include the sensing of temperature, and computing analog logarithms (see Operational amplifier applications#Logarithmic output).

The symbol used to represent a particular type of diode in a circuit diagram conveys the general electrical function to the reader. There are alternative symbols for some types of diodes, though the differences are minor. The triangle in the symbols points to the forward direction, i.e. in the direction of conventional current flow.

There are a number of common, standard and manufacturer-driven numbering and coding schemes for diodes; the two most common being the EIA/JEDEC standard and the European Pro Electron standard:

The standardized 1N-series numbering "EIA370" system was introduced in the US by EIA/JEDEC (Joint Electron Device Engineering Council) about 1960. Most diodes have a 1-prefix designation (e.g., 1N4003). Among the most popular in this series were: 1N34A/1N270 (germanium signal), 1N914/1N4148 (silicon signal), 1N400x (silicon 1A power rectifier), and 1N580x (silicon 3A power rectifier).

The JIS semiconductor designation system has all semiconductor diode designations starting with "1S".

The European Pro Electron coding system for active components was introduced in 1966 and comprises two letters followed by the part code. The first letter represents the semiconductor material used for the component (A = germanium and B = silicon) and the second letter represents the general function of the part (for diodes, A = low-power/signal, B = variable capacitance, X = multiplier, Y = rectifier and Z = voltage reference); for example:


Other common numbering / coding systems (generally manufacturer-driven) include:


As well as these common codes, many manufacturers or organisations have their own systems toofor example:


In optics, an equivalent device for the diode but with laser light would be the Optical isolator, also known as an Optical Diode, that allows light to only pass in one direction. It uses a Faraday rotator as the main component.

The first use for the diode was the demodulation of amplitude modulated (AM) radio broadcasts. The history of this discovery is treated in depth in the radio article. In summary, an AM signal consists of alternating positive and negative peaks of a radio carrier wave, whose amplitude or envelope is proportional to the original audio signal. The diode rectifies the AM radio frequency signal, leaving only the positive peaks of the carrier wave. The audio is then extracted from the rectified carrier wave using a simple filter and fed into an audio amplifier or transducer, which generates sound waves.

In microwave and millimeter wave technology, beginning in the 1930s, researchers improved and miniaturized the crystal detector. Point contact diodes ("crystal diodes") and Schottky diodes are used in radar, microwave and millimeter wave detectors.

Rectifiers are constructed from diodes, where they are used to convert alternating current (ac) electricity into direct current (dc). Automotive alternators are a common example, where the diode, which rectifies the AC into dc, provides better performance than the commutator or earlier, dynamo. Similarly, diodes are also used in "Cockcroft–Walton voltage multipliers" to convert ac into higher ac voltages.

Diodes are frequently used to conduct damaging high voltages away from sensitive electronic devices. They are usually reverse-biased (non-conducting) under normal circumstances. When the voltage rises above the normal range, the diodes become forward-biased (conducting). For example, diodes are used in (stepper motor and H-bridge) motor controller and relay circuits to de-energize coils rapidly without the damaging voltage spikes that would otherwise occur. (A diode used in such an application is called a flyback diode). Many integrated circuits also incorporate diodes on the connection pins to prevent external voltages from damaging their sensitive transistors. Specialized diodes are used to protect from over-voltages at higher power (see Diode types above).

Diodes can be combined with other components to construct AND and OR logic gates. This is referred to as diode logic.

In addition to light, mentioned above, semiconductor diodes are sensitive to more energetic radiation. In electronics, cosmic rays and other sources of ionizing radiation cause noise pulses and single and multiple bit errors.
This effect is sometimes exploited by particle detectors to detect radiation. A single particle of radiation, with thousands or millions of electron volts of energy, generates many charge carrier pairs, as its energy is deposited in the semiconductor material. If the depletion layer is large enough to catch the whole shower or to stop a heavy particle, a fairly accurate measurement of the particle's energy can be made, simply by measuring the charge conducted and without the complexity of a magnetic spectrometer, etc.
These semiconductor radiation detectors need efficient and uniform charge collection and low leakage current. They are often cooled by liquid nitrogen. For longer-range (about a centimetre) particles, they need a very large depletion depth and large area. For short-range particles, they need any contact or un-depleted semiconductor on at least one surface to be very thin. The back-bias voltages are near breakdown (around a thousand volts per centimetre). Germanium and silicon are common materials. Some of these detectors sense position as well as energy.
They have a finite life, especially when detecting heavy particles, because of radiation damage. Silicon and germanium are quite different in their ability to convert gamma rays to electron showers.

Semiconductor detectors for high-energy particles are used in large numbers. Because of energy loss fluctuations, accurate measurement of the energy deposited is of less use.

A diode can be used as a temperature measuring device, since the forward voltage drop across the diode depends on temperature, as in a silicon bandgap temperature sensor. From the Shockley ideal diode equation given above, it might "appear" that the voltage has a "positive" temperature coefficient (at a constant current), but usually the variation of the reverse saturation current term is more significant than the variation in the thermal voltage term. Most diodes therefore have a "negative" temperature coefficient, typically −2 mV/˚C for silicon diodes. The temperature coefficient is approximately constant for temperatures above about 20 kelvins. Some graphs are given for 1N400x series, and CY7 cryogenic temperature sensor.

Diodes will prevent currents in unintended directions. To supply power to an electrical circuit during a power failure, the circuit can draw current from a battery. An uninterruptible power supply may use diodes in this way to ensure that current is only drawn from the battery when necessary. Likewise, small boats typically have two circuits each with their own battery/batteries: one used for engine starting; one used for domestics. Normally, both are charged from a single alternator, and a heavy-duty split-charge diode is used to prevent the higher-charge battery (typically the engine battery) from discharging through the lower-charge battery when the alternator is not running.

Diodes are also used in electronic musical keyboards. To reduce the amount of wiring needed in electronic musical keyboards, these instruments often use keyboard matrix circuits. The keyboard controller scans the rows and columns to determine which note the player has pressed. The problem with matrix circuits is that, when several notes are pressed at once, the current can flow backwards through the circuit and trigger "phantom keys" that cause "ghost" notes to play. To avoid triggering unwanted notes, most keyboard matrix circuits have diodes soldered with the switch under each key of the musical keyboard. The same principle is also used for the switch matrix in solid-state pinball machines.

Diodes can be used to limit the positive or negative excursion of a signal to a prescribed voltage.

A diode clamp circuit can take a periodic alternating current signal that oscillates between positive and negative values, and vertically displace it such that either the positive, or the negative peaks occur at a prescribed level. The clamper does not restrict the peak-to-peak excursion of the signal, it moves the whole signal up or down so as to place the peaks at the reference level.

Diodes are usually referred to as "D" for diode on PCBs. Sometimes the abbreviation "CR" for "crystal rectifier" is used.






</doc>
<doc id="8256" url="https://en.wikipedia.org/wiki?curid=8256" title="Drexel University">
Drexel University

Drexel University is a private research university with its main campus located in the University City neighborhood of Philadelphia, Pennsylvania, United States. It was founded in 1891 by Anthony J. Drexel, a noted financier and philanthropist. Founded as Drexel Institute of Art, Science, and Industry; it was renamed Drexel Institute of Technology in 1936, before assuming the name Drexel University in 1970.

As of 2015, more than 26,000 students are enrolled in over 70 undergraduate programs and more than 100 master's, doctoral, and professional programs at the university. Drexel's cooperative education program (co-op) is a unique aspect of the school's degree programs, offering students the opportunity to gain up to 18 months of paid, full-time work experience in a field relevant to their undergraduate major or graduate degree program prior to graduation.

Drexel University was founded in 1891 as the Drexel Institute of Art, Science and Industry, by Philadelphia financier and philanthropist Anthony J. Drexel. The original mission of the institution was to provide educational opportunities in the "practical arts and sciences" for women and men of all backgrounds. The institution became known as the Drexel Institute of Technology in 1936, and in 1970 the Drexel Institute of Technology gained university status, becoming Drexel University.

Although there were many changes during its first century, the university's identity has been held constant as a privately controlled, non-sectarian, coeducational center of higher learning, distinguished by a commitment to practical education and hands-on experience in an occupational setting. The central aspect of Drexel University's focus on career preparation, in the form of its cooperative education program, was introduced in 1919. The program became integral to the university's unique educational experience. Participating students alternate periods of classroom-based study with periods of full-time, practical work experience related to their academic major and career interests.

Between 1995 and 2009, Drexel University underwent a period of significant change to its programs, enrollment, and facilities under the leadership of Dr. Constantine Papadakis, the university's president during that time. Papadakis oversaw Drexel's largest expansion in its history, with a 471 percent increase in its endowment and a 102 percent increase in student enrollment. His leadership also guided the university toward improved performance in collegiate rankings, a more selective approach to admissions, and a more rigorous academic program at all levels. It was during this period of expansion that Drexel acquired and assumed management of the former MCP Hahnemann University, creating the Drexel University College of Medicine in 2002. In 2006, the university established the Thomas R. Kline School of Law, and in 2011 the School of Law achieved full accreditation by the American Bar Association.

Dr. Constantine Papadakis died of pneumonia in April 2009 while still employed as the university's president. His successor, John Anderson Fry, was formerly the president of Franklin & Marshall College and served as the Executive Vice President of the University of Pennsylvania. Under Fry's leadership, Drexel has continued its expansion, including the July 2011 acquisition of The Academy of Natural Sciences.

The College of Arts and Sciences was formed in 1990 when Drexel merged the two existing College of Sciences and College of Humanities together.

The College of Media Arts and design "fosters the study, exploration and management of the arts: media, design, the performing and visual". The college offers sixteen undergraduate programs, and 6 graduate programs, in modern art and design fields that range from graphic design and dance to fashion design and television management. Its wide range of programs has helped the college earn full accreditation from the National Association of Schools of Art & Design, the National Architectural Accrediting Board, and the Council for Interior Design Accreditation.

The Bennett S. LeBow College of Business history dates to the founding in 1891 of the Drexel Institute, that later became Drexel University, and of its Business Department in 1896. Today LeBow offers thirteen undergraduate majors, eight graduate programs, and two doctoral programs; 22 percent of Drexel University's undergraduate students are enrolled in a LeBow College of Business program. 

The LeBow College of Business has been ranked as the 38th best private business school in the nation. Its online MBA program is ranked 14th in the world by the "Financial Times"; the publication also ranks the undergraduate business program at LeBow as 19th in the United States. The part-time MBA program ranks 1st in academic quality in the 2015 edition of "Business Insider's" rankings. Undergraduate and graduate entrepreneurship programs are ranked 19th in the country by the "Princeton Review".

Economics programs at the LeBow College of Business are housed within the School of Economics. In addition to the undergraduate program in economics, the school is home to a recently launched M.S. in Economics program as well as a PhD program in economics. Faculty members in the School of Economics have been published in the "American Economic Review", "Rand Journal of Economics", and "Review of Economics and Statistics." The school has been ranked among the best in the world for its extensive research into matters of international trade.

Drexel's College of Engineering is one of its oldest and largest academic colleges, and served as the original focus of the career-oriented school upon its founding in 1891. The College of Engineering is home to several notable alumni, including two astronauts; financier Bennett S. LeBow, for whom the university's College of Business is named; and Paul Baran, inventor of the packet-switched network. Today, Drexel University's College of Engineering, which is home to 19 percent of the undergraduate student body, is known for creating the world's first engineering degree in appropriate technology. The college is also one of only 17 U.S. universities to offer a bachelor's degree in architectural engineering, and only one of five private institutions to do so.
The 2006 edition of U.S. News ranks the undergraduate engineering program #57 in the country and the 2007 edition of graduate schools ranks the graduate program #61. The 2008 edition ranks the University Engineering Program at #55 and in the 2009 US News Ranking, the university has moved up to the #52 position.

The engineering curriculum used by the school was originally called E4 (Enhanced Educational Experience for Engineers) which was established in 1986 and funded in part by the Engineering Directorate of the National Science Foundation. In 1988 the program evolved into tDEC (the Drexel Engineering Curriculum) which is composed of two full years of rigorous core engineering courses which encompass the freshman and sophomore years of the engineering student. The College of Engineering hasn't used the tDEC curriculum since approximately 2005.

The College of Computing and Informatics is a recent addition to Drexel University, though its programs have been offered to students for many years. The college was formed by the consolidation of the former College of Information Science & Technology (often called the "iSchool"), the Department of Computer Science, and the Computing and Security Technology program. Undergraduate and graduate programs in computer science, software engineering, information systems, and computer security are offered by the college.

The Drexel University College of Medicine is a recent addition to the colleges and schools of the university, having been formed upon the acquisition of MCP Hahnemann University in 2002. The College of Medicine was ranked #83 in the "Best Medical Schools: Research" category by U.S. News & World Report in 2015. In addition to its M.D. program, the College of Medicine offers several graduate programs in professional studies and biomedical sciences.

The Graduate School of Biomedical Sciences and Professional studies offers both Master of Science and Doctor of Philosophy degree programs in fields like biochemistry, biotechnology, clinical research, and forensic science. The school also serves as the center for biomedical research at Drexel University.

Founded to combine Drexel's College of Medicine academic principles with its rigorous College of Engineering curriculum, the School of Biomedical Engineering, Science and Health Systems focuses on the emerging field of biomedical science at the undergraduate, graduate, and doctoral levels. Primary research areas within the school include bioinformatics, biomechanics, biomaterials, and cardiovascular engineering.

Formed in 2002 along with the College of Medicine, Drexel's College of Nursing and Health Professions offers more than 25 programs to undergraduate and graduate students in the fields of nursing, nutrition, health sciences, health services, and radiologic technology. The college's research into matters of nutrition and rehabilitation have garnered approximately $2.9 million in external research funding on an annual basis. The physician assistant program at Drexel's College of Nursing and Health Professions is ranked in the top 15 such programs in the United States; its anesthesia programs and physical therapy programs are, respectively, ranked as top-50 programs nationwide.

Established in 1892, the department now known as the College of Professional Studies has focused exclusively on educational programs and pursuits for nontraditional adult learners. Today, the Goodwin College of Professional Studies offers several options designed for adult learners at all stages of career and educational development. Bachelor of Science degree completion programs are offered in part-time evening or weekend formats; graduate programs and doctoral programs are offered at the graduate level, as are self-paced "continuing education" courses and nearly a dozen self-paced certification programs.

The Pennoni Honors College, named for Drexel alumnus and trustee Dr. C.R. "Chuck" Pennoni '63, '66, Hon. '92, and his wife Annette, recognizes and promotes excellence among Drexel students. Students admitted to the Honors College live together and take many of the same classes; the college provides these students with access to unique cultural and social activities and a unique guest speaker series. Students are also involved in the university's Honors Student Advisory Committee and have the opportunity to take part in Drexel's "Alternative Spring Break", an international study tour held each spring.

Upon its founding in 2006, the Thomas R. Kline School of Law, originally known as the Earle Mack School of Law, was the first law school founded in Philadelphia in more than three decades. The School of Law offers L.L.M. and Master of Legal Studies degrees, in addition to the flagship Juris Doctorate program, and uniquely offers cooperative education as part of its curriculum across all programs. In 2015, "Bloomberg Business" ranked the Kline School of Law as the second most underrated law school in the United States.

One of the oldest schools within Drexel University, the modern School of Education dates back to the 1891 founding of the school. Originally, the Department of Education offered teacher training to women as one of its original, career-focused degree programs. Today, the School of Education offers a coeducational approach to teacher training at the elementary and secondary levels for undergraduates. Other undergraduate programs include those focused on the intersection between learning and technology, teacher certification for non-education majors, and a minor in education for students with an interest in instruction. Graduate degrees offered by the School of Education include those in administration and leadership, special education, higher education, mathematics education, international education, and educational creativity and innovation. Doctoral degrees are offered in educational leadership and learning technologies.

The School of Public Health states that its mission is to "provide education, conduct research, and partner with communities and organizations to improve the health of populations". To that end, the school offers both a B.S. and a minor in public health for undergraduate students as well as several options for students pursuing graduate and doctoral degrees in the field. At the graduate level, the Dornsife School offers both a Master of Public Health and an Executive Master of Public Health, as well as an M.S. in biostatistics and an M.S. in epidemiology. Two Doctor of Public Health degrees are also offered, as isa Doctor of Philosophy in epidemiology. The school's graduate and doctoral students are heavily invested in the research activities of the Dornsife School of Public Health, which has helped the school attract annual funding for its four research centers.

The Center for Hospitality and Sport Management was formed in 2013, in an effort to house and consolidate academic programs in hospitality, tourism management, the culinary arts, and sport management. Academic programs combine the unique skills required of the sports and hospitality industries with the principles and curriculum espoused by the management programs within Drexel's LeBow College of Business.

Focusing specifically on the skills required to successfully start and launch a business, the Charles D. Close School of Entrepreneurship is the first and only freestanding school of entrepreneurship in the United States. Undergraduate students take part in a B.A. program in entrepreneurship and innovation, while graduate students a combined Master of Science degree in biomedicine and entrepreneurship. Minors in entrepreneurship are also offered to undergraduate students.

Housed within the Close School is the Baiada Institute for Entrepreneurship. The institute serves as an incubator for Drexel student startups, providing resources and mentorships to students who are starting their own business while enrolled in one of the Close School's degree programs or academic minors.

Drexel University launched its first Internet-based education program, a master's degree in Library & Information Science, in 1996. In 2001, Drexel created its wholly owned, for-profit online education subsidiary, Drexel e-Learning, Inc., better known as Drexel University Online. It was announced in October 2013 that Drexel University Online would no longer be a for-profit venture, but rather become an internal division within the university to better serve its online student population. Although headquartered in Philadelphia, Drexel announced a new Washington, D.C., location in December 2012 to serve as both an academic and outreach center, catering to the online student population.

In an effort to create greater awareness of distance learning and to recognize exceptional leaders and best practices in the field, Drexel University Online founded National Distance Learning Week, in conjunction with the United States Distance Learning Association, in 2007. In September 2010, Drexel University Online received the Sloan-C award for institution-wide excellence in online education indicating that it had exceptional programs of "demonstrably high quality" at the regional and national levels and across disciplines. Drexel University Online won the 2008 United States Distance Learning Association's Best Practices Awards for Distance Learning Programming. In 2007, the online education subsidiary had a revenue of $40 million. In March 2013, Drexel Online had more than 7,000 unique students from all 50 states and more than 20 countries pursuing a bachelor's, master's, or certificate. As of December 2013, Drexel University Online offers more than 100 fully accredited master's degrees, bachelor's degrees and certificate programs.

Drexel's longstanding cooperative education, or "co-op" program is one of the largest and oldest in the United States. Drexel has a fully internet-based job database, where students can submit résumés and request interviews with any of the thousands of companies that offer positions. They interview with employers during three rounds of applications: A round, B round, and C round. Students also have the option of obtaining an internship via independent search. A student graduating from Drexel's 5-year degree program typically has a total of 18 months of internship with up to three different companies. The majority of co-ops are paid, averaging $15,912 per 6-month period, however this figure changes with major. About one third of Drexel graduates are offered full-time positions by their co-op employers right after graduation.

Drexel's knowledge community of researchers and scholars are socially, professionally and intellectually diverse. Research Centers and Institutes at Drexel include:


In its 2017 rankings, "U.S. News & World Report" ranked Drexel tied for 96th among national universities in the United States, and tied for 14th in the "Most Innovative Schools" category. The publication also ranked the Library and Information Studies program tied for 10th in the nation for 2017.

In 2016, "Bloomberg Businessweek" ranked the undergraduate business program 78th in the country. In 2014, Business Insider ranked Drexel's graduate business school 19th in the country for networking.

The Department of Materials Science and Engineering was ranked 18th of 88 programs in the 2011 National Research Council survey rankings.

The Physician Assistant program is ranked tied for 13th in the nation by "U.S. News & World Report" in its 2017 rankings.

In 2014, "The Princeton Review" ranked Drexel 20th in its list of worst college libraries.

Drexel University's programs are divided across three Philadelphia-area campuses: the University City Campus, the Center City Hahnemann Campus including Hahnemann University Hospital, and the Queen Lane College of Medicine Campus.
The University City Main Campus of Drexel University is located just west of the Schuylkill River in the University City district of Philadelphia. It is Drexel's largest and oldest campus; the campus contains the university's administrative offices and serves as the main academic center for students. The northern, residential portion of the main campus is located in the Powelton Village section of West Philadelphia. The two prominent performing stages at Drexel University are the Mandell Theater and the Main Auditorium. The Main Auditorium dates back to the founding of Drexel and construction of its main hall. It features over 1000 seats, and a pipe organ installed in 1928. The organ was purchased by Saturday Evening Post publisher Cyrus H. K. Curtis after he had donated a similar organ, the Curtis Organ, to nearby University of Pennsylvania and it was suggested that he do the same for Drexel. The 424-seat Mandell Theater was built in 1973 and features a more performance-oriented stage, including a full fly system, modern stage lighting facilities, stadium seating, and accommodations for wheelchairs. It is used for the semiannual spring musical, as well as various plays and many events.

The Queen Lane Medical Campus was purchased in 2003 by Drexel University as part of its acquisition of MCP Hahnemann University. It is located in the East Falls neighborhood of northwest Philadelphia and is primarily utilized by first- and second-year medical students. A free shuttle is available, connecting the Queen Lane Campus to the Center City Hahnemann and University City Main campuses.

The Center City Hahnemann Campus is in the middle of Philadelphia, straddling the Vine Street Expressway and centered on Hahnemann University Hospital. Shuttle service is offered between the Center City Hahnemann Campus and both the University City and Queen Lane campuses of the university.

In 2011, The Academy of Natural Sciences entered into an agreement to become a subsidiary of Drexel University. Founded in 1812, the Academy of Natural Sciences is America's oldest natural history museum and is a world leader in biodiversity and environmental research.

On January 5, 2009, Drexel University opened the Center for Graduate Studies in Sacramento, California. Eventually renamed Drexel University Sacramento upon the addition of an undergraduate program in business administration, the campus also offered an Ed.D. program in Educational Leadership and Management and master's degree programs in Business Administration, Finance, Higher Education, Human Resource Development, Public Health, and Interdepartmental Medical Science. On March 5, 2015, Drexel University announced the closure of the Sacramento campus, with an 18-month "phase out" period designed to allow current students to complete their degrees.

The Undergraduate Student Government Association of Drexel University works with administrators to solve student problems and tries to promote communication between the students and the administration.

The Graduate Student Association "advocates the interests and addresses concerns of graduate students at Drexel; strives to enhance graduate student life at the University in all aspects, from academic to campus security; and provides a formal means of communication between graduate students and the University community".

The Campus Activities Board (CAB) is an undergraduate, student-run event planning organization. CAB creates events for the undergraduate population. To assist with planning and organization, the Campus Activities Board is broken down into 5 committees: Special Events, Traditions, Marketing, Culture and Discovery, and Performing and Fine Arts.

WKDU is Drexel's student-run FM radio station, with membership open to all undergraduate students. Its status as an 800-watt, non-commercial station in a major market city has given it a wider audience and a higher profile than many other college radio stations.

DUTV is Drexel's Philadelphia cable television station. The student operated station is part of the Paul F. Harron Studios at Drexel University. The purpose of DUTV is to provide "the people of Philadelphia with quality educational television, and providing Drexel students the opportunity to gain experience in television management and production". The Programing includes an eclectic variety of shows from a bi-monthly news show, DNews, to old films, talk shows dealing with important current issues and music appreciation shows.

"The Triangle" has been the university's newspaper since 1926 and currently publishes on a weekly basis every Friday. The yearbook was first published in 1911 and named the Lexerd in 1913. Prior to the publishing of a campus wide yearbook in 1911 "The Hanseatic" and "The Eccentric" were both published in 1896 as class books. Other publications include "MAYA", the undergraduate student literary and artistic magazine; "D&M Magazine", Design & Merchandising students crafted magazine; "The Smart Set from Drexel University", an online magazine founded in 2005; and "The Drexelist" a blog-style news source founded in 2010.

The Drexel Publishing Group serves as a medium for literary publishing on campus. The Drexel Publishing Group oversees "ASK" (The Journal of the College of Arts and Sciences at Drexel University), "Painted Bride Quarterly", a 36-year-old national literary magazine housed at Drexel; "The 33rd", an annual anthology of student and faculty writing at Drexel; "DPG Online Magazine", and "Maya", the undergraduate literary and artistic magazine. The Drexel Publishing Group also serves as a pedagogical organization by allowing students to intern and work on its publications.

Drexel requires all non-commuting first- and second-year students to live in one of its ten residence halls or in "university approved housing". First year students must live in one of the residence halls designated specifically for first-years. These residence halls include Millennium, Calhoun, Kelly, Myers, Towers, Van Rensselaer and Race Halls. Kelly, Myers, Towers, and Calhoun Halls are traditional residence halls (a bedroom shared with one or more roommate(s) and one bathroom per floor), while Race and Van Rensselaer Halls are suite-style residence halls (shared bedrooms, private bathrooms, kitchens, and common area within the suite). Millennium Hall, Drexel's newest residence hall, is a modified suite (a bedroom shared with one roommate, and bathrooms and showers that look like closets with open sinks in the hallway).

Each residence hall is designed to facilitate the Freshman Experience in a slightly different way. Calhoun, Kelly and Towers Halls are all typical residence halls. Myers Hall offers "Living Learning Communities" where a group of students who share common interests such as language or major live together. Most of Millennium Hall is reserved for students of the Pennoni Honors College, although some floors are occupied by other students.

Second-year students have the option of living in a residence hall designated for upperclassmen, or "university approved housing". The residence halls for upperclassmen are North and Caneris Halls. North Hall operates under the For Students By Students Residential Experience Engagement Model, developed by the Residential Living Office. There are many apartments that are university approved that second-year students can choose to live in. Three of the largest apartment buildings that fit this description are Chestnut Square, University Crossings, and The Summit, all owned by American Campus Communities. Many other students live in smaller apartment buildings or individual townhouse-style apartments in Powelton Village. A second-year student can choose one of the already listed university approved housing options or petition the university to add a new property to the approved list. While living in a university approved apartment offers the freedom of living outside a residence hall, due to the Drexel co-op system, many students end up in the residence halls because they operate on a quarter to quarter basis, and don't require students to be locked into leases.

Graduate students can live in Stiles Hall.

All residence halls except Caneris Hall, University Crossings, and Stiles Memorial Hall are located north of Arch Street between 34th Street and 32nd Street in the Powelton Village area.

Drexel University recognizes over 250 student organizations in the following categories:

The following groups are recognized as honors or professional organizations under the Office of Campus Activities and are not considered part of social Greek life at Drexel University.

Approximately 12 percent of Drexel's undergraduate population are members of a social Greek-letter organization. There are currently thirteen Interfraternity Council (IFC) chapters, seven Panhellenic Council (PHC) chapters and thirteen Multi-cultural Greek Council (MGC) chapters.

Two IFC chapters have been awarded Top Chapters in 2008 by their respective national organizations; Pi Kappa Alpha, and Alpha Chi Rho. In 2013, Sigma Phi Epsilon and Alpha Epsilon Pi were awarded the Top Chapter award by their respective national headquarters.


Drexel's school mascot is a dragon known, as "Mario the Magnificent", named in honor of alumnus and Board of Trustees member Mario V. Mascioli. The Dragon has been the mascot of the school since around the mid-1920s; the first written reference to the Dragons occurred in 1928, when the football team was called "The Dragons in The Triangle". Before becoming known as the Dragons, the athletic teams had been known by such names as the Blue & Gold, the Engineers, and the Drexelites. The school's sports teams, now known as the Drexel Dragons, participate in the NCAA's Division I as a member of the Colonial Athletic Association. They do not currently field a varsity football team.

In addition to its NCAA Division I teams, Drexel University is home to 33 active club teams including lacrosse, water polo, squash, triathlon, and cycling. Other club teams include soccer, baseball, rugby, field hockey, and roller hockey. The club teams operate under the direction of the Club Sports Council and the Recreational Sports Office.

Tradition suggests that rubbing the toe of the bronze "Waterboy" statue, located in the Main Building atrium, can result in receiving good grades on exams. Although the rest of the bronze statue has developed a dark brown patina over the years, the toe has remained highly polished and shines like new.

Frustrated by unresponsive university administrators, students throughout Drexel's history have spoken of a "Drexel Shaft" to describe their interactions with the administration during their academic career at the school. The "Drexel Shaft" was once associated with the Flame of Knowledge fountain, now located in front of North Hall. As the legend of the Drexel Shaft grew larger, however, the "shaft" itself grew alongside the legend. Eventually, the chimney atop the Amtrak Boiler House in Penn Coach Yard, located just east of 32nd Street on the University City main campus, came to embody the unresponsive treatment that frustrated many students during their time at Drexel. The smokestack was demolished, to cheers by students and faculty members alike, in November 15, 2009, in what the university community hopes will be a transformation of both the campus' aesthetics and the legend of the "Drexel Shaft" itself.

During the fall of 2014, a chemical leak was detected at Millennium Hall, one of the student resident buildings. A hazmat crew was called in to deal with the leak, and the building was evacuated. No one was injured in the event. The chemical turned out to be Freon, with the source being one of the air conditioning units in the building. Along with the Drexel Shaft, Freon has become a meme within the university’s student community.

Drexel has appeared in news and television media several times. In 2006 Drexel served as the location for ABC Family's reality show "Back on Campus". Also in 2006, the Epsilon Zeta chapter of Delta Zeta won ABC Daytime's Summer of Fun contest. As a result, the sorority was featured in national television spots for a week and hosted an ABC party on campus, which was attended by cast members from "General Hospital" and "All My Children".

John Langdon, adjunct professor in the Antoinette Westphal College of Media Arts & Design, created the ambigram featured on the cover of Dan Brown's Angels & Demons; a number of other ambigrams served as the central focus of the book and its corresponding film. It is believed Prof. Langdon was the inspiration for the name of the lead character, played by Tom Hanks in the film adaptation.

Howard Benson, a Drexel alumnus and a music producer associated with Hoobastank, Creed and Kelly Clarkson, teaches a music production master class at Drexel.

Drexel University was a sponsor of Matthew Quick's novel "Silver Linings Playbook," which was made into a movie in 2012. Matthew Quick held several lectures at Drexel University.

In 2007, Drexel was the host of the 2008 Democratic Presidential candidate debate in Philadelphia, televised by MSNBC. The university hosted the US Table Tennis Olympic Trials between January 10 and January 13, 2008. Drexel University also hosted the 2011 U.S. Open Squash Championships from October 1–6, 2011, as well as the 2012 U.S. Open Squash Championships from October 4–12, 2012.

In the U.S. TV Series "House of Cards", Congressman Peter Russo (played by Corey Stoll) is a graduate of Drexel University.

Since its founding the university has graduated over 100,000 alumni. Certificate-earning alumni such as artist Violet Oakley and illustrator Frank Schoonover reflect the early emphasis on art as part of the university's curriculum. With World War II, the university's technical programs swelled, and as a result Drexel graduated alumni such as Paul Baran, one of the founding fathers of the Internet and one of the inventors of the packet switching network, and Norman Joseph Woodland the inventor of barcode technology. In addition to its emphasis on technology Drexel has graduated several notable athletes such as National Basketball Association (NBA) basketball players Michael Anderson and Malik Rose, and several notable business people such as Raj Gupta, former President and Chief executive officer (CEO) of Rohm and Haas, and Kenneth C. Dahlberg, former CEO of Science Applications International Corporation (SAIC). Alassane Dramane Ouattara President of the Republic of Ivory Coast. In 2018, Tirthak Saha -a 2016 graduate of the ECE school - was named to the Forbes 30 Under 30 list for achievements in the Energy field.

In 1991, the university's centennial anniversary, Drexel created an association called the Drexel 100, for alumni who have demonstrated excellence work, philanthropy, or public service. After the creation of the association 100 alumni were inducted in 1992 and since then the induction process has been on a biennial basis. In 2006 164 total alumni had been inducted into the association.

Drexel University created the annual $100,000 Anthony J. Drexel Exceptional Achievement Award to recognize a faculty member from a U.S. institution whose work transforms both research and the society it serves. The first recipient was bioengineer James J. Collins of Boston University (now at MIT) and the Howard Hughes Medical Institute.

In 2004, in conjunction with BAYADA Home Health Care, Drexel University's College of Nursing and Health Professions created the BAYADA Award for Technological Innovation in Nursing Education and Practice. The award honors nursing educators and practicing nurses whose innovation leads to improved patient care or improved nursing education.



</doc>
<doc id="8258" url="https://en.wikipedia.org/wiki?curid=8258" title="Daedalus">
Daedalus

In Greek mythology, Daedalus (; "Daidalos" "cunningly wrought", perhaps related to δαιδάλλω "to work artfully"; ; Etruscan: "Taitale") was a skillful craftsman and artist. He is the father of Icarus, the uncle of Perdix, and possibly also the father of Iapyx, although this is unclear.

Daedalus's parentage was supplied as a later addition, providing him with a father in Metion, Eupalamus, or Palamaon, and a mother, Alcippe, Iphinoe, or Phrasmede. Daedalus had two sons: Icarus and Iapyx, along with a nephew either Talos or Perdix.

Athenians transferred Cretan Daedalus to make him Athenian-born, the grandson of the ancient king Erechtheus, claiming that Daedalus fled to Crete after killing his nephew Talos. Over time, other stories were told of Daedalus.

Some Greeks claimed to be descendants of Daedalus. People in ancient Greece often believed that their family was descended from mythical characters, though it is likely these characters were seen as historical rather than fictional. The Greek philosopher Socrates claimed to be a descendant of Daedalus in at least three of Plato's books: "Euthyphro", "Alcibaides", and "Meno". In "Euthyphro," Socrates says, "Your statements, Euthyphro, seem to belong to my ancestor, Daedalus." In "Alcibaides", Socrates says, "And mine, noble Alcibiades, to Daedalus, and he to Hephaestus, son of Zeus." This suggests that Daedalus was related to Hephaestus, the god of blacksmiths, and therefore to Zeus himself.

Daedalus is first mentioned by Homer as the creator of a wide dancing-ground for Ariadne. He also created the Labyrinth on Crete, in which the Minotaur (part man, part bull) was kept. In the story of the labyrinth as told by the Hellenes, the Athenian hero Theseus is challenged to kill the Minotaur, finding his way with the help of Ariadne's thread. Daedalus' appearance in Homer is in an extended metaphor, "plainly not Homer's invention", Robin Lane Fox observes: "He is a point of comparison and so he belongs in stories which Homer's audience already recognized." In Bronze Age Crete, an inscription "da-da-re-jo-de" has been read as referring to a place at Knossos, and a place of worship.

In Homer's language, "daidala" refers to finely crafted objects. They are mostly objects of armor, but fine bowls and furnishings are also "daidala", and on one occasion so are the "bronze-working" of "clasps, twisted brooches, earrings and necklaces" made by Hephaestus while cared for in secret by the goddesses of the sea.

Ignoring Homer, later writers envisaged the Labyrinth as an edifice rather than a single dancing path to the center and out again, and gave it numberless winding passages and turns that opened into one another, seeming to have neither beginning nor end. Ovid, in his "Metamorphoses", suggests that Daedalus constructed the Labyrinth so cunningly that he himself could barely escape it after he built it. Daedalus built the labyrinth for King Minos, who needed it to imprison his wife's son the Minotaur. The story is told that Poseidon had given a white bull to Minos so that he might use it as a sacrifice. Instead, Minos kept it for himself; and in revenge, Poseidon, with the help of Aphrodite, made Pasiphaë, King Minos's wife, lust for the bull. For Pasiphaë, as Greek mythologers interpreted it, Daedalus also built a wooden cow so she could mate with the bull, for the Greeks imagined the Minoan bull of the sun to be an actual, earthly bull, the slaying of which later required a heroic effort by Theseus.

This story thus encourages others to consider the long-term consequences of their own inventions with great care, lest those inventions do more harm than good. As in the tale of Icarus' wings, Daedalus is portrayed assisting in the creation of something that has subsequent negative consequences, in this case with his creation of the monstrous Minotaur's almost impenetrable Labyrinth, which made slaying the beast an endeavour of legendary difficulty.

The most familiar literary telling explaining Daedalus' wings is a late one, that of Ovid: in his "Metamorphoses" (VIII:183–235) Daedalus was shut up in a tower to prevent the knowledge of his Labyrinth from spreading to the public. He could not leave Crete by sea, as the king kept a strict watch on all vessels, permitting none to sail without being carefully searched. Since Minos controlled the land and sea routes, Daedalus set to work to fabricate wings for himself and his young son Icarus. He tied feathers together, from smallest to largest so as to form an increasing surface. He secured the feathers at their midpoints with string and at their bases with wax, and gave the whole a gentle curvature like the wings of a bird. When the work was done, the artist, waving his wings, found himself buoyed upward and hung suspended, poising himself on the beaten air. He next equipped his son in the same manner, and taught him how to fly. When both were prepared for flight, Daedalus warned Icarus not to fly too high, because the heat of the sun would melt the wax, nor too low, because the sea foam would soak the feathers.

They had passed Samos, Delos and Lebynthos by the time the boy, forgetting himself, began to soar upward toward the sun. The blazing sun softened the wax that held the feathers together and they came off. Icarus quickly fell in the sea and drowned. His father cried, bitterly lamenting his own arts, and called the island near the place where Icarus fell into the ocean Icaria in memory of his child. Some time later, the goddess Athena visited Daedalus and gave him wings, telling him to fly like a god.

An early image of winged Daedalus appears on an Etruscan jug of ca 630 BC found at Cerveteri, where a winged figure captioned "Taitale" appears on one side of the vessel, paired on the other side, uniquely, with "Metaia", Medea: "its linking of these two mythical figures is unparalleled," Robin Lane Fox observes: "The link was probably based on their wondrous, miraculous art. Magically, Daedalus could fly, and magically Medea was able to rejuvenate the old (the scene on the jug seems to show her doing just this)". The image of Daedalus demonstrates that he was already well known in the West.

Further to the west Daedalus arrived safely in Sicily, in the care of King Cocalus of Kamikos on the island's south coast; there Daedalus built a temple to Apollo, and hung up his wings, an offering to the god. In an invention of Virgil ("Aeneid" VI), Daedalus flies to Cumae and founds his temple there, rather than in Sicily; long afterward Aeneas confronts the sculpted golden doors of the temple.

Minos, meanwhile, searched for Daedalus by traveling from city to city asking a riddle. He presented a spiral seashell and asked for a string to be run through it. When he reached Kamikos, King Cocalus, knowing Daedalus would be able to solve the riddle, privately fetched the old man to him. He tied the string to an ant which, lured by a drop of honey at one end, walked through the seashell stringing it all the way through. Minos then knew Daedalus was in the court of King Cocalus and demanded he be handed over. Cocalus managed to convince Minos to take a bath first, where Cocalus' daughters killed Minos. In some versions, Daedalus himself poured boiling water on Minos and killed him.

The anecdotes are literary and late; however, in the founding tales of the Greek colony of Gela, founded in the 680s on the southwest coast of Sicily, a tradition was preserved that the Greeks had seized cult images wrought by Daedalus from their local predecessors, the Sicani.

Daedalus was so proud of his achievements that he could not bear the idea of a rival. His sister had placed her son, named variously as Perdix, Talos, or Calos, under his charge to be taught the mechanical arts. He was an art scholar and showed striking evidence of ingenuity. Walking on the seashore, he picked up the spine of a fish. According to Ovid, imitating it, he took a piece of iron and notched it on the edge, and thus invented the saw. He put two pieces of iron together, connecting them at one end with a rivet, and sharpening the other ends, and made a pair of compasses. Daedalus was so envious of his nephew's accomplishments that he took an opportunity and caused him to fall from the Acropolis. Athena turned Perdix into a partridge and left a scar that looked like a partridge on Daedalus' right shoulder and Daedalus left Athens due to this.

Such anecdotal details as these were embroideries upon the reputation of Daedalus as an innovator in many arts. In Pliny's Natural History (7.198) he is credited with inventing carpentry "and with it the saw, axe, plumb-line, drill, glue, and isinglass". Pausanias, in travelling around Greece, attributed to Daedalus numerous archaic wooden cult figures (see "xoana") that impressed him: "All the works of this artist, though somewhat uncouth to look at, nevertheless have a touch of the divine in them."

It is said he first conceived masts and sails for ships for the navy of Minos. He is said to have carved statues so well they looked as if alive; even possessing self-motion. They would have escaped if not for the chain that bound them to the wall.

Daedalus gave his name, eponymously, to any Greek artificer and to many Greek contraptions that represented dextrous skill. At Plataea there was a festival, the Daedala, in which a temporary wooden altar was fashioned, and an effigy was made from an oak-tree and dressed in bridal attire. It was carried in a cart with a woman who acted as bridesmaid. The image was called "Daedale" and the archaic ritual given an explanation through a myth to the purpose

In the period of Romanticism, Daedalus came to denote the classic artist, a skilled mature craftsman, while Icarus symbolized the romantic artist, whose impetuous, passionate and rebellious nature, as well as his defiance of formal aesthetic and social conventions, may ultimately prove to be self-destructive. Stephen Dedalus, in Joyce's "Portrait of the Artist as a Young Man" envisages his future artist-self "a winged form flying above the waves ... a hawk-like man flying sunward above the sea, a prophecy of the end he had been born to serve”.

Daedalus is said to have created statues that were so realistic that they had to be tied down to stop them from wandering off. In "Meno", Socrates and Meno are debating the nature of knowledge and true belief when Socrates refers to Daedalus' statues: "... if they are not fastened up they play truant and run away; but, if fastened, they stay where they are."





</doc>
<doc id="8259" url="https://en.wikipedia.org/wiki?curid=8259" title="Deception Pass">
Deception Pass

Deception Pass is a strait separating Whidbey Island from Fidalgo Island, in the northwest part of the U.S. state of Washington. It connects Skagit Bay, part of Puget Sound, with the Strait of Juan de Fuca. A pair of bridges known collectively as Deception Pass Bridge cross Deception Pass, and the bridges are on the National Register of Historic Places.

The Deception Pass area has been home to various Coast Salish tribes for thousands of years. The first Europeans to see Deception Pass were members of the 1790 expedition of Manuel Quimper on the "Princesa Real". The Spanish gave it the name "Boca de Flon". A group of sailors led by Joseph Whidbey, part of the Vancouver Expedition, found and mapped Deception Pass on June 7, 1792. George Vancouver gave it the name "Deception" because it had misled him into thinking Whidbey Island was a peninsula. The "deception" was heightened due to Whidbey's failure to find the strait at first. In May 1792, Vancouver was anchored near the southern end of Whidbey Island. He sent Joseph Whidbey to explore the waters east of Whidbey Island, now known as Saratoga Passage, using small boats. Whidbey reached the northern end of Saratoga Passage and explored eastward into Skagit Bay, which is shallow and difficult to navigate. He returned south to rejoin Vancouver without having found Deception Pass. It appeared that Skagit Bay was a dead-end and that Whidbey Island and Fidalgo Island were a long peninsula attached to the mainland. In June the expedition sailed north along the west coast of Whidbey Island. Vancouver sent Joseph Whidbey to explore inlets leading to the east. The first inlet turned out to be a "very narrow and intricate channel, which...abounded with rocks above and beneath the surface of the water". This channel led to Skagit Bay, thus separating Whidbey Island from the mainland. Vancouver apparently felt he and Joseph Whidbey had been deceived by the tricky strait. Vancouver wrote of Whidbey's efforts: "This determined [the shore they had been exploring] to be an island, which, in consequence of Mr. Whidbey’s circumnavigation, I distinguished by the name of Whidbey’s Island: and this northern pass, leading into [Skagit Bay], Deception Passage".

In the waters of Deception Pass, just east of the present-day Deception Pass Bridge, is a small island known as Ben Ure Island. The island became infamous for its activity of human smuggling of migrant Chinese people for local labor. Ben Ure and his partner Lawrence "Pirate" Kelly were quite profitable at their human smuggling business and played hide-and-seek with the United States Customs Department for years. Ure's own operation at Deception Pass in the late 1880s consisted of Ure and his Native-American wife. Local tradition has it that his wife would camp on the nearby Strawberry Island (which was visible from the open sea) and signal him with a fire on the island's summit to alert him to whether or not it was safe to attempt to bring the human cargo he illegally transported ashore. For transport, Ure would tie the people up in burlap bags so that if customs agents were to approach he could easily toss the people in bags overboard. The tidal currents would carry the entrapped drowned migrants' bodies to San Juan Island to the north and west of the pass and many ended up in what became known as Dead Man's Bay.

Between the years 1910 and 1914, a prison rock quarry was operated on the Fidalgo Island side of the pass. Nearby barracks housed some 40 prisoners, members of an honors program out of Walla Walla State Penitentiary and the prison population was made up of several types of prisoners, including those convicted of murder. Guards stood watch at the quarry as the prisoners cut the rock into gravel and loaded it onto barges located at the base of the cliff atop the pass's waters. The quarried rock was then taken by barge to the Seattle waterfront. The camp was dismantled in 1924 and although abandoned as a quarry, the remains of the camp can still be found. The location, however, is hazardous and over the years there have been several fatal accidents when visitors have ventured onto the steep cliffs.

Upon completion on July 31, 1935, the span Deception Pass Bridge connected Whidbey Island to the tiny Pass Island, and Pass Island to Fidalgo Island. Prior to the bridge, travellers and businessmen would use an inter-island ferry to commute between Fidalgo and Whidbey islands.

Deception Pass is a dramatic seascape where the tidal flow and whirlpools beneath the twin bridges connecting Fidalgo Island to Whidbey Island move quickly. During ebb and flood tide current speed reaches about , flowing in opposite directions between ebb and flood. This swift current can lead to standing waves, large whirlpools, and roiling eddies. This swift current phenomenon can be viewed from the twin bridges' pedestrian walkways or from the trail leading below the larger south bridge from the parking lot on the Whidbey Island side. Boats can be seen waiting on either side of the pass for the current to stop or change direction before going through. Thrill-seeking kayakers go there during large tide changes to surf the standing waves and brave the class 2 and 3 rapid conditions.

Diving Deception Pass is dangerous and only for the most competent and prepared divers. There are a few times each year that the tides are right for a drift dive from the cove, under the bridge, and back to the cove as the tide changes. These must be planned well in advance by divers who know how to read currents and are aware of the dangerous conditions. However, because of the large tidal exchange, Deception Pass hosts some of the most spectacular colors and life in the Pacific Northwest. The walls and bottom are covered in colorful invertebrates, lingcod, greenlings, and barnacles everywhere.

Deception Pass is today surrounded by Deception Pass State Park, the most-visited park in Washington with over 2 million visitors each year. The park was officially established in 1923, when the original of a military reserve was transferred to Washington State Parks. The park's facilities were greatly enhanced in the 1930s when the Civilian Conservation Corps (CCC) built roads, trails, and buildings in order to develop the park.

The road to West Beach was created in 1950, opening up a stretch of beach to hordes of vehicles. The former fish hatchery at Bowman Bay became a part of the park in the early 1970s. The old entrance to the park was closed in 1997 when a new entrance was created at the intersection of Highway 20 and Cornet Bay road, improving access into and out of the park.

Deception Pass State Park has a number of recreational opportunities, including three campgrounds, several hiking trails, beaches, and tidepools. Several miles of the Pacific Northwest Trail are within the park, most notably including the section that crosses Deception Pass on the Highway 20 bridge. In addition, the Cornet Bay Retreat Center provides cabins and dining and recreation facilities. Cornet Bay offers boat launches and fishing opportunities, while Bowman Bay has an interpretive center that explains the story of the Civilian Conservation Corps throughout Washington state. Near the center is a CCC honor statue, which can be found in 30 different states in the country. Fishing is popular in Pass Lake, on the north side of the bridge. Boat rentals and guided tours of the park are also offered.

Included in the park are ten islands: Northwest Island, Deception Island, Pass Island, Strawberry, Ben Ure, Kiket, Skagit, Hope, and Big and Little Deadman Islands. Ben Ure Island is partially privately owned. The island is not open to the public except for a small rentable cabin available via the state park, which is only accessible by rowboat.

The 2002 horror movie "The Ring" was in part filmed near the pass.

The bridge is fictionalized as a toll bridge named "Desolation Bridge" in season one of The Killing.

Seattle shoegaze act The Sight Below filmed the 2008 video for their track "Further Away" at Deception Pass, with Deception Island's scenic imagery prominently featured.

Seattle grunge band Mudhoney named a song on their 1993 EP Five Dollar Bob's Mock Cooter Stew "Deception Pass."

Seattle progressive rock band Queensrÿche filmed scenes of their video "Anybody Listening" near Deception Pass and Deception Island.




</doc>
<doc id="8262" url="https://en.wikipedia.org/wiki?curid=8262" title="Dominoes">
Dominoes

Dominoes is a family of tile-based games played with rectangular "domino" tiles. Each domino is a rectangular tile with a line dividing its face into two square "ends". Each end is marked with a number of spots (also called "pips", "nips", or "dobs") or is blank. The backs of the dominoes in a set are indistinguishable, either blank or having some common design. The domino gaming pieces (colloquially nicknamed "bones", "cards", "tiles", "tickets", "stones", "chips", or "spinners") make up a domino set, sometimes called a "deck" or "pack". The traditional Sino-European domino set consists of 28 dominoes, featuring all combinations of spot counts between zero and six. A domino set is a generic gaming device, similar to playing cards or dice, in that a variety of games can be played with a set.

The earliest mention of dominoes is from Song dynasty China found in the text "Former Events in Wulin" by Zhou Mi (1232–1298). Modern dominoes first appeared in Italy during the 18th century, but how Chinese dominoes developed into the modern game is unknown. Italian missionaries in China may have brought the game to Europe.

The name "domino" is most likely from the resemblance to a kind of carnival costume worn during the Venetian Carnival, often consisting of a black-hooded robe and a white mask. Despite the coinage of the word polyomino as a generalization, there is no connection between the word "domino" and the number 2 in any language.

European-style dominoes are traditionally made of bone or ivory, or a dark hardwood such as ebony, with contrasting black or white pips (inlaid or painted). Alternatively, domino sets have been made from many different natural materials: stone (e.g., marble, granite or soapstone); other hardwoods (e.g., ash, oak, redwood, and cedar); metals (e.g., brass or pewter); ceramic clay, or even frosted glass or crystal. These sets have a more novel look, and the often heavier weight makes them feel more substantial; also, such materials and the resulting products are usually much more expensive than polymer materials. 

Modern commercial domino sets are usually made of synthetic materials, such as ABS or polystyrene plastics, or Bakelite and other phenolic resins; many sets approximate the look and feel of ivory while others use colored or even translucent plastics to achieve a more contemporary look. Modern sets also commonly use a different color for the dots of each different end value (one-spots might have black pips while two-spots might be green, three red, etc.) to facilitate finding matching ends. Occasionally, one may find a domino set made of card stock like that for playing cards. Such sets are lightweight, compact, and inexpensive, and like cards are more susceptible to minor disturbances such as a sudden breeze. Sometimes, dominoes have a metal pin (called a spinner or pivot) in the middle.

The traditional set of dominoes contains one unique piece for each possible combination of two ends with zero to six spots, and is known as a double-six set because the highest-value piece has six pips on each end (the "double six"). The spots from one to six are generally arranged as they are on six-sided dice, but because blank ends having no spots are used, seven faces are possible, allowing 28 unique pieces in a double-six set.

However, this is a relatively small number especially when playing with more than four people, so many domino sets are "extended" by introducing ends with greater numbers of spots, which increases the number of unique combinations of ends and thus of pieces. Each progressively larger set increases the maximum number of pips on an end by three, so the common extended sets are double-nine, double-12, double-15, and double-18. Larger sets such as double-21 can theoretically exist, but are rarely seen in retail stores, as identifying the number of pips on each domino becomes difficult, and a double-21 set would have 253 pieces, far more than is normally necessary for most domino games even with eight players.

The oldest confirmed written mention of dominoes in China comes from the "Former Events in Wulin" (i.e., the capital Hangzhou) written by the Yuan Dynasty (1271–1368) author Zhou Mi (1232–1298), who listed "pupai" (gambling plaques or dominoes), as well as dice as items sold by peddlers during the reign of Emperor Xiaozong of Song (r. 1162–1189). Andrew Lo asserts that Zhou Mi meant dominoes when referring to "pupai", since the Ming author Lu Rong (1436–1494) explicitly defined "pupai" as dominoes (in regard to a story of a suitor who won a maiden's hand by drawing out four winning "pupai" from a set).

The earliest known manual written about dominoes is the "(Manual of the Xuanhe Period)" written by Qu You (1341–1437), but some Chinese scholars believe this manual is a forgery from a later time.

In the "Encyclopedia of a Myriad of Treasures", Zhang Pu (1602–1641) described the game of laying out dominoes as "pupai", although the character for "pu" had changed, yet retained the same pronunciation. Traditional Chinese domino games include "Tien Gow, Pai Gow, Che Deng", and others. The 32-piece Chinese domino set, made to represent each possible face of two thrown dice and thus have no blank faces, differs from the 28-piece domino set found in the West during the mid 18th century. Chinese dominoes with blank faces were known during the 17th century.

Many different domino sets have been used for centuries in various parts of the world to play a variety of domino games. Each domino originally represented one of the 21 results of throwing two six-sided dice (2d6). One half of each domino is set with the pips from one die and the other half contains the pips from the second die. Chinese sets also introduce duplicates of some throws and divide the dominoes into two suits: military and civil. Chinese dominoes are also longer than typical European dominoes.

The early 18th century had dominoes making their way to Europe, making their first appearance in Italy. The game changed somewhat in the translation from Chinese to the European culture. European domino sets contain neither suit distinctions nor the duplicates that went with them. Instead, European sets contain seven additional dominoes, with six of these representing the values that result from throwing a single die with the other half of the tile left blank, and the seventh domino representing the blank-blank (0–0) combination.

Ivory dominoes were routinely used in 19th-century rural England in the settling of disputes over traditional grazing boundaries, and were commonly referred to as "bonesticks".

Domino tiles, also known as bones, are twice as long as they are wide and usually have a line in the middle dividing them into two squares. The value of either side is the number of spots or pips. In the most common variant (double-six), the values range from blank or no pips to six. The sum of the two values, i.e. the total number of pips, may be referred to as the rank or weight of a tile, and a tile with more pips may be called heavier than a lighter tile with fewer pips.

Tiles are generally named after their two values; e.g. deuce-five or five-deuce (2–5 or 5–2) are alternative ways of describing the tile with the values two and five. Tiles that have the same value on both ends are called doubles, and are typically referred to as double-zero, double-one, etc. Tiles with two different values are called singles.

Every tile belongs to the two suits of its two values, e.g. 0–3 belongs both to the blank suit (or 0 suit) and to the 3 suit. Naturally the doubles form an exception in that each double belongs to only one suit. In 42, the doubles can be treated as an additional suit of doubles, so the double-six (6–6) belongs both to the six suit and the suit of doubles.

The most common domino sets commercially available are double six (with 28 tiles) and double nine (with 55 tiles). Larger sets exist and are popular for games involving several players or for players looking for long domino games. The number of tiles in a set has the formula formula_1 for a double-"n" set.

The most popular type of play are layout games, which fall into two main categories, blocking games and scoring games.


The most basic domino variant is for two players and requires a double-six set. The 28 tiles are shuffled face down and form the "stock" or "boneyard". Each player draws seven tiles; the remainder are not used. Once the players begin drawing tiles, they are typically placed on-edge in front of the players, so each player can see their own tiles, but none can see the value of other players' tiles. Every player can thus see how many tiles remain in the opponent's hands at all times during gameplay.

One player begins by downing (playing the first tile) one of their tiles. This tile starts the line of play, in which values of adjacent pairs of tile ends must match. The players alternately extend the line of play with one tile at one of its two ends; if a player is unable to place a valid tile, they must keep on pulling tiles from the stock until they can. The game ends when one player wins by playing their last tile, or when the game is blocked because neither player can play. If that occurs, whoever caused the block gets all of the remaining player points not counting their own.

Players accrue points during game play for certain configurations, moves, or emptying one's hand. Most scoring games use variations of the draw game. If a player does not call "domino" before the tile is laid on the table, and another player says domino after the tile is laid, the first player must pick up an extra domino.

In a draw game (blocking or scoring), players are additionally allowed to draw as many tiles as desired from the stock before playing a tile, and they are not allowed to pass before the stock is (nearly) empty. The score of a game is the number of pips in the losing player's hand plus the number of pips in the stock. Most rules prescribe that two tiles need to remain in the stock. The draw game is often referred to as simply "dominoes".

Adaptations of both games can accommodate more than two players, who may play individually or in teams.

The line of play is the configuration of played tiles on the table. It starts with a single tile and typically grows in two opposite directions when players add matching tiles. In practice, players often play tiles at right angles when the line of play gets too close to the edge of the table.

The rules for the line of play often differ from one variant to another. In many rules, the doubles serve as spinners, i.e., they can be played on all four sides, causing the line of play to branch. Sometimes, the first tile is required to be a double, which serves as the only spinner. In some games such as Chicken Foot, all sides of a spinner must be occupied before anybody is allowed to play elsewhere. Matador has unusual rules for matching. Bendomino uses curved tiles, so one side of the line of play (or both) may be blocked for geometrical reasons.

In Mexican Train and other train games, the game starts with a spinner from which various trains branch off. Most trains are owned by a player and in most situations players are allowed to extend only their own train.

In blocking games, scoring happens at the end of the game. After a player has emptied their hand, thereby winning the game for the team, the score consists of the total pip count of the losing team's hands. In some rules, the pip count of the remaining stock is added. If a game is blocked because no player can move, the winner is often determined by adding the pips in players' hands.

In scoring games, each individual can potentially add to the score. For example, in Bergen, players score two points whenever they cause a configuration in which both open ends have the same value and three points if additionally one open end is formed by a double. In Muggins, players score by ensuring the total pip count of the open ends is a multiple of a certain number. In variants of Muggins, the line of play may branch due to spinners.

In British public houses and social clubs, a scoring version of "5s-and-3s" is used. The game is normally played in pairs (two against two) and is played as a series of "ends". In each "end", the objective is for players to attach a domino from their hand to one end of those already played so that the sum of the end dominoes is divisible by five or three. One point is scored for each time five or three can be divided into the sum of the two dominoes, i.e. four at one end and five at the other makes nine, which is divisible by three three times, resulting in three points. Double five at one end and five at the other makes 15, which is divisible by three five times (five points) and divisible by five three times (three points) for a total of eight points.

An "end" stops when one of the players is out, i.e., has played all of their dominoes. In the event no player is able to empty their hand, then the player with the lowest domino left in hand is deemed to be out and scores one point. A game consists of any number of ends with points scored in the ends accumulating towards a total. The game ends when one of the pair's total score exceeds a set number of points. A running total score is often kept on a cribbage board. 5s-and-3s is played in a number of competitive leagues in the British Isles.

For 40 years the game has been played by four people, with the winner being the first player to score 150 points, in multiples of five, by using 27 bones, using mathematical strategic defenses and explosive offense. At times, it has been played with pairs of partners. The double-six set is the preferred deck with the lowest denomination of game pieces, with 28 dominoes.

In many versions of the game, the player with the highest double leads with that double, for example "double-six". If no one has it, the next-highest double is called: "double-five?", then "double-four?", etc. until the highest double in any of the players' hands is played. If no player has an "opening" double, the next heaviest domino in the highest suit is called - "six-five?", "six-four?". In some variants, players take turns picking dominoes from the stock until an opening double is picked and played. In other variants, the hand is reshuffled and each player picks seven dominoes. After the first hand, the winner (or winning team) of the previous hand is allowed to pick first and begins by playing any domino in his or her hand.

Playing the first bone of a hand is sometimes called setting, leading, downing, or posing the first bone. Dominoes aficionados often call this procedure smacking down the bone. After each hand, bones are shuffled and each player draws the number of bones required, normally seven. Play proceeds clockwise. Players, in turn, must play a bone with an end that matches one of the open ends of the layouts.

In some versions of the games, the pips or points on the end, and the section to be played next to it must add up to a given number. For example, in a double-six set, the "sum" would be six, requiring a blank to be played next to a six, an ace (one) next to a five, a deuce (two) next to a four, etc.

The stock of bones left behind, if any, is called the bone yard, and the bones therein are said to be sleeping. In draw games, players take part in the bone selection, typically drawing from the bone yard when they do not have a "match" in their hands.

If a player inadvertently picks up and sees one or more extra dominoes, those dominoes become part of his or her hand.

A player who can play a tile may be allowed to pass anyway. Passing can be signalled by tapping twice on the table or by saying "go" or "pass".

Play continues until one of the players has played all the dominoes in his or her hand, calls "Out!", "I win", or "Domino!" and wins the hand, or until all players are blocked and no legal plays remain. This is sometimes referred to as locked down or sewed up. In a common version of the game, the next player after the block picks up all the dominoes in the bone yard as if trying to find a (nonexistent) match. If all the players are blocked, or locked out, the player with the lowest hand (pip count) wins. In team play, the team with the lowest individual hand wins. In the case of a tie, the first of tied players or the first "team" in the play rotation wins.

In games where points accrue, the winning player scores a point for each pip on each bone still held by each opponent or the opposing team. If no player went out, the win is determined by the lightest hand, sometimes only the excess points held by opponents.

A game is generally played to 100 points, the tally being kept on paper. In more common games, mainly urban rules, games are played to 150, 200, or 250 points.

In some games, the tally is kept by creating , where the beginning of the house (the first 10 points) is a large +, the next 10 points are O, and scoring with a five is a /, and are placed in the four corners of the house. One house is equal to 50 points.

In some versions, if a lock down occurs, the first person to call a lock-down gains the other players bones and adds the amount of the pips to his or her house. If a person who calls rocks after a call of lock-down or domino finds the number of pips a player called is incorrect, those points become his.

When a player plays out of turn or knocks when he could have played and someone calls bogus play, the other person is awarded 50 points.

Apart from the usual blocking and scoring games, also domino games of a very different character are played, such as solitaire or trick-taking games. Most of these are adaptations of card games and were once popular in certain areas to circumvent religious proscriptions against playing cards.
A very simple example is a Concentration variant played with a double-six set; two tiles are considered to match if their total pip count is 12.

A popular domino game in Texas is 42. The game is similar to the card game spades. It is played with four players paired into teams. Each player draws seven dominoes, and the dominoes are played into tricks. Each trick counts as one point, and any domino with a multiple of five dots counts toward the total of the hand. These 35 points of "five count" and seven tricks equals 42 points, hence the name.

Dominoes is played at a professional level, similar to poker. Numerous organisations and clubs of amateur domino players exist around the world. Some organizations, including the "Fédération Internationale de Domino (FIDO)" organize international competitions. The 2008 and 2009 Double FIDO domino world champion from the UK is Darren Elhindi.

Besides playing games, another use of dominoes is the domino show, which involves standing them on end in long lines so that when the first tile is toppled, it topples the second, which topples the third, etc., resulting in all of the tiles falling. By analogy, the phenomenon of small events causing similar events leading to eventual catastrophe is called the domino effect.

Arrangements of millions of tiles have been made that have taken many minutes, even hours to fall. For large and elaborate arrangements, special blockages (also known as firebreaks) are employed at regular distances to prevent a premature toppling from undoing more than a section of the dominoes while still being able to be removed without damage.

The phenomenon also has some theoretical relevance (amplifier, digital signal, information processing), and this amounts to the theoretical possibility of building domino computers. Dominoes are also commonly used as components in Rube Goldberg machines.

The Netherlands has hosted an annual domino-toppling exhibition called Domino Day since 1986. The event held on 18 November 2005 knocked over 4 million dominoes by a team from Weijers Domino Productions. On Domino Day 2008 (14 November 2008), the Weijers Domino Productions team attempted to set 10 records:
This record attempt was held in the in Leeuwarden. The artist who toppled the first stone was the Finnish acrobat Salima Peippo.

At one time, Pressman Toys manufactured a product called Domino Rally that contained tiles and mechanical devices for setting up toppling exhibits.

In Berlin on 9 November 2009, giant dominoes were toppled in a 20th-anniversary commemoration of the fall of the Berlin Wall. Former Polish president and Solidarity leader Lech Wałęsa set the toppling in motion.

Since April 2008, the character encoding standard Unicode includes characters that represent the double-six domino tiles in various orientations. All combinations of blank through six pips on the left or right provides 49 glyphs, the same combinations vertically for another 49, and also a horizontal and a vertical "back" for a total of 100 glyphs. In this arrangement, both orientations are present: horizontally both tiles [1|6] and [6|1] exist, while a regular game set only has one such tile. The Unicode range for dominoes is U+1F030–U+1F09F. The naming pattern in Unicode is, by example, . Few fonts are known to support these glyphs. While the complete domino set has only 28 tiles, for printing layout reasons, the Unicode set needs both horizontal and vertical forms for each tile, plus the 01-03 (plain) 03-01 (reversed) pairs, and generic backsides.






</doc>
<doc id="8263" url="https://en.wikipedia.org/wiki?curid=8263" title="Dissociation constant">
Dissociation constant

In chemistry, biochemistry, and pharmacology, a dissociation constant (formula_1) is a specific type of equilibrium constant that measures the propensity of a larger object to separate (dissociate) reversibly into smaller components, as when a complex falls apart into its component molecules, or when a salt splits up into its component ions. The dissociation constant is the inverse of the association constant. In the special case of salts, the dissociation constant can also be called an ionization constant.

For a general reaction:

</chem>

in which a complex formula_2 breaks down into "x" A subunits and "y" B subunits, the dissociation constant is defined

where [A], [B], and [AB] are the concentrations of A, B, and the complex AB, respectively.

One reason for the popularity of the dissociation constant in biochemistry and pharmacology is that in the frequently encountered case where x=y=1, K has a simple physical interpretation: when formula_4, formula_5 or equivalently formula_6

The dissociation constant of water is denoted "K":

The concentration of water <chem>H2O</chem> is omitted by convention, which means that the value of "K" differs from the value of "K" that would be computed using that concentration.

The value of "K" varies with temperature, as shown in the table below. This variation must be taken into account when making precise measurements of quantities such as pH.



</doc>
<doc id="8267" url="https://en.wikipedia.org/wiki?curid=8267" title="Dimensional analysis">
Dimensional analysis

In engineering and science, dimensional analysis is the analysis of the relationships between different physical quantities by identifying their base quantities (such as length, mass, time, and electric charge) and units of measure (such as miles vs. kilometers, or pounds vs. kilograms) and tracking these dimensions as calculations or comparisons are performed. The conversion of units from one dimensional unit to another is often somewhat complex. Dimensional analysis, or more specifically the factor-label method, also known as the unit-factor method, is a widely used technique for such conversions using the rules of algebra.

The concept of physical dimension was introduced by Joseph Fourier in 1822. Physical quantities that are of the same kind (also called "commensurable") have the same dimension (length, time, mass) and can be directly compared to each other, even if they are originally expressed in differing units of measure (such as yards and meters). If physical quantities have different dimensions (such as length vs. mass), they cannot be expressed in terms of similar units and cannot be compared in quantity (also called "incommensurable"). For example, asking whether a kilogram is larger than an hour is meaningless.

Any physically meaningful equation (and any inequality) will have the same dimensions on its left and right sides, a property known as "dimensional homogeneity". Checking for dimensional homogeneity is a common application of dimensional analysis, serving as a plausibility check on derived equations and computations. It also serves as a guide and constraint in deriving equations that may describe a physical system in the absence of a more rigorous derivation.

A lot of parameters and the measurements(m) in the physical sciences and engineering are expressed as a concrete number – a numerical quantity(q) and a corresponding dimensional unit. Often a quantity is expressed in terms of several other quantities; for example, speed is a combination of length and time, e.g. 60 miles per hour or 1.4 kilometers per second. Compound relations with "per" are expressed with division, e.g. 60 mi/1 h. Other relations can involve multiplication (often shown with a centered dot or juxtaposition), powers (like m for square meters), or combinations thereof.

A set of base units for a system of measurement is a conventionally chosen set of units, none of which can be expressed as a combination of the others, and in terms of which all the remaining units of the system can be expressed. For example, units for length and time are normally chosen as base units. Units for volume, however, can be factored into the base units of length (m), thus they are considered derived or compound units.

Sometimes the names of units obscure the fact that they are derived units. For example, a newton (N) is a unit of force, which will have units of mass (kg) times acceleration (m⋅s). The newton is defined as .

Percentages are dimensionless quantities, since they are ratios of two quantities with the same dimensions. In other words, the % sign can be read as "hundredths", since .

Taking a derivative with respect to a quantity adds the dimension of the variable one is differentiating with respect to, in the denominator. Thus:
In economics, one distinguishes between stocks and flows: a stock has units of "units" (say, widgets or dollars), while a flow is a derivative of a stock, and has units of "units/time" (say, dollars/year).

In some contexts, dimensional quantities are expressed as dimensionless quantities or percentages by omitting some dimensions. For example, debt-to-GDP ratios are generally expressed as percentages: total debt outstanding (dimension of currency) divided by annual GDP (dimension of currency) – but one may argue that in comparing a stock to a flow, annual GDP should have dimensions of currency/time (dollars/year, for instance), and thus Debt-to-GDP should have units of years, which indicates that Debt-to-GDP is the number of years needed for a constant GDP to pay the debt, if all GDP is spent on the debt and the debt is otherwise unchanged.

In dimensional analysis, a ratio which converts one unit of measure into another without changing the quantity is called a conversion factor. For example, kPa and bar are both units of pressure, and . The rules of algebra allow both sides of an equation to be divided by the same expression, so this is equivalent to . Since any quantity can be multiplied by 1 without changing it, the expression "" can be used to convert from bars to kPa by multiplying it with the quantity to be converted, including units. For example, because , and bar/bar cancels out, so .

The most basic rule of dimensional analysis is that of dimensional homogeneity. Only commensurable quantities (physical quantities having the same dimension) may be "compared," "equated," "added," or "subtracted."
However, the dimensions form an abelian group under multiplication, so:

For example, it makes no sense to ask whether 1 hour is more, the same, or less than 1 kilometer, as these have different dimensions, nor to add 1 hour to 1 kilometer. However, it makes perfect sense to ask whether 1 mile is more, the same, or less than 1 kilometer being the same dimension of physical quantity even though the units are different. On the other hand, if an object travels 100 km in 2 hours, one may divide these and conclude that the object's average speed was 50 km/h.

The rule implies that in a physically meaningful "expression" only quantities of the same dimension can be added, subtracted, or compared. For example, if "m", "m" and "L" denote, respectively, the mass of some man, the mass of a rat and the length of that man, the dimensionally homogeneous expression is meaningful, but the heterogeneous expression is meaningless. However, "m"/"L" is fine. Thus, dimensional analysis may be used as a sanity check of physical equations: the two sides of any equation must be commensurable or have the same dimensions.

Even when two physical quantities have identical dimensions, it may nevertheless be meaningless to compare or add them. For example, although torque and energy share the dimension , they are fundamentally different physical quantities.

To compare, add, or subtract quantities with the same dimensions but expressed in different units, the standard procedure is first to convert them all to the same units. For example, to compare 32 metres with 35 yards, use 1 yard = 0.9144 m to convert 35 yards to 32.004 m.

A related principle is that any physical law that accurately describes the real world must be independent of the units used to measure the physical variables. For example, Newton's laws of motion must hold true whether distance is measured in miles or kilometers. This principle gives rise to the form that conversion factors must take between units that measure the same dimension: multiplication by a simple constant. It also ensures equivalence; for example, if two buildings are the same height in feet, then they must be the same height in meters.

The factor-label method is the sequential application of conversion factors expressed as fractions and arranged so that any dimensional unit appearing in both the numerator and denominator of any of the fractions can be cancelled out until only the desired set of dimensional units is obtained. For example, 10 miles per hour can be converted to meters per second by using a sequence of conversion factors as shown below:

It can be seen that each conversion factor is equivalent to the value of one. For example, starting with 1 mile = 1609.344 meters and dividing both sides of the equation by 1 mile yields 1 mile / 1 mile = 1609.344 meters / 1 mile, which when simplified yields 1 = 1609.344 meters / 1 mile.

So, when the units "mile" and "hour" are cancelled out and the arithmetic is done, 10 miles per hour converts to 4.4704 meters per second.

As a more complex example, the concentration of nitrogen oxides (i.e., formula_2) in the flue gas from an industrial furnace can be converted to a mass flow rate expressed in grams per hour (i.e., g/h) of formula_3 by using the following information as shown below:


After canceling out any dimensional units that appear both in the numerators and denominators of the fractions in the above equation, the NO concentration of 10 ppm converts to mass flow rate of 24.63 grams per hour.

The factor-label method can also be used on any mathematical equation to check whether or not the dimensional units on the left hand side of the equation are the same as the dimensional units on the right hand side of the equation. Having the same units on both sides of an equation does not ensure that the equation is correct, but having different units on the two sides (when expressed in terms of base units) of an equation implies that the equation is wrong.

For example, check the Universal Gas Law equation of , when:

As can be seen, when the dimensional units appearing in the numerator and denominator of the equation's right hand side are cancelled out, both sides of the equation have the same dimensional units.

The factor-label method can convert only unit quantities for which the units are in a linear relationship intersecting at 0. Most units fit this paradigm. An example for which it cannot be used is the conversion between degrees Celsius and kelvins (or degrees Fahrenheit). Between degrees Celsius and kelvins, there is a constant difference rather than a constant ratio, while between degrees Celsius and degrees Fahrenheit there is neither a constant difference nor a constant ratio. There is, however, an affine transform (formula_6, rather than a linear transform formula_7) between them.

For example, the freezing point of water is 0 °C and 32 °F, and a 5 °C change is the same as a 9 °F change. Thus, to convert from units of Fahrenheit to units of Celsius, one subtracts 32 °F (the offset from the point of reference), divides by 9 °F and multiplies by 5 °C (scales by the ratio of units), and adds 0 °C (the offset from the point of reference). Reversing this yields the formula for obtaining a quantity in units of Celsius from units of Fahrenheit; one could have started with the equivalence between 100 °C and 212 °F, though this would yield the same formula at the end.

Hence, to convert the numerical quantity value of a temperature "T"[F] in degrees Fahrenheit to a numerical quantity value "T"[C] in degrees Celsius, this formula may be used:

To convert "T"[C] in degrees Celsius to "T"[F] in degrees Fahrenheit, this formula may be used:

Dimensional analysis is most often used in physics and chemistry – and in the mathematics thereof – but finds some applications outside of those fields as well.

A simple application of dimensional analysis to mathematics is in computing the form of the volume of an "n"-ball (the solid ball in "n" dimensions), or the area of its surface, the "n"-sphere: being an "n"-dimensional figure, the volume scales as formula_8 while the surface area, being formula_9-dimensional, scales as formula_10 Thus the volume of the "n"-ball in terms of the radius is formula_11 for some constant formula_12 Determining the constant takes more involved mathematics, but the form can be deduced and checked by dimensional analysis alone.

In finance, economics, and accounting, dimensional analysis is most commonly referred to in terms of the distinction between stocks and flows. More generally, dimensional analysis is used in interpreting various financial ratios, economics ratios, and accounting ratios.

In fluid mechanics, dimensional analysis is performed in order to obtain dimensionless Pi terms or groups. According to the principles of dimensional analysis, any prototype can be described by a series of these terms or groups that describe the behaviour of the system. Using suitable Pi terms or groups, it is possible to develop a similar set of Pi terms for a model that has the same dimensional relationships. In other words, Pi terms provide a shortcut to developing a model representing a certain prototype. Common dimensionless groups in fluid mechanics include:


The origins of dimensional analysis have been disputed by historians. The 19th-century French mathematician Joseph Fourier is generally credited with having made important contributions based on the idea that physical laws like should be independent of the units employed to measure the physical variables. This led to the conclusion that meaningful laws must be homogeneous equations in their various units of measurement, a result which was eventually formalized in the Buckingham π theorem. However, the first application of dimensional analysis has been credited to the Italian scholar François Daviet de Foncenex (1734–1799). It was published in 1761, 61 years before the publication of Fourier's work.

James Clerk Maxwell played a major role in establishing modern use of dimensional analysis by distinguishing mass, length, and time as fundamental units, while referring to other units as derived. Although Maxwell defined length, time and mass to be "the three fundamental units", he also noted that gravitational mass can be derived from length and time by assuming a form of Newton's law of universal gravitation in which the gravitational constant "G" is taken as unity, thereby defining . By assuming a form of Coulomb's law in which Coulomb's constant "k" is taken as unity, Maxwell then determined that the dimensions of an electrostatic unit of charge were , which, after substituting his equation for mass, results in charge having the same dimensions as mass, viz. .

Dimensional analysis is also used to derive relationships between the physical quantities that are involved in a particular phenomenon that one wishes to understand and characterize. It was used for the first time in this way in 1872 by Lord Rayleigh, who was trying to understand why the sky is blue. Rayleigh first published the technique in his 1877 book "The Theory of Sound".

The original meaning of the word "dimension", in Fourier's "Theorie de la Chaleur", was the numerical value of the exponents of the base units. For example, acceleration was considered to have the dimension 1 with respect to the unit of length, and the dimension −2 with respect to the unit of time. This was slightly changed by Maxwell, who said the dimensions of acceleration are LT, instead of just the exponents.

The Buckingham π theorem describes how every physically meaningful equation involving "n" variables can be equivalently rewritten as an equation of dimensionless parameters, where "m" is the rank of the dimensional matrix. Furthermore, and most importantly, it provides a method for computing these dimensionless parameters from the given variables.

A dimensional equation can have the dimensions reduced or eliminated through nondimensionalization, which begins with dimensional analysis, and involves scaling quantities by characteristic units of a system or natural units of nature. This gives insight into the fundamental properties of the system, as illustrated in the examples below.

The dimension of a physical quantity can be expressed as a product of the basic physical dimensions such as length, mass and time, each raised to a rational power. The "dimension" of a physical quantity is more fundamental than some "scale" unit used to express the amount of that physical quantity. For example, "mass" is a dimension, while the kilogram is a particular scale unit chosen to express a quantity of mass. Except for natural units, the choice of scale is cultural and arbitrary.

There are many possible choices of basic physical dimensions. The SI standard recommends the usage of the following dimensions and corresponding symbols: length (L), mass (M), time (T), electric current (I), absolute temperature (Θ), amount of substance (N) and luminous intensity (J). The symbols are by convention usually written in roman sans serif typeface. Mathematically, the dimension of the quantity "Q" is given by 
where "a", "b", "c", "d", "e", "f", "g" are the dimensional exponents. Other physical quantities could be defined as the base quantities, as long as they form a linearly independent basis. For instance, one could replace the dimension of electrical current (I) of the SI basis with a dimension of electric charge (Q), since Q = IT.

As examples, the dimension of the physical quantity speed "v" is
and the dimension of the physical quantity force "F" is

The unit chosen to express a physical quantity and its dimension are related, but not identical concepts. The units of a physical quantity are defined by convention and related to some standard; e.g., length may have units of metres, feet, inches, miles or micrometres; but any length always has a dimension of L, no matter what units of length are chosen to express it. Two different units of the same physical quantity have conversion factors that relate them. For example, 1 in = 2.54 cm; in this case (2.54 cm/in) is the conversion factor, which is itself dimensionless. Therefore, multiplying by that conversion factor does not change the dimensions of a physical quantity.

There are also physicists that have cast doubt on the very existence of incompatible fundamental dimensions of physical quantity, although this does not invalidate the usefulness of dimensional analysis.

The dimensions that can be formed from a given collection of basic physical dimensions, such as M, L, and T, form an abelian group: The identity is written as 1; , and the inverse to L is 1/L or L. L raised to any rational power "p" is a member of the group, having an inverse of L or 1/L. The operation of the group is multiplication, having the usual rules for handling exponents ().

This group can be described as a vector space over the rational numbers, with for example dimensional symbol MLT corresponding to the vector . When physical measured quantities (be they like-dimensioned or unlike-dimensioned) are multiplied or divided by one other, their dimensional units are likewise multiplied or divided; this corresponds to addition or subtraction in the vector space. When measurable quantities are raised to a rational power, the same is done to the dimensional symbols attached to those quantities; this corresponds to scalar multiplication in the vector space.

A basis for such a vector space of dimensional symbols is called a set of base quantities, and all other vectors are called derived units. As in any vector space, one may choose different bases, which yields different systems of units (e.g., choosing whether the unit for charge is derived from the unit for current, or vice versa).

The group identity 1, the dimension of dimensionless quantities, corresponds to the origin in this vector space.

The set of units of the physical quantities involved in a problem correspond to a set of vectors (or a matrix). The nullity describes some number (e.g., "m") of ways in which these vectors can be combined to produce a zero vector. These correspond to producing (from the measurements) a number of dimensionless quantities, {π, ..., π}. (In fact these ways completely span the null subspace of another different space, of powers of the measurements.) Every possible way of multiplying (and exponentiating) together the measured quantities to produce something with the same units as some derived quantity "X" can be expressed in the general form

Consequently, every possible commensurate equation for the physics of the system can be rewritten in the form

Knowing this restriction can be a powerful tool for obtaining new insight into the system.

The dimension of physical quantities of interest in mechanics can be expressed in terms of base dimensions M, L, and T – these form a 3-dimensional vector space. This is not the only valid choice of base dimensions, but it is the one most commonly used. For example, one might choose force, length and mass as the base dimensions (as some have done), with associated dimensions F, L, M; this corresponds to a different basis, and one may convert between these representations by a change of basis. The choice of the base set of dimensions is thus a convention, with the benefit of increased utility and familiarity. The choice of base dimensions is not arbitrary, because the dimensions must form a basis: they must span the space, and be linearly independent.

For example, F, L, M form a set of fundamental dimensions because they form a basis that is equivalent to M, L, T: the former can be expressed as [F = ML/T], L, M, while the latter can be expressed as M, L, [T = (ML/F)].

On the other hand, length, velocity and time do not form a set of as base dimensions, for two reasons:

Depending on the field of physics, it may be advantageous to choose one or another extended set of dimensional symbols. In electromagnetism, for example, it may be useful to use dimensions of M, L, T, and Q, where Q represents the dimension of electric charge. In thermodynamics, the base set of dimensions is often extended to include a dimension for temperature, Θ. In chemistry the number of moles of substance (the number of molecules divided by Avogadro's constant, ≈ 6.02 × 10) is defined as a base unit, N, as well.
In the interaction of relativistic plasma with strong laser pulses, a dimensionless relativistic similarity parameter, connected with the symmetry properties of the collisionless Vlasov equation, is constructed from the plasma-, electron- and critical-densities in addition to the electromagnetic vector potential. The choice of the dimensions or even the number of dimensions to be used in different fields of physics is to some extent arbitrary, but consistency in use and ease of communications are common and necessary features.

Scalar arguments to transcendental functions such as exponential, trigonometric and logarithmic functions, or to inhomogeneous polynomials, must be dimensionless quantities. (Note: this requirement is somewhat relaxed in Siano's orientational analysis described below, in which the square of certain dimensioned quantities are dimensionless.)
While most mathematical identities about dimensionless numbers translate in a straightforward manner to dimensional quantities, care must be taken with logarithms of ratios: the identity log(a/b) = log a − log b, where the logarithm is taken in any base, holds for dimensionless numbers a and b, but it does "not" hold if a and b are dimensional, because in this case the left-hand side is well-defined but the right-hand side is not.

Similarly, while one can evaluate monomials ("x") of dimensional quantities, one cannot evaluate polynomials of mixed degree with dimensionless coefficients on dimensional quantities: for "x", the expression (3 m) = 9 m makes sense (as an area), while for "x" + "x", the expression (3 m) + 3 m = 9 m + 3 m does not make sense.

However, polynomials of mixed degree can make sense if the coefficients are suitably chosen physical quantities that are not dimensionless. For example,

This is the height to which an object rises in time "t" if the acceleration of gravity is 32 feet per second per second and the initial upward speed is 500 feet per second. It is not even necessary for "t" to be in "seconds". For example, suppose "t" = 0.01 minutes. Then the first term would be

The value of a dimensional physical quantity "Z" is written as the product of a unit ["Z"] within the dimension and a dimensionless numerical factor, "n".

When like-dimensioned quantities are added or subtracted or compared, it is convenient to express them in consistent units so that the numerical values of these quantities may be directly added or subtracted. But, in concept, there is no problem adding quantities of the same dimension expressed in different units. For example, 1 meter added to 1 foot is a length, but one cannot derive that length by simply adding 1 and 1. A conversion factor, which is a ratio of like-dimensioned quantities and is equal to the dimensionless unity, is needed:

The factor formula_27 is identical to the dimensionless 1, so multiplying by this conversion factor changes nothing. Then when adding two quantities of like dimension, but expressed in different units, the appropriate conversion factor, which is essentially the dimensionless 1, is used to convert the quantities to identical units so that their numerical values can be added or subtracted.

Only in this manner is it meaningful to speak of adding like-dimensioned quantities of differing units.

Some discussions of dimensional analysis implicitly describe all quantities as mathematical vectors. (In mathematics scalars are considered a special case of vectors; vectors can be added to or subtracted from other vectors, and, inter alia, multiplied or divided by scalars. If a vector is used to define a position, this assumes an implicit point of reference: an origin. While this is useful and often perfectly adequate, allowing many important errors to be caught, it can fail to model certain aspects of physics. A more rigorous approach requires distinguishing between position and displacement (or moment in time versus duration, or absolute temperature versus temperature change).

Consider points on a line, each with a position with respect to a given origin, and distances among them. Positions and displacements all have units of length, but their meaning is not interchangeable:
This illustrates the subtle distinction between "affine" quantities (ones modeled by an affine space, such as position) and "vector" quantities (ones modeled by a vector space, such as displacement).

Properly then, positions have dimension of "affine" length, while displacements have dimension of "vector" length. To assign a number to an "affine" unit, one must not only choose a unit of measurement, but also a point of reference, while to assign a number to a "vector" unit only requires a unit of measurement.

Thus some physical quantities are better modeled by vectorial quantities while others tend to require affine representation, and the distinction is reflected in their dimensional analysis.

This distinction is particularly important in the case of temperature, for which the numeric value of absolute zero is not the origin 0 in some scales. For absolute zero,
but for temperature differences,
(Here °R refers to the Rankine scale, not the Réaumur scale).
Unit conversion for temperature differences is simply a matter of multiplying by, e.g., 1 °F / 1 K (although the ratio is not a constant value). But because some of these scales have origins that do not correspond to absolute zero, conversion from one temperature scale to another requires accounting for that. As a result, simple dimensional analysis can lead to errors if it is ambiguous whether 1 K means the absolute temperature equal to −272.15 °C, or the temperature difference equal to 1 °C.

Similar to the issue of a point of reference is the issue of orientation: a displacement in 2 or 3 dimensions is not just a length, but is a length together with a "direction". (This issue does not arise in 1 dimension, or rather is equivalent to the distinction between positive and negative.) Thus, to compare or combine two dimensional quantities in a multi-dimensional space, one also needs an orientation: they need to be compared to a frame of reference.

This leads to the extensions discussed below, namely Huntley's directed dimensions and Siano's orientational analysis.

What is the period of oscillation of a mass attached to an ideal linear spring with spring constant suspended in gravity of strength ? That period is the solution for of some dimensionless equation in the variables , , , and .
The four quantities have the following dimensions: [T]; [M]; [M/T]; and [L/T]. From these we can form only one dimensionless product of powers of our chosen variables, formula_28 = formula_29 , and putting formula_30 for some dimensionless constant gives the dimensionless equation sought. The dimensionless product of powers of variables is sometimes referred to as a dimensionless group of variables; here the term "group" means "collection" rather than mathematical group. They are often called dimensionless numbers as well.

Note that the variable does not occur in the group. It is easy to see that it is impossible to form a dimensionless product of powers that combines with , , and , because is the only quantity that involves the dimension L. This implies that in this problem the is irrelevant. Dimensional analysis can sometimes yield strong statements about the "irrelevance" of some quantities in a problem, or the need for additional parameters. If we have chosen enough variables to properly describe the problem, then from this argument we can conclude that the period of the mass on the spring is independent of : it is the same on the earth or the moon. The equation demonstrating the existence of a product of powers for our problem can be written in an entirely equivalent way: formula_31, for some dimensionless constant κ (equal to formula_32 from the original dimensionless equation).

When faced with a case where dimensional analysis rejects a variable (, here) that one intuitively expects to belong in a physical description of the situation, another possibility is that the rejected variable is in fact relevant, but that some other relevant variable has been omitted, which might combine with the rejected variable to form a dimensionless quantity. That is, however, not the case here.

When dimensional analysis yields only one dimensionless group, as here, there are no unknown functions, and the solution is said to be "complete" – although it still may involve unknown dimensionless constants, such as .

Consider the case of a vibrating wire of length "ℓ" (L) vibrating with an amplitude "A" (L). The wire has a linear density "ρ" (M/L) and is under tension "s" (ML/T), and we want to know the energy "E" (ML/T) in the wire. Let "π" and "π" be two dimensionless products of powers of the variables chosen, given by

The linear density of the wire is not involved. The two groups found can be combined into an equivalent form as an equation

where "F" is some unknown function, or, equivalently as

where "f" is some other unknown function. Here the unknown function implies that our solution is now incomplete, but dimensional analysis has given us something that may not have been obvious: the energy is proportional to the first power of the tension. Barring further analytical analysis, we might proceed to experiments to discover the form for the unknown function "f". But our experiments are simpler than in the absence of dimensional analysis. We'd perform none to verify that the energy is proportional to the tension. Or perhaps we might guess that the energy is proportional to "ℓ", and so infer that . The power of dimensional analysis as an aid to experiment and forming hypotheses becomes evident.

The power of dimensional analysis really becomes apparent when it is applied to situations, unlike those given above, that are more complicated, the set of variables involved are not apparent, and the underlying equations hopelessly complex. Consider, for example, a small pebble sitting on the bed of a river. If the river flows fast enough, it will actually raise the pebble and cause it to flow along with the water. At what critical velocity will this occur? Sorting out the guessed variables is not so easy as before. But dimensional analysis can be a powerful aid in understanding problems like this, and is usually the very first tool to be applied to complex problems where the underlying equations and constraints are poorly understood. In such cases, the answer may depend on a dimensionless number such as the Reynolds number, which may be interpreted by dimensional analysis.

Consider the case of a thin, solid, parallel-sided rotating disc of axial thickness "t" (L) and radius "R" (L). The disc has a density "ρ" (M/L), rotates at an angular velocity "ω" (T) and this leads to a stress "S" (MLT) in the material. There is a theoretical linear elastic solution, given by Lame, to this problem when the disc is thin relative to its radius, the faces of the disc are free to move axially, and the plane stress constitutive relations can be assumed to be valid. As the disc becomes thicker relative to the radius then the plane stress solution breaks down. If the disc is restrained axially on its free faces then a state of plane strain will occur. However, if this is not the case then the state of stress may only be determined though consideration of three-dimensional elasticity and there is no known theoretical solution for this case. An engineer might, therefore, be interested in establishing a relationship between the five variables. Dimensional analysis for this case leads to the following (5 − 3 = 2) non-dimensional groups:

Through the use of numerical experiments using, for example, the finite element method, the nature of the relationship between the two non-dimensional groups can be obtained as shown in the figure. As this problem only involves two non-dimensional groups, the complete picture is provided in a single plot and this can be used as a design/assessment chart for rotating discs

Huntley has pointed out that it is sometimes productive to refine our concept of dimension. Two possible refinements are:

As an example of the usefulness of the first refinement, suppose we wish to calculate the distance a cannonball travels when fired with a vertical velocity component formula_36 and a horizontal velocity component formula_37, assuming it is fired on a flat surface. Assuming no use of directed lengths, the quantities of interest are then formula_37, formula_36, both dimensioned as LT, , the distance travelled, having dimension L, and the downward acceleration of gravity, with dimension LT.

With these four quantities, we may conclude that the equation for the range may be written:

Or dimensionally

from which we may deduce that formula_42 and formula_43, which leaves one exponent undetermined. This is to be expected since we have two fundamental dimensions L and T, and four parameters, with one equation.

If, however, we use directed length dimensions, then formula_37 will be dimensioned as LT, formula_36 as LT, as L and as LT. The dimensional equation becomes:

and we may solve completely as formula_47, formula_48 and formula_49. The increase in deductive power gained by the use of directed length dimensions is apparent.

In a similar manner, it is sometimes found useful (e.g., in fluid mechanics and thermodynamics) to distinguish between mass as a measure of inertia (inertial mass), and mass as a measure of quantity (substantial mass). For example, consider the derivation of Poiseuille's Law. We wish to find the rate of mass flow of a viscous fluid through a circular pipe. Without drawing distinctions between inertial and substantial mass we may choose as the relevant variables

There are three fundamental variables so the above five equations will yield two dimensionless variables which we may take to be formula_52 and formula_53 and we may express the dimensional equation as

where and are undetermined constants. If we draw a distinction between inertial mass with dimension formula_55 and substantial mass with dimension formula_56, then mass flow rate and density will use substantial mass as the mass parameter, while the pressure gradient and coefficient of viscosity will use inertial mass. We now have four fundamental parameters, and one dimensionless constant, so that the dimensional equation may be written:

where now only is an undetermined constant (found to be equal to formula_58 by methods outside of dimensional analysis). This equation may be solved for the mass flow rate to yield Poiseuille's law.

Huntley's extension has some serious drawbacks:


It also is often quite difficult to assign the L, L, L, L, symbols to the physical variables involved in the problem of interest. He invokes a procedure that involves the "symmetry" of the physical problem. This is often very difficult to apply reliably: It is unclear as to what parts of the problem that the notion of "symmetry" is being invoked. Is it the symmetry of the physical body that forces are acting upon, or to the points, lines or areas at which forces are being applied? What if more than one body is involved with different symmetries?

Consider the spherical bubble attached to a cylindrical tube, where one wants the flow rate of air as a function of the pressure difference in the two parts. What are the Huntley extended dimensions of the viscosity of the air contained in the connected parts? What are the extended dimensions of the pressure of the two parts? Are they the same or different? These difficulties are responsible for the limited application of Huntley's addition to real problems.

Angles are, by convention, considered to be dimensionless variables, and so the use of angles as physical variables in dimensional analysis can give less meaningful results. As an example, consider the projectile problem mentioned above. Suppose that, instead of the x- and y-components of the initial velocity, we had chosen the magnitude of the velocity and the angle at which the projectile was fired. The angle is, by convention, considered to be dimensionless, and the magnitude of a vector has no directional quality, so that no dimensionless variable can be composed of the four variables , , , and . Conventional analysis will correctly give the powers of and , but will give no information concerning the dimensionless angle .

Note that the orientational symbols form a group (the Klein four-group or "Viergruppe"). In this system, scalars always have the same orientation as the identity element, independent of the "symmetry of the problem". Physical quantities that are vectors have the orientation expected: a force or a velocity in the z-direction has the orientation of . For angles, consider an angle that lies in the z-plane. Form a right triangle in the z-plane with being one of the acute angles. The side of the right triangle adjacent to the angle then has an orientation and the side opposite has an orientation . Then, since we conclude that an angle in the xy-plane must have an orientation , which is not unreasonable. Analogous reasoning forces the conclusion that has orientation while has orientation 1. These are different, so one concludes (correctly), for example, that there are no solutions of physical equations that are of the form , where and are real scalars. Note that an expression such as formula_60 is not dimensionally inconsistent since it is a special case of the sum of angles formula and should properly be written:

which for formula_62 and formula_63 yields formula_64. Physical quantities may be expressed as complex numbers (e.g. formula_65) which imply that the complex quantity has an orientation equal to that of the angle it is associated with ( in the above example).

The assignment of orientational symbols to physical quantities and the requirement that physical equations be orientationally homogeneous can actually be used in a way that is similar to dimensional analysis to derive a little more information about acceptable solutions of physical problems. In this approach one sets up the dimensional equation and solves it as far as one can. If the lowest power of a physical variable is fractional, both sides of the solution is raised to a power such that all powers are integral. This puts it into "normal form". The orientational equation is then solved to give a more restrictive condition on the unknown powers of the orientational symbols, arriving at a solution that is more complete than the one that dimensional analysis alone gives. Often the added information is that one of the powers of a certain variable is even or odd.

As an example, for the projectile problem, using orientational symbols, θ, being in the xy-plane will thus have dimension and the range of the projectile will be of the form:

Dimensional homogeneity will now correctly yield and , and orientational homogeneity requires that be an odd integer. In fact the required function of theta will be which is a series of odd powers of .

It is seen that the Taylor series of and are orientationally homogeneous using the above multiplication table, while expressions like and are not, and are (correctly) deemed unphysical.

It should be clear that the multiplication rule used for the orientational symbols is not the same as that for the cross product of two vectors. The cross product of two identical vectors is zero, while the product of two identical orientational symbols is the identity element.

The dimensionless constants that arise in the results obtained, such as the C in the Poiseuille's Law problem and the formula_67 in the spring problems discussed above, come from a more detailed analysis of the underlying physics and often arise from integrating some differential equation. Dimensional analysis itself has little to say about these constants, but it is useful to know that they very often have a magnitude of order unity. This observation can allow one to sometimes make "back of the envelope" calculations about the phenomenon of interest, and therefore be able to more efficiently design experiments to measure it, or to judge whether it is important, etc.

Paradoxically, dimensional analysis can be a useful tool even if all the parameters in the underlying theory are dimensionless, e.g., lattice models such as the Ising model can be used to study phase transitions and critical phenomena. Such models can be formulated in a purely dimensionless way. As we approach the critical point closer and closer, the distance over which the variables in the lattice model are correlated (the so-called correlation length, formula_68 ) becomes larger and larger. Now, the correlation length is the relevant length scale related to critical phenomena, so one can, e.g., surmise on "dimensional grounds" that the non-analytical part of the free energy per lattice site should be formula_69 where formula_70 is the dimension of the lattice.

It has been argued by some physicists, e.g., M. J. Duff, that the laws of physics are inherently dimensionless. The fact that we have assigned incompatible dimensions to Length, Time and Mass is, according to this point of view, just a matter of convention, borne out of the fact that before the advent of modern physics, there was no way to relate mass, length, and time to each other. The three independent dimensionful constants: "c", "ħ", and "G", in the fundamental equations of physics must then be seen as mere conversion factors to convert Mass, Time and Length into each other.

Just as in the case of critical properties of lattice models, one can recover the results of dimensional analysis in the appropriate scaling limit; e.g., dimensional analysis in mechanics can be derived by reinserting the constants "ħ", "c", and "G" (but we can now consider them to be dimensionless) and demanding that a nonsingular relation between quantities exists in the limit formula_71, formula_72 and formula_73. In problems involving a gravitational field the latter limit should be taken such that the field stays finite.

Following are tables of commonly occurring expressions in physics, related to the dimensions of energy, momentum, and force.

If , where "c" is the speed of light and "ħ" is the reduced Planck constant, and a suitable fixed unit of energy is chosen, then all quantities of length "L", mass "M" and time "T" can be expressed (dimensionally) as a power of energy "E", because length, mass and time can be expressed using speed "v", action "S", and energy "E":

though speed and action are dimensionless ( and ) – so the only remaining quantity with dimension is energy. In terms of powers of dimensions:

This is particularly useful in particle physics and high energy physics, in which case the energy unit is the electron volt (eV). Dimensional checks and estimates become very simple in this system.

However, if electric charges and currents are involved, another unit to be fixed is for electric charge, normally the electron charge "e" though other choices are possible.







</doc>
<doc id="8270" url="https://en.wikipedia.org/wiki?curid=8270" title="December 25">
December 25




</doc>
<doc id="8271" url="https://en.wikipedia.org/wiki?curid=8271" title="Digital television">
Digital television

Digital television (DTV) is the transmission of television signals, including the sound channel, using digital encoding, in contrast to the earlier television technology, analog television, in which the video and audio are carried by analog signals. It is an innovative advance that represents the first significant evolution in television technology since color television in the 1950s. Digital TV makes more economical use of scarce radio spectrum space; it can transmit multiple channels in the same bandwidth occupied by a single channel of analog television, and provides many new features that analog television cannot. A switchover from analog to digital broadcasting began around 2006 in some countries, and many industrial countries have now completed the changeover, while other countries are in various stages of adaptation. Different digital television broadcasting standards have been adopted in different parts of the world; below are the more widely used standards:

Digital TV's roots have been tied very closely to the availability of inexpensive, high performance computers. It wasn't until the 1990s that digital TV became a real possibility.

In the mid-1980s, as Japanese consumer electronics firms forged ahead with the development of HDTV technology, and as the MUSE analog format was proposed by Japan's public broadcaster NHK as a worldwide standard, Japanese advancements were seen as pacesetters that threatened to eclipse U.S. electronics companies. Until June 1990, the Japanese MUSE standard—based on an analog system—was the front-runner among the more than 23 different technical concepts under consideration. Then, an American company, General Instrument, demonstrated the feasibility of a digital television signal. This breakthrough was of such significance that the FCC was persuaded to delay its decision on an ATV standard until a digitally based standard could be developed.

In March 1990, when it became clear that a digital standard was feasible, the FCC made a number of critical decisions. First, the Commission declared that the new ATV standard must be more than an enhanced analog signal, but be able to provide a genuine HDTV signal with at least twice the resolution of existing television images. Then, to ensure that viewers who did not wish to buy a new digital television set could continue to receive conventional television broadcasts, it dictated that the new ATV standard must be capable of being "simulcast" on different channels. The new ATV standard also allowed the new DTV signal to be based on entirely new design principles. Although incompatible with the existing NTSC standard, the new DTV standard would be able to incorporate many improvements.

The final standard adopted by the FCC did not require a single standard for scanning formats, aspect ratios, or lines of resolution. This outcome resulted from a dispute between the consumer electronics industry (joined by some broadcasters) and the computer industry (joined by the film industry and some public interest groups) over which of the two scanning processes—interlaced or progressive—is superior. Interlaced scanning, which is used in televisions worldwide, scans even-numbered lines first, then odd-numbered ones. Progressive scanning, which is the format used in computers, scans lines in sequences, from top to bottom. The computer industry argued that progressive scanning is superior because it does not "flicker" in the manner of interlaced scanning. It also argued that progressive scanning enables easier connections with the Internet, and is more cheaply converted to interlaced formats than vice versa. The film industry also supported progressive scanning because it offers a more efficient means of converting filmed programming into digital formats. For their part, the consumer electronics industry and broadcasters argued that interlaced scanning was the only technology that could transmit the highest quality pictures then (and currently) feasible, i.e., 1,080 lines per picture and 1,920 pixels per line. Broadcasters also favored interlaced scanning because their vast archive of interlaced programming is not readily compatible with a progressive format.

DirecTV in the U.S. launched the first commercial digital satellite platform in May 1994, using the Digital Satellite System (DSS) standard. Digital cable broadcasts were tested and launched in the U.S. in 1996 by TCI and Time Warner. The first digital terrestrial platform was launched in November 1998 as ONdigital in the United Kingdom, using the DVB-T standard.

Digital television supports many different picture formats defined by the broadcast television systems which are a combination of size and aspect ratio (width to height ratio).

With digital terrestrial television (DTT) broadcasting, the range of formats can be broadly divided into two categories: high definition television (HDTV) for the transmission of high-definition video and standard-definition television (SDTV). These terms by themselves are not very precise, and many subtle intermediate cases exist.

One of several different HDTV formats that can be transmitted over DTV is: 1280 × 720 pixels in progressive scan mode (abbreviated "720p") or 1920 × 1080 pixels in interlaced video mode ("1080i"). Each of these uses a aspect ratio. HDTV cannot be transmitted over analog television channels because of channel capacity issues.

SDTV, by comparison, may use one of several different formats taking the form of various aspect ratios depending on the technology used in the country of broadcast. In terms of rectangular pixels, NTSC countries can deliver a 640 × 480 resolution in 4:3 and 854 × 480 in , while PAL can give 768 × 576 in and 1024 × 576 in . However, broadcasters may choose to reduce these resolutions to reduce bit rate (e.g., many DVB-T channels in the United Kingdom use a horizontal resolution of 544 or 704 pixels per line).

Each commercial broadcasting terrestrial television DTV channel in North America is permitted to be broadcast at a bit rate up to 19 megabits per second. However, the broadcaster does not need to use this entire bandwidth for just one broadcast channel. Instead the broadcast can use the channel to include PSIP and can also subdivide across several video subchannels (a.k.a. feeds) of varying quality and compression rates, including non-video datacasting services that allow one-way high-bit-rate streaming of data to computers like National Datacast.

A broadcaster may opt to use a standard-definition (SDTV) digital signal instead of an HDTV signal, because current convention allows the bandwidth of a DTV channel (or "multiplex") to be subdivided into multiple digital subchannels, (similar to what most FM radio stations offer with HD Radio), providing multiple feeds of entirely different television programming on the same channel. This ability to provide either a single HDTV feed or multiple lower-resolution feeds is often referred to as distributing one's "bit budget" or multicasting. This can sometimes be arranged automatically, using a statistical multiplexer (or "stat-mux"). With some implementations, image resolution may be less directly limited by bandwidth; for example in DVB-T, broadcasters can choose from several different modulation schemes, giving them the option to reduce the transmission bit rate and make reception easier for more distant or mobile viewers.

There are several different ways to receive digital television. One of the oldest means of receiving DTV (and TV in general) is from terrestrial transmitters using an antenna (known as an "aerial" in some countries). This way is known as Digital terrestrial television (DTT). With DTT, viewers are limited to channels that have a terrestrial transmitter in range of their antenna.

Other ways have been devised to receive digital television. Among the most familiar to people are digital cable and digital satellite. In some countries where transmissions of TV signals are normally achieved by microwaves, digital MMDS is used. Other standards, such as Digital multimedia broadcasting (DMB) and DVB-H, have been devised to allow handheld devices such as mobile phones to receive TV signals. Another way is IPTV, that is receiving TV via Internet Protocol, relying on digital subscriber line (DSL) or optical cable line. Finally, an alternative way is to receive digital TV signals via the open Internet (Internet television), whether from a central streaming service or a P2P (peer-to-peer) system.

Some signals carry encryption and specify use conditions (such as "may not be recorded" or "may not be viewed on displays larger than 1 m in diagonal measure") backed up with the force of law under the World Intellectual Property Organization Copyright Treaty (WIPO Copyright Treaty) and national legislation implementing it, such as the U.S. Digital Millennium Copyright Act. Access to encrypted channels can be controlled by a removable smart card, for example via the Common Interface (DVB-CI) standard for Europe and via Point Of Deployment (POD) for IS or named differently CableCard.

Digital television signals must not interfere with each other, and they must also coexist with analog television until it is phased out.
The following table gives allowable signal-to-noise and signal-to-interference ratios for various interference scenarios. This table is a crucial regulatory tool for controlling the placement and power levels of stations. Digital TV is more tolerant of interference than analog TV, and this is the reason a smaller range of channels can carry an all-digital set of television stations.

People can interact with a DTV system in various ways. One can, for example, browse the electronic program guide. Modern DTV systems sometimes use a return path providing feedback from the end user to the broadcaster. This is possible with a coaxial or fiber optic cable, a dialup modem, or Internet connection but is not possible with a standard antenna.

Some of these systems support video on demand using a communication channel localized to a neighborhood rather than a city (terrestrial) or an even larger area (satellite).

1seg (1-segment) is a special form of ISDB. Each channel is further divided into 13 segments. The 12 segments of them are allocated for HDTV and remaining segment, the 13th, is used for narrow-band receivers such as mobile television or cell phone.

DTV has several advantages over analog TV, the most significant being that digital channels take up less bandwidth, and the bandwidth needs are continuously variable, at a corresponding reduction in image quality depending on the level of compression as well as the resolution of the transmitted image. This means that digital broadcasters can provide more digital channels in the same space, provide high-definition television service, or provide other non-television services such as multimedia or interactivity. DTV also permits special services such as multiplexing (more than one program on the same channel), electronic program guides and additional languages (spoken or subtitled). The sale of non-television services may provide an additional revenue source.

Digital and analog signals react to interference differently. For example, common problems with analog television include ghosting of images, noise from weak signals, and many other potential problems which degrade the quality of the image and sound, although the program material may still be watchable. With digital television, the audio and video must be synchronized digitally, so reception of the digital signal must be very nearly complete; otherwise, neither audio nor video will be usable. Short of this complete failure, "blocky" video is seen when the digital signal experiences interference.

Analog TV began with monophonic sound, and later developed multichannel television sound with two independent audio signal channels. DTV allows up to 5 audio signal channels plus a sub-woofer bass channel, with broadcasts similar in quality to movie theaters and DVDs.

DTV images have some picture defects that are not present on analog television or motion picture cinema, because of present-day limitations of bit rate and compression algorithms such as MPEG-2. This defect is sometimes referred to as "mosquito noise".

Because of the way the human visual system works, defects in an image that are localized to particular features of the image or that come and go are more perceptible than defects that are uniform and constant. However, the DTV system is designed to take advantage of other limitations of the human visual system to help mask these flaws, e.g. by allowing more compression artifacts during fast motion where the eye cannot track and resolve them as easily and, conversely, minimizing artifacts in still backgrounds that may be closely examined in a scene (since time allows). 

Broadcast, cable, satellite, and Internet DTV operators control the picture quality of television signal encodes using sophisticated, neuroscience-based algorithms, such as the structural similarity (SSIM) video quality measurement tool, which was accorded each of its inventors a Primetime Emmy because of its global use. Another tool, called Visual Information Fidelity (VIF), is a top-performing algorithm at the core of the Netflix VMAF video quality monitoring system, which accounts for about 35% of all U.S. bandwidth consumption.

Changes in signal reception from factors such as degrading antenna connections or changing weather conditions may gradually reduce the quality of analog TV. The nature of digital TV results in a perfectly decodable video initially, until the receiving equipment starts picking up interference that overpowers the desired signal or if the signal is too weak to decode. Some equipment will show a garbled picture with significant damage, while other devices may go directly from perfectly decodable video to no video at all or lock up. This phenomenon is known as the digital cliff effect.

For remote locations, distant channels that, as analog signals, were previously usable in a snowy and degraded state may, as digital signals, be perfectly decodable or may become completely unavailable. The use of higher frequencies will add to these problems, especially in cases where a clear line-of-sight from the receiving antenna to the transmitter is not available.

Television sets with only analog tuners cannot decode digital transmissions. When analog broadcasting over the air ceases, users of sets with analog-only tuners may use other sources of programming (e.g. cable, recorded media) or may purchase set-top converter boxes to tune in the digital signals. In the United States, a government-sponsored coupon was available to offset the cost of an external converter box. Analog switch-off (of full-power stations) took place on December 11, 2006 in The Netherlands, June 12, 2009 in the United States for full-power stations, and later for Class-A Stations on September 1, 2016, July 24, 2011 in Japan, August 31, 2011 in Canada, February 13, 2012 in Arab states, May 1, 2012 in Germany, October 24, 2012 in the United Kingdom and Ireland, October 31, 2012 in selected Indian cities, and December 10, 2013 in Australia. Completion of analog switch-off is scheduled for December 31, 2017 in the whole of India, December 2018 in Costa Rica and around 2020 for the Philippines.

Prior to the conversion to digital TV, analog television broadcast audio for TV channels on a separate FM carrier signal from the video signal. This FM audio signal could be heard using standard radios equipped with the appropriate tuning circuits.

However, after the transition of many countries to digital TV, no portable radio manufacturer has yet developed an alternative method for portable radios to play just the audio signal of digital TV channels. (DTV radio is not the same thing.)

The adoption of a broadcast standard incompatible with existing analog receivers has created the problem of large numbers of analog receivers being discarded during digital television transition. One superintendent of public works was quoted in 2009 saying; "some of the studies I’ve read in the trade magazines say up to a quarter of American households could be throwing a TV out in the next two years following the regulation change". In 2009, an estimated 99 million analog TV receivers were sitting unused in homes in the US alone and, while some obsolete receivers are being retrofitted with converters, many more are simply dumped in landfills where they represent a source of toxic metals such as lead as well as lesser amounts of materials such as barium, cadmium and chromium.

According to one campaign group, a CRT computer monitor or TV contains an average of of lead. According to another source, the lead in glass of a CRT varies from 1.08 lb to 11.28 lb, depending on screen size and type, but the lead is in the form of "stable and immobile" lead oxide mixed into the glass. It is claimed that the lead can have long-term negative effects on the environment if dumped as landfill. However, the glass envelope can be recycled at suitably equipped facilities. Other portions of the receiver may be subject to disposal as hazardous material.

Local restrictions on disposal of these materials vary widely; in some cases second-hand stores have refused to accept working color television receivers for resale due to the increasing costs of disposing of unsold TVs. Those thrift stores which are still accepting donated TVs have reported significant increases in good-condition working used television receivers abandoned by viewers who often expect them not to work after digital transition.

In Michigan in 2009, one recycler estimated that as many as one household in four would dispose of or recycle a TV set in the following year. The digital television transition, migration to high-definition television receivers and the replacement of CRTs with flatscreens are all factors in the increasing number of discarded analog CRT-based television receivers.





</doc>
<doc id="8274" url="https://en.wikipedia.org/wiki?curid=8274" title="Declaration of Arbroath">
Declaration of Arbroath

The Declaration of Arbroath is a declaration of Scottish independence, made in 1320. It is in the form of a letter in Latin submitted to Pope John XXII, dated 6 April 1320, intended to confirm Scotland's status as an independent, sovereign state and defending Scotland's right to use military action when unjustly attacked.

Generally believed to have been written in the Arbroath Abbey by Bernard of Kilwinning, then Chancellor of Scotland and Abbot of Arbroath, and sealed by fifty-one magnates and nobles, the letter is the sole survivor of three created at the time. The others were a letter from the King of Scots, Robert I, and a letter from four Scottish bishops which all presumably made similar points.

The Declaration was part of a broader diplomatic campaign which sought to assert Scotland's position as an independent kingdom, rather than being a feudal land controlled by England's Norman kings, as well as lift the excommunication of Robert the Bruce. The Pope had recognised Edward I of England's claim to overlordship of Scotland in 1305 and Bruce was excommunicated by the Pope for murdering John Comyn before the altar in Greyfriars Church in Dumfries in 1306.

The Declaration made a number of points: that Scotland had always been independent, indeed for longer than England; that Edward I of England had unjustly attacked Scotland and perpetrated atrocities; that Robert the Bruce had delivered the Scottish nation from this peril; and, most controversially, that the independence of Scotland was the prerogative of the Scottish people, rather than the King of Scots. In fact it stated that the nobility would choose someone else to be king if Bruce proved to be unfit in maintaining Scotland's independence.

Some have interpreted this last point as an early expression of 'popular sovereignty' – that government is contractual and that kings can be chosen by the community rather than by God alone. Modern Scottish nationalists point to the “Declaration" as evidence of the long-term persistence of the Scots as a distinct national community, giving a very early date for the emergence of nationalism. However "the overwhelming majority of academics challenge this vision. Scholars point out that definitions change with time. The meaning ascribed to words similar to nation during the ancient and medieval periods was often quite different than it is today."

It has also been argued that the Declaration was not a statement of popular sovereignty (and that its signatories would have had no such concept) but a statement of royal propaganda supporting Bruce's faction. A justification had to be given for the rejection of King John Balliol in whose name William Wallace and Andrew de Moray had rebelled in 1297. The reason given in the Declaration is that Bruce was able to defend Scotland from English aggression whereas, by implication, King John could not.

Whatever the true motive, the idea of a contract between King and people was advanced to the Pope as a justification for Bruce's coronation whilst John de Balliol still lived in Papal custody.

There are 39 names—eight earls and thirty one barons—at the start of the document, all of whom may have had their seals appended, probably over the space of some weeks and months, with nobles sending in their seals to be used. On the extant copy of the Declaration there are only 19 seals, and of those 19 people only 12 are named within the document. It is thought likely that at least 11 more seals than the original 39 might have been appended. The Declaration was then taken to the papal court at Avignon by Bishop Kininmund, Sir Adam Gordon and Sir Odard de Maubuisson.
The Pope heeded the arguments contained in the Declaration, influenced by the offer of support from the Scots for his long-desired crusade if they no longer had to fear English invasion. He exhorted Edward II in a letter to make peace with the Scots, but the following year was again persuaded by the English to take their side and issued six bulls to that effect.

On 1 March 1328 the new English king, Edward III signed a peace treaty between Scotland and England, the Treaty of Edinburgh-Northampton. In this treaty, which was in effect for five years until 1333, Edward renounced all English claims to Scotland. Eight months later, in October 1328, the interdict on Scotland, and the excommunication of its king, were removed by the Pope.

The original copy of the Declaration that was sent to Avignon is lost. A copy of the Declaration survives among Scotland's state papers, held by the National Archives of Scotland in Edinburgh. The most widely known English language translation was made by Sir James Fergusson, formerly Keeper of the Records of Scotland, from text that he reconstructed using this extant copy and early copies of the original draft. One passage in particular, strongly suggesting Sallust (86–35 BC) as the direct source, is often quoted from the Fergusson translation:

Listed below are the signatories of the Declaration of Arbroath in 1320. Although it includes several consistent Bruce loyalists, it includes others who had opposed Bruce, or whom Bruce tried for plotting against him a few months later, and others of whom little is known.

The declaration itself is written in Latin. It uses the Latin versions of the signatories' titles, and in some cases the spelling of names has changed over the years. This list generally uses the titles of the signatories' Wikipedia biographies.


In addition, the names of the following do not appear in the document's text, but their names are written on seal tags and their seals are present:


US Senate Resolution 155 of 10 November 1997 states that the Declaration of Arbroath, the Scottish Declaration of Independence [sic], was signed on 6 April 1320 and the American Declaration of Independence was modeled on that inspirational document. However, although this influence is accepted by some historians, it is disputed by others. Even advocates of the link concede that it is speculative and not based on any verifiable sources.

In 2016 the Declaration of Arbroath was placed on UNESCO's Memory of the World register.





</doc>
<doc id="8276" url="https://en.wikipedia.org/wiki?curid=8276" title="Digital data">
Digital data

Digital data, in information theory and information systems, is the discrete, discontinuous representation of information or works. Numbers and letters are commonly used representations.

Digital data can be contrasted with analog signals which behave in a continuous manner, and with continuous functions such as sounds, images, and other measurements.

The word "digital" comes from the same source as the words digit and "digitus" (the Latin word for "finger"), as fingers are often used for discrete counting. Mathematician George Stibitz of Bell Telephone Laboratories used the word "digital" in reference to the fast electric pulses emitted by a device designed to aim and fire anti-aircraft guns in 1942. The term is most commonly used in computing and electronics, especially where real-world information is converted to binary numeric form as in digital audio and digital photography.

Since symbols (for example, alphanumeric characters) are not continuous, representing symbols digitally is rather simpler than conversion of continuous or analog information to digital. Instead of sampling and quantization as in analog-to-digital conversion, such techniques as polling and encoding are used.

A symbol input device usually consists of a group of switches that are polled at regular intervals to see which switches are switched. Data will be lost if, within a single polling interval, two switches are pressed, or a switch is pressed, released, and pressed again. This polling can be done by a specialized processor in the device to prevent burdening the main CPU. When a new symbol has been entered, the device typically sends an interrupt, in a specialized format, so that the CPU can read it.

For devices with only a few switches (such as the buttons on a joystick), the status of each can be encoded as bits (usually 0 for released and 1 for pressed) in a single word. This is useful when combinations of key presses are meaningful, and is sometimes used for passing the status of modifier keys on a keyboard (such as shift and control). But it does not scale to support more keys than the number of bits in a single byte or word.

Devices with many switches (such as a computer keyboard) usually arrange these switches in a scan matrix, with the individual switches on the intersections of x and y lines. When a switch is pressed, it connects the corresponding x and y lines together. Polling (often called scanning in this case) is done by activating each x line in sequence and detecting which y lines then have a signal, thus which keys are pressed. When the keyboard processor detects that a key has changed state, it sends a signal to the CPU indicating the scan code of the key and its new state. The symbol is then encoded, or converted into a number, based on the status of modifier keys and the desired character encoding.

A custom encoding can be used for a specific application with no loss of data. However, using a standard encoding such as ASCII is problematic if a symbol such as 'ß' needs to be converted but is not in the standard.

It is estimated that in the year 1986 less than 1% of the world's technological capacity to store information was digital and in 2007 it was already 94%. The year 2002 is assumed to be the year when human kind was able to store more information in digital than in analog format (the "beginning of the digital age").

Digital data come in these three states: data at rest, data in transit and data in use. The confidentiality, integrity and availability have to be managed during the entire lifecycle from 'birth' to the destruction of the data.

All digital information possesses common properties that distinguish it from analog data with respect to communications:

Even though digital signals are generally associated with the binary electronic digital systems used in modern electronics and computing, digital systems are actually ancient, and need not be binary or electronic.



</doc>
<doc id="8278" url="https://en.wikipedia.org/wiki?curid=8278" title="Deduction">
Deduction

Deduction may refer to:





</doc>
<doc id="8280" url="https://en.wikipedia.org/wiki?curid=8280" title="Demon">
Demon

A demon (from Koine Greek "daimónion") is a supernatural and often malevolent being prevalent in religion, occultism, literature, fiction, mythology and folklore.

The original Greek word "daimon" does not carry such negative connotations. The Ancient Greek word "daimōn" denotes a spirit or divine power, much like the Latin "genius" or "numen". The Greek conception of a "daimōn" notably appears in the works of Plato, where it describes the divine inspiration of Socrates. 

However, in Ancient Near Eastern religions as well as in the Abrahamic traditions, including ancient and medieval Christian demonology, a demon is considered a harmful spiritual entity which may cause demonic possession, calling for an exorcism. 
In Western occultism and Renaissance magic, which grew out of an amalgamation of Greco-Roman magic, Jewish Aggadah and Christian demonology, a demon is believed to be a spiritual entity that may be conjured and controlled.

The Ancient Greek word "daimōn" denotes a spirit or divine power, much like the Latin "genius" or "numen". "Daimōn" most likely came from the Greek verb "daiesthai" (to divide, distribute). The Greek conception of a "daimōn" notably appears in the works of Plato, where it describes the divine inspiration of Socrates. To distinguish the classical Greek concept from its later Christian interpretation, the former is anglicized as either "daemon" or "daimon" rather than "demon". The original Greek word "daimon" does not carry the negative connotation initially understood by implementation of the Koine ("daimonion"), and later ascribed to any cognate words sharing the root.

The Greek terms do not have any connotations of evil or malevolence. In fact, "eudaimonia", (literally good-spiritedness) means happiness. By the early Roman Empire, cult statues were seen, by pagans and their Christian neighbors alike, as inhabited by the numinous presence of the gods: "Like pagans, Christians still sensed and saw the gods and their power, and as something, they had to assume, lay behind it, by an easy traditional shift of opinion they turned these pagan "daimones" into malevolent 'demons', the troupe of Satan... Far into the Byzantine period Christians eyed their cities' old pagan statuary as a seat of the demons' presence. It was no longer beautiful, it was infested." The term had first acquired its negative connotations in the Septuagint translation of the Hebrew Bible into Greek, which drew on the mythology of ancient Semitic religions. This was then inherited by the Koine text of the New Testament. The Western medieval and neo-medieval conception of a "demon" derives seamlessly from the ambient popular culture of Late Antiquity. The Hellenistic "daemon" eventually came to include many Semitic and Near Eastern gods as evaluated by Christianity.

The supposed existence of demons remains an important concept in many modern religions and occultist traditions. Demons are still feared largely due to their alleged power to possess living creatures. In the contemporary Western occultist tradition (perhaps epitomized by the work of Aleister Crowley), a demon (such as Choronzon, which is Crowley's interpretation of the so-called 'Demon of the Abyss') is a useful metaphor for certain inner psychological processes (inner demons), though some may also regard it as an objectively real phenomenon. Some scholars believe that large portions of the demonology (see Asmodai) of Judaism, a key influence on Christianity and Islam, originated from a later form of Zoroastrianism, and were transferred to Judaism during the Persian era.

According to the Jewish Encyclopedia, "In Chaldean mythology the seven evil deities were known as "shedu", storm-demons, represented in ox-like form." They were represented as winged bulls, derived from the colossal bulls used as protective jinn of royal palaces.

From Chaldea, the term "shedu" traveled to the Israelites. The writers of the Tanach applied the word as a dialogism to Canaanite deities.

There are indications that demons in popular Hebrew mythology were believed to come from the nether world. Various diseases and ailments were ascribed to them, particularly those affecting the brain and those of internal nature. Examples include catalepsy, headache, epilepsy and nightmares. There also existed a demon of blindness, "Shabriri" (lit. "dazzling glare") who rested on uncovered water at night and blinded those who drank from it.

Demons supposedly entered the body and caused the disease while overwhelming or "seizing" the victim. To cure such diseases, it was necessary to draw out the evil demons by certain incantations and talismanic performances, at which the Essenes excelled. Josephus, who spoke of demons as "spirits of the wicked which enter into men that are alive and kill them", but which could be driven out by a certain root, witnessed such a performance in the presence of the Emperor Vespasian and ascribed its origin to King Solomon. In mythology, there were few defences against Babylonian demons. The mythical mace Sharur had the power to slay demons such as Asag, a legendary gallu or edimmu of hideous strength.

As referring to the existence or non-existence of demons ("shedim" or "Se'irim") there are converse opinions in Judaism. There are "practically nil" roles assigned to demons in the Jewish Bible. In Judaism today, beliefs in "demons" or "evil spirits" are either midot hasidut (Hebr. for "customs of the pious"), and therefore not halachah, or notions based on a superstition that are non-essential, non-binding parts of Judaism, and therefore not normative Jewish practice. In conclusion, Jews are not obligated to believe in the existence of "shedim", as posek rabbi David Bar-Hayim points out.

The Tanakh mentions two classes of demonic spirits, the "se'irim" and the "shedim". The word "shedim" appears only in two places in the Tanakh (, ). The "se'irim" are mentioned once in , probably a re-calling of Assyrian demons in shape of goats. The "shedim" in return are not pagan demigods, but the foreign gods themselves. Both entities appear in a scriptural context of animal or child sacrifice to "non-existent" false gods.

In the Jerusalem Talmud notions of "shedim" ("demons" or "spirits") are almost unknown or occur only very rarely, whereas in the Babylon Talmud there are many references to "shedim" and magical incantations. The existence of "shedim" in general was not questioned by most of the Babylonian Talmudists. As a consequence of the rise of influence of the Babylonian Talmud over that of the Jerusalem Talmud, late rabbis in general took as fact the existence of "shedim", nor did most of the medieval thinkers question their reality. However, rationalists like Maimonides, Saadia Gaon and Abraham ibn Ezra and others explicitly denied their existence, and completely rejected concepts of demons, evil spirits, negative spiritual influences, attaching and possessing spirits. Their point of view eventually became mainstream Jewish understanding.

In Kabbalah demons are regarded a necessary part of the divine emanation in the material world and a byproduct of human sin (Qliphoth). However spirits such as the "shedim" may also be benevolent and were used in kabbalistic ceremonies (as with the "golem" of Rabbi Yehuda Loevy) and malevolent "shedim" ("mazikin", from the root meaning "to damage") were often credited with possession.

Aggadic tales from the Persian tradition describe the "shedim", the " mazziḳim" ("harmers"), and the " ruḥin" ("spirits"). There were also "lilin" ("night spirits"), " ṭelane" ("shade", or "evening spirits"), " ṭiharire" ("midday spirits"), and " ẓafrire" ("morning spirits"), as well as the "demons that bring famine" and "such as cause storm and earthquake". According to some aggadic stories, demons were under the dominion of a king or chief, either Asmodai or, in the older Aggadah, Samael ("the angel of death"), who killed via poison. Stories in the fashion of this kind of folklore never became an essential feature of Jewish theology. Although occasionally an angel is called "satan" in the Babylon Talmud, this does not refer to a demon: "Stand not in the way of an ox when coming from the pasture, for Satan dances between his horns".

To the Qumran community during the Second Temple period this apotropaic prayer was assigned, stating: "And, I the Sage, declare the grandeur of his radiance in order to frighten and terri[fy] all the spirits of the ravaging angels and the bastard spirits, demons, Liliths, owls" ("Dead Sea Scrolls", "Songs of the Sage," Lines 4–5).

In the Dead Sea Scrolls, there exists a fragment entitled "Curses of Belial" ("Curses of Belial (Dead Sea Scrolls, 394, 4Q286(4Q287, fr. 6)=4QBerakhot)"). This fragment holds much rich language that reflects the sentiment shared between the Qumran towards Belial. In many ways this text shows how these people thought Belial influenced sin through the way they address him and speak of him. By addressing "Belial and all his guilty lot," (4Q286:2) they make it clear that he is not only impious, but also guilty of sins. Informing this state of uncleanliness are both his "hostile" and "wicked design" (4Q286:3,4). Through this design, Belial poisons the thoughts of those who are not necessarily sinners. Thus a dualism is born from those inclined to be wicked and those who aren't. It is clear that Belial directly influences sin by the mention of "abominable plots" and "guilty inclination" (4Q286:8,9). These are both mechanisms by which Belial advances his evil agenda that the Qumran have exposed and are calling upon God to protect them from. There is a deep sense of fear that Belial will "establish in their heart their evil devices" (4Q286:11,12). This sense of fear is the stimulus for this prayer in the first place. Without the worry and potential of falling victim to Belial's demonic sway, the Qumran people would never feel impelled to craft a curse. This very fact illuminates the power Belial was believed to hold over mortals, and the fact that sin proved to be a temptation that must stem from an impure origin.

In Jubilees 1:20, Belial's appearance continues to support the notion that sin is a direct product of his influence. Moreover, Belial's presence acts as a placeholder for all negative influences or those that would potentially interfere with God's will and a pious existence. Similarly to the "gentiles ... [who] cause them to sin against you" (Jubilees 1:19), Belial is associated with a force that drives one away from God. Coupled in this plea for protection against foreign rule, in this case the Egyptians, is a plea for protection from "the spirit of Belial" (Jubilees 1:19). Belial's tendency is to "ensnare [you] from every path of righteousness" (Jubilees 1:19). This phrase is intentionally vague, allowing room for interpretation. Everyone, in one way or another, finds themselves straying from the path of righteousness and by pawning this transgression off on Belial, he becomes a scapegoat for all misguidance, no matter what the cause. By associating Belial with all sorts of misfortune and negative external influence, the Qumran people are henceforth allowed to be let off for the sins they commit.

Belial's presence is found throughout the War Scrolls, located in the Dead Sea Scrolls, and is established as the force occupying the opposite end of the spectrum of God. In Col. I, verse 1, the very first line of the document, it is stated that "the first attack of the Sons of Light shall be undertaken against the forces of the Sons of Darkness, the army of Belial" (1Q33;1:1). This dichotomy sheds light on the negative connotations that Belial held at the time. Where God and his Sons of Light are forces that protect and promote piety, Belial and his Sons of Darkness cater to the opposite, instilling the desire to sin and encouraging destruction. This opposition is only reinforced later in the document; it continues to read that the "holy ones" will "strike a blow at wickedness", ultimately resulting in the "annihilation of the Sons of Darkness" (1Q33:1:13). This epic battle between good and evil described in such abstract terms, however it is also applicable to everyday life and serves as a lens through which the Qumran see the world. Every day is the Sons of Light battle evil and call upon God to help them overcome evil in ways small and large.

Belial's influence is not taken lightly. In Col. XI, verse 8, the text depicts God conquering the "hordes of Belial" (1Q33;11:8). This defeat is indicative of God's power over Belial and his forces of temptation. However the fact that Belial is the leader of hordes is a testament to how persuasive he can be. If Belial was obviously an arbiter of wrongdoing and was blatantly in the wrong, he wouldn't be able to amass an army. This fact serves as a warning message, reasserting God's strength, while also making it extremely clear the breadth of Belial's prowess. Belial's "council is to condemn and convict", so the Qumran feel strongly that their people are not only aware of his purpose, but also equipped to combat his influence (1Q33;13:11).

In the Damascus Document, Belial also makes a prominent appearance, being established as a source of evil and an origin of several types of sin. In Column 4, the first mention of Belial reads: "Belial shall be unleashed against Israel" (4Q266). This phrase is able to be interpreted myriad different ways. Belial is characterized in a wild and uncontrollable fashion, making him seem more dangerous and unpredictable. The notion of being unleashed is such that once he is free to roam; he is unstoppable and able to carry out his agenda uninhibited. The passage then goes to enumerate the "three nets" (4Q266;4:16) by which Belial captures his prey and forces them to sin. "Fornication ..., riches ..., [and] the profanation of the temple" (4Q266;4:17,18) make up the three nets. These three temptations were three agents by which people were driven to sin, so subsequently, the Qumran people crafted the nets of Belial to rationalize why these specific temptations were so toxic. Later in Column 5, Belial is mentioned again as one of "the removers of bound who led Israel astray" (4Q266;5:20). This statement is a clear display of Belial's influence over man regarding sin. The passage goes on to state: "they preached rebellion against ... God" (4Q266;5:21,22). Belial's purpose is to undermine the teachings of God, and he achieves this by imparting his nets on humans, or the incentive to sin.

In the "War of the Sons of Light Against the Sons of Darkness", Belial controls scores of demons, which are specifically allotted to him by God for the purpose of performing evil. Belial, despite his malevolent disposition, is considered an angel.

Demons in the Old Testament of the Christian Bible are of two classes: the "satyrs" or "shaggy goats" (from Hebr. "se'irim" "hairy beings" and Greek Old Testament σάτυρος "satyros", "satyr"; , ) and the "demons" (from Hebr. "shedim", and Koine Greek δαιμόνιον "daimonion"; , ).

The term "demon" (from the Greek New Testament δαιμόνιον "daimonion") appears 63 times in the New Testament of the Christian Bible.

Demons are sometimes included into biblical interpretation. In the story of Passover, the Bible tells the story as "the Lord struck down all the firstborn in Egypt" (Exodus 12:21–29). In the Book of Jubilees, which is considered canonical only by the Ethiopian Orthodox Church, this same event is told slightly differently: "All the powers of [the demon] Mastema had been let loose to slay all the first-born in the land of Egypt...And the powers of the Lord did everything according as the Lord commanded them" (Jubilees 49:2–4).

In the Genesis flood narrative the author explains how God was noticing "how corrupt the earth had become, for all the people on earth had corrupted their ways" (Genesis 6:12). In Jubilees the sins of man are attributed to "the unclean demons [who] began to lead astray the children of the sons of Noah, and to make to err and destroy them" (Jubilees 10:1). In Jubilees Mastema questions the loyalty of Abraham and tells God to "bid him offer him as a burnt offering on the altar, and Thou wilt see if he will do this command" (Jubilees 17:16). The discrepancy between the story in Jubilees and the story in Genesis 22 exists with the presence of Mastema. In Genesis, God tests the will of Abraham merely to determine whether he is a true follower, however; in Jubilees Mastema has an agenda behind promoting the sacrifice of Abraham's son, "an even more demonic act than that of the Satan in Job." In Jubilees, where Mastema, an angel tasked with the tempting of mortals into sin and iniquity, requests that God give him a tenth of the spirits of the children of the watchers, demons, in order to aid the process. These demons are passed into Mastema’s authority, where once again, an angel is in charge of demonic spirits.
The sources of demonic influence were thought to originate from the Watchers or Nephilim, who are first mentioned in Genesis 6 and are the focus of 1 Enoch Chapters 1–16, and also in Jubilees 10. The Nephilim were seen as the source of the sin and evil on earth because they are referenced in Genesis 6:4 before the story of the Flood. In Genesis 6:5, God sees evil in the hearts of men. The passage states, "the wickedness of humankind on earth was great", and that "Every inclination of the thoughts of their hearts was only continually evil" (Genesis 5). The mention of the Nephilim in the preceding sentence connects the spread of evil to the Nephilim. Enoch is a very similar story to Genesis 6:4–5, and provides further description of the story connecting the Nephilim to the corruption of humans. In Enoch, sin originates when angels descend from heaven and fornicate with women, birthing giants as tall as 300 cubits. The giants and the angels' departure of Heaven and mating with human women are also seen as the source of sorrow and sadness on Earth. The book of Enoch shows that these fallen angels can lead humans to sin through direct interaction or through providing forbidden knowledge. In Enoch, Semyaz leads the angels to mate with women. Angels mating with humans is against God's commands and is a cursed action, resulting in the wrath of God coming upon Earth. Azazel indirectly influences humans to sin by teaching them divine knowledge not meant for humans. Asael brings down the "stolen mysteries" (Enoch 16:3). Asael gives the humans weapons, which they use to kill each other. Humans are also taught other sinful actions such as beautification techniques, alchemy, astrology and how to make medicine (considered forbidden knowledge at the time). Demons originate from the evil spirits of the giants that are cursed by God to wander the earth. These spirits are stated in Enoch to "corrupt, fall, be excited, and fall upon the earth, and cause sorrow" (Enoch 15:11).

The Book of Jubilees conveys that sin occurs when Cainan accidentally transcribes astrological knowledge used by the Watchers (Jubilees 8). This differs from Enoch in that it does not place blame on the Angels. However, in Jubilees 10:4 the evil spirits of the Watchers are discussed as evil and still remain on earth to corrupt the humans. God binds only 90 percent of the Watchers and destroys them, leaving 10 percent to be ruled by Mastema. Because the evil in humans is great, only 10 percent would be needed to corrupt and lead humans astray. These spirits of the giants also referred to as "the bastards" in the Apotropaic prayer Songs of the Sage, which lists the names of demons the narrator hopes to expel.

In Christianity, demons are corrupted spirits carrying the execution of Satan's desires. They are generally regarded as three different types of spirits:

Often deities of other religions are interpreted or identified as such "demons" (from the Greek Old Testament δαιμόνιον "daimonion"). The evolution of the Christian Devil and pentagram are examples of early rituals and images that showcase evil qualities, as seen by the Christian churches.

Since Early Christianity, demonology has developed from a simple acceptance of demons to a complex study that has grown from the original ideas taken from Jewish demonology and Christian scriptures. Christian demonology is studied in depth within the Roman Catholic Church, although many other Christian churches affirm and discuss the existence of demons.

Building upon the few references to "daemons" in the New Testament, especially the poetry of the Book of Revelation, Christian writers of apocrypha from the 2nd century onwards created a more complicated tapestry of beliefs about "demons" that was largely independent of Christian scripture.

The contemporary Roman Catholic Church unequivocally teaches that angels and demons are real beings rather than just symbolic devices. The Catholic Church has a cadre of officially sanctioned exorcists which perform many exorcisms each year. The exorcists of the Catholic Church teach that demons attack humans continually but that afflicted persons can be effectively healed and protected either by the formal rite of exorcism, authorized to be performed only by bishops and those they designate, or by prayers of deliverance, which any Christian can offer for themselves or others.

At various times in Christian history, attempts have been made to classify demons according to various proposed demonic hierarchies.

In the Gospels, particularly the Gospel of Mark, Jesus cast out many demons from those afflicted with various ailments. He also lent this power to some of his disciples ().

Apuleius, by Augustine of Hippo, is ambiguous as to whether "daemons" had become "demonized" by the early 5th century:

He [Apulieus] also states that the blessed are called in Greek "eudaimones", because they are good souls, that is to say, good demons, confirming his opinion that the souls of men are demons.

Islam and Islam-related beliefs acknowledges the concept of evil spirits known as malevolent Jinn, Afarit and Shayatin. Unlike the belief in angels, belief in demons is not obligated by the six articles of Islamic faith. However, several demonic spirits is generally assumed by Islamic theology and further elaborated beliefs persist in Islamic folklore.

Rather than demonic, Jinn are depicted as close to humans regarded as living in societies, in need of dwelling places, eating and drinking, and although their lifespan exceeds those of humans over centuries, they die and also need to procreate, but because they are created from "smokeless fire" in contrast to humans made from "solid earth", the latter can not see them. As for humans, Jinn are also subject to temptations of the Shayatin and Satan therefore may either be good or evil. Evil Jinn are comparable to demons, scaring or possessing humans. In folklore some Jinn may also lurk on lonely travelers to dissuade them from their paths and eat their corpses. Although not evil, a Jinni may haunt a person, because it feels offended by him. Islam has no binding origin story of Jinn, but Islamic beliefs commonly assume that the Jinn were created on a Thursday thousands of years before mankind. Therefore Islamic medieval narratives often called them "Pre-Adamites".

The Shayatin are the Islamic equivalent of "demons" in western usage.
Islam differs in regard of the origin of demons. They may either be a class of heavenly creatures cast out of heaven or the descendants of Iblis. Unlike Jinn and humans, Shayatin are immortal and will just die, then the world perishes, however prayers could dissolve or banish them. However unlike Jinn and human, Shayatin can not attain salvation. Further they are thought to attempt to reach to heaven, but are chased away from the Angels or shooting stars. The Shayatin do not possess people, but "whisper" to their minds and seduce them into falsehood and sin. These are called "waswās" and may enter the hearts of humans to support negative feelings, especially in states of strong emotion like depression or anger.

Another demonic spirit is called Ifrit. While some regard them as a kind of malevolent and powerful Jinn, other traditions depict them as a subcategory of Shayatin. However they are not exactly Shayatin since they differ in their origin, rather the Ifrit is a kind of Ghost drawn the life-force of those who were murdered.

Hindu beliefs include numerous varieties of spirits that might be classified as demigods, including Vetalas, Bhutas and Pishachas. Rakshasas and Asuras are often misunderstood to be as demons. There are no demons in Hinduism as hinduism is not based on good and evil and is not constructed by the principle of duality. 

Originally, "Asura", in the earliest hymns of the Rig Veda, meant any supernatural spirit, either good or bad. Since the /s/ of the Indic linguistic branch is cognate with the /h/ of the Early Iranian languages, the word "Asura", representing a category of celestial beings, became the word "Ahura" (Mazda), the Supreme God of the monotheistic Zoroastrians. Ancient Hinduism tells that Devas (also called "suras") and Asuras are half-brothers, sons of the same father Kashyapa; although some of the Devas, such as Varuna, are also called Asuras. Later, during Puranic age, Asura and Rakshasa came to exclusively mean any of a race of anthropomorphic, powerful, possibly evil beings. Daitya (lit. sons of the mother "Diti"), Rakshasa (lit. from "harm to be guarded against"), and Asura are incorrectly translated into English as "demon".

Post Vedic, Hindu scriptures, pious, highly enlightened Asuras, such as Prahlada and Vibhishana, are not uncommon. The Asura are not fundamentally against the gods, nor do they tempt humans to fall. Many people metaphorically interpret the Asura as manifestations of the ignoble passions in the human mind and as a symbolic devices. There were also cases of power-hungry Asuras challenging various aspects of the Gods, but only to be defeated eventually and seek forgiveness—see Surapadman and Narakasura.

Hinduism advocates the reincarnation and transmigration of souls according to one's karma. Souls (Atman) of the dead are adjudged by the Yama and are accorded various purging punishments before being reborn. Humans that have committed extraordinary wrongs are condemned to roam as lonely, often mischief mongers, spirits for a length of time before being reborn. Many kinds of such spirits (Vetalas, Pishachas, Bhūta) are recognized in the later Hindu texts.

In the Bahá'í Faith, demons are not regarded as independent evil spirits as they are in some faiths. Rather, evil spirits described in various faiths' traditions, such as Satan, fallen angels, demons and jinns, are metaphors for the base character traits a human being may acquire and manifest when he turns away from God and follows his lower nature. Belief in the existence of ghosts and earthbound spirits is rejected and considered to be the product of superstition.

While some people fear demons, or attempt to exorcise them, others willfully attempt to summon them for knowledge, assistance, or power. The ceremonial magician usually consults a grimoire, which gives the names and abilities of demons as well as detailed instructions for conjuring and controlling them. Grimoires are not limited to demons – some give the names of angels or spirits which can be called, a process called theurgy. The use of ceremonial magic to call demons is also known as goetia, the name taken from a section in the famous grimoire known as the "Lesser Key of Solomon".

According to Rosemary Ellen Guiley, "Demons are not courted or worshipped in contemporary Wicca and Paganism. The existence of negative energies is acknowledged."

Psychologist Wilhelm Wundt remarked that "among the activities attributed by myths all over the world to demons, the harmful predominate, so that in popular belief bad demons are clearly older than good ones." Sigmund Freud developed this idea and claimed that the concept of demons was derived from the important relation of the living to the dead: "The fact that demons are always regarded as the spirits of those who have died "recently" shows better than anything the influence of mourning on the origin of the belief in demons."

M. Scott Peck, an American psychiatrist, wrote two books on the subject, "People of the Lie: The Hope For Healing Human Evil" and "Glimpses of the Devil: A Psychiatrist's Personal Accounts of Possession, Exorcism, and Redemption". Peck describes in some detail several cases involving his patients. In "People of the Lie" he provides identifying characteristics of an evil person, whom he classified as having a character disorder. In "Glimpses of the Devil" Peck goes into significant detail describing how he became interested in exorcism in order to debunk the "myth" of possession by evil spirits – only to be convinced otherwise after encountering two cases which did not fit into any category known to psychology or psychiatry. Peck came to the conclusion that possession was a rare phenomenon related to evil and that possessed people are not actually evil; rather, they are doing battle with the forces of evil.

Although Peck's earlier work was met with widespread popular acceptance, his work on the topics of evil and possession has generated significant debate and derision. Much was made of his association with (and admiration for) the controversial Malachi Martin, a Roman Catholic priest and a former Jesuit, despite the fact that Peck consistently called Martin a liar and a manipulator. Richard Woods, a Roman Catholic priest and theologian, has claimed that Dr. Peck misdiagnosed patients based upon a lack of knowledge regarding dissociative identity disorder (formerly known as multiple personality disorder) and had apparently transgressed the boundaries of professional ethics by attempting to persuade his patients into accepting Christianity. Father Woods admitted that he has never witnessed a genuine case of demonic possession in all his years.

According to S. N. Chiu, God is shown sending a demon against Saul in 1 Samuel 16 and 18 in order to punish him for the failure to follow God's instructions, showing God as having the power to use demons for his own purposes, putting the demon under his divine authority. According to the "Britannica Concise Encyclopedia", demons, despite being typically associated with evil, are often shown to be under divine control, and not acting of their own devices.





</doc>
<doc id="8286" url="https://en.wikipedia.org/wiki?curid=8286" title="Domino effect">
Domino effect

A domino effect or chain reaction is the cumulative effect produced when one event sets off a chain of similar events. The term is best known as a mechanical effect, and is used as an analogy to a falling row of dominoes. It typically refers to a linked sequence of events where the time between successive events is relatively small. It can be used literally (an observed series of actual collisions) or metaphorically (causal linkages within systems such as global finance or politics). The term "domino effect" is used both to imply that an event is inevitable or highly likely (as it has already started to happen), and conversely to imply that an event is impossible or highly unlikely (the one domino left standing).


Relevant physical theory:

Mathematical theory

Political theory

Social




</doc>
<doc id="8293" url="https://en.wikipedia.org/wiki?curid=8293" title="Diffusion pump">
Diffusion pump

Diffusion pumps use a high speed jet of vapor to direct gas molecules in the pump throat down into the bottom of the pump and out the exhaust. Invented in 1915 by Wolfgang Gaede using mercury vapor, and improved by Irving Langmuir and W. Crawford, they were the first type of high vacuum pumps operating in the regime of free molecular flow, where the movement of the gas molecules can be better understood as diffusion than by conventional fluid dynamics. Gaede used the name "diffusion pump" since his design was based on the finding that gas cannot diffuse against the vapor stream, but will be carried with it to the exhaust. However, the principle of operation might be more precisely described as gas-jet pump, since diffusion plays a role also in other high vacuum pumps. In modern text books, the diffusion pump is categorized as a momentum transfer pump.

The diffusion pump is widely used in both industrial and research applications. Most modern diffusion pumps use silicone oil or polyphenyl ethers as the working fluid. Cecil Reginald Burch discovered the possibility of using silicone oil in 1928.

An oil diffusion pump is used to achieve higher vacuum (lower pressure) than is possible by use of positive displacement pumps alone. Although its use has been mainly associated within the high-vacuum range (down to 10 mbar), diffusion pumps today can produce pressures approaching 10 mbar when properly used with modern fluids and accessories. The features that make the diffusion pump attractive for high and ultra-high vacuum use are its high pumping speed for all gases and low cost per unit pumping speed when compared with other types of pump used in the same vacuum range. Diffusion pumps cannot discharge directly into the atmosphere, so a mechanical forepump is typically used to maintain an outlet pressure around 0.1 mbar.

The oil diffusion pump is operated with an oil of low vapor pressure. The high speed jet is generated by boiling the fluid and directing the vapor through a jet assembly. Note that the oil is gaseous when entering the nozzles. Within the nozzles, the flow changes from laminar, to supersonic and molecular. Often, several jets are used in series to enhance the pumping action. The outside of the diffusion pump is cooled using either air flow or a water line. As the vapor jet hits the outer cooled shell of the diffusion pump, the working fluid condenses and is recovered and directed back to the boiler. The pumped gases continue flowing to the base of the pump at increased pressure, flowing out through the diffusion pump outlet, where they are compressed to ambient pressure by the secondary mechanical forepump and exhausted. 

Unlike turbomolecular pumps and cryopumps, diffusion pumps have no moving parts and as a result are quite durable and reliable. They can function over pressure ranges of 10 to 10 mbar. They are driven only by convection and thus have a very low energy efficiency.

One major disadvantage of diffusion pumps is the tendency to backstream oil into the vacuum chamber. This oil can contaminate surfaces inside the chamber or upon contact with hot filaments or electrical discharges may result in carbonaceous or siliceous deposits. Due to backstreaming, oil diffusion pumps are not suitable for use with highly sensitive analytical equipment or other applications which require an extremely clean vacuum environment, but mercury diffusion pumps may be in the case of ultra high vacuum chambers used for metal deposition. Often cold traps and baffles are used to minimize backstreaming, although this results in some loss of pumping ability.

The oil of a diffusion pump cannot be exposed to the atmosphere when hot. If this occurs, the oil will burn and has to be replaced.

 
The steam ejector is a popular form of diffusion pump for vacuum distillation and freeze-drying. A jet of steam entrains the vapour that must be removed from the vacuum chamber. Steam ejectors can have single or multiple stages, with and without condensers in between the stages.

One class of diffusion vacuum pumps is the multistage compressed-air driven ejector. It is very popular in applications where objects are moved around using suction cups and vacuum lines.




</doc>
<doc id="8299" url="https://en.wikipedia.org/wiki?curid=8299" title="Domenico Alberti">
Domenico Alberti

Domenico Alberti (c. 1710 – 14 October 1740 or 1746) was an Italian singer, harpsichordist, and composer.

Alberti was born in Venice and studied music with Antonio Lotti. He wrote operas, songs, and sonatas for keyboard instruments, for which he is best known today. These sonatas frequently employ arpeggiated accompaniment in the left hand in one of several patterns that are now collectively known as "Alberti bass". Alberti was one of the earliest composers to use these patterns, but was not the first or only. The most well-known of these patterns consists of regular broken chords, with the lowest note sounding first, then the highest, then the middle and then the highest again. This pattern is repeated. Today, Alberti is regarded as a minor composer, and his works are played or recorded only irregularly. The Alberti bass was used by many later composers, and it became an important element in much keyboard music of the Classical music era.

An example of Alberti bass (Mozart's "Piano Sonata, K 545"):

In his own lifetime, Alberti was known as a singer. He often used to accompany himself on the harpsichord. In 1736, he served as a page for Pietro Andrea Cappello, the Venetian ambassador to Spain. While at the Spanish court, the famous castrato singer Farinelli heard him sing. Farinelli was said to have been impressed, although Alberti was an amateur.

Alberti's best known pieces are his keyboard sonatas, although even they are very rarely performed. It is thought he wrote around 36 sonatas, of which 14 have survived. They all have two movements, each in binary form.

It is probable that Mozart's first violin sonatas, written at the age of seven, were modeled on Alberti's work.

Alberti died in 1740 or 1746 in Rome.


</doc>
<doc id="8300" url="https://en.wikipedia.org/wiki?curid=8300" title="Doris Day">
Doris Day

Doris Day (born Doris Mary Kappelhoff; April 3, 1922) is an American actress, singer, and animal welfare activist. After she began her career as a big band singer in 1939, her popularity increased with her first hit recording "Sentimental Journey" (1945). After leaving Les Brown & His Band of Renown to embark on a solo career, she recorded more than 650 songs from 1947 to 1967, which made her one of the most popular and acclaimed singers of the 20th century.

Day's film career began with the 1948 film "Romance on the High Seas", and its success sparked her twenty-year career as a motion picture actress. She starred in a series of successful films, including musicals, comedies, and dramas. She played the title role in "Calamity Jane" (1953), and starred in Alfred Hitchcock's "The Man Who Knew Too Much" (1956) with James Stewart. Her most successful films were the "pioneering" bedroom comedies she made co-starring Rock Hudson and James Garner, such as "Pillow Talk" (1959) and "Move Over, Darling" (1963), respectively. She also co-starred in films with such leading men as Clark Gable, Cary Grant, David Niven, and Rod Taylor. After her final film in 1968, she went on to star in the CBS sitcom "The Doris Day Show" (1968–73).

She was usually one of the top ten singers between 1951 and 1966. As an actress, she became the biggest female film star in the early 1960s, and ranked sixth among the box office performers by 2012. In 2011, she released her 29th studio album, "My Heart", which became a UK Top 10 album featuring new material. Among her awards, Day has received the Grammy Lifetime Achievement Award and a Legend Award from the Society of Singers. In 1960, she was nominated for the Academy Award for Best Actress, and in 1989 was given the Cecil B. DeMille Award for lifetime achievement in motion pictures. In 2004, she was awarded the Presidential Medal of Freedom by President George W. Bush followed in 2011 by the Los Angeles Film Critics Association's Career Achievement Award.

Doris Mary Kappelhoff was born on April 3, 1922, in Cincinnati, Ohio, the daughter of Alma Sophia (née Welz), a housewife, and William Joseph Kappelhoff, a music teacher and choir master. All of her grandparents were German immigrants. For most of her life, Day reportedly believed she had been born in 1924 and reported her age accordingly; it was not until her 95th birthday--when the Associated Press found her birth certificate, showing a 1922 date of birth--that she learned otherwise.

The youngest of three siblings, she had two older brothers: Richard (who died before her birth) and Paul, two to three years older. Due to her father's alleged infidelity, her parents separated. She developed an early interest in dance, and in the mid-1930s formed a dance duo with Jerry Doherty that performed locally in Cincinnati. A car accident on October 13, 1937, injured her right leg and curtailed her prospects as a professional dancer.

While recovering from an auto accident, Day started to sing along with the radio and discovered a talent she did not know she had. Day said: "During this long, boring period, I used to while away a lot of time listening to the radio, sometimes singing along with the likes of Benny Goodman, Duke Ellington, Tommy Dorsey, and Glenn Miller [...]. But the one radio voice I listened to above others belonged to Ella Fitzgerald. There was a quality to her voice that fascinated me, and I'd sing along with her, trying to catch the subtle ways she shaded her voice, the casual yet clean way she sang the words."

Observing her daughter sing rekindled Alma's interest in show business, and she decided to give Doris singing lessons. She engaged a teacher, Grace Raine. After three lessons, Raine told Alma that young Doris had "tremendous potential"; Raine was so impressed that she gave Doris three lessons a week for the price of one. Years later, Day said that Raine had the biggest effect on her singing style and career.

During the eight months she was taking singing lessons, Day had her first professional jobs as a vocalist, on the WLW radio program "Carlin's Carnival", and in a local restaurant, Charlie Yee's Shanghai Inn. During her radio performances, Day first caught the attention of Barney Rapp, who was looking for a girl vocalist and asked if Day would like to audition for the job. According to Rapp, he had auditioned about 200 singers when Day got the job.

While working for Rapp in 1939, she adopted the stage surname "Day", at Rapp's suggestion. Rapp felt that "Kappelhoff" was too long for marquees, and he admired her rendition of the song "Day After Day". After working with Rapp, Day worked with bandleaders Jimmy James, Bob Crosby, and Les Brown.

While working with Brown, Day scored her first hit recording, "Sentimental Journey", released in early 1945. It soon became an anthem of the desire of World War II demobilizing troops to return home. This song is still associated with Day, and she rerecorded it on several occasions, including a version in her 1971 television special. During 1945–46, Day (as vocalist with the Les Brown Band) had six other top ten hits on the" Billboard" chart: "My Dreams Are Getting Better All the Time", "'Tain't Me", "Till The End of Time", "You Won't Be Satisfied (Until You Break My Heart)", "The Whole World is Singing My Song", and "I Got the Sun in the Mornin'". In the 1950s she became the most popular and one of the highest paid singers in America.

While singing with the Les Brown band and for nearly two years on Bob Hope's weekly radio program, she toured extensively across the United States. Her popularity as a radio performer and vocalist, which included a second hit record "My Dreams Are Getting Better All the Time", led directly to a career in films. In 1941, Day appeared as a singer in three Soundies with the Les Brown band.

Her performance of the song "Embraceable You" impressed songwriter Jule Styne and his partner, Sammy Cahn, and they recommended her for a role in "Romance on the High Seas" (1948). Day got the part after auditioning for director Michael Curtiz. She was shocked at being offered the role in that film, and admitted to Curtiz that she was a singer without acting experience. But he said he liked that "she was honest," not afraid to admit it, and he wanted someone who "looked like the All-American Girl," which he felt she did. She was the discovery he was most proud of during his career.

The film provided her with a #2 hit recording as a soloist, "It's Magic", which followed by two months her first #1 hit ("Love Somebody" in 1948) recorded as a duet with Buddy Clark. Day recorded "Someone Like You", before the 1949 film "My Dream Is Yours", which featured the song.

In 1950, U.S. servicemen in Korea voted her their favorite star. She continued to make minor and frequently nostalgic period musicals such as "On Moonlight Bay", "By the Light of the Silvery Moon", and "Tea For Two" for Warner Brothers.

Her most commercially successful film for Warner was "I'll See You in My Dreams" (1951), which broke box-office records of 20 years. The film is a musical biography of lyricist Gus Kahn. It was Day's fourth film directed by Curtiz.

In 1953, Day appeared as the title character in the comedic western-themed musical, "Calamity Jane". A song from the film, "Secret Love", won the Academy Award for Best Original Song and became Day's fourth No. 1 hit single in the U.S.

Between 1950 and 1953, the albums from six of her movie musicals charted in the Top 10, three of them at No. 1. After filming "Lucky Me" with Bob Cummings and "Young at Heart" (both 1954) with Frank Sinatra, Day chose not to renew her contract with Warner Brothers.

During this period, Day also had her own radio program, "The Doris Day Show". It was broadcast on CBS in 1952–1953.

Having become primarily recognized as a musical-comedy actress, Day gradually took on more dramatic roles to broaden her range. Her dramatic star-turn as singer Ruth Etting in "Love Me or Leave Me" (1955), co-starring James Cagney, received critical and commercial success, becoming Day's biggest hit thus far. Day said it was her best film performance. Producer Joe Pasternak said, "I was stunned that Doris did not get an Oscar nomination." The soundtrack album from that movie was a No. 1 hit.

Day starred in Alfred Hitchcock's suspense film, "The Man Who Knew Too Much" (1956) with James Stewart. She sang two songs in the film, "Que Sera, Sera (Whatever Will Be, Will Be)", which won an Academy Award for Best Original Song, and "We'll Love Again". The film was Day's 10th movie to be in the Top 10 at the box office. In 1956, Day played the title role in the thriller/noir "Julie" with Louis Jourdan.

After three successive dramatic films, Day returned to her musical/comedic roots in 1957's "The Pajama Game" with John Raitt. The film was based on the Broadway play of the same name. She worked with Paramount Pictures for the comedy "Teacher's Pet" (1958), alongside Clark Gable and Gig Young. She co-starred with Richard Widmark and Gig Young in the romantic comedy film, "The Tunnel of Love" (1958), but found scant success opposite Jack Lemmon in "It Happened to Jane" (1959).

"Billboard" annual nationwide poll of disc jockeys had ranked Day as the No. 1 female vocalist nine times in ten years (1949 through 1958), but her success and popularity as a singer was now being overshadowed by her box-office appeal.

In 1959, Day entered her most successful phase as a film actress with a series of romantic comedies. This success began with "Pillow Talk" (1959), co-starring Rock Hudson, who became a lifelong friend, and Tony Randall. Day received a nomination for an Academy Award for Best Actress. Day, Hudson, and Randall made two more films together, "Lover Come Back" (1961) and "Send Me No Flowers" (1964).

In 1960, she starred with David Niven and Janis Paige in the hit "Please Don't Eat the Daisies". In 1962, Day appeared with Cary Grant in the comedy "That Touch of Mink", the first film in history ever to gross $1 million in one theatre (Radio City Music Hall). During 1960 and the 1962 to 1964 period, she ranked number one at the box office, the second woman to be number one four times. She set an unprecedented record that has yet to be equaled, receiving seven consecutive Laurel Awards as the top female box office star.

Day teamed up with James Garner, starting with "The Thrill of It All", followed by "Move Over, Darling" (both 1963). The film's theme song, "Move Over Darling", co-written by her son, reached #8 in the U.K. In between these comedic roles, Day co-starred with Rex Harrison in the movie thriller "Midnight Lace" (1960), an updating of the classic stage thriller, "Gaslight".

By the late 1960s, the sexual revolution of the baby boomer generation had refocused public attitudes about sex. Times changed, but Day's films did not. Day's next film, "Do Not Disturb" (1965), was popular with audiences, but her popularity soon waned. Critics and comics dubbed Day "The World's Oldest Virgin", and audiences began to shy away from her films. As a result, she slipped from the list of top box-office stars, last appearing in the top ten in 1966 with the hit film "The Glass Bottom Boat". One of the roles she turned down was that of "Mrs. Robinson" in "The Graduate", a role that eventually went to Anne Bancroft. In her published memoirs, Day said she had rejected the part on moral grounds: she found the script "vulgar and offensive".

She starred in the western film "The Ballad of Josie" (1967). That same year, Day recorded "The Love Album", although it was not released until 1994. The following year (1968), she starred in the comedy film "Where Were You When the Lights Went Out?" which centers on the Northeast blackout of November 9, 1965. Her final feature, the comedy "With Six You Get Eggroll", was released in 1968.

From 1959 to 1970, Day received nine Laurel Award nominations (and won four times) for best female performance in eight comedies and one drama. From 1959 through 1969, she received six Golden Globe nominations for best female performance in three comedies, one drama ("Midnight Lace"), one musical ("Jumbo"), and her television series.

When her third husband Martin Melcher died on April 20, 1968, a shocked Day discovered that Melcher and his business partner Jerome Bernard Rosenthal had squandered her earnings, leaving her deeply in debt. Rosenthal had been her attorney since 1949, when he represented her in her uncontested divorce action against her second husband, saxophonist George W. Weidler. Day filed suit against Rosenthal in February 1969, won a successful decision in 1974, but did not receive compensation until a settlement in 1979.

Day also learned to her displeasure that Melcher had committed her to a television series, which became "The Doris Day Show".

Day hated the idea of performing on television, but felt obliged to it. The first episode of "The Doris Day Show" aired on September 24, 1968, and, from 1968 to 1973, employed "Que Sera, Sera" as its theme song. Day persevered (she needed the work to help pay off her debts), but only after CBS ceded creative control to her and her son. The successful show enjoyed a five-year run, and functioned as a curtain raiser for the popular "Carol Burnett Show". It is remembered today for its abrupt season-to-season changes in casting and premise.

By the end of its run in 1973, public tastes had changed and her firmly established persona was regarded as passé. She largely retired from acting after "The Doris Day Show", but did complete two television specials, "The Doris Mary Anne Kappelhoff Special" (1971) and "Doris Day to Day" (1975). She appeared in a John Denver TV special in 1974.

In the 1985–86 season, Day hosted her own television talk show, "Doris Day's Best Friends", on CBN. The network canceled the show after 26 episodes, despite the worldwide publicity it received.

In October 1985, the California Supreme Court rejected Rosenthal's appeal of the multimillion-dollar judgment against him for legal malpractice, and upheld conclusions of a trial court and a Court of Appeal that Rosenthal acted improperly. In April 1986, the U.S. Supreme Court refused to review the lower court's judgment. In June 1987, Rosenthal filed a $30 million lawsuit against lawyers he claimed cheated him out of millions of dollars in real estate investments. He named Day as a co-defendant, describing her as an "unwilling, involuntary plaintiff whose consent cannot be obtained". Rosenthal claimed that millions of dollars Day lost were in real estate sold after Melcher died in 1968, in which Rosenthal asserted that the attorneys gave Day bad advice, telling her to sell, at a loss, three hotels, in Palo Alto, California, Dallas, Texas and Atlanta, Georgia and some oil leases in Kentucky and Ohio.

Rosenthal claimed he had made the investments under a long-term plan, and did not intend to sell them until they appreciated in value. Two of the hotels sold in 1970 for about $7 million, and their estimated worth in 1986 was $50 million. In July 1984, after a hearing panel of the State Bar Court, after 80 days of testimony and consideration of documentary evidence, the panel accused Rosenthal of 13 separate acts of misconduct and urged his disbarment in a 34-page unsigned opinion. The State Bar Court's review department upheld the panel's findings, which asked the justices to order Rosenthal's disbarment. He continued representing clients in federal courts until the U.S. Supreme Court ruled against him on March 21, 1988. Disbarment by the Ninth U.S. Circuit Court of Appeals followed on August 19, 1988. The Supreme Court of California, in affirming the disbarment, held that Rosenthal had engaged in transactions involving undisclosed conflicts of interest, took positions adverse to his former clients, overstated expenses, double-billed for legal fees, failed to return client files, failed to provide access to records, failed to give adequate legal advice, failed to provide clients with an opportunity to obtain independent counsel, filed fraudulent claims, gave false testimony, engaged in conduct designed to harass his clients, delayed court proceedings, obstructed justice and abused legal process. Rosenthal died August 15, 2007, at the age of 96.

Terry Melcher stated that his adoptive father's premature death saved Day from financial ruin. It remains unresolved whether Martin Melcher had himself also been duped. Day stated publicly that she believed her husband innocent of any deliberate wrongdoing, stating that he "simply trusted the wrong person". According to Day's autobiography, as told to A. E. Hotchner, the usually athletic and healthy Martin Melcher had an enlarged heart. Most of the interviews on the subject given to Hotchner (and included in Day's autobiography) paint an unflattering portrait of Melcher. Author David Kaufman asserts that one of Day's costars, actor Louis Jourdan, maintained that Day herself disliked her husband, but Day's public statements regarding Melcher appear to contradict that assertion.

Day was scheduled to present, along with Patrick Swayze and Marvin Hamlisch, the Best Original Score Oscar at the 61st Academy Awards in March 1989 but she suffered a deep leg cut and was unable to attend. She had been walking through the gardens of her hotel when she cut her leg on a sprinkler. The cut required stitches.

Day was inducted into the Ohio Women's Hall of Fame in 1981 and received the Cecil B. DeMille Award for career achievement in 1989. In 1994, Day's Greatest Hits album became another entry into the British charts. The song "Perhaps, Perhaps, Perhaps" was included in the soundtrack of the Australian film "Strictly Ballroom" and was the theme song for the British TV show "Coupling", with Mari Wilson performing the song for the title sequence.

Day has participated in interviews and celebrations of her birthday with an annual Doris Day music marathon. In July 2008, she appeared on the Southern California radio show of longtime friend, newscaster George Putnam.

Day turned down a tribute offer from the American Film Institute and from the Kennedy Center Honors because they require attendance in person. In 2004, she was awarded the Presidential Medal of Freedom by President George W. Bush for her achievements in the entertainment industry and for her work on behalf of animals. President Bush stated:

Columnist Liz Smith and film critic Rex Reed mounted vigorous campaigns to gather support for an Honorary Academy Award for Day to herald her film career and her status as the top female box-office star of all time. Day received a Grammy for Lifetime Achievement in Music in 2008, albeit again in absentia.

She received three Grammy Hall of Fame Awards, in 1998, 1999 and 2012 for her recordings of "Sentimental Journey", "Secret Love", and "Que Sera, Sera", respectively. Day was inducted into the Hit Parade Hall of Fame in 2007, and in 2010 received the first Legend Award ever presented by the Society of Singers.

Day, aged 89, released "My Heart" in the United Kingdom on September 5, 2011, her first new album in nearly two decades, since the release of "The Love Album", which, although recorded in 1967, was not released until 1994. The album is a compilation of previously unreleased recordings produced by Day's son, Terry Melcher, before his death in 2004. Tracks include the 1970s Joe Cocker hit "You Are So Beautiful", The Beach Boys' "Disney Girls" and jazz standards such as "My Buddy", which Day originally sang in her 1951 film "I'll See You in My Dreams".

After the disc was released in the US it soon climbed to No. 12 on Amazon's bestseller list, and helped raise funds for the Doris Day Animal League. Day became the oldest artist to score a UK Top 10 with an album featuring new material.

In January 2012, the Los Angeles Film Critics Association presented Day with a Lifetime Achievement Award.

In April 2014, Day made an unexpected public appearance to attend the annual Doris Day Animal Foundation benefit. The benefit raises money for her Animal Foundation.

Clint Eastwood offered Day a role in a film he was planning to direct in 2015. Although she reportedly was in talks with Eastwood, her neighbour in Carmel, about a role in the film, she eventually declined.

Since her retirement from films, Day has lived in Carmel-by-the-Sea, California. She has many pets and adopts stray animals. She granted an ABC telephone interview on her birthday in 2016, which was accompanied by photos of her life and career.

Day is a lifelong Republican, and supported George W. Bush's presidential campaign in 2000. Her only child, music producer and songwriter Terry Melcher, who had a hit in the 1960s with "Hey Little Cobra" under the name The Rip Chords, died of melanoma in 2004, about five months after Day had received the Presidential Medal of Freedom. She owns a hotel in Carmel-by-the-Sea, the Cypress Inn, which Melcher co-owned with his mother.

Day has been married four times. She was married to Al Jorden, a trombonist whom she first met in Barney Rapp's Band, from March 1941 to February 1943. Her only child, son Terrence Paul Jorden (later known as Terry Melcher), resulted from this marriage. Jorden, who was reportedly physically abusive to Day, committed suicide in 1967 by gunshot. Her second marriage was to George William Weidler, a saxophonist and the brother of actress Virginia Weidler, from March 30, 1946, to May 31, 1949. Weidler and Day met again several years later; during a brief reconciliation, he helped introduce her to Christian Science.

On April 3, 1951, her 29th birthday, she married Martin Melcher. This marriage lasted until Melcher's death in April 1968. Melcher adopted Day's son Terry, who, with the name Terry Melcher, became a successful musician and record producer. Martin Melcher produced many of Day's movies. She and Melcher were both practicing Christian Scientists, resulting in her not seeing a doctor for some time after symptoms that suggested cancer. This distressing period ended when, finally consulting a physician, and thereby finding the lump was benign, she fully recovered.

Day's fourth marriage, from April 14, 1976, until April 2, 1982, was to Barry Comden (March 30, 1935 – May 25, 2009). Comden was the "maître d'hôtel" at one of Day's favorite restaurants. Knowing of her great love of dogs, Comden endeared himself to Day by giving her a bag of meat scraps and bones on her way out of the restaurant. When this marriage unraveled, Comden complained that Day cared more for her "animal friends" than she did for him.

Day's interest in animal welfare and related issues apparently dates to her teen years. While recovering from an automobile accident, she took her dog Tiny for a walk without a leash. Tiny ran into the street and was killed by a passing car. Day later expressed guilt and loneliness about Tiny's untimely death. In 1971, she co-founded Actors and Others for Animals, and appeared in a series of newspaper advertisements denouncing the wearing of fur, alongside Mary Tyler Moore, Angie Dickinson, and Jayne Meadows. Day's friend, Cleveland Amory, wrote about these events in "Man Kind? Our Incredible War on Wildlife" (1974).

In 1978, Day founded the Doris Day Pet Foundation, now the Doris Day Animal Foundation (DDAF). A non-profit 501(c)(3) grant-giving public charity, DDAF funds other non-profit causes throughout the US that share DDAF's mission of helping animals and the people who love them. The DDAF continues to operate independently under Day's personal supervision.

To complement the Doris Day Animal Foundation, Day formed the Doris Day Animal League (DDAL) in 1987, a national non-profit citizen's lobbying organization whose mission is to reduce pain and suffering and protect animals through legislative initiatives. Day actively lobbied the United States Congress in support of legislation designed to safeguard animal welfare on a number of occasions and in 1995 she originated the annual Spay Day USA. The DDAL merged into The Humane Society of the United States (HSUS) in 2006. The HSUS now manages World Spay Day, the annual one-day spay/neuter event that Day originated.

A facility to help abused and neglected horses opened in 2011 and bears her name—the Doris Day Horse Rescue and Adoption Center, located in Murchison, Texas, on the grounds of an animal sanctuary started by her late friend, author Cleveland Amory. Day contributed $250,000 towards the founding of the center.




</doc>
<doc id="8301" url="https://en.wikipedia.org/wiki?curid=8301" title="Distillation">
Distillation

Distillation is the process of separating the components or substances from a liquid mixture by selective boiling and condensation. Distillation may result in essentially complete separation (nearly pure components), or it may be a partial separation that increases the concentration of selected components of the mixture. In either case the process exploits differences in the volatility of the mixture's components. In industrial chemistry, distillation is a unit operation of practically universal importance, but it is a physical separation process and not a chemical reaction.

Distillation has many applications. For example:

An installation for distillation, especially of distilled beverages, is a distillery. The distillation equipment is a still.

Early evidence of distillation comes from Akkadian tablets dated "circa" 1200 BC describing perfumery operations, providing textual evidence that an early primitive form of distillation was known to the Babylonians of ancient Mesopotamia. Early evidence of distillation also comes from alchemists working in Alexandria, Roman Egypt, in the 1st century. Distilled water has been known since at least c. 200, when Alexander of Aphrodisias described the process. Work on distilling other liquids continued in early Byzantine Egypt, under Zosimus of Panopolis in the 3rd century. Distillation was known in the ancient Indian subcontinent, evident from baked clay retorts and receivers found at Taxila and Charsadda in modern Pakistan, dating back to the early centuries of the Common Era. These "Gandhara stills" were only capable of producing very weak liquor, as there was no efficient means of collecting the vapors at low heat. Distillation in China could have begun during the Eastern Han dynasty (1st–2nd centuries), but the distillation of beverages began in the Jin (12th–13th centuries) and Southern Song (10th–13th centuries) dynasties according to archaeological evidence.

Clear evidence of the distillation of alcohol comes from the Arab chemist Al-Kindi, in 9th-century Iraq. The process later spread to Italy, where it was described by the School of Salerno in the 12th century. Fractional distillation was developed by Tadeo Alderotti in the 13th century. A still was found in an archaeological site in Qinglong, Hebei province, in China, dating to the 12th century. Distilled beverages were common during the Yuan dynasty (13th–14th centuries).

In 1500, German alchemist Hieronymus Braunschweig published "Liber de arte destillandi" (The Book of the Art of Distillation), the first book solely dedicated to the subject of distillation, followed in 1512 by a much expanded version. In 1651, John French published "The Art of Distillation", the first major English compendium of practice, though it has been claimed that much of it derives from Braunschweig's work. This includes diagrams with people in them showing the industrial rather than bench scale of the operation.

As alchemy evolved into the science of chemistry, vessels called retorts became used for distillations. Both alembics and retorts are forms of glassware with long necks pointing to the side at a downward angle which acted as air-cooled condensers to condense the distillate and let it drip downward for collection. Later, copper alembics were invented. Riveted joints were often kept tight by using various mixtures, for instance a dough made of rye flour. These alembics often featured a cooling system around the beak, using cold water for instance, which made the condensation of alcohol more efficient. These were called pot stills. Today, the retorts and pot stills have been largely supplanted by more efficient distillation methods in most industrial processes. However, the pot still is still widely used for the elaboration of some fine alcohols such as cognac, Scotch whisky, Irish whiskey, tequila and some vodkas. Pot stills made of various materials (wood, clay, stainless steel) are also used by bootleggers in various countries. Small pot stills are also sold for the domestic production of flower water or essential oils.

Early forms of distillation were batch processes using one vaporization and one condensation. Purity was improved by further distillation of the condensate. Greater volumes were processed by simply repeating the distillation. Chemists were reported to carry out as many as 500 to 600 distillations in order to obtain a pure compound.

In the early 19th century the basics of modern techniques including pre-heating and reflux were developed. In 1822, Anthony Perrier developed one of the first continuous stills. In 1826, Robert Stein improved that design to make his patent still. In 1830, Aeneas Coffey got a patent for improving that design. Coffey's continuous still may be regarded as the archetype of modern petrochemical units. The French engineer Armand Savalle developed his steam regulator around 1846. In 1877, Ernest Solvay was granted a U.S. Patent for a tray column for ammonia distillation and the same and subsequent years saw developments of this theme for oil and spirits.

With the emergence of chemical engineering as a discipline at the end of the 19th century, scientific rather than empirical methods could be applied. The developing petroleum industry in the early 20th century provided the impetus for the development of accurate design methods such as the McCabe–Thiele method by Ernest Thiele and the Fenske equation. The availability of powerful computers has also allowed direct computer simulation of distillation columns.

The application of distillation can roughly be divided in four groups: laboratory scale, industrial distillation, distillation of herbs for perfumery and medicinals (herbal distillate), and food processing. The latter two are distinctively different from the former two in that in the processing of beverages and herbs, the distillation is not used as a true purification method but more to transfer all volatiles from the source materials to the distillate.

The main difference between laboratory scale distillation and industrial distillation is that laboratory scale distillation is often performed batch-wise, whereas industrial distillation often occurs continuously. In batch distillation, the composition of the source material, the vapors of the distilling compounds and the distillate change during the distillation. In batch distillation, a still is charged (supplied) with a batch of feed mixture, which is then separated into its component fractions which are collected sequentially from most volatile to less volatile, with the bottoms (remaining least or non-volatile fraction) removed at the end. The still can then be recharged and the process repeated.

In continuous distillation, the source materials, vapors, and distillate are kept at a constant composition by carefully replenishing the source material and removing fractions from both vapor and liquid in the system. This results in a more detailed control of the separation process.

The boiling point of a liquid is the temperature at which the vapor pressure of the liquid equals the pressure around the liquid, enabling bubbles to form without being crushed. A special case is the normal boiling point, where the vapor pressure of the liquid equals the ambient atmospheric pressure.

It is a common misconception that in a liquid mixture at a given pressure, each component boils at the boiling point corresponding to the given pressure and the vapors of each component will collect separately and purely. This, however, does not occur even in an idealized system. Idealized models of distillation are essentially governed by Raoult's law and Dalton's law, and assume that vapor–liquid equilibria are attained.

Raoult's law states that the vapor pressure of a solution is dependent on 1) the vapor pressure of each chemical component in the solution and 2) the fraction of solution each component makes up a.k.a. the mole fraction. This law applies to ideal solutions, or solutions that have different components but whose molecular interactions are the same as or very similar to pure solutions.

Dalton's law states that the total pressure is the sum of the partial pressures of each individual component in the mixture. When a multi-component liquid is heated, the vapor pressure of each component will rise, thus causing the total vapor pressure to rise. When the total vapor pressure reaches the pressure surrounding the liquid, boiling occurs and liquid turns to gas throughout the bulk of the liquid. Note that a mixture with a given composition has one boiling point at a given pressure, when the components are mutually soluble. A mixture of constant composition does not have multiple boiling points.

An implication of one boiling point is that lighter components never cleanly "boil first". At boiling point, all volatile components boil, but for a component, its percentage in the vapor is the same as its percentage of the total vapor pressure. Lighter components have a higher partial pressure and thus are concentrated in the vapor, but heavier volatile components also have a (smaller) partial pressure and necessarily vaporize also, albeit being less concentrated in the vapor. Indeed, batch distillation and fractionation succeed by varying the composition of the mixture. In batch distillation, the batch vaporizes, which changes its composition; in fractionation, liquid higher in the fractionation column contains more lights and boils at lower temperatures. Therefore, starting from a given mixture, it appears to have a boiling range instead of a boiling "point", although this is because its composition changes: each intermediate mixture has its own, singular boiling point.

The idealized model is accurate in the case of chemically similar liquids, such as benzene and toluene. In other cases, severe deviations from Raoult's law and Dalton's law are observed, most famously in the mixture of ethanol and water. These compounds, when heated together, form an azeotrope, which is when the vapor phase and liquid phase contain the same composition. Although there are computational methods that can be used to estimate the behavior of a mixture of arbitrary components, the only way to obtain accurate vapor–liquid equilibrium data is by measurement.

It is not possible to "completely" purify a mixture of components by distillation, as this would require each component in the mixture to have a zero partial pressure. If ultra-pure products are the goal, then further chemical separation must be applied. When a binary mixture is vaporized and the other component, e.g. a salt, has zero partial pressure for practical purposes, the process is simpler.

Heating an ideal mixture of two volatile substances A and B (with A having the higher volatility, or lower boiling point) in a batch distillation setup (such as in an apparatus depicted in the opening figure) until the mixture is boiling results in a vapor above the liquid which contains a mixture of A and B. The ratio between A and B in the vapor will be different from the ratio in the liquid: the ratio in the liquid will be determined by how the original mixture was prepared, while the ratio in the vapor will be enriched in the more volatile compound, A (due to Raoult's Law, see above). The vapor goes through the condenser and is removed from the system. This in turn means that the ratio of compounds in the remaining liquid is now different from the initial ratio (i.e., more enriched in B than the starting liquid).

The result is that the ratio in the liquid mixture is changing, becoming richer in component B. This causes the boiling point of the mixture to rise, which in turn results in a rise in the temperature in the vapor, which results in a changing ratio of A : B in the gas phase (as distillation continues, there is an increasing proportion of B in the gas phase). This results in a slowly changing ratio A : B in the distillate.

If the difference in vapor pressure between the two components A and B is large (generally expressed as the difference in boiling points), the mixture in the beginning of the distillation is highly enriched in component A, and when component A has distilled off, the boiling liquid is enriched in component B.

Continuous distillation is an ongoing distillation in which a liquid mixture is continuously (without interruption) fed into the process and separated fractions are removed continuously as output streams occur over time during the operation. Continuous distillation produces a minimum of two output fractions, including at least one volatile distillate fraction, which has boiled and been separately captured as a vapor, and then condensed to a liquid. There is always a bottoms (or residue) fraction, which is the least volatile residue that has not been separately captured as a condensed vapor.

Continuous distillation differs from batch distillation in the respect that concentrations should not change over time. Continuous distillation can be run at a steady state for an arbitrary amount of time. For any source material of specific composition, the main variables that affect the purity of products in continuous distillation are the reflux ratio and the number of theoretical equilibrium stages, in practice determined by the number of trays or the height of packing. Reflux is a flow from the condenser back to the column, which generates a recycle that allows a better separation with a given number of trays. Equilibrium stages are ideal steps where compositions achieve vapor–liquid equilibrium, repeating the separation process and allowing better separation given a reflux ratio. A column with a high reflux ratio may have fewer stages, but it refluxes a large amount of liquid, giving a wide column with a large holdup. Conversely, a column with a low reflux ratio must have a large number of stages, thus requiring a taller column.

Both batch and continuous distillations can be improved by making use of a fractionating column on top of the distillation flask. The column improves separation by providing a larger surface area for the vapor and condensate to come into contact. This helps it remain at equilibrium for as long as possible. The column can even consist of small subsystems ('trays' or 'dishes') which all contain an enriched, boiling liquid mixture, all with their own vapor–liquid equilibrium.

There are differences between laboratory-scale and industrial-scale fractionating columns, but the principles are the same. Examples of laboratory-scale fractionating columns (in increasing efficiency) include

Laboratory scale distillations are almost exclusively run as batch distillations. The device used in distillation, sometimes referred to as a "still", consists at a minimum of a reboiler or "pot" in which the source material is heated, a condenser in which the heated vapour is cooled back to the liquid state, and a receiver in which the concentrated or purified liquid, called the distillate, is collected. Several laboratory scale techniques for distillation exist (see also ).

In simple distillation, the vapor is immediately channeled into a condenser. Consequently, the distillate is not pure but rather its composition is identical to the composition of the vapors at the given temperature and pressure. That concentration follows Raoult's law.

As a result, simple distillation is effective only when the liquid boiling points differ greatly (rule of thumb is 25 °C) or when separating liquids from non-volatile solids or oils. For these cases, the vapor pressures of the components are usually different enough that the distillate may be sufficiently pure for its intended purpose.

For many cases, the boiling points of the components in the mixture will be sufficiently close that Raoult's law must be taken into consideration. Therefore, fractional distillation must be used in order to separate the components by repeated vaporization-condensation cycles within a packed fractionating column. This separation, by successive distillations, is also referred to as rectification.

As the solution to be purified is heated, its vapors rise to the fractionating column. As it rises, it cools, condensing on the condenser walls and the surfaces of the packing material. Here, the condensate continues to be heated by the rising hot vapors; it vaporizes once more. However, the composition of the fresh vapors are determined once again by Raoult's law. Each vaporization-condensation cycle (called a "theoretical plate") will yield a purer solution of the more volatile component. In reality, each cycle at a given temperature does not occur at exactly the same position in the fractionating column; "theoretical plate" is thus a concept rather than an accurate description.

More theoretical plates lead to better separations. A spinning band distillation system uses a spinning band of Teflon or metal to force the rising vapors into close contact with the descending condensate, increasing the number of theoretical plates.

Like vacuum distillation, steam distillation is a method for distilling compounds which are heat-sensitive. The temperature of the steam is easier to control than the surface of a heating element, and allows a high rate of heat transfer without heating at a very high temperature. This process involves bubbling steam through a heated mixture of the raw material. By Raoult's law, some of the target compound will vaporize (in accordance with its partial pressure). The vapor mixture is cooled and condensed, usually yielding a layer of oil and a layer of water.

Steam distillation of various aromatic herbs and flowers can result in two products; an essential oil as well as a watery herbal distillate. The essential oils are often used in perfumery and aromatherapy while the watery distillates have many applications in aromatherapy, food processing and skin care.

Some compounds have very high boiling points. To boil such compounds, it is often better to lower the pressure at which such compounds are boiled instead of increasing the temperature. Once the pressure is lowered to the vapor pressure of the compound (at the given temperature), boiling and the rest of the distillation process can commence. This technique is referred to as vacuum distillation and it is commonly found in the laboratory in the form of the rotary evaporator.

This technique is also very useful for compounds which boil beyond their decomposition temperature at atmospheric pressure and which would therefore be decomposed by any attempt to boil them under atmospheric pressure.

Molecular distillation is vacuum distillation below the pressure of 0.01 torr. 0.01 torr is one order of magnitude above high vacuum, where fluids are in the free molecular flow regime, i.e. the mean free path of molecules is comparable to the size of the equipment. The gaseous phase no longer exerts significant pressure on the substance to be evaporated, and consequently, rate of evaporation no longer depends on pressure. That is, because the continuum assumptions of fluid dynamics no longer apply, mass transport is governed by molecular dynamics rather than fluid dynamics. Thus, a short path between the hot surface and the cold surface is necessary, typically by suspending a hot plate covered with a film of feed next to a cold plate with a line of sight in between. Molecular distillation is used industrially for purification of oils.

Some compounds have high boiling points as well as being air sensitive. A simple vacuum distillation system as exemplified above can be used, whereby the vacuum is replaced with an inert gas after the distillation is complete. However, this is a less satisfactory system if one desires to collect fractions under a reduced pressure. To do this a "cow" or "pig" adaptor can be added to the end of the condenser, or for better results or for very air sensitive compounds a Perkin triangle apparatus can be used.

The Perkin triangle, has means via a series of glass or Teflon taps to allows fractions to be isolated from the rest of the still, without the main body of the distillation being removed from either the vacuum or heat source, and thus can remain in a state of reflux. To do this, the sample is first isolated from the vacuum by means of the taps, the vacuum over the sample is then replaced with an inert gas (such as nitrogen or argon) and can then be stoppered and removed. A fresh collection vessel can then be added to the system, evacuated and linked back into the distillation system via the taps to collect a second fraction, and so on, until all fractions have been collected.

Short path distillation is a distillation technique that involves the distillate travelling a short distance, often only a few centimeters, and is normally done at reduced pressure. A classic example would be a distillation involving the distillate travelling from one glass bulb to another, without the need for a condenser separating the two chambers. This technique is often used for compounds which are unstable at high temperatures or to purify small amounts of compound. The advantage is that the heating temperature can be considerably lower (at reduced pressure) than the boiling point of the liquid at standard pressure, and the distillate only has to travel a short distance before condensing. A short path ensures that little compound is lost on the sides of the apparatus. The Kugelrohr is a kind of a short path distillation apparatus which often contain multiple chambers to collect distillate fractions.

Zone distillation is a distillation process in long container with partial melting of refined matter in moving liquid zone and condensation of vapor in the solid phase at condensate pulling in cold area. The process is worked in theory. When zone heater is moving from the top to the bottom of the container then solid condensate with irregular impurity distribution is forming. Then most pure part of the condensate may be extracted as product. The process may be iterated many times by moving (without turnover) the received condensate to the bottom part of the container on the place of refined matter. The irregular impurity distribution in the condensate (that is efficiency of purification) increases with number of repetitions of the process.
Zone distillation is a distillation analog of zone recrystallization. Impurity distribution in the condensate is described by known equations of zone recrystallization with various numbers of iteration of process – with replacement distribution efficient k of crystallization on separation factor α of distillation.


The unit process of evaporation may also be called "distillation":

Other uses:

Interactions between the components of the solution create properties unique to the solution, as most processes entail nonideal mixtures, where Raoult's law does not hold. Such interactions can result in a constant-boiling azeotrope which behaves as if it were a pure compound (i.e., boils at a single temperature instead of a range). At an azeotrope, the solution contains the given component in the same proportion as the vapor, so that evaporation does not change the purity, and distillation does not effect separation. For example, ethyl alcohol and water form an azeotrope of 95.6% at 78.1 °C.

If the azeotrope is not considered sufficiently pure for use, there exist some techniques to break the azeotrope to give a pure distillate. This set of techniques are known as azeotropic distillation. Some techniques achieve this by "jumping" over the azeotropic composition (by adding another component to create a new azeotrope, or by varying the pressure). Others work by chemically or physically removing or sequestering the impurity. For example, to purify ethanol beyond 95%, a drying agent (or desiccant, such as potassium carbonate) can be added to convert the soluble water into insoluble water of crystallization. Molecular sieves are often used for this purpose as well.

Immiscible liquids, such as water and toluene, easily form azeotropes. Commonly, these azeotropes are referred to as a low boiling azeotrope because the boiling point of the azeotrope is lower than the boiling point of either pure component. The temperature and composition of the azeotrope is easily predicted from the vapor pressure of the pure components, without use of Raoult's law. The azeotrope is easily broken in a distillation set-up by using a liquid–liquid separator (a decanter) to separate the two liquid layers that are condensed overhead. Only one of the two liquid layers is refluxed to the distillation set-up.

High boiling azeotropes, such as a 20 weight percent mixture of hydrochloric acid in water, also exist. As implied by the name, the boiling point of the azeotrope is greater than the boiling point of either pure component.

To break azeotropic distillations and cross distillation boundaries, such as in the DeRosier Problem, it is necessary to increase the composition of the light key in the distillate.

The boiling points of components in an azeotrope overlap to form a band. By exposing an azeotrope to a vacuum or positive pressure, it's possible to bias the boiling point of one component away from the other by exploiting the differing vapour pressure curves of each; the curves may overlap at the azeotropic point, but are unlikely to be remain identical further along the pressure axis either side of the azeotropic point. When the bias is great enough, the two boiling points no longer overlap and so the azeotropic band disappears.

This method can remove the need to add other chemicals to a distillation, but it has two potential drawbacks.

Under negative pressure, power for a vacuum source is needed and the reduced boiling points of the distillates requires that the condenser be run cooler to prevent distillate vapours being lost to the vacuum source. Increased cooling demands will often require additional energy and possibly new equipment or a change of coolant.

Alternatively, if positive pressures are required, standard glassware can not be used, energy must be used for pressurization and there is a higher chance of side reactions occurring in the distillation, such as decomposition, due to the higher temperatures required to effect boiling.

A unidirectional distillation will rely on a pressure change in one direction, either positive or negative.

Pressure-swing distillation is essentially the same as the unidirectional distillation used to break azeotropic mixtures, but here both positive and negative pressures may be employed.

This improves the selectivity of the distillation and allows a chemist to optimize distillation by avoiding extremes of pressure and temperature that waste energy. This is particularly important in commercial applications.

One example of the application of pressure-swing distillation is during the industrial purification of ethyl acetate after its catalytic synthesis from ethanol.

Large scale industrial distillation applications include both batch and continuous fractional, vacuum, azeotropic, extractive, and steam distillation. The most widely used industrial applications of continuous, steady-state fractional distillation are in petroleum refineries, petrochemical and chemical plants and natural gas processing plants.

To control and optimize such industrial distillation, a standardized laboratory method, ASTM D86, is established. This test method extends to the atmospheric distillation of petroleum products using a laboratory batch distillation unit to quantitatively determine the boiling range characteristics of petroleum products.

Industrial distillation is typically performed in large, vertical cylindrical columns known as distillation towers or distillation columns with diameters ranging from about 65 centimeters to 16 meters and heights ranging from about 6 meters to 90 meters or more. When the process feed has a diverse composition, as in distilling crude oil, liquid outlets at intervals up the column allow for the withdrawal of different "fractions" or products having different boiling points or boiling ranges. The "lightest" products (those with the lowest boiling point) exit from the top of the columns and the "heaviest" products (those with the highest boiling point) exit from the bottom of the column and are often called the bottoms.
Industrial towers use reflux to achieve a more complete separation of products. Reflux refers to the portion of the condensed overhead liquid product from a distillation or fractionation tower that is returned to the upper part of the tower as shown in the schematic diagram of a typical, large-scale industrial distillation tower. Inside the tower, the downflowing reflux liquid provides cooling and condensation of the upflowing vapors thereby increasing the efficiency of the distillation tower. The more reflux that is provided for a given number of theoretical plates, the better the tower's separation of lower boiling materials from higher boiling materials. Alternatively, the more reflux that is provided for a given desired separation, the fewer the number of theoretical plates required. Chemical engineers must choose what combination of reflux rate and number of plates is both economically and physically feasible for the products purified in the distillation column.

Such industrial fractionating towers are also used in cryogenic air separation, producing liquid oxygen, liquid nitrogen, and high purity argon. Distillation of chlorosilanes also enables the production of high-purity silicon for use as a semiconductor.
Design and operation of a distillation tower depends on the feed and desired products. Given a simple, binary component feed, analytical methods such as the McCabe–Thiele method or the Fenske equation can be used. For a multi-component feed, simulation models are used both for design and operation. Moreover, the efficiencies of the vapor–liquid contact devices (referred to as "plates" or "trays") used in distillation towers are typically lower than that of a theoretical 100% efficient equilibrium stage. Hence, a distillation tower needs more trays than the number of theoretical vapor–liquid equilibrium stages. A variety of models have been postulated to estimate tray efficiencies.

In modern industrial uses, a packing material is used in the column instead of trays when low pressure drops across the column are required. Other factors that favor packing are: vacuum systems, smaller diameter columns, corrosive systems, systems prone to foaming, systems requiring low liquid holdup, and batch distillation. Conversely, factors that favor plate columns are: presence of solids in feed, high liquid rates, large column diameters, complex columns, columns with wide feed composition variation, columns with a chemical reaction, absorption columns, columns limited by foundation weight tolerance, low liquid rate, large turn-down ratio and those processes subject to process surges.

This packing material can either be random dumped packing (1–3" wide) such as Raschig rings or structured sheet metal. Liquids tend to wet the surface of the packing and the vapors pass across this wetted surface, where mass transfer takes place. Unlike conventional tray distillation in which every tray represents a separate point of vapor–liquid equilibrium, the vapor–liquid equilibrium curve in a packed column is continuous. However, when modeling packed columns, it is useful to compute a number of "theoretical stages" to denote the separation efficiency of the packed column with respect to more traditional trays. Differently shaped packings have different surface areas and void space between packings. Both of these factors affect packing performance.

Another factor in addition to the packing shape and surface area that affects the performance of random or structured packing is the liquid and vapor distribution entering the packed bed. The number of theoretical stages required to make a given separation is calculated using a specific vapor to liquid ratio. If the liquid and vapor are not evenly distributed across the superficial tower area as it enters the packed bed, the liquid to vapor ratio will not be correct in the packed bed and the required separation will not be achieved. The packing will appear to not be working properly. The height equivalent to a theoretical plate (HETP) will be greater than expected. The problem is not the packing itself but the mal-distribution of the fluids entering the packed bed. Liquid mal-distribution is more frequently the problem than vapor. The design of the liquid distributors used to introduce the feed and reflux to a packed bed is critical to making the packing perform to it maximum efficiency. Methods of evaluating the effectiveness of a liquid distributor to evenly distribute the liquid entering a packed bed can be found in references. Considerable work has been done on this topic by Fractionation Research, Inc. (commonly known as FRI).

The goal of multi-effect distillation is to increase the energy efficiency of the process, for use in desalination, or in some cases one stage in the production of ultrapure water. The number of effects is inversely proportional to the kW·h/m of water recovered figure, and refers to the volume of water recovered per unit of energy compared with single-effect distillation. One effect is roughly 636 kW·h/m.

There are many other types of multi-effect distillation processes, including one referred to as simply multi-effect distillation (MED), in which multiple chambers, with intervening heat exchangers, are employed.

Carbohydrate-containing plant materials are allowed to ferment, producing a dilute solution of ethanol in the process. Spirits such as whiskey and rum are prepared by distilling these dilute solutions of ethanol. Components other than ethanol, including water, esters, and other alcohols, are collected in the condensate, which account for the flavor of the beverage. Some of these beverages are then stored in barrels or other containers to acquire more flavor compounds and characteristic flavors.





</doc>
<doc id="8302" url="https://en.wikipedia.org/wiki?curid=8302" title="David Hilbert">
David Hilbert

David Hilbert (; ; 23 January 1862 – 14 February 1943) was a German mathematician. He is recognized as one of the most influential and universal mathematicians of the 19th and early 20th centuries. Hilbert discovered and developed a broad range of fundamental ideas in many areas, including invariant theory and the axiomatization of geometry. He also formulated the theory of Hilbert spaces, one of the foundations of functional analysis.

Hilbert adopted and warmly defended Georg Cantor's set theory and transfinite numbers. A famous example of his leadership in mathematics is his 1900 presentation of a collection of problems that set the course for much of the mathematical research of the 20th century.

Hilbert and his students contributed significantly to establishing rigor and developed important tools used in modern mathematical physics. Hilbert is known as one of the founders of proof theory and mathematical logic, as well as for being among the first to distinguish between mathematics and metamathematics.

Hilbert, the first of two children of Otto and Maria Therese (Erdtmann) Hilbert, was born in the Province of Prussia, Kingdom of Prussia, either in Königsberg (according to Hilbert's own statement) or in Wehlau (known since 1946 as Znamensk) near Königsberg where his father worked at the time of his birth.

In late 1872, Hilbert entered the Friedrichskolleg Gymnasium ("Collegium fridericianum", the same school that Immanuel Kant had attended 140 years before); but, after an unhappy period, he transferred to (late 1879) and graduated from (early 1880) the more science-oriented Wilhelm Gymnasium. Upon graduation, in autumn 1880, Hilbert enrolled at the University of Königsberg, the "Albertina". In early 1882, Hermann Minkowski (two years younger than Hilbert and also a native of Königsberg but had gone to Berlin for three semesters), returned to Königsberg and entered the university. Hilbert developed a lifelong friendship with the shy, gifted Minkowski.

In 1884, Adolf Hurwitz arrived from Göttingen as an Extraordinarius (i.e., an associate professor). An intense and fruitful scientific exchange among the three began, and Minkowski and Hilbert especially would exercise a reciprocal influence over each other at various times in their scientific careers. Hilbert obtained his doctorate in 1885, with a dissertation, written under Ferdinand von Lindemann, titled "Über invariante Eigenschaften spezieller binärer Formen, insbesondere der Kugelfunktionen" ("On the invariant properties of special binary forms, in particular the spherical harmonic functions").

Hilbert remained at the University of Königsberg as a "Privatdozent" (senior lecturer) from 1886 to 1895. In 1895, as a result of intervention on his behalf by Felix Klein, he obtained the position of Professor of Mathematics at the University of Göttingen. During the Klein and Hilbert years, Göttingen became the preeminent institution in the mathematical world. He remained there for the rest of his life.

Among Hilbert's students were Hermann Weyl, chess champion Emanuel Lasker, Ernst Zermelo, and Carl Gustav Hempel. John von Neumann was his assistant. At the University of Göttingen, Hilbert was surrounded by a social circle of some of the most important mathematicians of the 20th century, such as Emmy Noether and Alonzo Church.

Among his 69 Ph.D. students in Göttingen were many who later became famous mathematicians, including (with date of thesis): Otto Blumenthal (1898), Felix Bernstein (1901), Hermann Weyl (1908), Richard Courant (1910), Erich Hecke (1910), Hugo Steinhaus (1911), and Wilhelm Ackermann (1925). Between 1902 and 1939 Hilbert was editor of the "Mathematische Annalen", the leading mathematical journal of the time.

Around 1925, Hilbert developed pernicious anemia, a then-untreatable vitamin deficiency whose primary symptom is exhaustion; his assistant Eugene Wigner described him as subject to "enormous fatigue" and how he "seemed quite old", and that even after eventually being diagnosed and treated, he "was hardly a scientist after 1925, and certainly not a Hilbert."

Hilbert lived to see the Nazis purge many of the prominent faculty members at University of Göttingen in 1933. Those forced out included Hermann Weyl (who had taken Hilbert's chair when he retired in 1930), Emmy Noether and Edmund Landau. One who had to leave Germany, Paul Bernays, had collaborated with Hilbert in mathematical logic, and co-authored with him the important book "Grundlagen der Mathematik" (which eventually appeared in two volumes, in 1934 and 1939). This was a sequel to the Hilbert-Ackermann book "Principles of Mathematical Logic" from 1928. Hermann Weyl's successor was Helmut Hasse.

About a year later, Hilbert attended a banquet and was seated next to the new Minister of Education, Bernhard Rust. Rust asked whether "the "Mathematical Institute" really suffered so much because of the departure of the Jews". Hilbert replied,
"Suffered? It doesn't exist any longer, does it!"
By the time Hilbert died in 1943, the Nazis had nearly completely restaffed the university, as many of the former faculty had either been Jewish or married to Jews. Hilbert's funeral was attended by fewer than a dozen people, only two of whom were fellow academics, among them Arnold Sommerfeld, a theoretical physicist and also a native of Königsberg. News of his death only became known to the wider world six months after he had died.

The epitaph on his tombstone in Göttingen consists of the famous lines he spoke at the conclusion of his retirement address to the Society of German Scientists and Physicians on 8 September 1930. The words were given in response to the Latin maxim: "Ignoramus et ignorabimus" or "We do not know, we shall not know":

In English:

The day before Hilbert pronounced these phrases at the 1930 annual meeting of the Society of German Scientists and Physicians, Kurt Gödel—in a round table discussion during the Conference on Epistemology held jointly with the Society meetings—tentatively announced the first expression of his incompleteness theorem. Gödel's incompleteness theorems show that even elementary axiomatic systems such as Peano arithmetic are either self-contradicting or contain logical propositions that are impossible to prove or disprove.

In 1892, Hilbert married Käthe Jerosch (1864–1945), "the daughter of a Königsberg merchant, an outspoken young lady with an independence of mind that matched his own". While at Königsberg they had their one child, Franz Hilbert (1893–1969).

Hilbert's son Franz suffered throughout his life from an undiagnosed mental illness. His inferior intellect was a terrible disappointment to his father and this misfortune was a matter of distress to the mathematicians and students at Göttingen.

Hilbert considered the mathematician Hermann Minkowski to be his "best and truest friend".

Hilbert was baptized and raised a Calvinist in the Prussian Evangelical Church. He later on left the Church and became an agnostic. He also argued that mathematical truth was independent of the existence of God or other "a priori" assumptions.

Hilbert's first work on invariant functions led him to the demonstration in 1888 of his famous "finiteness theorem". Twenty years earlier, Paul Gordan had demonstrated the theorem of the finiteness of generators for binary forms using a complex computational approach. Attempts to generalize his method to functions with more than two variables failed because of the enormous difficulty of the calculations involved. In order to solve what had become known in some circles as "Gordan's Problem", Hilbert realized that it was necessary to take a completely different path. As a result, he demonstrated "Hilbert's basis theorem", showing the existence of a finite set of generators, for the invariants of quantics in any number of variables, but in an abstract form. That is, while demonstrating the existence of such a set, it was not a constructive proof — it did not display "an object" — but rather, it was an existence proof and relied on use of the law of excluded middle in an infinite extension.

Hilbert sent his results to the "Mathematische Annalen". Gordan, the house expert on the theory of invariants for the "Mathematische Annalen", could not appreciate the revolutionary nature of Hilbert's theorem and rejected the article, criticizing the exposition because it was insufficiently comprehensive. His comment was:

Klein, on the other hand, recognized the importance of the work, and guaranteed that it would be published without any alterations. Encouraged by Klein, Hilbert extended his method in a second article, providing estimations on the maximum degree of the minimum set of generators, and he sent it once more to the "Annalen". After having read the manuscript, Klein wrote to him, saying:

Later, after the usefulness of Hilbert's method was universally recognized, Gordan himself would say:

For all his successes, the nature of his proof stirred up more trouble than Hilbert could have imagined at the time. Although Kronecker had conceded, Hilbert would later respond to others' similar criticisms that "many different constructions are subsumed under one fundamental idea" — in other words (to quote Reid): "Through a proof of existence, Hilbert had been able to obtain a construction"; "the proof" (i.e. the symbols on the page) "was" "the object". Not all were convinced. While Kronecker would die soon afterwards, his constructivist philosophy would continue with the young Brouwer and his developing intuitionist "school", much to Hilbert's torment in his later years. Indeed, Hilbert would lose his "gifted pupil" Weyl to intuitionism — "Hilbert was disturbed by his former student's fascination with the ideas of Brouwer, which aroused in Hilbert the memory of Kronecker". Brouwer the intuitionist in particular opposed the use of the Law of Excluded Middle over infinite sets (as Hilbert had used it). Hilbert would respond:

The text "Grundlagen der Geometrie" (tr.: "Foundations of Geometry") published by Hilbert in 1899 proposes a formal set, called Hilbert's axioms, substituting for the traditional axioms of Euclid. They avoid weaknesses identified in those of Euclid, whose works at the time were still used textbook-fashion. It is difficult to specify the axioms used by Hilbert without referring to the publication history of the "Grundlagen" since Hilbert changed and modified them several times. The original monograph was quickly followed by a French translation, in which Hilbert added V.2, the Completeness Axiom. An English translation, authorized by Hilbert, was made by E.J. Townsend and copyrighted in 1902. This translation incorporated the changes made in the French translation and so is considered to be a translation of the 2nd edition. Hilbert continued to make changes in the text and several editions appeared in German. The 7th edition was the last to appear in Hilbert's lifetime. New editions followed the 7th, but the main text was essentially not revised.
Hilbert's approach signaled the shift to the modern axiomatic method. In this, Hilbert was anticipated by Moritz Pasch's work from 1882. Axioms are not taken as self-evident truths. Geometry may treat "things", about which we have powerful intuitions, but it is not necessary to assign any explicit meaning to the undefined concepts. The elements, such as point, line, plane, and others, could be substituted, as Hilbert is reported to have said to Schoenflies and Kötter, by tables, chairs, glasses of beer and other such objects. It is their defined relationships that are discussed.

Hilbert first enumerates the undefined concepts: point, line, plane, lying on (a relation between points and lines, points and planes, and lines and planes), betweenness, congruence of pairs of points (line segments), and congruence of angles. The axioms unify both the plane geometry and solid geometry of Euclid in a single system.

Hilbert put forth a most influential list of 23 unsolved problems at the International Congress of Mathematicians in Paris in 1900. This is generally reckoned as the most successful and deeply considered compilation of open problems ever to be produced by an individual mathematician.

After re-working the foundations of classical geometry, Hilbert could have extrapolated to the rest of mathematics. His approach differed, however, from the later 'foundationalist' Russell-Whitehead or 'encyclopedist' Nicolas Bourbaki, and from his contemporary Giuseppe Peano. The mathematical community as a whole could enlist in problems, which he had identified as crucial aspects of the areas of mathematics he took to be key.

The problem set was launched as a talk "The Problems of Mathematics" presented during the course of the Second International Congress of Mathematicians held in Paris. The introduction of the speech that Hilbert gave said:

He presented fewer than half the problems at the Congress, which were published in the acts of the Congress. In a subsequent publication, he extended the panorama, and arrived at the formulation of the now-canonical 23 Problems of Hilbert. See also Hilbert's twenty-fourth problem. The full text is important, since the exegesis of the questions still can be a matter of inevitable debate, whenever it is asked how many have been solved.

Some of these were solved within a short time. Others have been discussed throughout the 20th century, with a few now taken to be unsuitably open-ended to come to closure. Some even continue to this day to remain a challenge for mathematicians.

In an account that had become standard by the mid-century, Hilbert's problem set was also a kind of manifesto, that opened the way for the development of the formalist school, one of three major schools of mathematics of the 20th century. According to the formalist, mathematics is manipulation of symbols according to agreed upon formal rules. It is therefore an autonomous activity of thought. There is, however, room to doubt whether Hilbert's own views were simplistically formalist in this sense.

In 1920 he proposed explicitly a research project (in "metamathematics", as it was then termed) that became known as Hilbert's program. He wanted mathematics to be formulated on a solid and complete logical foundation. He believed that in principle this could be done, by showing that:


He seems to have had both technical and philosophical reasons for formulating this proposal. It affirmed his dislike of what had become known as the "ignorabimus", still an active issue in his time in German thought, and traced back in that formulation to Emil du Bois-Reymond.

This program is still recognizable in the most popular philosophy of mathematics, where it is usually called "formalism". For example, the Bourbaki group adopted a watered-down and selective version of it as adequate to the requirements of their twin projects of (a) writing encyclopedic foundational works, and (b) supporting the axiomatic method as a research tool. This approach has been successful and influential in relation with Hilbert's work in algebra and functional analysis, but has failed to engage in the same way with his interests in physics and logic.

Hilbert wrote in 1919:

Hilbert published his views on the foundations of mathematics in the 2-volume work Grundlagen der Mathematik.

Hilbert and the mathematicians who worked with him in his enterprise were committed to the project. His attempt to support axiomatized mathematics with definitive principles, which could banish theoretical uncertainties, ended in failure.

Gödel demonstrated that any non-contradictory formal system, which was comprehensive enough to include at least arithmetic, cannot demonstrate its completeness by way of its own axioms. In 1931 his incompleteness theorem showed that Hilbert's grand plan was impossible as stated. The second point cannot in any reasonable way be combined with the first point, as long as the axiom system is genuinely finitary.

Nevertheless, the subsequent achievements of proof theory at the very least "clarified" consistency as it relates to theories of central concern to mathematicians. Hilbert's work had started logic on this course of clarification; the need to understand Gödel's work then led to the development of recursion theory and then mathematical logic as an autonomous discipline in the 1930s. The basis for later theoretical computer science, in the work of Alonzo Church and Alan Turing, also grew directly out of this 'debate'.

Around 1909, Hilbert dedicated himself to the study of differential and integral equations; his work had direct consequences for important parts of modern functional analysis. In order to carry out these studies, Hilbert introduced the concept of an infinite dimensional Euclidean space, later called Hilbert space. His work in this part of analysis provided the basis for important contributions to the mathematics of physics in the next two decades, though from an unanticipated direction.
Later on, Stefan Banach amplified the concept, defining Banach spaces. Hilbert spaces are an important class of objects in the area of functional analysis, particularly of the spectral theory of self-adjoint linear operators, that grew up around it during the 20th century.

Until 1912, Hilbert was almost exclusively a "pure" mathematician. When planning a visit from Bonn, where he was immersed in studying physics, his fellow mathematician and friend Hermann Minkowski joked he had to spend 10 days in quarantine before being able to visit Hilbert. In fact, Minkowski seems responsible for most of Hilbert's physics investigations prior to 1912, including their joint seminar in the subject in 1905.

In 1912, three years after his friend's death, Hilbert turned his focus to the subject almost exclusively. He arranged to have a "physics tutor" for himself. He started studying kinetic gas theory and moved on to elementary radiation theory and the molecular theory of matter. Even after the war started in 1914, he continued seminars and classes where the works of Albert Einstein and others were followed closely.

By 1907 Einstein had framed the fundamentals of the theory of gravity, but then struggled for nearly 8 years with a confounding problem of putting the theory into final form. By early summer 1915, Hilbert's interest in physics had focused on general relativity, and he invited Einstein to Göttingen to deliver a week of lectures on the subject. Einstein received an enthusiastic reception at Göttingen. Over the summer Einstein learned that Hilbert was also working on the field equations and redoubled his own efforts. During November 1915 Einstein published several papers culminating in "The Field Equations of Gravitation" (see Einstein field equations). Nearly simultaneously David Hilbert published "The Foundations of Physics", an axiomatic derivation of the field equations (see Einstein–Hilbert action). Hilbert fully credited Einstein as the originator of the theory, and no public priority dispute concerning the field equations ever arose between the two men during their lives. See more at priority.

Additionally, Hilbert's work anticipated and assisted several advances in the mathematical formulation of quantum mechanics. His work was a key aspect of Hermann Weyl and John von Neumann's work on the mathematical equivalence of Werner Heisenberg's matrix mechanics and Erwin Schrödinger's wave equation and his namesake Hilbert space plays an important part in quantum theory. In 1926 von Neumann showed that if atomic states were understood as vectors in Hilbert space, then they would correspond with both Schrödinger's wave function theory and Heisenberg's matrices.

Throughout this immersion in physics, Hilbert worked on putting rigor into the mathematics of physics. While highly dependent on higher mathematics, physicists tended to be "sloppy" with it. To a "pure" mathematician like Hilbert, this was both "ugly" and difficult to understand. As he began to understand physics and how physicists were using mathematics, he developed a coherent mathematical theory for what he found, most importantly in the area of integral equations. When his colleague Richard Courant wrote the now classic "Methoden der mathematischen Physik" (Methods of Mathematical Physics) including some of Hilbert's ideas, he added Hilbert's name as author even though Hilbert had not directly contributed to the writing. Hilbert said "Physics is too hard for physicists", implying that the necessary mathematics was generally beyond them; the Courant-Hilbert book made it easier for them.

Hilbert unified the field of algebraic number theory with his 1897 treatise "Zahlbericht" (literally "report on numbers"). He also resolved a significant number-theory problem formulated by Waring in 1770. As with the finiteness theorem, he used an existence proof that shows there must be solutions for the problem rather than providing a mechanism to produce the answers. He then had little more to publish on the subject; but the emergence of Hilbert modular forms in the dissertation of a student means his name is further attached to a major area.

He made a series of conjectures on class field theory. The concepts were highly influential, and his own contribution lives on in the names of the Hilbert class field and of the Hilbert symbol of local class field theory. Results were mostly proved by 1930, after work by Teiji Takagi.

Hilbert did not work in the central areas of analytic number theory, but his name has become known for the Hilbert–Pólya conjecture, for reasons that are anecdotal.

His collected works ("Gesammelte Abhandlungen") have been published several times. The original versions of his papers contained "many technical errors of varying degree"; when the collection was first published, the errors were corrected and it was found that this could be done without major changes in the statements of the theorems, with one exception—a claimed proof of the continuum hypothesis. The errors were nonetheless so numerous and significant that it took Olga Taussky-Todd three years to make the corrections.





</doc>
<doc id="8303" url="https://en.wikipedia.org/wiki?curid=8303" title="Down syndrome">
Down syndrome

Down syndrome (DS or DNS), also known as trisomy 21, is a genetic disorder caused by the presence of all or part of a third copy of chromosome 21. It is typically associated with physical growth delays, characteristic facial features, and mild to moderate intellectual disability. The average IQ of a young adult with Down syndrome is 50, equivalent to the mental ability of an 8 or 9-year-old child, but this can vary widely.
The parents of the affected individual are typically genetically normal. The extra chromosome occurs by chance. The probability increases from less than 0.1% in 20-year-old mothers to 3% in those age 45. There is no known behavioral activity or environmental factor that changes the probability. Down syndrome can be identified during pregnancy by prenatal screening followed by diagnostic testing or after birth by direct observation and genetic testing. Since the introduction of screening, pregnancies with the diagnosis are often terminated. Regular screening for health problems common in Down syndrome is recommended throughout the person's life.
There is no cure for Down syndrome. Education and proper care have been shown to improve quality of life. Some children with Down syndrome are educated in typical school classes, while others require more specialized education. Some individuals with Down syndrome graduate from high school, and a few attend post-secondary education. In adulthood, about 20% in the United States do paid work in some capacity, with many requiring a sheltered work environment. Support in financial and legal matters is often needed. Life expectancy is around 50 to 60 years in the developed world with proper health care.
Down syndrome is one of the most common chromosome abnormalities in humans. It occurs in about one per 1,000 babies born each year. In 2015, Down syndrome was present in 5.4 million individuals and resulted in 27,000 deaths, down from 43,000 deaths in 1990. It is named after John Langdon Down, a British doctor who fully described the syndrome in 1866. Some aspects of the condition were described earlier by Jean-Étienne Dominique Esquirol in 1838 and Édouard Séguin in 1844. In 1959, the genetic cause of Down syndrome, an extra copy of chromosome 21, was discovered.
Those with Down syndrome nearly always have physical and intellectual disabilities. As adults, their mental abilities are typically similar to those of an 8- or 9-year-old. They also typically have poor immune function and generally reach developmental milestones at a later age. They have an increased risk of a number of other health problems, including congenital heart defect, epilepsy, leukemia, thyroid diseases, and mental disorders.

People with Down syndrome may have some or all of these physical characteristics: a small chin, slanted eyes, poor muscle tone, a flat nasal bridge, a single crease of the palm, and a protruding tongue due to a small mouth and relatively large tongue. These airway changes lead to obstructive sleep apnea in around half of those with Down syndrome. Other common features include: a flat and wide face, a short neck, excessive joint flexibility, extra space between big toe and second toe, abnormal patterns on the fingertips and short fingers. Instability of the atlantoaxial joint occurs in about 20% and may lead to spinal cord injury in 1–2%. Hip dislocations may occur without trauma in up to a third of people with Down syndrome.

Growth in height is slower, resulting in adults who tend to have short stature—the average height for men is 154 cm (5 ft 1 in) and for women is 142 cm (4 ft 8 in). Individuals with Down syndrome are at increased risk for obesity as they age. Growth charts have been developed specifically for children with Down syndrome.

Most individuals with Down syndrome have mild (IQ: 50–69) or moderate (IQ: 35–50) intellectual disability with some cases having severe (IQ: 20–35) difficulties. Those with mosaic Down syndrome typically have IQ scores 10–30 points higher. As they age, people with Down syndrome typically perform less well than their same-age peers. Some after 30 years of age may lose their ability to speak. This syndrome causes about a third of cases of intellectual disability. Many developmental milestones are delayed with the ability to crawl typically occurring around 8 months rather than 5 months and the ability to walk independently typically occurring around 21 months rather than 14 months.

Commonly, individuals with Down syndrome have better language understanding than ability to speak. Between 10 and 45% have either a stutter or rapid and irregular speech, making it difficult to understand them. They typically do fairly well with social skills. Behavior problems are not generally as great an issue as in other syndromes associated with intellectual disability. In children with Down syndrome, mental illness occurs in nearly 30% with autism occurring in 5–10%. People with Down syndrome experience a wide range of emotions. While people with Down syndrome are generally happy, symptoms of depression and anxiety may develop in early adulthood.

Children and adults with Down syndrome are at increased risk of epileptic seizures, which occur in 5–10% of children and up to 50% of adults. This includes an increased risk of a specific type of seizure called infantile spasms. Many (15%) who live 40 years or longer develop Alzheimer disease. In those who reach 60 years of age, 50–70% have the disease.

Hearing and vision disorders occur in more than half of people with Down syndrome. 

Vision problems occur in 38 to 80%. Between 20 and 50% have strabismus, in which the two eyes do not move together. Cataracts (cloudiness of the lens of the eye) occur in 15%, and may be present at birth. Keratoconus (a thin, cone-shaped cornea) and glaucoma (increased eye pressure) are also more common, as are refractive errors requiring glasses or contacts. Brushfield spots (small white or grayish/brown spots on the outer part of the iris) are present in 38 to 85% of individuals.
Hearing problems are found in 50–90% of children with Down syndrome. This is often the result of otitis media with effusion which occurs in 50–70% and chronic ear infections which occur in 40 to 60%. Ear infections often begin in the first year of life and are partly due to poor eustachian tube function. Excessive ear wax can also cause hearing loss due to obstruction of the outer ear canal. Even a mild degree of hearing loss can have negative consequences for speech, language understanding, and academics. Additionally, it is important to rule out hearing loss as a factor in social and cognitive deterioration. Age-related hearing loss of the sensorineural type occurs at a much earlier age and affects 10–70% of people with Down syndrome.

The rate of congenital heart disease in newborns with Down syndrome is around 40%. Of those with heart disease, about 80% have an atrioventricular septal defect or ventricular septal defect with the former being more common. Mitral valve problems become common as people age, even in those without heart problems at birth. Other problems that may occur include tetralogy of Fallot and patent ductus arteriosus. People with Down syndrome have a lower risk of hardening of the arteries.

Although the overall risk of cancer in DS is not changed, the risk of testicular cancer and certain blood cancers, including acute lymphoblastic leukemia (ALL) and acute megakaryoblastic leukemia (AMKL) is increased while the risk of other non blood cancers are decreased. People with DS are believed to have an increased risk of developing cancers derived from germ cells whether these cancers are blood or non-blood related.

Cancers of the blood are 10 to 15 times more common in children with Down syndrome. In particular, acute lymphoblastic leukemia is 20 times more common and the megakaryoblastic form of acute myeloid leukemia (acute megakaryoblastic leukemia), is 500 times more common. Acute megakaryoblastic leukemia (AMKL) is a leukemia of megakaryoblasts, the precursors cells to megakaryocytes which form blood platelets. Acute lymphoblastic leukemia in Down syndrome accounts for 1-3% of all childhood cases of ALL. It occurs most often in those older than 9 years or having a white blood cell count greater than 50,000 per microliter and is rare in those younger than 1 year old. ALL in DS tends to have poorer outcomes than other cases of ALL in people without DS.

In Down syndrome, AMKL is typically preceded by transient myeloproliferative disease (TMD), a disorder of blood cell production in which non-cancerous megakaryoblasts with a mutation in the "GATA1" gene rapidly divide during the later period of pregnancy. The condition affects 3–10% of babies with Down. Well it often spontaneously resolves within 3 months of birth, it can cause serious blood, liver, or other complications. In about 10% of cases, TMD progresses to AMKL during the 3 months to 5 years following its resolution.

People with DS have a lower risk of all major solid cancers including those of lung, breast, cervix, with the lowest relative rates occurring in those aged 50 years or older. This low risk is thought due to an increase in the expression of tumor suppressor genes present on chromosome 21. One exception is testicular germ cell cancer which occurs at a higher rate in DS.

Problems of the thyroid gland occur in 20–50% of individuals with Down syndrome. Low thyroid is the most common form, occurring in almost half of all individuals. Thyroid problems can be due to a poorly or nonfunctioning thyroid at birth (known as congenital hypothyroidism) which occurs in 1% or can develop later due to an attack on the thyroid by the immune system resulting in Graves' disease or autoimmune hypothyroidism. Type 1 diabetes mellitus is also more common.

Constipation occurs in nearly half of people with Down syndrome and may result in changes in behavior. One potential cause is Hirschsprung's disease, occurring in 2–15%, which is due to a lack of nerve cells controlling the colon. Other frequent congenital problems include duodenal atresia, pyloric stenosis, Meckel diverticulum, and imperforate anus. Celiac disease affects about 7–20% and gastroesophageal reflux disease is also more common.

Individuals with Down syndrome tend to be more susceptible to gingivitis as well as early, severe periodontal disease, necrotising ulcerative gingivitis, and early tooth loss, especially in the lower front teeth. While plaque and poor oral hygiene are contributing factors, the severity of these periodontal diseases cannot be explained solely by external factors. Research suggests that the severity is likely a result of a weakened immune system. The weakened immune system also contributes to increased incidence of yeast infections in the mouth (from Candida albicans).

Individuals with Down syndrome also tend to have a more alkaline saliva resulting in a greater resistance to tooth decay, despite decreased quantities of saliva, less effective oral hygiene habits, and higher plaque indexes.

Higher rates of tooth wear and bruxism are also common. Other common oral manifestations of Down syndrome include enlarged hypotonic tongue, crusted and hypotonic lips, mouth breathing, narrow palate with crowded teeth, class III malocclusion with an underdeveloped maxilla and posterior crossbite, delayed exfoliation of baby teeth and delayed eruption of adult teeth, shorter roots on teeth, and often missing and malformed (usually smaller) teeth. Less common manifestations include cleft lip and palate and enamel hypocalcification (20% prevalence).

Males with Down syndrome usually do not father children, while females have lower rates of fertility relative to those who are unaffected. Fertility is estimated to be present in 30–50% of females. Menopause typically occurs at an earlier age. The poor fertility in males is thought to be due to problems with sperm development; however, it may also be related to not being sexually active. As of 2006, three instances of males with Down syndrome fathering children and 26 cases of females having children have been reported. Without assisted reproductive technologies, around half of the children of someone with Down syndrome will also have the syndrome.

Down syndrome is caused by having three copies of the genes on chromosome 21, rather than the usual two. The parents of the affected individual are typically genetically normal. Those who have one child with Down syndrome have about a 1% risk of having a second child with the syndrome, if both parents are found to have normal karyotypes.

The extra chromosome content can arise through several different ways. The most common cause (about 92–95% of cases) is a complete extra copy of chromosome 21, resulting in trisomy 21. In 1.0 to 2.5% of cases, some of the cells in the body are normal and others have trisomy 21, known as mosaic Down syndrome. The other common mechanisms that can give rise to Down syndrome include: a Robertsonian translocation, isochromosome, or ring chromosome. These contain additional material from chromosome 21 and occur in about 2.5% of cases. An isochromosome results when the two long arms of a chromosome separate together rather than the long and short arm separating together during egg or sperm development.

Trisomy 21 (also known by the karyotype 47,XX,+21 for females and 47,XY,+21 for males) is caused by a failure of the 21st chromosome to separate during egg or sperm development. As a result, a sperm or egg cell is produced with an extra copy of chromosome 21; this cell thus has 24 chromosomes. When combined with a normal cell from the other parent, the baby has 47 chromosomes, with three copies of chromosome 21. About 88% of cases of trisomy 21 result from nonseparation of the chromosomes in the mother, 8% from nonseparation in the father, and 3% after the egg and sperm have merged.

The extra chromosome 21 material may also occur due to a Robertsonian translocation in 2–4% of cases. In this situation, the long arm of chromosome 21 is attached to another chromosome, often chromosome 14. In a male affected with Down syndrome, it results in a karyotype of 46XY,t(14q21q). This may be a new mutation or previously present in one of the parents. The parent with such a translocation is usually normal physically and mentally; however, during production of egg or sperm cells, a higher chance of creating reproductive cells with extra chromosome 21 material exists. This results in a 15% chance of having a child with Down syndrome when the mother is affected and a less than 5% probability if the father is affected. The probability of this type of Down syndrome is not related to the mother's age. Some children without Down syndrome may inherit the translocation and have a higher probability of having children of their own with Down syndrome. In this case it is sometimes known as familial Down syndrome.

The extra genetic material present in DS results in overexpression of a portion of the 310 genes located on chromosome 21. This overexpression has been estimated at around 50%. Some research has suggested the Down syndrome critical region is located at bands 21q22.1–q22.3, with this area including genes for amyloid, superoxide dismutase, and likely the ETS2 proto oncogene. Other research, however, has not confirmed these findings. microRNAs are also proposed to be involved.

The dementia which occurs in Down syndrome is due to an excess of amyloid beta peptide produced in the brain and is similar to Alzheimer's disease. This peptide is processed from amyloid precursor protein, the gene for which is located on chromosome 21. Senile plaques and neurofibrillary tangles are present in nearly all by 35 years of age, though dementia may not be present. Those with DS also lack a normal number of lymphocytes and produce less antibodies which contributes to their increased risk of infection.

Down syndrome is associated with an increased risk of many chronic diseases that are typically associated with older age such as Alzheimer's disease. The accelerated aging suggest that trisomy 21 increases the biological age of tissues, but molecular evidence for this hypothesis is sparse. According to a biomarker of tissue age known as epigenetic clock, trisomy 21 increases the age of blood and brain tissue (on average by 6.6 years).

When screening tests predict a high risk of Down syndrome, a more invasive diagnostic test (amniocentesis or chorionic villus sampling) is needed to confirm the diagnosis. If Down syndrome occurs in one in 500 pregnancies and the test used has a 5% false-positive rate, this means, of 26 women who test positive on screening, only one will have Down syndrome confirmed. If the screening test has a 2% false-positive rate, this means one of eleven who test positive on screening have a fetus with DS. Amniocentesis and chorionic villus sampling are more reliable tests, but they increase the risk of miscarriage between 0.5 and 1%. The risk of limb problems is increased in the offspring due to the procedure. The risk from the procedure is greater the earlier it is performed, thus amniocentesis is not recommended before 15 weeks gestational age and chorionic villus sampling before 10 weeks gestational age.

About 92% of pregnancies in Europe with a diagnosis of Down syndrome are terminated. In the United States, termination rates are around 67%, but this rate varied from 61% to 93% among different populations. Rates are lower among women who are younger and have decreased over time. When nonpregnant people are asked if they would have a termination if their fetus tested positive, 23–33% said yes, when high-risk pregnant women were asked, 46–86% said yes, and when women who screened positive are asked, 89–97% say yes.

The diagnosis can often be suspected based on the child's physical appearance at birth. An analysis of the child's chromosomes is needed to confirm the diagnosis, and to determine if a translocation is present, as this may help determine the risk of the child's parents having further children with Down syndrome. Parents generally wish to know the possible diagnosis once it is suspected and do not wish pity.

Guidelines recommend screening for Down syndrome to be offered to all pregnant women, regardless of age. A number of tests are used, with varying levels of accuracy. They are typically used in combination to increase the detection rate. None can be definitive, thus if screening is positive, either amniocentesis or chorionic villus sampling is required to confirm the diagnosis. Screening in both the first and second trimesters is better than just screening in the first trimester. The different screening techniques in use are able to pick up 90 to 95% of cases with a false-positive rate of 2 to 5%.

Ultrasound imaging can be used to screen for Down syndrome. Findings that indicate increased risk when seen at 14 to 24 weeks of gestation include a small or no nasal bone, large ventricles, nuchal fold thickness, and an abnormal right subclavian artery, among others. The presence or absence of many markers is more accurate. Increased fetal nuchal translucency (NT) indicates an increased risk of Down syndrome picking up 75–80% of cases and being falsely positive in 6%.

Several blood markers can be measured to predict the risk of Down syndrome during the first or second trimester. Testing in both trimesters is sometimes recommended and test results are often combined with ultrasound results. In the second trimester, often two or three tests are used in combination with two or three of: α-fetoprotein, unconjugated estriol, total hCG, and free βhCG detecting about 60–70% of cases.

Testing of the mother's blood for fetal DNA is being studied and appears promising in the first trimester. The International Society for Prenatal Diagnosis considers it a reasonable screening option for those women whose pregnancies are at a high risk for trisomy 21. Accuracy has been reported at 98.6% in the first trimester of pregnancy. Confirmatory testing by invasive techniques (amniocentesis, CVS) is still required to confirm the screening result.

Efforts such as early childhood intervention, screening for common problems, medical treatment where indicated, a good family environment, and work-related training can improve the development of children with Down syndrome. Education and proper care can improve quality of life. Raising a child with Down syndrome is more work for parents than raising an unaffected child. Typical childhood vaccinations are recommended.

A number of health organizations have issued recommendations for screening those with Down syndrome for particular diseases. This is recommended to be done systematically.

At birth, all children should get an electrocardiogram and ultrasound of the heart. Surgical repair of heart problems may be required as early as three months of age. Heart valve problems may occur in young adults, and further ultrasound evaluation may be needed in adolescents and in early adulthood. Due to the elevated risk of testicular cancer, some recommend checking the person's testicles yearly.

Hearing aids or other amplification devices can be useful for language learning in those with hearing loss. Speech therapy may be useful and is recommended to be started around 9 months of age. As those with Down syndrome typically have good hand-eye coordination, learning sign language may be possible. Augmentative and alternative communication methods, such as pointing, body language, objects, or pictures, are often used to help with communication. Behavioral issues and mental illness are typically managed with counseling or medications.
Education programs before reaching school age may be useful. School-age children with Down syndrome may benefit from inclusive education (whereby students of differing abilities are placed in classes with their peers of the same age), provided some adjustments are made to the curriculum. Evidence to support this, however, is not very strong. In the United States, the Individuals with Disabilities Education Act of 1975 requires public schools generally to allow attendance by students with Down syndrome.
Individuals with Down syndrome may learn better visually. Drawing may help with language, speech, and reading skills. Children with Down syndrome still often have difficulty with sentence structure and grammar, as well as developing the ability to speak clearly. Several types of early intervention can help with cognitive development. Efforts to develop motor skills include physical therapy, speech and language therapy, and occupational therapy. Physical therapy focuses specifically on motor development and teaching children to interact with their environment. Speech and language therapy can help prepare for later language. Lastly, occupational therapy can help with skills needed for later independence.

Tympanostomy tubes are often needed and often more than one set during the person's childhood. Tonsillectomy is also often done to help with sleep apnea and throat infections. Surgery, however, does not always address the sleep apnea and a continuous positive airway pressure (CPAP) machine may be useful. Physical therapy and participation in physical education may improve motor skills. Evidence to support this in adults, however, is not very good.

Efforts to prevent respiratory syncytial virus (RSV) infection with human monoclonal antibodies should be considered, especially in those with heart problems. In those who develop dementia there is no evidence for memantine, donepezil, rivastigmine, or galantamine.

Plastic surgery has been suggested as a method of improving the appearance and thus the acceptance of people with Down syndrome. It has also been proposed as a way to improve speech. Evidence, however, does not support a meaningful difference in either of these outcomes. Plastic surgery on children with Down syndrome is uncommon, and continues to be controversial. The U.S. National Down Syndrome Society views the goal as one of mutual respect and acceptance, not appearance.

Many alternative medical techniques are used in Down syndrome; however, they are poorly supported by evidence. These include: dietary changes, massage, animal therapy, chiropractic and naturopathy, among others. Some proposed treatments may also be harmful.

Between 5 and 15% of children with Down syndrome in Sweden attend regular school. Some graduate from high school; however, most do not. Of those with intellectual disability in the United States who attended high school about 40% graduated. Many learn to read and write and some are able to do paid work. In adulthood about 20% in the United States do paid work in some capacity. In Sweden, however, less than 1% have regular jobs. Many are able to live semi-independently, but they often require help with financial, medical, and legal matters. Those with mosaic Down syndrome usually have better outcomes.

Individuals with Down syndrome have a higher risk of early death than the general population. This is most often from heart problems or infections. Following improved medical care, particularly for heart and gastrointestinal problems, the life expectancy has increased. This increase has been from 12 years in 1912, to 25 years in the 1980s, to 50 to 60 years in the developed world in the 2000s. Currently between 4 and 12% die in the first year of life. The probability of long-term survival is partly determined by the presence of heart problems. In those with congenital heart problems 60% survive to 10 years and 50% survive to 30 years of age. In those without heart problems 85% survive to 10 years and 80% survive to 30 years of age. About 10% live to 70 years of age. The National Down Syndrome Society have developed information regarding the positive aspects of life with Down syndrome.

Globally, as of 2010, Down syndrome occurs in about 1 per 1000 births and results in about 17,000 deaths. More children are born with Down syndrome in countries where abortion is not allowed and in countries where pregnancy more commonly occurs at a later age. About 1.4 per 1000 live births in the United States and 1.1 per 1000 live births in Norway are affected. In the 1950s, in the United States, it occurred in 2 per 1000 live births with the decrease since then due to prenatal screening and abortions. The number of pregnancies with Down syndrome is more than two times greater with many spontaneously aborting. It is the cause of 8% of all congenital disorders.

Maternal age affects the chances of having a pregnancy with Down syndrome. At age 20, the chance is one in 1441; at age 30, it is one in 959; at age 40, it is one in 84; and at age 50 it is one in 44. Although the probability increases with maternal age, 70% of children with Down syndrome are born to women 35 years of age and younger, because younger people have more children. The father's older age is also a risk factor in women older than 35, but not in women younger than 35, and may partly explain the increase in risk as women age.

English physician John Langdon Down first described Down syndrome in 1862, recognizing it as a distinct type of mental disability, and again in a more widely published report in 1866. Édouard Séguin described it as separate from cretinism in 1844. By the 20th century, Down syndrome had become the most recognizable form of mental disability.

In antiquity, many infants with disabilities were either killed or abandoned. A number of historical pieces of art are believed to portray Down syndrome, including pottery from the pre-Columbian Tumaco-La Tolita culture in present-day Colombia and Ecuador, and the 16th-century painting "The Adoration of the Christ Child".

In the 20th century, many individuals with Down syndrome were institutionalized, few of the associated medical problems were treated, and most died in infancy or early adult life. With the rise of the eugenics movement, 33 of the then 48 U.S. states and several countries began programs of forced sterilization of individuals with Down syndrome and comparable degrees of disability. Action T4 in Nazi Germany made public policy of a program of systematic involuntary euthanization.

With the discovery of karyotype techniques in the 1950s, it became possible to identify abnormalities of chromosomal number or shape. In 1959, Jérôme Lejeune reported the discovery that Down syndrome resulted from an extra chromosome. However, Lejeune's claim to the discovery has been disputed, and in 2014, the Scientific Council of the French Federation of Human Genetics unanimously awarded its Grand Prize to his colleague Marthe Gautier for her role in this discovery. The discovery was in the laboratory of Raymond Turpin at the Hôpital Trousseau in Paris, France. Jérôme Lejeune and Marthe Gautier were both his students.

As a result of this discovery, the condition became known as trisomy 21. Even before the discovery of its cause, the presence of the syndrome in all races, its association with older maternal age, and its rarity of recurrence had been noticed. Medical texts had assumed it was caused by a combination of inheritable factors that had not been identified. Other theories had focused on injuries sustained during birth.
Due to his perception that children with Down syndrome shared facial similarities with those of Blumenbach's Mongolian race, John Langdon Down used the term "mongoloid". He felt that the existence of Down syndrome confirmed that all peoples were genetically related. In the 1950s with discovery of the underlying cause as being related to chromosomes, concerns about the race-based nature of the name increased.

In 1961, 19 scientists suggested that "mongolism" had "misleading connotations" and had become "an embarrassing term". The World Health Organization (WHO) dropped the term in 1965 after a request by the delegation from the Mongolian People's Republic. While the term mongoloid (also mongolism, Mongolian imbecility or idiocy) continued to be used until the early 1980s, it is now considered unacceptable and is no longer in common use.

In 1975, the United States National Institutes of Health (NIH) convened a conference to standardize the naming and recommended replacing the possessive form, "Down's syndrome" with "Down syndrome". However, both the possessive and nonpossessive forms remain in use by the general population. The term "trisomy 21" is also used frequently.

Some obstetricians argue that not offering screening for Down syndrome is unethical. As it is a medically reasonable procedure, per informed consent, people should at least be given information about it. It will then be the woman's choice, based on her personal beliefs, how much or how little screening she wishes. When results from testing become available, it is also considered unethical not to give the results to the person in question.

Some bioethicists deem it reasonable for parents to select a child who would have the highest well-being. One criticism of this reasoning is that it often values those with disabilities less. Some parents argue that Down syndrome shouldn't be prevented or cured and that eliminating Down syndrome amounts to genocide. The disability rights movement does not have a position on screening, although some members consider testing and abortion discriminatory. Some in the United States who are pro-life support abortion if the fetus is disabled, while others do not. Of a group of 40 mothers in the United States who have had one child with Down syndrome, half agreed to screening in the next pregnancy.

Within the US, some Protestant denominations see abortion as acceptable when a fetus has Down syndrome, while Orthodox Christians and Roman Catholics often do not. Some of those against screening refer to it as a form of "eugenics". Disagreement exists within Islam regarding the acceptability of abortion in those carrying a fetus with Down syndrome. Some Islamic countries allow abortion, while others do not. Women may face stigmatization whichever decision they make.

Advocacy groups for individuals with Down syndrome began to be formed after the Second World War. These were organizations advocating for the inclusion of people with Down syndrome into the general school system and for a greater understanding of the condition among the general population, as well as groups providing support for families with children living with Down syndrome. Before this individuals with Down syndrome were often placed in mental hospitals or asylums. Organizations included the Royal Society for Handicapped Children and Adults founded in the UK in 1946 by Judy Fryd, Kobato Kai founded in Japan in 1964, the National Down Syndrome Congress founded in the United States in 1973 by Kathryn McGee and others, and the National Down Syndrome Society founded in 1979 in the United States.

The first World Down Syndrome Day was held on 21 March 2006. The day and month were chosen to correspond with 21 and trisomy, respectively. It was recognized by the United Nations General Assembly in 2011.

Efforts are underway to determine how the extra chromosome 21 material causes Down syndrome, as currently this is unknown, and to develop treatments to improve intelligence in those with the syndrome. One hope is to use stem cells. Other methods being studied include the use of antioxidants, gamma secretase inhibition, adrenergic agonists, and memantine. Research is often carried out on an animal model, the Ts65Dn mouse.



</doc>
<doc id="8305" url="https://en.wikipedia.org/wiki?curid=8305" title="Dyslexia">
Dyslexia

Dyslexia, also known as reading disorder, is characterized by trouble with reading despite normal intelligence. Different people are affected to varying degrees. Problems may include difficulties in spelling words, reading quickly, writing words, "sounding out" words in the head, pronouncing words when reading aloud and understanding what one reads. Often these difficulties are first noticed at school. When someone who previously could read loses their ability, it is known as alexia. The difficulties are involuntary and people with this disorder have a normal desire to learn.
Dyslexia is believed to be caused by both genetic and environmental factors. Some cases run in families. It often occurs in people with attention deficit hyperactivity disorder (ADHD) and is associated with similar difficulties with numbers. It may begin in adulthood as the result of a traumatic brain injury, stroke, or dementia. The underlying mechanisms of dyslexia are problems within the brain's language processing. Dyslexia is diagnosed through a series of tests of memory, spelling, vision, and reading skills. Dyslexia is separate from reading difficulties caused by hearing or vision problems or by insufficient teaching.
Treatment involves adjusting teaching methods to meet the person's needs. While not curing the underlying problem, it may decrease the degree of symptoms. Treatments targeting vision are not effective. Dyslexia is the most common learning disability and occurs in all areas of the world. It affects 3–7% of the population, however, up to 20% may have some degree of symptoms. While dyslexia is more often diagnosed in men, it has been suggested that it affects men and women equally. Some believe that dyslexia should be best considered as a different way of learning, with both benefits and downsides.
Dyslexia is thought to have two types of cause, one related to language processing and another to visual processing. It is considered a cognitive disorder, not a problem with intelligence. However, emotional problems often arise because of it. Some published definitions are purely descriptive, whereas others propose causes. The latter usually cover a variety of reading skills and deficits, and difficulties with distinct causes rather than a single condition. The National Institute of Neurological Disorders and Stroke definition describes dyslexia as "difficulty with phonological processing (the manipulation of sounds), spelling, and/or rapid visual-verbal responding". The British Dyslexia Association definition describes dyslexia as "a learning difficulty that primarily affects the skills involved in accurate and fluent word reading and spelling" and is characterized by "difficulties in phonological awareness, verbal memory and verbal processing speed".

Acquired dyslexia or alexia may be caused by brain damage due to stroke or atrophy. Forms of alexia include pure alexia, surface dyslexia, semantic dyslexia, phonological dyslexia, and deep dyslexia.

There is some variability in the definition of dyslexia. Some sources, such as the U.S. National Institutes of Health, define it specifically as a learning disorder. Other sources, however, define it simply as an inability to read in the context of normal intelligence, and distinguish between "developmental dyslexia" (a learning disorder) and "acquired dyslexia" (loss of the ability to read caused by brain damage). ICD 10, the manual of medical diagnosis used in much of the world, includes separate diagnoses for "developmental dyslexia" (81.0) and for "dyslexia and alexia" (48.0). DSM 5, the manual of psychiatric diagnosis used in the United States, does not specifically define dyslexia, justifying this decision by stating that "the many definitions of dyslexia and dyscalculia meant those terms would not be useful as disorder names or in the diagnostic 
criteria". Instead it includes dyslexia in a category called specific learning disorders.

In early childhood, symptoms that correlate with a later diagnosis of dyslexia include delayed onset of speech and a lack of phonological awareness, as well as being easily distracted by background noise. A common myth closely associates dyslexia with mirror writing and reading letters or words backwards. These behaviors are seen in many children as they learn to read and write, and are not considered to be defining characteristics of dyslexia.

School-age children with dyslexia may exhibit signs of difficulty in identifying or generating rhyming words, or counting the number of syllables in words – both of which depend on phonological awareness. They may also show difficulty in segmenting words into individual sounds or may blend sounds when producing words, indicating reduced phonemic awareness. Difficulties with word retrieval or naming things is also associated with dyslexia. People with dyslexia are commonly poor spellers, a feature sometimes called dysorthographia or dysgraphia, which depends on orthographic coding.

Problems persist into adolescence and adulthood and may accompany difficulties with summarizing stories, memorization, reading aloud, or learning foreign languages. Adults with dyslexia can often read with good comprehension, though they tend to read more slowly than others without a learning difficulty and perform worse in spelling tests or when reading nonsense words – a measure of phonological awareness.

The orthographic complexity of a language directly impacts how difficult learning to read the language is. English and French have comparatively "deep" phonemic orthographies within the Latin alphabet writing system, with complex structures employing spelling patterns on several levels: letter-sound correspondence, syllables, and morphemes. Languages such as Spanish, Italian and Finnish have mostly alphabetic orthographies, which primarily employ letter-sound correspondence – so-called shallow orthographies – which for dyslexics makes them easier to learn. Logographic writing systems, such as Chinese characters, have extensive symbol use, and pose problems for dyslexic learners.

Dyslexia is often accompanied by several learning disabilities, but it is unclear whether they share underlying neurological causes. These associated disabilities include:

Researchers have been trying to find the neurobiological basis of dyslexia since the condition was first identified in 1881. For example, some have tried to associate the common problem among dyslexics of not being able to see letters clearly to abnormal development of their visual nerve cells.

Modern neuroimaging techniques such as functional magnetic resonance imaging (fMRI) and positron emission tomography (PET) have shown a correlation between both functional and structural differences in the brains of children with reading difficulties. Some dyslexics show less electrical activation in parts of the left hemisphere of the brain involved with reading, such as the inferior frontal gyrus, inferior parietal lobule, and the middle and ventral temporal cortex. Over the past decade, brain activation studies using PET to study language have produced a breakthrough in the understanding of the neural basis of language. Neural bases for the visual lexicon and for auditory verbal short-term memory components have been proposed, with some implication that the observed neural manifestation of developmental dyslexia is task-specific (i.e. functional rather than structural). fMRIs in dyslexics have provided important data which point to the interactive role of the cerebellum and cerebral cortex as well as other brain structures.

The cerebellar theory of dyslexia proposes that impairment of cerebellum-controlled muscle movement affects the formation of words by the tongue and facial muscles, resulting in the fluency problems that are characteristic of some dyslexics. The cerebellum is also involved in the automatization of some tasks, such as reading. The fact that some dyslexic children have motor task and balance impairments has been used as evidence for a cerebellar role in their reading difficulties. However, the cerebellar theory is not supported by controlled research studies.

Research into potential genetic causes of dyslexia has its roots in post-autopsy examination of the brains of people with dyslexia. Observed anatomical differences in the language centers of such brains include microscopic cortical malformations known as ectopias, more rarely, vascular micro-malformations, and microgyrus. The previously cited studies and others suggest that abnormal cortical development presumed to occur before or during the sixth month of fetal brain development was the cause of the abnormalities. Abnormal cell formations in dyslexics have also been reported in non-language cerebral and subcortical brain structures. Several genes have been associated with dyslexia, including DCDC2 and KIAA0319 on chromosome 6, and DYX1C1 on chromosome 15.

The contribution of gene–environment interaction to reading disability has been intensely studied using twin studies, which estimate the proportion of variance associated with a person's environment and the proportion associated with their genes. Studies examining the influence of environmental factors such as parental education and teacher quality have determined that genetics have greater influence in supportive, rather than less optimal, environments. However, more optimal conditions may just allow those genetic risk factors to account for more of the variance in outcome because the environmental risk factors have been minimized. As environment plays a large role in learning and memory, it is likely that epigenetic modifications play an important role in reading ability. Animal experiments and measures of gene expression and methylation in the human periphery are used to study epigenetic processes; however, both types of study have many limitations in the extrapolation of results for application to the human brain.

The dual-route theory of reading aloud was first described in the early 1970s. This theory suggests that two separate mental mechanisms, or cognitive routes, are involved in reading aloud. One mechanism is the lexical route, which is the process whereby skilled readers can recognize known words by sight alone, through a "dictionary" lookup procedure. The other mechanism is the nonlexical or sublexical route, which is the process whereby the reader can "sound out" a written word. This is done by identifying the word's constituent parts (letters, phonemes, graphemes) and applying knowledge of how these parts are associated with each other, for example, how a string of neighboring letters sound together. The dual-route system could explain the different rates of dyslexia occurrence between different languages (e.g. the Spanish language dependence on phonological rules accounts for the fact that Spanish-speaking children show a higher level of performance in non-word reading, when compared to English-speakers).

Dyslexia disorder is not caused by mutation in one gene; in fact, it appears to involve the combined effects of several genes. Studying the cognitive problems associated with other disorders helps to better understand the genotype-phenotype link of dyslexia. Neurophysiological and imaging procedures are being used to ascertain phenotypic characteristics in dyslexics, thus identifying the effects of certain genes.

There are tests that can indicate with high probability whether a person is a dyslexic. If diagnostic testing indicates that a person may be dyslexic, such tests are often followed up with a full diagnostic assessment to determine the extent and nature of the disorder. Tests can be administered by a teacher or computer. Some test results indicate how to carry out teaching strategies.

Central dyslexias include surface dyslexia, semantic dyslexia, phonological dyslexia, and deep dyslexia. ICD-10 reclassified the previous distinction between dyslexia (315.02 in ICD-9) and alexia (315.01 in ICD-9) into a single classification as R48.0. The terms are applied to developmental dyslexia and inherited dyslexia along with developmental aphasia and inherited alexia, which are considered synonymous.

In surface dyslexia, words with regular pronunciations (highly consistent with their spelling, e.g. "mint") are read more accurately than words with irregular pronunciation, such as "colonel". Difficulty distinguishing homophones is a diagnostic used for some forms of surface dyslexia. This disorder is usually accompanied by surface agraphia and fluent aphasia. Acquired surface dyslexia arises when a previously literate person experiences brain damage, which results in pronunciation errors that indicate impairment of the lexical route.

In phonological dyslexia, sufferers can read familiar words but have difficulty with unfamiliar words, such as invented pseudo-words. Phonological dyslexia is associated with lesions in the parts of the brain supplied with blood by the middle cerebral artery. The superior temporal lobe is often also involved. Furthermore, dyslexics compensate by overusing a front-brain region called Broca's area, which is associated with aspects of language and speech. The Lindamood Phoneme Sequencing Program (LiPS) is used to treat phonological dyslexia. This system is based on a three-way sensory feedback process, using auditory, visual, and oral skills to learn to recognize words and word patterns. Case studies with a total of three patients found a significant improvement in spelling and reading ability after using LiPS.

Individuals with deep dyslexia experience both semantic paralexia (para-dyslexia) and phonological dyslexia, which causes the person to read a word and then say a related meaning instead of the denoted meaning. Deep alexia is associated with clear phonological processing impairments. Deep dyslexia is caused by widespread damage to the brain that often includes the left hemisphere. The "continuum" hypothesis claims that deep dyslexia develops from phonological dyslexia.

Peripheral dyslexias have been described as affecting the visual analysis of letters as a result of brain injury. Hemianopsia, a visual field loss on the left/right side of the vertical midline, is associated with this condition.

Pure, or phonologically-based, dyslexia, also known as agnosic dyslexia, dyslexia without agraphia, and pure word blindness, is dyslexia due to difficulty in recognizing written sequences of letters (such as words), or sometimes even letters. It is considered '"pure" because it is not accompanied by other significant language-related impairments. Pure dyslexia does not affect speech, handwriting style, language or comprehension impairments. Pure dyslexia is caused by lesions on the visual word form area (VWFA). The VWFA is composed of the left lateral occipital sulcus and is activated during reading. A lesion in the VWFA stops transmission between the visual cortex and the left angular gyrus. It can also be caused by a lesion involving the left occipital lobe or the splenium. It is usually accompanied by a homonymous hemianopsia in the right side of the visual field. Multiple oral re-reading (MOR) is a treatment for pure dyslexia. It is considered a top-down processing technique in which affected individuals read and reread texts a predetermined number of times or until reading speed or accuracy improves a predetermined amount.

Hemianopic dyslexia is commonly considered to derive from visual field loss due to damage to the primary visual cortex. Sufferers may complain of abnormally slow reading but are able to read individual words normally. This is the most common form of peripheral alexia, and the form with the best evidence of effective treatments.

In neglect dyslexia, some letters, most commonly those at the beginning or left side of a word, are skipped or misread during reading. This alexia is associated with right parietal lesions. The use of prism glasses has been shown to substantially mitigate this condition.

People with attentional dyslexia complain of letter-crowding or migration, sometimes blending elements of two words into one. Sufferers read better when words are presented in isolation rather than flanked by other words and letters. Using a large magnifying glass may help mitigate this condition by reducing the effects of flanking from nearby words; however, no trials of this or indeed any other therapy for left parietal syndromes have been published as of 2014.

Through the use of compensation strategies, therapy and educational support, dyslexic individuals can learn to read and write. There are techniques and technical aids which help to manage or conceal symptoms of the disorder. Removing stress and anxiety alone can sometimes improve written comprehension. For dyslexia intervention with alphabet-writing systems, the fundamental aim is to increase a child's awareness of correspondences between graphemes (letters) and phonemes (sounds), and to relate these to reading and spelling by teaching how sounds blend into words. It has been found that reinforced collateral training focused on reading and spelling yields longer-lasting gains than oral phonological training alone. Early intervention that is done for children at a young age can be successful in reducing reading failure.

There is some evidence that the use of specially-tailored fonts may help with dyslexia. These fonts, which include Dyslexie, OpenDyslexic, and Lexia Readable, were created based on the idea that many of the letters of the Latin alphabet are visually similar and may, therefore, confuse people with dyslexia. Dyslexie and OpenDyslexic both put emphasis on making each letter more distinctive in order to be more easily identified. The benefits, however, might simply be due to the added spacing between words.

There have been many studies conducted regarding intervention in dyslexia. Among these studies one meta-analysis found that there was functional activation as a result.

There is no evidence demonstrating that the use of music education is effective in improving dyslexic adolescents' reading skills.

Dyslexic children require special instruction for word analysis and spelling from an early age. While there are fonts that may help people with dyslexia better understand writing, this might simply be due to the added spacing between words. The prognosis, generally speaking, is positive for individuals who are identified in childhood and receive support from friends and family.

The percentage of people with dyslexia is unknown, but it has been estimated to be as low as 5% and as high as 17% of the population. While it is diagnosed more often in males, some believe that it affects males and females equally.

There are different definitions of dyslexia used throughout the world, but despite significant differences in writing systems, dyslexia occurs in different populations. Dyslexia is not limited to difficulty in converting letters to sounds, and Chinese dyslexics may have difficulty converting Chinese characters into their meanings. The Chinese vocabulary uses logographic, monographic, non-alphabet writing where one character can represent an individual phoneme.

The phonological-processing hypothesis attempts to explain why dyslexia occurs in a wide variety of languages. Furthermore, the relationship between phonological capacity and reading appears to be influenced by orthography.

Dyslexia was identified by Oswald Berkhan in 1881, but the term "dyslexia" was coined in 1887 by Rudolf Berlin, an ophthalmologist in Stuttgart. He used the term to refer to the case of a young boy who had a severe impairment in learning to read and write, despite showing typical intelligence and physical abilities in all other respects. In 1896, W. Pringle Morgan, a British physician from Seaford, East Sussex, published a description of a reading-specific learning disorder in a report to the "British Medical Journal" titled "Congenital Word Blindness". The distinction between phonological and surface types of dyslexia is only descriptive, and without any etiological assumption as to the underlying brain mechanisms. However, studies have alluded to potential differences due to variation in performance.

The majority of currently available dyslexia research relates to alphabetic writing systems, and especially to European languages. However, substantial research is also available regarding dyslexics who speak Arabic, Chinese, Hebrew, or other languages.

As is the case with any disorder, society often makes an assessment based on incomplete information. Before the 1980s, dyslexia was thought to be a consequence of education, rather than a basic disability. As a result, society often misjudges those with the disorder. There is also sometimes a workplace stigma and negative attitude towards those with dyslexia. If a dyslexic's instructors lack the necessary training to support a child with the condition, there is often a negative effect on the student's learning participation.




</doc>
<doc id="8308" url="https://en.wikipedia.org/wiki?curid=8308" title="Delft">
Delft

Delft () is a city and municipality in the province of South Holland, Netherlands. It is located between Rotterdam, to the southeast, and The Hague, to the northwest. Together with them, it is part of both Rotterdam–The Hague metropolitan area and the Randstad.

Delft is a popular tourist attraction in the country. It is home to Delft University of Technology (TU Delft), regarded as center of technological research and development in the Netherlands, Delft Blue pottery and the currently reigning House of Orange-Nassau. Historically, Delft played a highly influential role in the Dutch Golden Age. Delft has a special place in the history of microbiology. In terms of science and technology, thanks to the pioneering contributions of Antonie van Leeuwenhoek and Martinus Beijerinck, Delft can be considered to be the true birthplace of microbiology, with its several sub-disciplines such as bacteriology, protozoology, and virology.

The city of Delft came into being beside a canal, the 'Delf', which comes from the word "delven", meaning delving or digging, and led to the name Delft. It presumably started around the 11th century as a landlord court.

From a rural village in the early Middle Ages, Delft developed into a city, that in the 13th century (1246) received its charter. (For some more information about the early development, see Gracht)."

The town's association with the House of Orange started when William of Orange (Willem van Oranje), nicknamed William the Silent (Willem de Zwijger), took up residence in 1572. At the time he was the leader of growing national Dutch resistance against Spanish occupation, known as the Eighty Years' War. By then Delft was one of the leading cities of Holland and it was equipped with the necessary city walls to serve as a headquarters. An attack by Spanish forces in October of that year was repelled.

After the Act of Abjuration was proclaimed in 1581, Delft became the "de facto" capital of the newly independent Netherlands, as the seat of the Prince of Orange.

When William was shot dead in 1584 by Balthazar Gerards in the hall of the Prinsenhof, the family's traditional burial place in Breda was still in the hands of the Spanish. Therefore, he was buried in the Delft Nieuwe Kerk (New Church), starting a tradition for the House of Orange that has continued to the present day.

The Delft Explosion, also known in history as the Delft Thunderclap, occurred on 12 October 1654 when a gunpowder store exploded, destroying much of the city. Over a hundred people were killed and thousands were wounded.
About of gunpowder were stored in barrels in a magazine in a former Clarissen convent in the Doelenkwartier district. Cornelis Soetens, the keeper of the magazine, opened the store to check a sample of the powder and a huge explosion followed. Luckily, many citizens were away, visiting a market in Schiedam or a fair in The Hague.

Today, the explosion is remembered primarily for killing Rembrandt's most promising pupil, Carel Fabritius, and destroying almost all his works.

Delft artist Egbert van der Poel painted several pictures of Delft showing the devastation.
The city centre retains a large number of monumental buildings, while in many streets there are canals of which the banks are connected by typical bridges, altogether making this city a notable tourist destination.

Historical buildings and other sights of interest include:

Delft is well known for the Delft pottery ceramic products which were styled on the imported Chinese porcelain of the 17th century. The city had an early start in this area since it was a home port of the Dutch East India Company. It can still be seen at the pottery factories De Koninklijke Porceleyne Fles (or Royal Delft) and De Delftse Pauw.

The painter Johannes Vermeer (1632–1675) was born in Delft. Vermeer used Delft streets and home interiors as the subject or background in his paintings.
Several other famous painters lived and worked in Delft at that time, such as Pieter de Hoogh, Carel Fabritius, Nicolaes Maes, Gerard Houckgeest and Hendrick Cornelisz. van Vliet. They were all members of the Delft School. The Delft School is known for its images of domestic life, views of households, church interiors, courtyards, squares and the streets of Delft. The painters also produced pictures showing historic events, flowers, portraits for patrons and the court as well as decorative pieces of art.
Delft supports creative arts companies. From 2001 the Bacinol, a building that had been disused since 1951, began to house small companies in the creative arts sector. However, demolition of the building started in December 2009, making way for the construction of the new railway tunnel in Delft. The occupants of the building, as well as the name 'Bacinol', moved to another building in the city. The name Bacinol relates to Dutch penicillin research during WWII.

Delft University of Technology (TU Delft) is one of four universities of technology in the Netherlands. It was founded as an academy for civil engineering in 1842 by King William II. Today well over 21,000 students are enrolled.

The UNESCO-IHE Institute for Water Education, providing postgraduate education for people from developing countries, draws on the strong tradition in water management and hydraulic engineering of the Delft university.

In the local economic field essential elements are:

East of Delft lies a relatively vast nature and recreation area called the "Delftse Hout" ("Delft Wood") is situated. Through the forest lie bike, horse-riding and footpaths. It also includes a vast lake (suitable for swimming and windsurfing), narrow beaches, a restaurant, community gardens, plus camping ground and other recreational and sports facilities. (There is also a facility for renting bikes from the station.)

Inside the city, apart from a central park, there are also several smaller town parks, like "Nieuwe Plantage", "Agnetapark", "Kalverbos" and others.
Furthermore, there is the Botanical Garden of the TU and an arboretum in Delftse Hout.

Delft was the birthplace of:

Before 1900

After 1900
Otherwise related


Delft is twinned with:

Delft's longstanding connection with Rishon LeZion ended in 2016 after the supporting organizations shut down in both countries.

Trains stopping at these stations connect Delft with, among others, the nearby cities of Rotterdam and The Hague, up to every five minutes, for most of the day.

There are several bus routes from Delft to similar destinations. Trams frequently travel between Delft and The Hague via special double tracks crossing the city. One of those two lines (19) is still under construction inside Delft and is meant to connect The Hague with a science park, which is being developed on the southern (Rotterdam) side of Delft and is a joint project by the Delft and Rotterdam municipalities.




</doc>
<doc id="8309" url="https://en.wikipedia.org/wiki?curid=8309" title="Duesberg hypothesis">
Duesberg hypothesis

The Duesberg hypothesis is the claim, associated with University of California, Berkeley professor Peter Duesberg, that various noninfectious factors such as but not limited to, recreational and pharmaceutical drug use are the cause of AIDS, and that HIV (human immunodeficiency virus) is merely a harmless passenger virus. The scientific consensus is that the Duesberg hypothesis is incorrect and that HIV is the cause of AIDS. The most prominent supporters of this hypothesis are Duesberg himself, biochemist vitamin proponent David Rasnick, and journalist Celia Farber. The scientific community contends that Duesberg's arguments are the result of cherry-picking predominantly outdated scientific data and selectively ignoring evidence in favor of HIV's role in AIDS.

Duesberg argues that there is a statistical correlation between trends in recreational drug use and trends in AIDS cases. He argues that the epidemic of AIDS cases in the 1980s corresponds to a supposed epidemic of recreational drug use in the United States and Europe during the same time frame.

These claims are not supported by epidemiologic data. The average yearly increase in opioid-related deaths from 1990 to 2002 was nearly three times the yearly increase from 1979–90, with the greatest increase in 2000–02, yet AIDS cases and deaths fell dramatically during the mid-to-late-1990s. Duesberg's claim that recreational drug use, rather than HIV, was the cause of AIDS has been specifically examined and found to be false. Cohort studies have found that only HIV-positive drug users develop opportunistic infections; HIV-negative drug users do not develop such infections, indicating that HIV rather than drug use is the cause of AIDS.

Duesberg has also argued that nitrite inhalants were the cause of the epidemic of Kaposi sarcoma (KS) in gay men. However, this argument has been described as an example of the fallacy of a statistical confounding effect; it is now known that a herpesvirus, potentiated by HIV, is responsible for AIDS-associated KS.

Moreover, in addition to recreational drugs, Duesberg argues that anti-HIV drugs such as zidovudine (AZT) can cause AIDS. Duesberg's claim that antiviral medication causes AIDS is regarded as disproven by the scientific community. Placebo-controlled studies have found that AZT as a single agent produces modest and short-lived improvements in survival and delays the development of opportunistic infections; it certainly did not cause AIDS, which develops in both treated and untreated study patients. With the subsequent development of protease inhibitors and highly active antiretroviral therapy, numerous studies have documented the fact that anti-HIV drugs prevent the development of AIDS and substantially prolong survival, further disproving the claim that these drugs "cause" AIDS.

Several studies have specifically addressed Duesberg's claim that recreational drug abuse or sexual promiscuity were responsible for the manifestations of AIDS. An early study of his claims, published in "Nature" in 1993, found Duesberg's drug abuse-AIDS hypothesis to have "no basis in fact."

A large prospective study followed a group of 715 homosexual men in the Vancouver, Canada, area; approximately half were HIV-seropositive or became so during the follow-up period, and the remainder were HIV-seronegative. After more than 8 years of follow-up, despite similar rates of drug use, sexual contact, and other supposed risk factors in both groups, only the HIV-positive group suffered from opportunistic infections. Similarly, CD4 counts dropped in the patients who were HIV-infected, but remained stable in the HIV-negative patients, despite similar rates of risk behavior. The authors concluded that "the risk-AIDS hypothesis ... is clearly rejected by our data," and that "the evidence supports the hypothesis that HIV-1 has an integral role in the CD4 depletion and progressive immune dysfunction that characterise AIDS."

Similarly, the Multicenter AIDS Cohort Study (MACS) and the Women's Interagency HIV Study (WIHS)—which between them observed more than 8,000 Americans—demonstrated that "the presence of HIV infection is the only factor that is strongly and consistently associated with the conditions that define AIDS." A 2008 study found that recreational drug use (including cannabis, cocaine, poppers, and amphetamines) had no effect on CD4 or CD8 T-cell counts, providing further evidence against a role of recreational drugs as a cause of AIDS.

Duesberg argued in 1989 that a significant number of AIDS victims had died without proof of HIV infection. However, with the use of modern culture techniques and polymerase chain reaction testing, HIV can be demonstrated in virtually all patients with AIDS. Since AIDS is now defined partially by the presence of HIV, Duesberg claims it is impossible by definition to offer evidence that AIDS doesn't require HIV. However, the first definitions of AIDS mentioned no cause and the first AIDS diagnoses were made before HIV was discovered. The addition of HIV positivity to surveillance criteria as an absolutely necessary condition for case reporting occurred only in 1993, after a scientific consensus was established that HIV caused AIDS.

According to the Duesberg hypothesis, AIDS is not found in Africa. What Duesberg calls "the myth of an African AIDS epidemic," among people" exists for several reasons, including:

Duesberg states that African AIDS cases are "a collection of long-established, indigenous diseases, such as chronic fevers, weight loss, alias "slim disease," diarrhea, and tuberculosis" that result from malnutrition and poor sanitation. African AIDS cases, though, have increased in the last three decades as HIV's prevalence has increased but as malnutrition percentages and poor sanitation have declined in many African regions. In addition, while HIV and AIDS are more prevalent in urban than in rural settings in Africa, malnutrition and poor sanitation are found more commonly in rural than in urban settings.

According to Duesberg, common diseases are easily misdiagnosed as AIDS in Africa because "the diagnosis of African AIDS is arbitrary" and does not include HIV testing. A definition of AIDS agreed upon in 1985 by the World Health Organization in Bangui did not require a positive HIV test, but since 1985, many African countries have added positive HIV tests to the Bangui criteria for AIDS or changed their definitions to match those of the U.S. Centers for Disease Control. One of the reasons for using more HIV tests despite their expense is that, rather than overestimating AIDS as Duesberg suggests, the Bangui definition alone excluded nearly half of African AIDS patients."

Duesberg notes that diseases associated with AIDS differ between African and Western populations, concluding that the causes of immunodeficiency must be different. Tuberculosis is much more commonly diagnosed among AIDS patients in Africa than in Western countries, while PCP conforms to the opposite pattern. Tuberculosis, though, had higher prevalence in Africa than in the West before the spread of HIV. In Africa and the United States, HIV has spurred a similar percentage increase in tuberculosis cases. PCP may be underestimated in Africa: since machinery "required for accurate testing is relatively rare in many resource-poor areas, including large parts of Africa, PCP is likely to be underdiagnosed in Africa. Consistent with this hypothesis, studies that report the highest rates of PCP in Africa are those that use the most advanced diagnostic methods" Duesberg also claims that Kaposi's Sarcoma is "exclusively diagnosed in male homosexual risk groups using nitrite inhalants and other psychoactive drugs as aphrodisiacs", but the cancer is fairly common among heterosexuals in some parts of Africa, and is found in heterosexuals in the United States as well.

Because reported AIDS cases in Africa and other parts of the developing world include a larger proportion of people who do not belong to Duesberg's preferred risk groups of drug addicts and male homosexuals, Duesberg writes on his website that "There are no risk groups in Africa, like drug addicts and homosexuals." However, many studies have addressed the issue of risk groups in Africa and concluded that the risk of AIDS is not equally distributed. In addition, AIDS in Africa largely kills sexually active working-age adults.

South African president Thabo Mbeki accepted Duesberg's hypothesis and, through the mid-2000s, rejected offers of medical assistance to fight HIV infection, a policy of inaction that cost over 300,000 lives.

Duesberg argues that retroviruses like HIV must be harmless to survive: they do not kill cells and they do not cause cancer, he maintains. Duesberg writes, "retroviruses do not kill cells because they depend on viable cells for the replication of their RNA from viral DNA integrated into cellular DNA." Duesberg elsewhere states that "the typical virus reproduces by entering a living cell and commandeering the cell's resources in order to make new virus particles, a process that ends with the disintegration of the dead cell."

Duesberg also rejects the involvement of retroviruses and other viruses in cancer. To him, virus-associated cancers are "freak accidents of nature" that do not warrant research programs such as the War on Cancer. Duesberg rejects a role in cancer for numerous viruses, including leukemia viruses, Epstein-Barr Virus, Human Papilloma Virus, Hepatitis B, Feline Leukemia Virus, and Human T-lymphotropic virus.

Duesberg claims that the supposedly innocuous nature of all retroviruses is supported by what he considers to be their normal mode of proliferation: infection from mother to child "in utero". Duesberg does not suggest that HIV is an endogenous retrovirus, a virus integrated into the germ line and genetically heritable:

The consensus in the scientific community is that the Duesberg hypothesis has been refuted by a large and growing mass of evidence showing that HIV causes AIDS, that the amount of virus in the blood correlates with disease progression, that a plausible mechanism for HIV's action has been proposed, and that anti-HIV medication decreases mortality and opportunistic infection in people with AIDS.

In the 9 December 1994 issue of "Science" (Vol. 266, No. 5191), Duesberg's methods and claims were evaluated in a group of articles. The authors concluded that

The vast majority of people with AIDS have never received antiretroviral drugs, including those in developed countries prior to the licensure of AZT (zidovudine) in 1987, and people in developing countries today where very few individuals have access to these medications.

The NIAID reports that "in the mid-1980s, clinical trials enrolling patients with AIDS found that AZT given as single-drug therapy conferred a modest survival advantage compared [with] placebo. Among HIV-infected patients who had not yet developed AIDS, placebo-controlled trials found that AZT given as single-drug therapy delayed, for a year or two, the onset of AIDS-related illnesses. Significantly, long-term follow-up of these trials did not show a prolonged benefit of AZT, but also did not indicate that the drug increased disease progression or mortality. The lack of excess AIDS cases and death in the AZT arms of these placebo-controlled trials in effect counters the argument that AZT causes AIDS. Subsequent clinical trials found that patients receiving two-drug combinations had up to 50 percent improvements in time to progression to AIDS and in survival when compared with people receiving single-drug therapy. In more recent years, three-drug combination therapies have produced another 50 to 80 percent improvement in progression to AIDS and in survival when compared with two-drug regimens in clinical trials." "Use of potent anti-HIV combination therapies has contributed to dramatic reductions in the incidence of AIDS and AIDS-related deaths in populations where these drugs are widely available, an effect which clearly would not be seen if antiretroviral drugs caused AIDS."

Duesberg claims as support for his idea that many drug-free HIV-positive people have not yet developed AIDS; HIV/AIDS scientists note that many drug-free HIV-positive people have developed AIDS, and that, in the absence of medical treatment or rare genetic factors postulated to delay disease progression, it is very likely that nearly all HIV-positive people will eventually develop AIDS. Scientists also note that HIV-negative drug users do not suffer from immune system collapse.




</doc>
<doc id="8310" url="https://en.wikipedia.org/wiki?curid=8310" title="DSL (disambiguation)">
DSL (disambiguation)

DSL or digital subscriber line is a family of technologies that provide digital data transmission over the wires of a local telephone network.

DSL may also refer to:


</doc>
<doc id="8311" url="https://en.wikipedia.org/wiki?curid=8311" title="Dinosaur">
Dinosaur

Dinosaurs are a diverse group of reptiles of the clade Dinosauria. They first appeared during the Triassic period, between 243 and 233.23 million years ago, although the exact origin and timing of the evolution of dinosaurs is the subject of active research. They became the dominant terrestrial vertebrates after the Triassic–Jurassic extinction event 201 million years ago; their dominance continued through the Jurassic and Cretaceous periods. Reverse genetic engineering and the fossil record both demonstrate that birds are modern feathered dinosaurs, having evolved from earlier theropods during the late Jurassic Period. As such, birds were the only dinosaur lineage to survive the Cretaceous–Paleogene extinction event 66 million years ago. Dinosaurs can therefore be divided into "avian dinosaurs", or birds; and "non-avian dinosaurs", which are all dinosaurs other than birds. This article deals primarily with non-avian dinosaurs.

Dinosaurs are a varied group of animals from taxonomic, morphological and ecological standpoints. Birds, at over 10,000 living species, are the most diverse group of vertebrates besides perciform fish. Using fossil evidence, paleontologists have identified over 500 distinct genera and more than 1,000 different species of non-avian dinosaurs. Dinosaurs are represented on every continent by both extant species (birds) and fossil remains. Through the first half of the 20th century, before birds were recognized to be dinosaurs, most of the scientific community believed dinosaurs to have been sluggish and cold-blooded. Most research conducted since the 1970s, however, has indicated that all dinosaurs were active animals with elevated metabolisms and numerous adaptations for social interaction. Some were herbivorous, others carnivorous. Evidence suggests that egg-laying and nest-building are additional traits shared by all dinosaurs, avian and non-avian alike.

While dinosaurs were ancestrally bipedal, many extinct groups included quadrupedal species, and some were able to shift between these stances. Elaborate display structures such as horns or crests are common to all dinosaur groups, and some extinct groups developed skeletal modifications such as bony armor and spines. While the dinosaurs' modern-day surviving avian lineage (birds) are generally small due to the constraints of flight, many prehistoric dinosaurs (non-avian and avian) were large-bodied—the largest sauropod dinosaurs are estimated to have reached lengths of and heights of and were the largest land animals of all time. Still, the idea that non-avian dinosaurs were uniformly gigantic is a misconception based in part on preservation bias, as large, sturdy bones are more likely to last until they are fossilized. Many dinosaurs were quite small: "Xixianykus", for example, was only about long.

Since the first dinosaur fossils were recognized in the early 19th century, mounted fossil dinosaur skeletons have been major attractions at museums around the world, and dinosaurs have become an enduring part of world culture. The large sizes of some dinosaur groups, as well as their seemingly monstrous and fantastic nature, have ensured dinosaurs' regular appearance in best-selling books and films, such as "Jurassic Park". Persistent public enthusiasm for the animals has resulted in significant funding for dinosaur science, and new discoveries are regularly covered by the media.

The taxon Dinosauria was formally named in 1841 by paleontologist Sir Richard Owen, who used it to refer to the "distinct tribe or sub-order of Saurian Reptiles" that were then being recognized in England and around the world. The term is derived . Though the taxonomic name has often been interpreted as a reference to dinosaurs' teeth, claws, and other fearsome characteristics, Owen intended it merely to evoke their size and majesty.

Other prehistoric animals, including mosasaurs, ichthyosaurs, pterosaurs, plesiosaurs, and "Dimetrodon", while often popularly conceived of as dinosaurs, are not taxonomically classified as dinosaurs.

Under phylogenetic nomenclature, dinosaurs are usually defined as the group consisting of the most recent common ancestor (MRCA) of "Triceratops" and Neornithes, and all its descendants. It has also been suggested that Dinosauria be defined with respect to the MRCA of "Megalosaurus" and "Iguanodon", because these were two of the three genera cited by Richard Owen when he recognized the Dinosauria. Both definitions result in the same set of animals being defined as dinosaurs: "Dinosauria = Ornithischia + Saurischia", encompassing ankylosaurians (armored herbivorous quadrupeds), stegosaurians (plated herbivorous quadrupeds), ceratopsians (herbivorous quadrupeds with horns and frills), ornithopods (bipedal or quadrupedal herbivores including "duck-bills"), theropods (mostly bipedal carnivores and birds), and sauropodomorphs (mostly large herbivorous quadrupeds with long necks and tails).

Birds are now recognized as being the sole surviving lineage of theropod dinosaurs. In traditional taxonomy, birds were considered a separate class that had evolved from dinosaurs, a distinct superorder. However, a majority of contemporary paleontologists concerned with dinosaurs reject the traditional style of classification in favor of phylogenetic taxonomy; this approach requires that, for a group to be natural, all descendants of members of the group must be included in the group as well. Birds are thus considered to be dinosaurs and dinosaurs are, therefore, not extinct. Birds are classified as belonging to the subgroup Maniraptora, which are coelurosaurs, which are theropods, which are saurischians, which are dinosaurs.

Research by Matthew Baron, David B. Norman, and Paul M. Barrett in 2017 suggested a radical revision of dinosaurian systematics. Phylogenetic analysis by Baron "et al." recovered the Ornithischia as being closer to the Theropoda than the Sauropodomorpha, as opposed to the traditional union of theropods with sauropodomorphs. They resurrected the clade Ornithoscelida to refer to the group containing Ornithischia and Theropoda. Dinosauria itself was re-defined as the last common ancestor of "Triceratops horridus", "Passer domesticus", "Diplodocus carnegii", and all of its descendants, to ensure that sauropods and kin remain included as dinosaurs.

Using one of the above definitions, dinosaurs can be generally described as archosaurs with hind limbs held erect beneath the body. Many prehistoric animal groups are popularly conceived of as dinosaurs, such as ichthyosaurs, mosasaurs, plesiosaurs, pterosaurs, and pelycosaurs (especially "Dimetrodon"), but are not classified scientifically as dinosaurs, and none had the erect hind limb posture characteristic of true dinosaurs. Dinosaurs were the dominant terrestrial vertebrates of the Mesozoic, especially the Jurassic and Cretaceous periods. Other groups of animals were restricted in size and niches; mammals, for example, rarely exceeded the size of a domestic cat, and were generally rodent-sized carnivores of small prey.

Dinosaurs have always been an extremely varied group of animals; according to a 2006 study, over 500 non-avian dinosaur genera have been identified with certainty so far, and the total number of genera preserved in the fossil record has been estimated at around 1850, nearly 75% of which remain to be discovered. An earlier study predicted that about 3,400 dinosaur genera existed, including many that would not have been preserved in the fossil record. By September 17, 2008, 1,047 different species of dinosaurs had been named.

In 2016, the estimated number of dinosaur species that existed in the Mesozoic era was estimated to be 1,543–2,468. Some are herbivorous, others carnivorous, including seed-eaters, fish-eaters, insectivores, and omnivores. While dinosaurs were ancestrally bipedal (as are all modern birds), some prehistoric species were quadrupeds, and others, such as "Anchisaurus" and "Iguanodon", could walk just as easily on two or four legs. Cranial modifications like horns and crests are common dinosaurian traits, and some extinct species had bony armor. Although known for large size, many Mesozoic dinosaurs were human-sized or smaller, and modern birds are generally small in size. Dinosaurs today inhabit every continent, and fossils show that they had achieved global distribution by at least the early Jurassic period. Modern birds inhabit most available habitats, from terrestrial to marine, and there is evidence that some non-avian dinosaurs (such as "Microraptor") could fly or at least glide, and others, such as spinosaurids, had semiaquatic habits.

While recent discoveries have made it more difficult to present a universally agreed-upon list of dinosaurs' distinguishing features, nearly all dinosaurs discovered so far share certain modifications to the ancestral archosaurian skeleton, or are clear descendants of older dinosaurs showing these modifications. Although some later groups of dinosaurs featured further modified versions of these traits, they are considered typical for Dinosauria; the earliest dinosaurs had them and passed them on to their descendants. Such modifications, originating in the most recent common ancestor of a certain taxonomic group, are called the synapomorphies of such a group.

A detailed assessment of archosaur interrelations by Sterling Nesbitt confirmed or found the following twelve unambiguous synapomorphies, some previously known:

Nesbitt found a number of further potential synapomorphies, and discounted a number of synapomorphies previously suggested. Some of these are also present in silesaurids, which Nesbitt recovered as a sister group to Dinosauria, including a large anterior trochanter, metatarsals II and IV of subequal length, reduced contact between ischium and pubis, the presence of a cnemial crest on the tibia and of an ascending process on the astragalus, and many others.

A variety of other skeletal features are shared by dinosaurs. However, because they are either common to other groups of archosaurs or were not present in all early dinosaurs, these features are not considered to be synapomorphies. For example, as diapsids, dinosaurs ancestrally had two pairs of temporal fenestrae (openings in the skull behind the eyes), and as members of the diapsid group Archosauria, had additional openings in the snout and lower jaw. Additionally, several characteristics once thought to be synapomorphies are now known to have appeared before dinosaurs, or were absent in the earliest dinosaurs and independently evolved by different dinosaur groups. These include an elongated scapula, or shoulder blade; a sacrum composed of three or more fused vertebrae (three are found in some other archosaurs, but only two are found in "Herrerasaurus"); and a perforate acetabulum, or hip socket, with a hole at the center of its inside surface (closed in "Saturnalia", for example). Another difficulty of determining distinctly dinosaurian features is that early dinosaurs and other archosaurs from the late Triassic are often poorly known and were similar in many ways; these animals have sometimes been misidentified in the literature.

Dinosaurs stand with their hind limbs erect in a manner similar to most modern mammals, but distinct from most other reptiles, whose limbs sprawl out to either side. This posture is due to the development of a laterally facing recess in the pelvis (usually an open socket) and a corresponding inwardly facing distinct head on the femur. Their erect posture enabled early dinosaurs to breathe easily while moving, which likely permitted stamina and activity levels that surpassed those of "sprawling" reptiles. Erect limbs probably also helped support the evolution of large size by reducing bending stresses on limbs. Some non-dinosaurian archosaurs, including rauisuchians, also had erect limbs but achieved this by a "pillar erect" configuration of the hip joint, where instead of having a projection from the femur insert on a socket on the hip, the upper pelvic bone was rotated to form an overhanging shelf.

Dinosaurs diverged from their archosaur ancestors during the middle to late Triassic period, roughly 20 million years after the Permian–Triassic extinction event wiped out an estimated 95% of all life on Earth. Radiometric dating of the rock formation that contained fossils from the early dinosaur genus "Eoraptor" at 231.4 million years old establishes its presence in the fossil record at this time. Paleontologists think that "Eoraptor" resembles the common ancestor of all dinosaurs; if this is true, its traits suggest that the first dinosaurs were small, bipedal predators. The discovery of primitive, dinosaur-like ornithodirans such as "Marasuchus" and "Lagerpeton" in Argentinian Middle Triassic strata supports this view; analysis of recovered fossils suggests that these animals were indeed small, bipedal predators. Dinosaurs may have appeared as early as 243 million years ago, as evidenced by remains of the genus "Nyasasaurus" from that period, though known fossils of these animals are too fragmentary to tell if they are dinosaurs or very close dinosaurian relatives. Recently, it has been determined that "Staurikosaurus" from the Santa Maria Formation dates to 233.23 Ma, making it older in geologic age than "Eoraptor".

When dinosaurs appeared, they were not the dominant terrestrial animals. The terrestrial habitats were occupied by various types of archosauromorphs and therapsids, like cynodonts and rhynchosaurs. Their main competitors were the pseudosuchia, such as aetosaurs, ornithosuchids and rauisuchians, which were more successful than the dinosaurs. Most of these other animals became extinct in the Triassic, in one of two events. First, at about 215 million years ago, a variety of basal archosauromorphs, including the protorosaurs, became extinct. This was followed by the Triassic–Jurassic extinction event (about 200 million years ago), that saw the end of most of the other groups of early archosaurs, like aetosaurs, ornithosuchids, phytosaurs, and rauisuchians. Rhynchosaurs and dicynodonts survived (at least in some areas) at least as late as early-mid Norian and early Rhaetian, respectively, and the exact date of their extinction is uncertain. These losses left behind a land fauna of crocodylomorphs, dinosaurs, mammals, pterosaurians, and turtles. The first few lines of early dinosaurs diversified through the Carnian and Norian stages of the Triassic, possibly by occupying the niches of the groups that became extinct. Also notably, there was a heightened rate of extinction during the Carnian Pluvial Event.

Dinosaur evolution after the Triassic follows changes in vegetation and the location of continents. In the late Triassic and early Jurassic, the continents were connected as the single landmass Pangaea, and there was a worldwide dinosaur fauna mostly composed of coelophysoid carnivores and early sauropodomorph herbivores. Gymnosperm plants (particularly conifers), a potential food source, radiated in the late Triassic. Early sauropodomorphs did not have sophisticated mechanisms for processing food in the mouth, and so must have employed other means of breaking down food farther along the digestive tract. The general homogeneity of dinosaurian faunas continued into the middle and late Jurassic, where most localities had predators consisting of ceratosaurians, spinosauroids, and carnosaurians, and herbivores consisting of stegosaurian ornithischians and large sauropods. Examples of this include the Morrison Formation of North America and Tendaguru Beds of Tanzania. Dinosaurs in China show some differences, with specialized sinraptorid theropods and unusual, long-necked sauropods like "Mamenchisaurus". Ankylosaurians and ornithopods were also becoming more common, but prosauropods had become extinct. Conifers and pteridophytes were the most common plants. Sauropods, like the earlier prosauropods, were not oral processors, but ornithischians were evolving various means of dealing with food in the mouth, including potential cheek-like organs to keep food in the mouth, and jaw motions to grind food. Another notable evolutionary event of the Jurassic was the appearance of true birds, descended from maniraptoran coelurosaurians.

By the early Cretaceous and the ongoing breakup of Pangaea, dinosaurs were becoming strongly differentiated by landmass. The earliest part of this time saw the spread of ankylosaurians, iguanodontians, and brachiosaurids through Europe, North America, and northern Africa. These were later supplemented or replaced in Africa by large spinosaurid and carcharodontosaurid theropods, and rebbachisaurid and titanosaurian sauropods, also found in South America. In Asia, maniraptoran coelurosaurians like dromaeosaurids, troodontids, and oviraptorosaurians became the common theropods, and ankylosaurids and early ceratopsians like "Psittacosaurus" became important herbivores. Meanwhile, Australia was home to a fauna of basal ankylosaurians, hypsilophodonts, and iguanodontians. The stegosaurians appear to have gone extinct at some point in the late early Cretaceous or early late Cretaceous. A major change in the early Cretaceous, which would be amplified in the late Cretaceous, was the evolution of flowering plants. At the same time, several groups of dinosaurian herbivores evolved more sophisticated ways to orally process food. Ceratopsians developed a method of slicing with teeth stacked on each other in batteries, and iguanodontians refined a method of grinding with tooth batteries, taken to its extreme in hadrosaurids. Some sauropods also evolved tooth batteries, best exemplified by the rebbachisaurid "Nigersaurus".

There were three general dinosaur faunas in the late Cretaceous. In the northern continents of North America and Asia, the major theropods were tyrannosaurids and various types of smaller maniraptoran theropods, with a predominantly ornithischian herbivore assemblage of hadrosaurids, ceratopsians, ankylosaurids, and pachycephalosaurians. In the southern continents that had made up the now-splitting Gondwana, abelisaurids were the common theropods, and titanosaurian sauropods the common herbivores. Finally, in Europe, dromaeosaurids, rhabdodontid iguanodontians, nodosaurid ankylosaurians, and titanosaurian sauropods were prevalent. Flowering plants were greatly radiating, with the first grasses appearing by the end of the Cretaceous. Grinding hadrosaurids and shearing ceratopsians became extremely diverse across North America and Asia. Theropods were also radiating as herbivores or omnivores, with therizinosaurians and ornithomimosaurians becoming common.

The Cretaceous–Paleogene extinction event, which occurred approximately 66 million years ago at the end of the Cretaceous period, caused the extinction of all dinosaur groups except for the neornithine birds. Some other diapsid groups, such as crocodilians, sebecosuchians, turtles, lizards, snakes, sphenodontians, and choristoderans, also survived the event.

The surviving lineages of neornithine birds, including the ancestors of modern ratites, ducks and chickens, and a variety of waterbirds, diversified rapidly at the beginning of the Paleogene period, entering ecological niches left vacant by the extinction of Mesozoic dinosaur groups such as the arboreal enantiornithines, aquatic hesperornithines, and even the larger terrestrial theropods (in the form of "Gastornis", eogruiids, bathornithids, ratites, geranoidids, mihirungs, and "terror birds"). It is often cited that mammals out-competed the neornithines for dominance of most terrestrial niches but many of these groups co-existed with rich mammalian faunas for most of the Cenozoic. Terror birds and bathornithids occupied carnivorous guilds alongside predatory mammals, and ratites are still fairly successful as mid-sized herbivores; eogruiids similarly lasted from the Eocene to Pliocene, only becoming extinct very recently after over 20 million years of co-existence with many mammal groups.

Dinosaurs belong to a group known as archosaurs, which also includes modern crocodilians. Within the archosaur group, dinosaurs are differentiated most noticeably by their gait. Dinosaur legs extend directly beneath the body, whereas the legs of lizards and crocodilians sprawl out to either side.

Collectively, dinosaurs as a clade are divided into two primary branches, Saurischia and Ornithischia. Saurischia includes those taxa sharing a more recent common ancestor with birds than with Ornithischia, while Ornithischia includes all taxa sharing a more recent common ancestor with "Triceratops" than with Saurischia. Anatomically, these two groups can be distinguished most noticeably by their pelvic structure. Early saurischians—"lizard-hipped", from the Greek "sauros" (σαῦρος) meaning "lizard" and "ischion" (ἰσχίον) meaning "hip joint"—retained the hip structure of their ancestors, with a pubis bone directed cranially, or forward. This basic form was modified by rotating the pubis backward to varying degrees in several groups ("Herrerasaurus", therizinosauroids, dromaeosaurids, and birds). Saurischia includes the theropods (exclusively bipedal and with a wide variety of diets) and sauropodomorphs (long-necked herbivores which include advanced, quadrupedal groups).

By contrast, ornithischians—"bird-hipped", from the Greek "ornitheios" (ὀρνίθειος) meaning "of a bird" and "ischion" (ἰσχίον) meaning "hip joint"—had a pelvis that superficially resembled a bird's pelvis: the pubic bone was oriented caudally (rear-pointing). Unlike birds, the ornithischian pubis also usually had an additional forward-pointing process. Ornithischia includes a variety of species which were primarily herbivores. (NB: the terms "lizard hip" and "bird hip" are misnomers – birds evolved from dinosaurs with "lizard hips".)

The following is a simplified classification of dinosaur groups based on their evolutionary relationships, and organized based on the list of Mesozoic dinosaur species provided by Holtz (2007). A more detailed version can be found at Dinosaur classification.
The dagger (†) is used to signify groups with no living members.


Knowledge about dinosaurs is derived from a variety of fossil and non-fossil records, including fossilized bones, feces, trackways, gastroliths, feathers, impressions of skin, internal organs and soft tissues. Many fields of study contribute to our understanding of dinosaurs, including physics (especially biomechanics), chemistry, biology, and the earth sciences (of which paleontology is a sub-discipline). Two topics of particular interest and study have been dinosaur size and behavior.

Current evidence suggests that dinosaur average size varied through the Triassic, early Jurassic, late Jurassic and Cretaceous periods. Predatory theropod dinosaurs, which occupied most terrestrial carnivore niches during the Mesozoic, most often fall into the category when sorted by estimated weight into categories based on order of magnitude, whereas recent predatory carnivoran mammals peak in the category. The mode of Mesozoic dinosaur body masses is between one and ten metric tonnes. This contrasts sharply with the size of Cenozoic mammals, estimated by the National Museum of Natural History as about .

The sauropods were the largest and heaviest dinosaurs. For much of the dinosaur era, the smallest sauropods were larger than anything else in their habitat, and the largest were an order of magnitude more massive than anything else that has since walked the Earth. Giant prehistoric mammals such as "Paraceratherium" (the largest land mammal ever) were dwarfed by the giant sauropods, and only modern whales approach or surpass them in size. There are several proposed advantages for the large size of sauropods, including protection from predation, reduction of energy use, and longevity, but it may be that the most important advantage was dietary. Large animals are more efficient at digestion than small animals, because food spends more time in their digestive systems. This also permits them to subsist on food with lower nutritive value than smaller animals. Sauropod remains are mostly found in rock formations interpreted as dry or seasonally dry, and the ability to eat large quantities of low-nutrient browse would have been advantageous in such environments.

Scientists will probably never be certain of the largest and smallest dinosaurs to have ever existed. This is because only a tiny percentage of animals ever fossilize, and most of these remain buried in the earth. Few of the specimens that are recovered are complete skeletons, and impressions of skin and other soft tissues are rare. Rebuilding a complete skeleton by comparing the size and morphology of bones to those of similar, better-known species is an inexact art, and reconstructing the muscles and other organs of the living animal is, at best, a process of educated guesswork.
The tallest and heaviest dinosaur known from good skeletons is "Giraffatitan brancai" (previously classified as a species of "Brachiosaurus"). Its remains were discovered in Tanzania between 1907 and 1912. Bones from several similar-sized individuals were incorporated into the skeleton now mounted and on display at the Museum für Naturkunde Berlin; this mount is tall and long, and would have belonged to an animal that weighed between and  kilograms ( and  lb). The longest complete dinosaur is the long "Diplodocus", which was discovered in Wyoming in the United States and displayed in Pittsburgh's Carnegie Natural History Museum in 1907. The longest dinosaur known from good fossil material is the "Patagotitan": the skeleton mount in the American Museum of Natural History is long. The Carmen Funes Museum has an "Argentinosaurus" reconstructed skeleton mount long.

There were larger dinosaurs, but knowledge of them is based entirely on a small number of fragmentary fossils. Most of the largest herbivorous specimens on record were discovered in the 1970s or later, and include the massive "Argentinosaurus", which may have weighed to  kilograms (90 to 110 short tons) and reached length of ; some of the longest were the long "Diplodocus hallorum" (formerly "Seismosaurus"), the long "Supersaurus" and long "Patagotitan"; and the tallest, the tall "Sauroposeidon", which could have reached a sixth-floor window. The heaviest and longest dinosaur may have been "Amphicoelias fragillimus", known only from a now lost partial vertebral neural arch described in 1878. Extrapolating from the illustration of this bone, the animal may have been long and weighed kg ( lb). However, as no further evidence of sauropods of this size has been found, and the discoverer, Edward Cope, had made typographic errors before, it is likely to have been an extreme overestimation. As of 2018, "Argentinosaurus" and "Patagotitan" are considered by paleontologists as the largest dinosaurs known from reasonable remains.

The largest carnivorous dinosaur was "Spinosaurus", reaching a length of , and weighing 7–20.9 tonnes (7.7–23 short tons). Other large carnivorous theropods included "Giganotosaurus", "Carcharodontosaurus" and "Tyrannosaurus". "Therizinosaurus" and "Deinocheirus" were among the tallest of the theropods. The largest Ornithischian dinosaur was probably the hadrosaurid "Shantungosaurus" which measured and weighed about .

The smallest dinosaur known is the bee hummingbird, with a length of only and mass of around . The smallest known non-avialan dinosaurs were about the size of pigeons and were those theropods most closely related to birds. For example, "Anchiornis huxleyi" is currently the smallest non-avialan dinosaur described from an adult specimen, with an estimated weight of 110 grams and a total skeletal length of . The smallest herbivorous non-avialan dinosaurs included "Microceratus" and "Wannanosaurus", at about long each.

Many modern birds are highly social, often found living in flocks. There is general agreement that some behaviors that are common in birds, as well as in crocodiles (birds' closest living relatives), were also common among extinct dinosaur groups. Interpretations of behavior in fossil species are generally based on the pose of skeletons and their habitat, computer simulations of their biomechanics, and comparisons with modern animals in similar ecological niches.

The first potential evidence for herding or flocking as a widespread behavior common to many dinosaur groups in addition to birds was the 1878 discovery of 31 "Iguanodon bernissartensis", ornithischians that were then thought to have perished together in Bernissart, Belgium, after they fell into a deep, flooded sinkhole and drowned. Other mass-death sites have been discovered subsequently. Those, along with multiple trackways, suggest that gregarious behavior was common in many early dinosaur species. Trackways of hundreds or even thousands of herbivores indicate that duck-bills (hadrosaurids) may have moved in great herds, like the American bison or the African Springbok. Sauropod tracks document that these animals traveled in groups composed of several different species, at least in Oxfordshire, England, although there is no evidence for specific herd structures. Congregating into herds may have evolved for defense, for migratory purposes, or to provide protection for young. There is evidence that many types of slow-growing dinosaurs, including various theropods, sauropods, ankylosaurians, ornithopods, and ceratopsians, formed aggregations of immature individuals. One example is a site in Inner Mongolia that has yielded the remains of over 20 "Sinornithomimus", from one to seven years old. This assemblage is interpreted as a social group that was trapped in mud. The interpretation of dinosaurs as gregarious has also extended to depicting carnivorous theropods as pack hunters working together to bring down large prey. However, this lifestyle is uncommon among modern birds, crocodiles, and other reptiles, and the taphonomic evidence suggesting mammal-like pack hunting in such theropods as "Deinonychus" and "Allosaurus" can also be interpreted as the results of fatal disputes between feeding animals, as is seen in many modern diapsid predators.
The crests and frills of some dinosaurs, like the marginocephalians, theropods and lambeosaurines, may have been too fragile to be used for active defense, and so they were likely used for sexual or aggressive displays, though little is known about dinosaur mating and territorialism. Head wounds from bites suggest that theropods, at least, engaged in active aggressive confrontations.

From a behavioral standpoint, one of the most valuable dinosaur fossils was discovered in the Gobi Desert in 1971. It included a "Velociraptor" attacking a "Protoceratops", providing evidence that dinosaurs did indeed attack each other. Additional evidence for attacking live prey is the partially healed tail of an "Edmontosaurus", a hadrosaurid dinosaur; the tail is damaged in such a way that shows the animal was bitten by a tyrannosaur but survived. Cannibalism amongst some species of dinosaurs was confirmed by tooth marks found in Madagascar in 2003, involving the theropod "Majungasaurus".

Comparisons between the scleral rings of dinosaurs and modern birds and reptiles have been used to infer daily activity patterns of dinosaurs. Although it has been suggested that most dinosaurs were active during the day, these comparisons have shown that small predatory dinosaurs such as dromaeosaurids, "Juravenator", and "Megapnosaurus" were likely nocturnal. Large and medium-sized herbivorous and omnivorous dinosaurs such as ceratopsians, sauropodomorphs, hadrosaurids, ornithomimosaurs may have been cathemeral, active during short intervals throughout the day, although the small ornithischian "Agilisaurus" was inferred to be diurnal.

Based on current fossil evidence from dinosaurs such as "Oryctodromeus", some ornithischian species seem to have led a partially fossorial (burrowing) lifestyle. Many modern birds are arboreal (tree climbing), and this was also true of many Mesozoic birds, especially the enantiornithines. While some early bird-like species may have already been arboreal as well (including dromaeosaurids such as "Microraptor") most non-avialan dinosaurs seem to have relied on land-based locomotion. A good understanding of how dinosaurs moved on the ground is key to models of dinosaur behavior; the science of biomechanics, pioneered by Robert McNeill Alexander, has provided significant insight in this area. For example, studies of the forces exerted by muscles and gravity on dinosaurs' skeletal structure have investigated how fast dinosaurs could run, whether diplodocids could create sonic booms via whip-like tail snapping, and whether sauropods could float.

Modern birds are known to communicate using visual and auditory signals, and the wide diversity of visual display structures among fossil dinosaur groups, such as horns, frills, crests, sails and feathers, suggests that visual communication has always been important in dinosaur biology. Reconstruction of the plumage color of "Anchiornis huxleyi", suggest the importance of color in visual communication in non-avian dinosaurs. The evolution of dinosaur vocalization is less certain. Paleontologist Phil Senter suggests that non-avian dinosaurs relied mostly on visual displays and possibly non-vocal acoustic sounds like hissing, jaw grinding or clapping, splashing and wing beating (possible in winged maniraptoran dinosaurs). He states they were unlikely to have been capable of vocalizing since their closest relatives, crocodilians and birds, use different means to vocalize, the former via the larynx and the latter through the unique syrinx, suggesting they evolved independently and their common ancestor was mute.

The earliest remains of a syrinx, which has enough mineral content for fossilization, was found in a specimen of the duck-like "Vegavis iaai" dated 69-66 million year ago, and this organ is unlikely to have existed in non-avian dinosaurs. However, in contrast to Senter, the researchers have suggested that dinosaurs could vocalize and that the syrinx-based vocal system of birds evolved from a larynx-based one, rather than the two systems evolving independently. A 2016 study suggests that dinosaurs produced closed mouth vocalizations like cooing, which occur in both crocodilians and birds as well as other reptiles. Such vocalizations evolved independently in extant archosaurs numerous times, following increases in body size. The crests of the Lambeosaurini and nasal chambers of ankylosaurids have been suggested to function in vocal resonance, though Senter states that the presence of resonance chambers in some dinosaurs is not necessarily evidence of vocalization as modern snakes have such chambers which intensify their hisses.

All dinosaurs lay amniotic eggs with hard shells made mostly of calcium carbonate. Eggs are usually laid in a nest. Most species create somewhat elaborate nests, which can be cups, domes, plates, beds scrapes, mounds, or burrows. Some species of modern bird have no nests; the cliff-nesting common guillemot lays its eggs on bare rock, and male emperor penguins keep eggs between their body and feet. Primitive birds and many non-avialan dinosaurs often lay eggs in communal nests, with males primarily incubating the eggs. While modern birds have only one functional oviduct and lay one egg at a time, more primitive birds and dinosaurs had two oviducts, like crocodiles. Some non-avialan dinosaurs, such as "Troodon", exhibited iterative laying, where the adult might lay a pair of eggs every one or two days, and then ensured simultaneous hatching by delaying brooding until all eggs were laid.

When laying eggs, females grow a special type of bone between the hard outer bone and the marrow of their limbs. This medullary bone, which is rich in calcium, is used to make eggshells. A discovery of features in a "Tyrannosaurus rex" skeleton provided evidence of medullary bone in extinct dinosaurs and, for the first time, allowed paleontologists to establish the sex of a fossil dinosaur specimen. Further research has found medullary bone in the carnosaur "Allosaurus" and the ornithopod "Tenontosaurus". Because the line of dinosaurs that includes "Allosaurus" and "Tyrannosaurus" diverged from the line that led to "Tenontosaurus" very early in the evolution of dinosaurs, this suggests that the production of medullary tissue is a general characteristic of all dinosaurs.
Another widespread trait among modern birds (but see below in regards to fossil groups and extant megapodes) is parental care for young after hatching. Jack Horner's 1978 discovery of a "Maiasaura" ("good mother lizard") nesting ground in Montana demonstrated that parental care continued long after birth among ornithopods. A specimen of the Mongolian oviraptorid "Citipati osmolskae" was discovered in a chicken-like brooding position in 1993, which may indicate that they had begun using an insulating layer of feathers to keep the eggs warm. A dinosaur embryo (pertaining to the prosauropod "Massospondylus") was found without teeth, indicating that some parental care was required to feed the young dinosaurs. Trackways have also confirmed parental behavior among ornithopods from the Isle of Skye in northwestern Scotland.

However, there is ample evidence of supreprecociality among many dinosaur species, particularly theropods. For instance, non-ornithuromorph birds have been abundantly demonstrated to have had slow growth rates, megapode-like egg burying behaviour and the ability to fly soon after birth. Both "Tyrannosaurus rex" and "Troodon formosus" display juveniles with clear supreprecociality and likely occupying different ecological niches than the adults. Superprecociality has been inferred for sauropods.

Because both modern crocodilians and birds have four-chambered hearts (albeit modified in crocodilians), it is likely that this is a trait shared by all archosaurs, including all dinosaurs. While all modern birds have high metabolisms and are "warm blooded" (endothermic), a vigorous debate has been ongoing since the 1960s regarding how far back in the dinosaur lineage this trait extends. Scientists disagree as to whether non-avian dinosaurs were endothermic, ectothermic, or some combination of both.

After non-avian dinosaurs were discovered, paleontologists first posited that they were ectothermic. This supposed "cold-bloodedness" was used to imply that the ancient dinosaurs were relatively slow, sluggish organisms, even though many modern reptiles are fast and light-footed despite relying on external sources of heat to regulate their body temperature. The idea of dinosaurs as ectothermic and sluggish remained a prevalent view until Robert T. "Bob" Bakker, an early proponent of dinosaur endothermy, published an influential paper on the topic in 1968.

Modern evidence indicates that even non-avian dinosaurs and birds thrived in cooler temperate climates, and that at least some early species must have regulated their body temperature by internal biological means (aided by the animals' bulk in large species and feathers or other body coverings in smaller species). Evidence of endothermy in Mesozoic dinosaurs includes the discovery of polar dinosaurs in Australia and Antarctica as well as analysis of blood-vessel structures within fossil bones that are typical of endotherms. Scientific debate continues regarding the specific ways in which dinosaur temperature regulation evolved.

In saurischian dinosaurs, higher metabolisms were supported by the evolution of the avian respiratory system, characterized by an extensive system of air sacs that extended the lungs and invaded many of the bones in the skeleton, making them hollow. Early avian-style respiratory systems with air sacs may have been capable of sustaining higher activity levels than those of mammals of similar size and build. In addition to providing a very efficient supply of oxygen, the rapid airflow would have been an effective cooling mechanism, which is essential for animals that are active but too large to get rid of all the excess heat through their skin.

Like other reptiles, dinosaurs are primarily uricotelic, that is, their kidneys extract nitrogenous wastes from their bloodstream and excrete it as uric acid instead of urea or ammonia via the ureters into the intestine. In most living species, uric acid is excreted along with feces as a semisolid waste. However, at least some modern birds (such as hummingbirds) can be facultatively ammonotelic, excreting most of the nitrogenous wastes as ammonia. They also excrete creatine, rather than creatinine like mammals. This material, as well as the output of the intestines, emerges from the cloaca. In addition, many species regurgitate pellets, and fossil pellets that may have come from dinosaurs are known from as long ago as the Cretaceous period.

The possibility that dinosaurs were the ancestors of birds was first suggested in 1868 by Thomas Henry Huxley. After the work of Gerhard Heilmann in the early 20th century, the theory of birds as dinosaur descendants was abandoned in favor of the idea of their being descendants of generalized thecodonts, with the key piece of evidence being the supposed lack of clavicles in dinosaurs. However, as later discoveries showed, clavicles (or a single fused wishbone, which derived from separate clavicles) were not actually absent; they had been found as early as 1924 in "Oviraptor", but misidentified as an interclavicle. In the 1970s, John Ostrom revived the dinosaur–bird theory, which gained momentum in the coming decades with the advent of cladistic analysis, and a great increase in the discovery of small theropods and early birds. Of particular note have been the fossils of the Yixian Formation, where a variety of theropods and early birds have been found, often with feathers of some type. Birds share over a hundred distinct anatomical features with theropod dinosaurs, which are now generally accepted to have been their closest ancient relatives.
They are most closely allied with maniraptoran coelurosaurs. A minority of scientists, most notably Alan Feduccia and Larry Martin, have proposed other evolutionary paths, including revised versions of Heilmann's basal archosaur proposal, or that maniraptoran theropods are the ancestors of birds but themselves are not dinosaurs, only convergent with dinosaurs.

Feathers are one of the most recognizable characteristics of modern birds, and a trait that was shared by all other dinosaur groups. Based on the current distribution of fossil evidence, it appears that feathers were an ancestral dinosaurian trait, though one that may have been selectively lost in some species. Direct fossil evidence of feathers or feather-like structures has been discovered in a diverse array of species in many non-avian dinosaur groups, both among saurischians and ornithischians. Simple, branched, feather-like structures are known from heterodontosaurids, primitive neornithischians and theropods, and primitive ceratopsians. Evidence for true, vaned feathers similar to the flight feathers of modern birds has been found only in the theropod subgroup Maniraptora, which includes oviraptorosaurs, troodontids, dromaeosaurids, and birds. Feather-like structures known as pycnofibres have also been found in pterosaurs, suggesting the possibility that feather-like filaments may have been common in the bird lineage and evolved before the appearance of dinosaurs themselves. Research into the genetics of American alligators has also revealed that crocodylian scutes do possess feather-keratins during embryonic development, but these keratins are not expressed by the animals before hatching.

"Archaeopteryx" was the first fossil found that revealed a potential connection between dinosaurs and birds. It is considered a transitional fossil, in that it displays features of both groups. Brought to light just two years after Darwin's seminal "The Origin of Species", its discovery spurred the nascent debate between proponents of evolutionary biology and creationism. This early bird is so dinosaur-like that, without a clear impression of feathers in the surrounding rock, at least one specimen was mistaken for "Compsognathus". Since the 1990s, a number of additional feathered dinosaurs have been found, providing even stronger evidence of the close relationship between dinosaurs and modern birds. Most of these specimens were unearthed in the lagerstätte of the Yixian Formation, Liaoning, northeastern China, which was part of an island continent during the Cretaceous. Though feathers have been found in only a few locations, it is possible that non-avian dinosaurs elsewhere in the world were also feathered. The lack of widespread fossil evidence for feathered non-avian dinosaurs may be because delicate features like skin and feathers are not often preserved by fossilization and thus are absent from the fossil record.

The description of feathered dinosaurs has not been without controversy; perhaps the most vocal critics have been Alan Feduccia and Theagarten Lingham-Soliar, who have proposed that some purported feather-like fossils are the result of the decomposition of collagenous fiber that underlaid the dinosaurs' skin, and that maniraptoran dinosaurs with vaned feathers were not actually dinosaurs, but convergent with dinosaurs. However, their views have for the most part not been accepted by other researchers, to the point that the scientific nature of Feduccia's proposals has been questioned.

In 2016, it was reported that a dinosaur tail with feathers had been found enclosed in amber. The fossil is about 99 million years old.

Because feathers are often associated with birds, feathered dinosaurs are often touted as the missing link between birds and dinosaurs. However, the multiple skeletal features also shared by the two groups represent another important line of evidence for paleontologists. Areas of the skeleton with important similarities include the neck, pubis, wrist (semi-lunate carpal), arm and pectoral girdle, furcula (wishbone), and breast bone. Comparison of bird and dinosaur skeletons through cladistic analysis strengthens the case for the link.

Large meat-eating dinosaurs had a complex system of air sacs similar to those found in modern birds, according to a 2005 investigation led by Patrick M. O'Connor. The lungs of theropod dinosaurs (carnivores that walked on two legs and had bird-like feet) likely pumped air into hollow sacs in their skeletons, as is the case in birds. "What was once formally considered unique to birds was present in some form in the ancestors of birds", O'Connor said. In 2008, scientists described "Aerosteon riocoloradensis", the skeleton of which supplies the strongest evidence to date of a dinosaur with a bird-like breathing system. CT-scanning of "Aerosteon"'s fossil bones revealed evidence for the existence of air sacs within the animal's body cavity.

Fossils of the troodonts "Mei" and "Sinornithoides" demonstrate that some dinosaurs slept with their heads tucked under their arms. This behavior, which may have helped to keep the head warm, is also characteristic of modern birds. Several deinonychosaur and oviraptorosaur specimens have also been found preserved on top of their nests, likely brooding in a bird-like manner. The ratio between egg volume and body mass of adults among these dinosaurs suggest that the eggs were primarily brooded by the male, and that the young were highly precocial, similar to many modern ground-dwelling birds.

Some dinosaurs are known to have used gizzard stones like modern birds. These stones are swallowed by animals to aid digestion and break down food and hard fibers once they enter the stomach. When found in association with fossils, gizzard stones are called gastroliths.

The discovery that birds are a type of dinosaur showed that dinosaurs in general are not, in fact, extinct as is commonly stated. However, all non-avian dinosaurs, estimated to have been 628-1078 species, as well as many groups of birds did suddenly become extinct approximately 66 million years ago. It has been suggested that because small mammals, squamata and birds occupied the ecological niches suited for small body size, non-avian dinosaurs never evolved a diverse fauna of small-bodied species, which led to their downfall when large-bodied terrestrial tetrapods were hit by the mass extinction event. Many other groups of animals also became extinct at this time, including ammonites (nautilus-like mollusks), mosasaurs, plesiosaurs, pterosaurs, and many groups of mammals. Significantly, the insects suffered no discernible population loss, which left them available as food for other survivors. This mass extinction is known as the Cretaceous–Paleogene extinction event. The nature of the event that caused this mass extinction has been extensively studied since the 1970s; at present, several related theories are supported by paleontologists. Though the consensus is that an impact event was the primary cause of dinosaur extinction, some scientists cite other possible causes, or support the idea that a confluence of several factors was responsible for the sudden disappearance of dinosaurs from the fossil record.

The asteroid collision theory, which was brought to wide attention in 1980 by Walter Alvarez and colleagues, links the extinction event at the end of the Cretaceous period to a bolide impact approximately 66 million years ago. Alvarez "et al." proposed that a sudden increase in iridium levels, recorded around the world in the period's rock stratum, was direct evidence of the impact. The bulk of the evidence now suggests that a bolide wide hit in the vicinity of the Yucatán Peninsula (in southeastern Mexico), creating the approximately Chicxulub Crater and triggering the mass extinction. Scientists are not certain whether dinosaurs were thriving or declining before the impact event. Some scientists propose that the meteorite impact caused a long and unnatural drop in Earth's atmospheric temperature, while others claim that it would have instead created an unusual heat wave. The consensus among scientists who support this theory is that the impact caused extinctions both directly (by heat from the meteorite impact) and also indirectly (via a worldwide cooling brought about when matter ejected from the impact crater reflected thermal radiation from the sun). Although the speed of extinction cannot be deduced from the fossil record alone, various models suggest that the extinction was extremely rapid, being down to hours rather than years.

Before 2000, arguments that the Deccan Traps flood basalts caused the extinction were usually linked to the view that the extinction was gradual, as the flood basalt events were thought to have started around 68 million years ago and lasted for over 2 million years. However, there is evidence that two thirds of the Deccan Traps were created in only 1 million years about 66 million years ago, and so these eruptions would have caused a fairly rapid extinction, possibly over a period of thousands of years, but still longer than would be expected from a single impact event.

The Deccan Traps in India could have caused extinction through several mechanisms, including the release into the air of dust and sulfuric aerosols, which might have blocked sunlight and thereby reduced photosynthesis in plants. In addition, Deccan Trap volcanism might have resulted in carbon dioxide emissions, which would have increased the greenhouse effect when the dust and aerosols cleared from the atmosphere. Before the mass extinction of the dinosaurs, the release of volcanic gases during the formation of the Deccan Traps "contributed to an apparently massive global warming. Some data point to an average rise in temperature of in the last half million years before the impact [at Chicxulub]."

In the years when the Deccan Traps theory was linked to a slower extinction, Luis Alvarez (who died in 1988) replied that paleontologists were being misled by sparse data. While his assertion was not initially well-received, later intensive field studies of fossil beds lent weight to his claim. Eventually, most paleontologists began to accept the idea that the mass extinctions at the end of the Cretaceous were largely or at least partly due to a massive Earth impact. However, even Walter Alvarez has acknowledged that there were other major changes on Earth even before the impact, such as a drop in sea level and massive volcanic eruptions that produced the Indian Deccan Traps, and these may have contributed to the extinctions.

Non-avian dinosaur remains are occasionally found above the Cretaceous–Paleogene boundary. In 2001, paleontologists Zielinski and Budahn reported the discovery of a single hadrosaur leg-bone fossil in the San Juan Basin, New Mexico, and described it as evidence of Paleocene dinosaurs. The formation in which the bone was discovered has been dated to the early Paleocene epoch, approximately 64.5 million years ago. If the bone was not re-deposited into that stratum by weathering action, it would provide evidence that some dinosaur populations may have survived at least a half million years into the Cenozoic Era. Other evidence includes the finding of dinosaur remains in the Hell Creek Formation up to above the Cretaceous–Paleogene boundary, representing  years of elapsed time. Similar reports have come from other parts of the world, including China. Many scientists, however, dismissed the supposed Paleocene dinosaurs as re-worked, that is, washed out of their original locations and then re-buried in much later sediments. Direct dating of the bones themselves has supported the later date, with U–Pb dating methods resulting in a precise age of 64.8 ± 0.9 million years ago. If correct, the presence of a handful of dinosaurs in the early Paleocene would not change the underlying facts of the extinction.

Dinosaur fossils have been known for millennia, although their true nature was not recognized. The Chinese, whose modern word for dinosaur is "kǒnglóng" (恐龍, or "terrible dragon"), considered them to be dragon bones and documented them as such. For example, "Hua Yang Guo Zhi", a book written by Chang Qu during the Western Jin Dynasty (265–316), reported the discovery of dragon bones at Wucheng in Sichuan Province. Villagers in central China have long unearthed fossilized "dragon bones" for use in traditional medicines, a practice that continues today. In Europe, dinosaur fossils were generally believed to be the remains of giants and other biblical creatures.

Scholarly descriptions of what would now be recognized as dinosaur bones first appeared in the late 17th century in England. Part of a bone, now known to have been the femur of a "Megalosaurus", was recovered from a limestone quarry at Cornwell near Chipping Norton, Oxfordshire, in 1676. The fragment was sent to Robert Plot, Professor of Chemistry at the University of Oxford and first curator of the Ashmolean Museum, who published a description in his "Natural History of Oxfordshire" in 1677. He correctly identified the bone as the lower extremity of the femur of a large animal, and recognized that it was too large to belong to any known species. He therefore concluded it to be the thigh bone of a giant human similar to those mentioned in the Bible. In 1699, Edward Lhuyd, a friend of Sir Isaac Newton, was responsible for the first published scientific treatment of what would now be recognized as a dinosaur when he described and named a sauropod tooth, "Rutellum implicatum", that had been found in Caswell, near Witney, Oxfordshire.
Between 1815 and 1824, the Rev William Buckland, a professor of geology at Oxford, collected more fossilized bones of "Megalosaurus" and became the first person to describe a dinosaur in a scientific journal. The second dinosaur genus to be identified, "Iguanodon", was discovered in 1822 by Mary Ann Mantell – the wife of English geologist Gideon Mantell. Gideon Mantell recognized similarities between his fossils and the bones of modern iguanas. He published his findings in 1825.

The study of these "great fossil lizards" soon became of great interest to European and American scientists, and in 1842 the English paleontologist Richard Owen coined the term "dinosaur". He recognized that the remains that had been found so far, "Iguanodon", "Megalosaurus" and "Hylaeosaurus", shared a number of distinctive features, and so decided to present them as a distinct taxonomic group. With the backing of Prince Albert, the husband of Queen Victoria, Owen established the Natural History Museum, London, to display the national collection of dinosaur fossils and other biological and geological exhibits.

In 1858, William Parker Foulke discovered the first known American dinosaur, in marl pits in the small town of Haddonfield, New Jersey. (Although fossils had been found before, their nature had not been correctly discerned.) The creature was named "Hadrosaurus foulkii". It was an extremely important find: "Hadrosaurus" was one of the first nearly complete dinosaur skeletons found (the first was in 1834, in Maidstone, England), and it was clearly a bipedal creature. This was a revolutionary discovery as, until that point, most scientists had believed dinosaurs walked on four feet, like other lizards. Foulke's discoveries sparked a wave of dinosaur mania in the United States.

Dinosaur mania was exemplified by the fierce rivalry between Edward Drinker Cope and Othniel Charles Marsh, both of whom raced to be the first to find new dinosaurs in what came to be known as the Bone Wars. The feud probably originated when Marsh publicly pointed out that Cope's reconstruction of an "Elasmosaurus" skeleton was flawed: Cope had inadvertently placed the plesiosaur's head at what should have been the animal's tail end. The fight between the two scientists lasted for over 30 years, ending in 1897 when Cope died after spending his entire fortune on the dinosaur hunt. Marsh 'won' the contest primarily because he was better funded through a relationship with the US Geological Survey. Unfortunately, many valuable dinosaur specimens were damaged or destroyed due to the pair's rough methods: for example, their diggers often used dynamite to unearth bones (a method modern paleontologists would find appalling). Despite their unrefined methods, the contributions of Cope and Marsh to paleontology were vast: Marsh unearthed 86 new species of dinosaur and Cope discovered 56, a total of 142 new species. Cope's collection is now at the American Museum of Natural History in New York, while Marsh's is on display at the Peabody Museum of Natural History at Yale University.

After 1897, the search for dinosaur fossils extended to every continent, including Antarctica. The first Antarctic dinosaur to be discovered, the ankylosaurid "Antarctopelta oliveroi", was found on James Ross Island in 1986, although it was 1994 before an Antarctic species, the theropod "Cryolophosaurus ellioti", was formally named and described in a scientific journal.

Current dinosaur "hot spots" include southern South America (especially Argentina) and China. China in particular has produced many exceptional feathered dinosaur specimens due to the unique geology of its dinosaur beds, as well as an ancient arid climate particularly conducive to fossilization.

The field of dinosaur research has enjoyed a surge in activity that began in the 1970s and is ongoing. This was triggered, in part, by John Ostrom's discovery of "Deinonychus", an active predator that may have been warm-blooded, in marked contrast to the then-prevailing image of dinosaurs as sluggish and cold-blooded. Vertebrate paleontology has become a global science. Major new dinosaur discoveries have been made by paleontologists working in previously unexploited regions, including India, South America, Madagascar, Antarctica, and most significantly China (the amazingly well-preserved feathered dinosaurs in China have further consolidated the link between dinosaurs and their living descendants, modern birds). The widespread application of cladistics, which rigorously analyzes the relationships between biological organisms, has also proved tremendously useful in classifying dinosaurs. Cladistic analysis, among other modern techniques, helps to compensate for an often incomplete and fragmentary fossil record.

One of the best examples of soft-tissue impressions in a fossil dinosaur was discovered in Pietraroia, Italy. The discovery was reported in 1998, and described the specimen of a small, very young coelurosaur, "Scipionyx samniticus". The fossil includes portions of the intestines, colon, liver, muscles, and windpipe of this immature dinosaur.

In the March 2005 issue of "Science", the paleontologist Mary Higby Schweitzer and her team announced the discovery of flexible material resembling actual soft tissue inside a 68-million-year-old "Tyrannosaurus rex" leg bone from the Hell Creek Formation in Montana. After recovery, the tissue was rehydrated by the science team. When the fossilized bone was treated over several weeks to remove mineral content from the fossilized bone-marrow cavity (a process called demineralization), Schweitzer found evidence of intact structures such as blood vessels, bone matrix, and connective tissue (bone fibers). Scrutiny under the microscope further revealed that the putative dinosaur soft tissue had retained fine structures (microstructures) even at the cellular level. The exact nature and composition of this material, and the implications of Schweitzer's discovery, are not yet clear.

In 2009, a team including Schweitzer announced that, using even more careful methodology, they had duplicated their results by finding similar soft tissue in a duck-billed dinosaur, "Brachylophosaurus canadensis", found in the Judith River Formation of Montana. This included even more detailed tissue, down to preserved bone cells that seem even to have visible remnants of nuclei and what seem to be red blood cells. Among other materials found in the bone was collagen, as in the "Tyrannosaurus" bone. The type of collagen an animal has in its bones varies according to its DNA and, in both cases, this collagen was of the same type found in modern chickens and ostriches.

The extraction of ancient DNA from dinosaur fossils has been reported on two separate occasions; upon further inspection and peer review, however, neither of these reports could be confirmed. However, a functional peptide involved in the vision of a theoretical dinosaur has been inferred using analytical phylogenetic reconstruction methods on gene sequences of related modern species such as reptiles and birds. In addition, several proteins, including hemoglobin, have putatively been detected in dinosaur fossils.

In 2015, researchers reported finding structures similar to blood cells and collagen fibers, preserved in the bone fossils of six Cretaceous dinosaur specimens, which are approximately 75 million years old.

By human standards, dinosaurs were creatures of fantastic appearance and often enormous size. As such, they have captured the popular imagination and become an enduring part of human culture. Entry of the word "dinosaur" into the common vernacular reflects the animals' cultural importance: in English, "dinosaur" is commonly used to describe anything that is impractically large, obsolete, or bound for extinction.

Public enthusiasm for dinosaurs first developed in Victorian England, where in 1854, three decades after the first scientific descriptions of dinosaur remains, a menagerie of lifelike dinosaur sculptures were unveiled in London's Crystal Palace Park. The Crystal Palace dinosaurs proved so popular that a strong market in smaller replicas soon developed. In subsequent decades, dinosaur exhibits opened at parks and museums around the world, ensuring that successive generations would be introduced to the animals in an immersive and exciting way. Dinosaurs' enduring popularity, in its turn, has resulted in significant public funding for dinosaur science, and has frequently spurred new discoveries. In the United States, for example, the competition between museums for public attention led directly to the Bone Wars of the 1880s and 1890s, during which a pair of feuding paleontologists made enormous scientific contributions.

The popular preoccupation with dinosaurs has ensured their appearance in literature, film, and other media. Beginning in 1852 with a passing mention in Charles Dickens "Bleak House", dinosaurs have been featured in large numbers of fictional works. Jules Verne's 1864 novel "Journey to the Center of the Earth", Sir Arthur Conan Doyle's 1912 book "The Lost World", the iconic 1933 film "King Kong", the 1954 "Godzilla" and its many sequels, the best-selling 1990 novel "Jurassic Park" by Michael Crichton and its 1993 film adaptation are just a few notable examples of dinosaur appearances in fiction. Authors of general-interest non-fiction works about dinosaurs, including some prominent paleontologists, have often sought to use the animals as a way to educate readers about science in general. Dinosaurs are ubiquitous in advertising; numerous companies have referenced dinosaurs in printed or televised advertisements, either in order to sell their own products or in order to characterize their rivals as slow-moving, dim-witted, or obsolete.



General

Images

Video

Popular

Technical


</doc>
<doc id="8315" url="https://en.wikipedia.org/wiki?curid=8315" title="Diamagnetism">
Diamagnetism

Diamagnetic materials are repelled by a magnetic field; an applied magnetic field creates an induced magnetic field in them in the opposite direction, causing a repulsive force. In contrast, paramagnetic and ferromagnetic materials are attracted by a magnetic field. Diamagnetism is a quantum mechanical effect that occurs in all materials; when it is the only contribution to the magnetism, the material is called diamagnetic. In paramagnetic and ferromagnetic substances the weak diamagnetic force is overcome by the attractive force of magnetic dipoles in the material. The magnetic permeability of diamagnetic materials is less than μ, the permeability of vacuum. In most materials diamagnetism is a weak effect which can only be detected by sensitive laboratory instruments, but a superconductor acts as a strong diamagnet because it repels a magnetic field entirely from its interior.

Diamagnetism was first discovered when Sebald Justinus Brugmans observed in 1778 that bismuth and antimony were repelled by magnetic fields. In 1845, Michael Faraday demonstrated that it was a property of matter and concluded that every material responded (in either a diamagnetic or paramagnetic way) to an applied magnetic field. On a suggestion by William Whewell, Faraday first referred to the phenomenon as "diamagnetic" (the prefix "dia-" meaning "through" or "across"), then later changed it to "diamagnetism".

Diamagnetism, to a greater or lesser degree, is a property of all materials and always makes a weak contribution to the material's response to a magnetic field. For materials that show some other form of magnetism (such as ferromagnetism or paramagnetism), the diamagnetic contribution becomes negligible. Substances that mostly display diamagnetic behaviour are termed diamagnetic materials, or diamagnets. Materials called diamagnetic are those that laypeople generally think of as "non-magnetic", and include water, wood, most organic compounds such as petroleum and some plastics, and many metals including copper, particularly the heavy ones with many core electrons, such as mercury, gold and bismuth. The magnetic susceptibility values of various molecular fragments are called Pascal's constants.

Diamagnetic materials, like water, or water-based materials, have a relative magnetic permeability that is less than or equal to 1, and therefore a magnetic susceptibility less than or equal to 0, since susceptibility is defined as . This means that diamagnetic materials are repelled by magnetic fields. However, since diamagnetism is such a weak property, its effects are not observable in everyday life. For example, the magnetic susceptibility of diamagnets such as water is . The most strongly diamagnetic material is bismuth, , although pyrolytic carbon may have a susceptibility of in one plane. Nevertheless, these values are orders of magnitude smaller than the magnetism exhibited by paramagnets and ferromagnets. Note that because χ is derived from the ratio of the internal magnetic field to the applied field, it is a dimensionless value.

All conductors exhibit an effective diamagnetism when they experience a changing magnetic field. The Lorentz force on electrons causes them to circulate around forming eddy currents. The eddy currents then produce an induced magnetic field opposite the applied field, resisting the conductor's motion.

Superconductors may be considered perfect diamagnets (), because they expel all magnetic fields (except in a thin surface layer) due to the Meissner effect.

If a powerful magnet (such as a supermagnet) is covered with a layer of water (that is thin compared to the diameter of the magnet) then the field of the magnet significantly repels the water. This causes a slight dimple in the water's surface that may be seen by its reflection.

Diamagnets may be levitated in stable equilibrium in a magnetic field, with no power consumption. Earnshaw's theorem seems to preclude the possibility of static magnetic levitation. However, Earnshaw's theorem applies only to objects with positive susceptibilities, such as ferromagnets (which have a permanent positive moment) and paramagnets (which induce a positive moment). These are attracted to field maxima, which do not exist in free space. Diamagnets (which induce a negative moment) are attracted to field minima, and there can be a field minimum in free space.

A thin slice of pyrolytic graphite, which is an unusually strong diamagnetic material, can be stably floated in a magnetic field, such as that from rare earth permanent magnets. This can be done with all components at room temperature, making a visually effective demonstration of diamagnetism.

The Radboud University Nijmegen, the Netherlands, has conducted experiments where water and other substances were successfully levitated. Most spectacularly, a live frog (see figure) was levitated.

In September 2009, NASA's Jet Propulsion Laboratory (JPL) in Pasadena, California announced it had successfully levitated mice using a superconducting magnet, an important step forward since mice are closer biologically to humans than frogs. JPL said it hopes to perform experiments regarding the effects of microgravity on bone and muscle mass.

Recent experiments studying the growth of protein crystals have led to a technique using powerful magnets to allow growth in ways that counteract Earth's gravity.

A simple homemade device for demonstration can be constructed out of bismuth plates and a few permanent magnets that levitate a permanent magnet.

The electrons in a material generally circulate in orbitals, with effectively zero resistance and act like current loops. Thus it might be imagined that diamagnetism effects in general would be very, very common, since any applied magnetic field would generate currents in these loops that would oppose the change, in a similar way to superconductors, which are essentially perfect diamagnets. However, since the electrons are rigidly held in orbitals by the charge of the protons and are further constrained by the Pauli exclusion principle, many materials exhibit diamagnetism, but typically respond very little to the applied field.

The Bohr–van Leeuwen theorem proves that there cannot be any diamagnetism or paramagnetism in a purely classical system. However, the classical theory for Langevin diamagnetism gives the same prediction as the quantum theory. The classical theory is given below.

Paul Langevin's theory of diamagnetism (1905) applies to materials containing atoms with closed shells (see dielectrics). A field with intensity , applied to an electron with charge and mass , gives rise to Larmor precession with frequency . The number of revolutions per unit time is , so the current for an atom with electrons is (in SI units)

The magnetic moment of a current loop is equal to the current times the area of the loop. Suppose the field is aligned with the axis. The average loop area can be given as formula_2, where formula_3 is the mean square distance of the electrons perpendicular to the axis. The magnetic moment is therefore

If the distribution of charge is spherically symmetric, we can suppose that the distribution of coordinates are independent and identically distributed. Then formula_5, where formula_6 is the mean square distance of the electrons from the nucleus. Therefore, formula_7. If formula_8 is the number of atoms per unit volume, the volume diamagnetic susceptibility in SI units is

The Langevin theory is not the full picture for metals because they have non-localized electrons. The theory that describes diamagnetism in a free electron gas is called Landau diamagnetism, named after Lev Landau, and instead considers the weak counteracting field that forms when the electrons' trajectories are curved due to the Lorentz force. Landau diamagnetism, however, should be contrasted with Pauli paramagnetism, an effect associated with the polarization of delocalized electrons' spins. For the bulk case of a 3D system and low magnetic fields, the (volume) diamagnetic susceptibility can be calculated using Landau quantization, which in SI units is

where formula_11 is the Fermi energy. This is equivalent to formula_12, exactly formula_13 times Pauli paramagnetic susceptibility, where formula_14 is the Bohr magneton and formula_15 is the density of states (number of states per energy per volume). This formula takes into account the spin degeneracy of the carriers (spin ½ electrons).

In doped semiconductors the ratio between Landau and Pauli susceptibilities may change due to the effective mass of the charge carriers differing from the electron mass in vacuum, increasing the diamagnetic contribution. The formula presented here only applies for the bulk; in confined systems like quantum dots, the description is altered due to quantum confinement. Additionally, for strong magnetic fields, the susceptibility of delocalized electrons oscillates as a function of the field strength, a phenomenon known as the de Haas–van Alphen effect, also first described theoretically by Landau.




</doc>
<doc id="8317" url="https://en.wikipedia.org/wiki?curid=8317" title="Duke of Marlborough (title)">
Duke of Marlborough (title)

Duke of Marlborough ( ) is a title in the Peerage of England. It was created by Queen Anne in 1702 for John Churchill, 1st Earl of Marlborough (1650–1722), the noted military leader. In historical texts, it is often to him that an unqualified use of the title refers. The name of the dukedom refers to Marlborough in Wiltshire. It is one of the few titles in the peerage which allows for "suo jure" female inheritance, and the only current dukedom to do so.

The earldom of Marlborough was held by the family of Ley from its creation 1626 until its extinction with the death of the 4th earl in 1679. The title was recreated 10 years later for John Churchill (in 1689).

Churchill had been made "Lord Churchill of Eyemouth" (1682) in the Scottish peerage, and "Baron Churchill" of Sandridge (1685) and "Earl of Marlborough" (1689) in the Peerage of England. Shortly after her accession to the throne in 1702, Queen Anne made Churchill the first "Duke of Marlborough" and granted him the subsidiary title "Marquess of Blandford".

In 1678, Churchill married Sarah Jennings (1660–1744), a courtier and influential favourite of the queen. They had seven children, of whom four daughters married into some of the most important families in Great Britain; one daughter and one son died in infancy. He was pre-deceased by his son, John Churchill, Marquess of Blandford, in 1703; so, to prevent the extinction of the titles, a special Act of Parliament was passed. When the 1st Duke of Marlborough died in 1722 his title as "Lord Churchill of Eyemouth" in the Scottish peerage became extinct and the Marlborough titles passed, according to the Act, to his eldest daughter Henrietta (1681–1733), the 2nd Duchess of Marlborough. She was married to the 2nd Earl of Godolphin and had a son who predeceased her.

When Henrietta died in 1733, the Marlborough titles passed to her nephew Charles Spencer (1706–1758), the third son of her late sister Anne (1683–1716), who had married the 3rd Earl of Sunderland in 1699. After his older brother's death in 1729, Charles Spencer had already inherited the Spencer family estates and the titles of "Earl of Sunderland" (1643) and "Baron Spencer" of Wormleighton (1603), all in the Peerage of England. Upon his maternal aunt Henrietta's death in 1733, Charles Spencer succeeded to the Marlborough family estates and titles and became the 3rd Duke. When he died in 1758, his titles passed to his eldest son George (1739–1817), who was succeeded by his eldest son George, the 5th Duke (1766–1840). In 1815, Francis Spencer (the younger son of the 4th Duke) was created "Baron Churchill" in the Peerage of the United Kingdom. In 1902, his grandson, the 3rd Baron Churchill, was created "Viscount Churchill".

In 1817, the 5th Duke obtained permission to assume and bear the surname of Churchill in addition to his surname of Spencer, to perpetuate the name of his illustrious great-great-grandfather. At the same time he received Royal Licence to quarter the coat of arms of Churchill with his paternal arms of Spencer. The modern Dukes thus originally bore the surname "Spencer": the double-barrelled surname of "Spencer-Churchill" as used since 1817 remains in the family, though some members have preferred to style themselves "Churchill".

The 7th Duke was the paternal grandfather of the British Prime Minister Sir Winston Churchill, born at Blenheim Palace on 30 November 1874.

The 11th Duke, John Spencer-Churchill died in 2014, having assumed the title in 1972. The 12th and present Duke is Charles James Spencer-Churchill.

The family seat is Blenheim Palace in Woodstock, Oxfordshire.

After his leadership in the victory against the French in the Battle of Blenheim on 13 August 1704, the 1st Duke was honoured by Queen Anne granting him the royal manor of Woodstock, and building him a house at her expense to be called Blenheim. Construction started in 1705 and the house was completed in 1722, the year of the 1st Duke's death. Blenheim Palace has since remained in the Churchill and Spencer-Churchill family.

With the exception of the 10th Duke and his first wife, the Dukes and Duchesses of Marlborough are buried in Blenheim Palace's chapel. Most other members of the Spencer-Churchill family are interred in St. Martin's parish churchyard at Bladon, a short distance from the palace.

The dukedom is the only one in the United Kingdom that can still pass through a female line. However, unlike the remainder to heirs general found in most other peerages that allow male-preference primogeniture, the grant does not allow for abeyance and follows a more restrictive Semi-Salic formula designed to keep succession wherever possible in the male line. The succession is as follows:


Succession to the title under the first and second contingencies have lapsed; holders of the title from the 3rd Duke trace their status from the third contingency.

It is now very unlikely that the Dukedom will be passed to a woman or through a woman, since all the male-line descendants of the 1st Duke's second daughter Anne Spencer, Countess of Sunderland—including the lines of the Viscounts Churchill and Barons Churchill of Whichwood and of the Earls Spencer and of the entire Spencer-Churchill and Spencer family—would have to become extinct.

If that were to happen, the Churchill titles would pass to the Earl of Jersey and his family, the heir-male of the 1st Duke's granddaughter Anne Villiers, Countess of Jersey, daughter of Elizabeth Egerton, Duchess of Bridgewater, the third daughter of the first Duke.

The next heir would be the Duke of Buccleuch and his family, the heir-male of the 1st Duke's great-granddaughter Elizabeth Montagu, Duchess of Buccleuch, the daughter of Mary Montagu, Duchess of Montagu (1766 creation), the daughter of the 1st Duke's youngest daughter Mary, Duchess of Montagu (1705 creation).

The fourth surviving line is represented by the Earl of Chichester and his family, the heir-male of the 1st Duke's most senior great-great-granddaughter Mary Henrietta Osborne, Countess of Chichester, daughter of Francis Osborne, 5th Duke of Leeds, only child of Mary Godolphin, Duchess of Leeds, daughter of the 1st Duke's eldest daughter Henrietta Godolphin, 2nd Duchess of Marlborough by her husband Francis Godolphin, 2nd Earl of Godolphin.

The Duke holds subsidiary titles: "Marquess of Blandford" (created in 1702 for John Churchill), "Earl of Sunderland" (created in 1643 for the Spencer family), "Earl of Marlborough" (created in 1689 for John Churchill), "Baron Spencer" of Wormleighton (created in 1603 for the Spencer family), and "Baron Churchill" of Sandridge (created in 1685 for John Churchill), all in the Peerage of England.

The title "Marquess of Blandford" is used as the courtesy title for the Duke's eldest son and heir. The Duke's eldest son's eldest son can use the courtesy title "Earl of Sunderland", and the duke's eldest son's eldest son's eldest son (not necessarily the eldest great-grandson) the title "Lord Spencer of Wormleighton" (not to be confused with Earl Spencer).

The title of "Earl of Marlborough", created for John Churchill in 1689, had previously been created for James Ley, in 1626, becoming extinct in 1679.

The 1st Duke was honoured with land and titles in the Holy Roman Empire: Emperor Leopold I created him a Prince in 1704, and in 1705, his successor Emperor Joseph I gave him the principality of Mindelheim (once the lordship of the noted soldier Georg von Frundsberg). He was obliged to surrender Mindelheim in 1714 by the Treaty of Utrecht, which returned it to Bavaria. He tried to obtain Nellenburg in Austria in exchange, which at that time was only a county ('Landgrafschaft'), but this failed, partially because Austrian law did not allow for Nellenburg being converted into a sovereign principality. The 1st Duke's principality title of Mindelheim became extinct either on the return of the land to Bavaria or on his death, as the Empire operated Salic Law, which prevented female succession.

The original arms of Sir Winston Churchill (1620–1688), father of the 1st Duke of Marlborough, were simple and in use by his own father in 1619. The shield was Sable a lion rampant Argent, debruised by a bendlet Gules. The addition of a canton of Saint George (see below) rendered the distinguishing mark of the bendlet unnecessary.

The Churchill crest is blazoned as a lion couchant guardant Argent, supporting with its dexter forepaw a banner Gules, charged with a dexter hand appaumée of the first, staff Or.

In recognition of Sir Winston's services to King Charles I as Captain of the Horse, and his loyalty to King Charles II as a Member of Parliament, he was awarded an augmentation of honour to his arms around 1662. This rare mark of royal favour took the form of a canton of Saint George. At the same time, he was authorised to omit the bendlet, which had served the purpose of distinguishing this branch of the Churchill family from others which bore an undifferenced lion.

Sir Winston's shield and crest were inherited by his son John Churchill, 1st Duke of Marlborough. Minor modifications reflected the bearer's social rise: the helm was now shown in profile and had a closed grille to signify the bearer's rank as a peer, and there were now supporters placed on either side of the shield. They were the mythical Griffin (part lion, part eagle) and Wyvern (a dragon without hind legs). The supporters were derived from the arms of the family of the 1st Duke's mother, Drake of Ash (Argent, a wyvern gules; these arms can be seen on the monument in Musbury Church to Sir Bernard Drake, d.1586).

The motto was "Fiel pero desdichado" (Spanish for "Faithful but unfortunate"). The 1st Duke was also entitled to a coronet indicating his rank.

When the 1st Duke was made a Prince of the Holy Roman Empire in 1705, two unusual features were added: the Imperial Eagle and a Princely Coronet. His estates in Germany, such as Mindelheim, were represented in his arms by additional quarterings.

In 1817, the 5th Duke received Royal Licence to place the quarter of Churchill ahead of his paternal arms of Spencer. The shield of the Spencer family arms is: quarterly Argent and Gules, in the second and third quarters a fret Or, over all on a bend Sable three escallops of the first. The Spencer crest is: out of a ducal coronet Or, a griffin's head between two wings expanded Argent, gorged with a collar gemel and armed Gules. Paul Courtenay observes that "It would be normal in these circumstances for the paternal arms (Spencer) to take precedence over the maternal (Churchill), but because the Marlborough dukedom was senior to the Sunderland earldom, the procedure was reversed in this case."

Also in 1817, a further augmentation of honour was added to his armorial achievement. This incorporated the bearings from the standard of the Manor of Woodstock and was borne on an escutcheon, displayed over all in the centre chief point, as follows: Argent a cross of Saint George surmounted by an inescutcheon Azure, charged with three fleurs-de-lys Or, two over one. This inescutcheon represents the royal arms of France.

These quartered arms, incorporating the two augmentations of honour, have been the arms of all subsequent Dukes of Marlborough.

The motto "Fiel pero desdichado" is Spanish for "Faithful though Joyless". "Desdichado" means without happiness or without joy, alluding to the first Duke's father, Winston, who was a royalist and faithful supporter of the king during the English Civil War but was not compensated for his losses after the restoration. Charles II knighted Winston Churchill and other Civil War royalists but did not compensate them for their wartime losses, thereby inducing Winston to adopt the motto. It is unusual for the motto of an Englishman of the era to be in Spanish rather than Latin, and it is not known why this is the case.

The earldom of Marlborough was held by the family of Ley from 1626 to 1679. James Ley, the 1st Earl (c. 1550-1629), was lord chief justice of the King’s Bench in Ireland and then in England; he was an English member of parliament and was lord high treasurer from 1624 to 1628. In 1624 he was created Baron Ley and in 1626 Earl of Marlborough. The 3rd earl was his grandson James (1618–1665), a naval officer who was killed in action with the Dutch. James was succeeded by his uncle William, a younger son of the 1st earl, on whose death in 1679 the earldom became extinct.



The heir apparent to the Dukedom is George John Godolphin Spencer-Churchill, Marquess of Blandford (b. 1992), eldest son of the 12th Duke.

 
<section begin=FamilyTree />

<section end="FamilyTree" />


</doc>
<doc id="8322" url="https://en.wikipedia.org/wiki?curid=8322" title="December 17">
December 17




</doc>
<doc id="8324" url="https://en.wikipedia.org/wiki?curid=8324" title="Difference engine">
Difference engine

A difference engine is an automatic mechanical calculator designed to tabulate polynomial functions. The name derives from the method of divided differences, a way to interpolate or tabulate functions by using a small set of polynomial coefficients. Most mathematical functions commonly used by engineers, scientists and navigators, including logarithmic and trigonometric functions, can be approximated by polynomials, so a difference engine can compute many useful tables of numbers.

The historical difficulty in producing error-free tables by teams of mathematicians and human "computers" spurred Charles Babbage's desire to build a mechanism to automate the process.

The notion of a mechanical calculator for mathematical functions can be traced back to the Antikythera mechanism of the 2nd century BC, while early modern examples are attributed to Pascal and Leibniz in the 17th century. 
In 1784 J. H. Müller, an engineer in the Hessian army, devised and built an adding machine and described the basic principles of a difference machine in a book published in 1786 (the first written reference to a difference machine is dated to 1784), but he was unable to obtain funding to progress with the idea.

Charles Babbage began to construct a small difference engine in c. 1819 and had completed it by 1822 (Difference Engine 0). He announced his invention on June 14, 1822, in a paper to the Royal Astronomical Society, entitled "Note on the application of machinery to the computation of astronomical and mathematical tables". This machine used the decimal number system and was powered by cranking a handle. The British government was interested, since producing tables was time-consuming and expensive and they hoped the difference engine would make the task more economical.

In 1823, the British government gave Babbage £1700 to start work on the project. Although Babbage's design was feasible, the metalworking techniques of the era could not economically make parts in the precision and quantity required. Thus the implementation proved to be much more expensive and doubtful of success than the government's initial estimate. In 1832, Babbage and Joseph Clement produced a small working model (1/7 of the calculating section of Difference Engine No. 1, which was intended to operate on 20-digit numbers and sixth-order differences) which operated on 6-digit numbers and second-order differences. Lady Byron described seeing the working prototype in 1833: "We both went to see the thinking machine (for so it seems) last Monday. It raised several Nos. to the 2nd and 3rd powers, and extracted the root of a Quadratic equation." Work on the larger engine was suspended in 1833.

By the time the government abandoned the project in 1842, Babbage had received and spent over £17,000 on development, which still fell short of achieving a working engine. The government valued only the machine's output (economically produced tables), not the development (at unknown and unpredictable cost to complete) of the machine itself. Babbage did not, or was unwilling to, recognize that predicament. Meanwhile, Babbage's attention had moved on to developing an analytical engine, further undermining the government’s confidence in the eventual success of the difference engine. By improving the concept as an analytical engine, Babbage had made the difference engine concept obsolete, and the project to implement it an utter failure in the view of the government.

Babbage went on to design his much more general analytical engine, but later produced an improved "Difference Engine No. 2" design (31-digit numbers and seventh-order differences), between 1846 and 1849. Babbage was able to take advantage of ideas developed for the analytical engine to make the new difference engine calculate more quickly while using fewer parts.

Inspired by Babbage's difference engine in 1834, Per Georg Scheutz built several experimental models. In 1837 his son Edward proposed to construct a working model in metal, and in 1840 finished the calculating part, capable of calculating series with 5-digit numbers and first-order differences, which was later extended to third-order (1842). In 1843, after adding the printing part, the model was completed.

In 1851, funded by the government, construction of the larger and improved (15-digit numbers and fourth-order differences) machine began, and finished in 1853. The machine was demonstrated at the World's Fair in Paris, 1855 and then sold in 1856 to the Dudley Observatory in Albany, New York. In 1857 British government ordered next Scheutz's difference machine, which was built in 1859. It had the same basic construction as the previous one.

Martin Wiberg improved Scheutz's construction (c. 1859, his machine has the same capacity as Scheutz's - 15-digit and fourth-order) but used his device only for producing and publishing printed tables (interest tables in 1860, and logarithmic tables in 1875).

Alfred Deacon of London in c. 1862 produced a small difference engine (20-digit numbers and third-order differences).

American George B. Grant started working on his calculating machine in 1869, unaware of the works of Babbage and Scheutz (Schentz). One year later (1870) he learned about difference engines and proceed to design one himself, describing his construction in 1871. In 1874 the Boston Thursday Club raised a subscription for the construction of a large-scale model, which was built in 1876. It could be expanded to enhance precision.

Christel Hamann built one machine (16-digit numbers and second-order differences) in 1909 for the "Tables of Bauschinger and Peters" ("Logarithmic-Trigonometrical Tables with eight decimal places"), which was first published in Leipzig in 1910.

Burroughs Corporation in about 1912 built a machine for Nautical Almanac Office which was used as a difference engine of second-order. It was later replaced in 1929 by a Burroughs Class 11 (13-digit numbers and second-order differences, or 11-digit numbers and <nowiki>[at least up to]</nowiki> fifth-order differences).

Alexander John Thompson in about 1927-1928 built "integrating and differencing machine" (13-digit numbers and fourth-order differences) for his table of logarithms "Logarithmetica britannica". This machine was composed of four modified Triumphator calculators.

Leslie Comrie in 1928 described how to use the Brunsviga-Dupla calculating machine as a difference engine of second-order (15-digit numbers). He also noted in 1931 that National Accounting Machine Class 3000 could be used as a difference engine of sixth-order.

During the 1980s, Allan G. Bromley, an associate professor at the University of Sydney, Australia, studied Babbage's original drawings for the Difference and Analytical Engines at the Science Museum library in London. This work led the Science Museum to construct a working difference engine No. 2 from 1989 to 1991, under Doron Swade, the then Curator of Computing. This was to celebrate the 200th anniversary of Babbage's birth in 2001. In 2000, the printer which Babbage originally designed for the difference engine was also completed. The conversion of the original design drawings into drawings suitable for engineering manufacturers' use revealed some minor errors in Babbage's design (possibly introduced as a protection in case the plans were stolen), which had to be corrected. Once completed, both the engine and its printer worked flawlessly, and still do. The difference engine and printer were constructed to tolerances achievable with 19th-century technology, resolving a long-standing debate as to whether Babbage's design would actually have worked. (One of the reasons formerly advanced for the non-completion of Babbage's engines had been that engineering methods were insufficiently developed in the Victorian era.)

The printer's primary purpose is to produce stereotype plates for use in printing presses, which it does by pressing type into soft plaster to create a flong. Babbage intended that the Engine's results be conveyed directly to mass printing, having recognized that many errors in previous tables were not the result of human calculating mistakes but from error in the manual typesetting process. The printer's paper output is mainly a means of checking the Engine's performance.

In addition to funding the construction of the output mechanism for the Science Museum's Difference Engine No. 2, Nathan Myhrvold commissioned the construction of a second complete Difference Engine No. 2, which was on exhibit at the Computer History Museum in Mountain View, California until 31 January 2016.
It has since been transferred to Intellectual Ventures in Seattle where it is on display just outside the main lobby.

The difference engine consists of a number of columns, numbered from 1 to N. The machine is able to store one decimal number in each column. The machine can only add the value of a column "n" + 1 to column "n" to produce the new value of "n". Column "N" can only store a constant, column 1 displays (and possibly prints) the value of the calculation on the current iteration.

The engine is programmed by setting initial values to the columns. Column 1 is set to the value of the polynomial at the start of computation. Column 2 is set to a value derived from the first and higher derivatives of the polynomial at the same value of X. Each of the columns from 3 to "N" is set to a value derived from the formula_1 first and higher derivatives of the polynomial.

In the Babbage design, one iteration (i.e., one full set of addition and carry operations) happens for each rotation of the main shaft. Odd and even columns alternately perform an addition in one cycle. The sequence of operations for column formula_2 is thus:


Steps 1,2,3,4 occur for every odd column, while steps 3,4,1,2 occur for every even column.

While Babbage's original design placed the crank directly on the main shaft, it was later realized that the force required to crank the machine would have been too great for a human to handle comfortably. Therefore, the two models that were built incorporate a 4:1 reduction gear at the crank, and four revolutions of the crank are required to perform one full cycle.

Each iteration creates a new result, and is accomplished in four steps corresponding to four complete turns of the handle shown at the far right in the picture below. The four steps are:


The engine represents negative numbers as ten's complements. Subtraction amounts to addition of a negative number. This works in the same manner that modern computers perform subtraction, known as two's complement.

The principle of a difference engine is Newton's method of divided differences. If the initial value of a polynomial (and of its finite differences) is calculated by some means for some value of X, the difference engine can calculate any number of nearby values, using the method generally known as the method of finite differences. For example, consider the quadratic polynomial

with the goal of tabulating the values "p"(0), "p"(1), "p"(2), "p"(3), "p"(4), and so forth. The table below is constructed as follows: the second column contains the values of the polynomial, the third column contains the differences of the two left neighbors in the second column, and the fourth column contains the differences of the two neighbors in the third column:

The numbers in the third values-column are constant. In fact, by starting with any polynomial of degree "n", the column number "n" + 1 will always be constant. This is the crucial fact behind the success of the method.

This table was built from left to right, but it is possible to continue building it from right to left down a diagonal in order to compute more values. To calculate "p"(5) use the values from the lowest diagonal. Start with the fourth column constant value of 4 and copy it down the column. Then continue the third column by adding 4 to 11 to get 15. Next continue the second column by taking its previous value, 22 and adding the 15 from the third column. Thus "p"(5) is 22 + 15 = 37. In order to compute "p"(6), we iterate the same algorithm on the "p"(5) values: take 4 from the fourth column, add that to the third column's value 15 to get 19, then add that to the second column's value 37 to get 56, which is "p"(6). This process may be continued ad infinitum. The values of the polynomial are produced without ever having to multiply. A difference engine only needs to be able to add. From one loop to the next, it needs to store 2 numbers—in this example (the last elements in the first and second columns). To tabulate polynomials of degree "n", one needs sufficient storage to hold "n" numbers.

Babbage's difference engine No. 2, finally built in 1991, could hold 8 numbers of 31 decimal digits each and could thus tabulate 7th degree polynomials to that precision. The best machines from Scheutz could store 4 numbers with 15 digits each.

The initial values of columns can be calculated by first manually calculating N consecutive values of the function and by backtracking, i.e. calculating the required differences.

Col formula_6 gets the value of the function at the start of computation formula_7. Col formula_8 is the difference between formula_9 and formula_7…

If the function to be calculated is a polynomial function, expressed as
the initial values can be calculated directly from the constant coefficients "a", "a","a", …, "a" without calculating any data points. The initial values are thus:


Many commonly used functions are analytic functions, which can be expressed as power series, for example as a Taylor series. The initial values can be calculated to any degree of accuracy; if done correctly the engine will give exact results for first N steps. After that, the engine will only give an approximation of the function.

The Taylor series expresses the function as a sum obtained from its derivatives at one point. For many functions the higher derivatives are trivial to obtain; for instance, the sine function at 0 has values of 0 or formula_19 for all derivatives. Setting 0 as the start of computation we get the simplified Maclaurin series

The same method of calculating the initial values from the coefficients can be used as for polynomial functions. The polynomial constant coefficients will now have the value

The problem with the methods described above is that errors will accumulate and the series will tend to diverge from the true function. A solution which guarantees a constant maximum error is to use curve fitting. A minimum of "N" values are calculated evenly spaced along the range of the desired calculations. Using a curve fitting technique like Gaussian reduction an "N"−1th degree polynomial interpolation of the function is found. With the optimized polynomial, the initial values can be calculated as above.

William Gibson and Bruce Sterling's "The Difference Engine" is an alternate history novel that looks how society would have progressed had the difference engine worked the way Babbage envisioned it.

The story takes places in Victorian England where technological advancement is in the rise. This is due to the effect of the success of Babbage's analytical machine. The convention of steampunk where Victorian fashion is combined with the technological elements of the Industrial Revolution is seen throughout the story due to technology being so advanced in that era.




</doc>
<doc id="8326" url="https://en.wikipedia.org/wiki?curid=8326" title="Draupnir">
Draupnir

In Norse mythology, Draupnir (Old Norse "the dripper") is a gold ring possessed by the god Odin with the ability to multiply itself: Every ninth night, eight new rings 'drip' from Draupnir, each one of the same size and weight as the original.

Draupnir was forged by the dwarven brothers Brokkr and Eitri (or Sindri). Brokkr and Eitri made this ring as one of a set of three gifts which included Mjöllnir and Gullinbursti. They made these gifts in accordance with a wager Loki made saying that Brokkr and Eitri could not make better gifts than the three made by the Sons of Ivaldi. In the end, Mjöllnir, Thor's hammer, won the contest for Brokkr and Eitri. Loki used a loophole to get out of the wager for his head (the wager was for Loki's head only, but he argued that, to remove his head, they would have to injure his neck, which was not in the bargain) and Brokkr punished him by sealing his lips shut with wire.

The ring was placed by Odin on the funeral pyre of his son Baldr:

Odin laid upon the pyre the gold ring called Draupnir; this quality attended it: that every ninth night there fell from it eight gold rings of equal weight. (from the "Gylfaginning").
The ring was subsequently retrieved by Hermóðr. It was offered as a gift by Freyr's servant Skírnir in the wooing of Gerðr, which is described in the poem "Skírnismál".

Draupnir is represented as a card in the Yu-Gi-Oh Trading Card Game. It has an effect that mimics the multiplication ability of the mythological version. If it is destroyed by another card's effect, you can add another "Nordic Relic" card to your hand. The art represents it as an arm brace, with another brace seemingly growing from it, once again mimicking the story.

"DRAUPNIR" was revealed as the key to a website that Neal Caffrey and Mozzie used to view their stolen Nazi U-boat treasure in "Taking Account", the seventh episode of the third season of "White Collar".

It also appeared in episode 11 of "" as a tool to seal Loki's spirit.

The Draupnir is never called by name but is simply known as Odin's ring in the first three books of the Witches of East End novels. This rings allows the wearer to teleport to any place of the nine worlds, and a copy of equal power was once owned by Loki before it was destroyed by Freya.



</doc>
<doc id="8328" url="https://en.wikipedia.org/wiki?curid=8328" title="Divergence">
Divergence

In vector calculus, divergence is a vector operator that produces a scalar field, giving the quantity of a vector field's source at each point. More technically, the divergence represents the volume density of the outward flux of a vector field from an infinitesimal volume around a given point.

As an example, consider air as it is heated or cooled. The velocity of the air at each point defines a vector field. While air is heated in a region, it expands in all directions, and thus the velocity field points outward from that region. The divergence of the velocity field in that region would thus have a positive value. While the air is cooled and thus contracting, the divergence of the velocity has a negative value.

In physical terms, the divergence of a three-dimensional vector field is the extent to which the vector field flow behaves like a source at a given point. It is a local measure of its "outgoingness" – the extent to which there is more of some quantity exiting an infinitesimal region of space than entering it. If the divergence is nonzero at some point then there is compression or expansion at that point. (Note that we are imagining the vector field to be like the velocity vector field of a fluid (in motion) when we use the terms "flow" and so on.)

More rigorously, the divergence of a vector field at a point can be defined as the limit of the net flow of across the smooth boundary of a three-dimensional region divided by the volume of as shrinks to . Formally,

where is the volume of , is the boundary of , and the integral is a surface integral with being the outward unit normal to that surface. The result, , is a function of . From this definition it also becomes obvious that can be seen as the "source density" of the flux of .

In light of the physical interpretation, a vector field with zero divergence everywhere is called "incompressible" or "solenoidal" – in which case any closed surface has no net flow across it.

The intuition that the sum of all sources minus the sum of all sinks should give the net flow outwards of a region is made precise by the divergence theorem.

Let , , be a system of Cartesian coordinates in 3-dimensional Euclidean space, and let , , be the corresponding basis of unit vectors. The divergence of a continuously differentiable vector field is defined as the scalar-valued function:

Although expressed in terms of coordinates, the result is invariant under rotations, as the physical interpretation suggests. This is because the trace of the Jacobian matrix of an -dimensional vector field in -dimensional space is invariant under any invertible linear transformation.

The common notation for the divergence is a convenient mnemonic, where the dot denotes an operation reminiscent of the dot product: take the components of the operator (see del), apply them to the corresponding components of , and sum the results. Because applying an operator is different from multiplying the components, this is considered an abuse of notation.

The divergence of a continuously differentiable second-order tensor field is a first-order tensor field:

For a vector expressed in local unit cylindrical coordinates as
where is the unit vector in direction , the divergence is
The use of local coordinates is vital for the validity of the expression. If we consider the position vector and the functions formula_6, formula_7, and formula_8, which assign the corresponding global cylindrical coordinate to a vector, in general formula_9, formula_10, and formula_11. In particular, if we consider the identity function formula_12, we find that:

formula_13.

In spherical coordinates, with the angle with the axis and the rotation around the axis, and formula_14 again written in local unit coordinates, the divergence is

Using Einstein notation we can consider the divergence in general coordinates, which we write as , where is the number of dimensions of the domain. Here, the upper index refers to the number of the coordinate or component, so refers to the second component, and not the quantity squared. The index variable is used to refer to an arbitrary element, such as . The divergence can then be written via the Voss-Weyl formula, as:

where formula_17 is the local coefficient of the volume element and are the components of with respect to the local unnormalized covariant basis (sometimes written as formula_18). The Einstein notation implies summation over , since it appears as both an upper and lower index.

The volume coefficient formula_17 is a function of position which depends on the coordinate system. In Cartesian, cylindrical and polar coordinates, formula_20 and formula_21 respectively, using the same conventions as above. It can also be expressed as formula_22, where formula_23 is the metric tensor. Since the determinant is a scalar quantity which doesn't depend on the indices, we can suppress them and simply write formula_24. Another expression comes from computing the determinant of the Jacobian for transforming from Cartesian coordinates, which for n=3 gives formula_25

Some conventions expect all local basis elements to be normalized to unit length, as was done in the previous sections. If we write formula_26 for the normalized basis, and formula_27 for the components of with respect to it, we have that 
formula_28, using one of the properties of the metric tensor. By dotting both sides of the last equality with the contravariant element formula_29, we can conclude that formula_30. After substituting, the formula becomes:

formula_31.

See "" for further discussion.

It can be shown that any stationary flux that is at least twice continuously differentiable in and vanishes sufficiently fast for can be decomposed into an "irrotational part" and a "source-free part" . Moreover, these parts are explicitly determined by the respective "source densities" (see above) and "circulation densities" (see the article Curl):

For the irrotational part one has

with

The source-free part, , can be similarly written: one only has to replace the "scalar potential" by a "vector potential" and the terms by , and the source density 
by the circulation density .

This "decomposition theorem" is a by-product of the stationary case of electrodynamics. It is a special case of the more general Helmholtz decomposition which works in dimensions greater than three as well.

The following properties can all be derived from the ordinary differentiation rules of calculus. Most importantly, the divergence is a linear operator, i.e.

for all vector fields and and all real numbers and .

There is a product rule of the following type: if is a scalar-valued function and is a vector field, then

or in more suggestive notation

Another product rule for the cross product of two vector fields and in three dimensions involves the curl and reads as follows:

or

The Laplacian of a scalar field is the divergence of the field's gradient:

The divergence of the curl of any vector field (in three dimensions) is equal to zero: 

If a vector field with zero divergence is defined on a ball in , then there exists some vector field on the ball with . For regions in more topologically complicated than this, the latter statement might be false (see Poincaré lemma). The degree of "failure" of the truth of the statement, measured by the homology of the chain complex

serves as a nice quantification of the complicatedness of the underlying region . These are the beginnings and main motivations of de Rham cohomology.

One can express the divergence as a particular case of the exterior derivative, which takes a 2-form to a 3-form in . Define the current two-form as
It measures the amount of "stuff" flowing through a surface per unit time in a "stuff fluid" of density moving with local velocity . Its exterior derivative is then given by

Thus, the divergence of the vector field can be expressed as:
Here the superscript is one of the two musical isomorphisms, and is the Hodge star operator. Working with the current two-form and the exterior derivative is usually easier than working with the vector field and divergence, because unlike the divergence, the exterior derivative commutes with a change of (curvilinear) coordinate system.

The divergence of a vector field can be defined in any number of dimensions. If 

in a Euclidean coordinate system with coordinates , define

The appropriate expression is more complicated in curvilinear coordinates.

In the case of one dimension, reduces to a regular function, and the divergence reduces to the derivative.

For any , the divergence is a linear operator, and it satisfies the "product rule"

for any scalar-valued function .

The divergence of a vector field extends naturally to any differentiable manifold of dimension that has a volume form (or density) , e.g. a Riemannian or Lorentzian manifold. Generalising the construction of a two-form for a vector field on , on such a manifold a vector field defines an -form obtained by contracting with . The divergence is then the function defined by

Standard formulas for the Lie derivative allow us to reformulate this as

This means that the divergence measures the rate of expansion of a volume element as we let it flow with the vector field.

On a pseudo-Riemannian manifold, the divergence with respect to the metric volume form can be computed in terms of the Levi-Civita connection :

where the second expression is the contraction of the vector field valued 1-form with itself and the last expression is the traditional coordinate expression from Ricci calculus.

An equivalent expression without using connection is

where is the metric and denotes the partial derivative with respect to coordinate .

Divergence can also be generalised to tensors. In Einstein notation, the divergence of a contravariant vector is given by

where denotes the covariant derivative.

Equivalently, some authors define the divergence of a mixed tensor by using the musical isomorphism : if is a -tensor ( for the contravariant vector and for the covariant one), then we define the "divergence of " to be the -tensor

that is, we take the trace over the "first two" covariant indices of the covariant derivative





</doc>
<doc id="8334" url="https://en.wikipedia.org/wiki?curid=8334" title="December 18">
December 18





</doc>
<doc id="8336" url="https://en.wikipedia.org/wiki?curid=8336" title="Decision problem">
Decision problem

In computability theory and computational complexity theory, a decision problem is a problem that can be posed as a yes-no question of the input values. Decision problems typically appear in mathematical questions of decidability, that is, the question of the existence of an effective method to determine the existence of some object or its membership in a set; some of the most important problems in mathematics are undecidable.

For example, the problem "given two numbers "x" and "y", does "x" evenly divide "y"?" is a decision problem. The answer can be either 'yes' or 'no', and depends upon the values of "x" and "y". A method for solving a decision problem, given in the form of an algorithm, is called a decision procedure for that problem. A decision procedure for the decision problem "given two numbers "x" and "y", does "x" evenly divide "y"?" would give the steps for determining whether "x" evenly divides "y", given "x" and "y". One such algorithm is long division, taught to many school children. If the remainder is zero the answer produced is 'yes', otherwise it is 'no'. A decision problem which can be solved by an algorithm, such as this example, is called "decidable".

The field of computational complexity categorizes "decidable" decision problems by how difficult they are to solve. "Difficult", in this sense, is described in terms of the computational resources needed by the most efficient algorithm for a certain problem. The field of recursion theory, meanwhile, categorizes "undecidable" decision problems by Turing degree, which is a measure of the noncomputability inherent in any solution.

A "decision problem" is any arbitrary yes-or-no question on an infinite set of inputs. Because of this, it is traditional to define the decision problem equivalently as: the set of possible inputs together with the set of inputs for which the problem returns "yes".

These inputs can be natural numbers, but may also be values of some other kind, such as strings over the binary alphabet {0,1} or over some other finite set of symbols. The subset of strings for which the problem returns "yes" is a formal language, and often decision problems are defined in this way as formal languages.

Alternatively, using an encoding such as Gödel numberings, any string can be encoded as a natural number, via which a decision problem can be defined as a subset of the natural numbers.

A classic example of a decidable decision problem is the set of prime numbers. It is possible to effectively decide whether a given natural number is prime by testing every possible nontrivial factor. Although much more efficient methods of primality testing are known, the existence of any effective method is enough to establish decidability.

A decision problem "A" is called "decidable" or "effectively solvable" if "A" is a recursive set. A problem is called "partially decidable", "semidecidable", "solvable", or "provable" if "A" is a recursively enumerable set. Problems that are not decidable are called "undecidable".

The halting problem is an important undecidable decision problem; for more examples, see list of undecidable problems.

Decision problems can be ordered according to many-one reducibility and related to feasible reductions such as polynomial-time reductions. A decision problem "P" is said to be "complete" for a set of decision problems "S" if "P" is a member of "S" and every problem in "S" can be reduced to "P". Complete decision problems are used in computational complexity to characterize complexity classes of decision problems. For example, the Boolean satisfiability problem is complete for the class NP of decision problems under polynomial-time reducibility.

Decision problems are closely related to function problems, which can have answers that are more complex than a simple 'yes' or 'no'. A corresponding function problem is "given two numbers "x" and "y", what is "x" divided by "y"?".

A function problem consists of a partial function "f"; the informal "problem" is to compute the values of "f" on the inputs for which it is defined.

Every function problem can be turned into a decision problem; the decision problem is just the graph of the associated function. (The graph of a function "f" is the set of pairs ("x","y") such that "f"("x") = "y".) If this decision problem were effectively solvable then the function problem would be as well. This reduction does not respect computational complexity, however. For example, it is possible for the graph of a function to be decidable in polynomial time (in which case running time is computed as a function of the pair ("x","y") ) when the function is not computable in polynomial time (in which case running time is computed as a function of "x" alone). The function "f"("x") = "2" has this property.

Every decision problem can be converted into the function problem of computing the characteristic function of the set associated to the decision problem. If this function is computable then the associated decision problem is decidable. However, this reduction is more liberal than the standard reduction used in computational complexity (sometimes called polynomial-time many-one reduction); for example, the complexity of the characteristic functions of an NP-complete problem and its co-NP-complete complement is exactly the same even though the underlying decision problems may not be considered equivalent in some typical models of computation.

Unlike decision problems, for which there is only one correct answer for each input, optimization problems are concerned with finding the "best" answer to a particular input. Optimization problems arise naturally in many applications, such as the traveling salesman problem and many questions in linear programming.

There are standard techniques for transforming function and optimization problems into decision problems. For example, in the traveling salesman problem, the optimization problem is to produce a tour with minimal weight. The associated decision problem is: for each "N", to decide whether the graph has any tour with weight less than "N". By repeatedly answering the decision problem, it is possible to find the minimal weight of a tour.

Because the theory of decision problems is very well developed, research in complexity theory has typically focused on decision problems. Optimization problems themselves are still of interest in computability theory, as well as in fields such as operations research.




</doc>
<doc id="8339" url="https://en.wikipedia.org/wiki?curid=8339" title="Domain Name System">
Domain Name System

The Domain Name System (DNS) is a hierarchical decentralized naming system for computers, services, or other resources connected to the Internet or a private network. It associates various information with domain names assigned to each of the participating entities. Most prominently, it translates more readily memorized domain names to the numerical IP addresses needed for locating and identifying computer services and devices with the underlying network protocols. By providing a worldwide, distributed directory service, the Domain Name System is an essential component of the functionality on the Internet, that has been in use since 1985.

The Domain Name System delegates the responsibility of assigning domain names and mapping those names to Internet resources by designating authoritative name servers for each domain. Network administrators may delegate authority over sub-domains of their allocated name space to other name servers. This mechanism provides distributed and fault tolerant service and was designed to avoid a single large central database.

The Domain Name System also specifies the technical functionality of the database service that is at its core. It defines the DNS protocol, a detailed specification of the data structures and data communication exchanges used in the DNS, as part of the Internet Protocol Suite.

The Internet maintains two principal namespaces, the domain name hierarchy and the Internet Protocol (IP) address spaces. The Domain Name System maintains the domain name hierarchy and provides translation services between it and the address spaces. Internet name servers and a communication protocol implement the Domain Name System. A DNS name server is a server that stores the DNS records for a domain; a DNS name server responds with answers to queries against its database.

The most common types of records stored in the DNS database are for Start of Authority (SOA), IP addresses (A and AAAA), SMTP mail exchangers (MX), name servers (NS), pointers for reverse DNS lookups (PTR), and domain name aliases (CNAME). Although not intended to be a general purpose database, DNS can store records for other types of data for either automatic lookups, such as DNSSEC records, or for human queries such as "responsible person" (RP) records. As a general purpose database, the DNS has also been used in combating unsolicited email (spam) by storing a real-time blackhole list. The DNS database is traditionally stored in a structured text file, the zone file, but other database systems are common.

An often-used analogy to explain the Domain Name System is that it serves as the phone book for the Internet by translating human-friendly computer hostnames into IP addresses. For example, the domain name www.example.com translates to the addresses 93.184.216.34 (IPv4) and 2606:2800:220:1:248:1893:25c8:1946 (IPv6). Unlike a phone book, DNS can be quickly updated, allowing a service's location on the network to change without affecting the end users, who continue to use the same hostname. Users take advantage of this when they use meaningful Uniform Resource Locators (URLs), and e-mail addresses without having to know how the computer actually locates the services.

An important and ubiquitous function of DNS is its central role in distributed Internet services such as cloud services and content delivery networks. When a user accesses a distributed Internet service using a URL, the domain name of the URL is translated to the IP address of a server that is proximal to the user. The key functionality of DNS exploited here is that different users can "simultaneously" receive different translations for the "same" domain name, a key point of divergence from a traditional phone-book view of the DNS. This process of using the DNS to assign proximal servers to users is key to providing faster and more reliable responses on the Internet and is widely used by most major Internet services.

The DNS reflects the structure of administrative responsibility in the Internet. Each subdomain is a zone of administrative autonomy delegated to a manager. For zones operated by a registry, administrative information is often complemented by the registry's RDAP and WHOIS services. That data can be used to gain insight on, and track responsibility for, a given host on the Internet.

Using a simpler, more memorable name in place of a host's numerical address dates back to the ARPANET era. The Stanford Research Institute (now SRI International) maintained a text file named HOSTS.TXT that mapped host names to the numerical addresses of computers on the ARPANET. Maintenance of numerical addresses, called the Assigned Numbers List, was handled by Jon Postel at the University of Southern California's Information Sciences Institute (ISI), whose team worked closely with SRI.

Addresses were assigned manually. To request a hostname and an address and add a computer to the master file, users contacted the SRI's Network Information Center (NIC), directed by Elizabeth Feinler, by telephone during business hours.

By the early 1980s, maintaining a single, centralized host table had become slow and unwieldy and the emerging network required an automated naming system to address technical and personnel issues. Postel directed the task of forging a compromise between five competing proposals of solutions to Paul Mockapetris. Mockapetris instead created the Domain Name System.

The Internet Engineering Task Force published the original specifications in RFC 882 and RFC 883 in November 1983.

In 1984, four UC Berkeley students, Douglas Terry, Mark Painter, David Riggle, and Songnian Zhou, wrote the first Unix name server implementation for the Berkeley Internet Name Domain, commonly referred to as BIND. In 1985, Kevin Dunlap of DEC substantially revised the DNS implementation. Mike Karels, Phil Almquist, and Paul Vixie have maintained BIND since then. In the early 1990s, BIND was ported to the Windows NT platform. It was widely distributed, especially on Unix systems, and is still the most widely used DNS software on the Internet.

In November 1987, RFC 1034 and RFC 1035 superseded the 1983 DNS specifications. Several additional Request for Comments have proposed extensions to the core DNS protocols.

The domain name space consists of a tree data structure. Each node or leaf in the tree has a "label" and zero or more "resource records" (RR), which hold information associated with the domain name. The domain name itself consists of the label, possibly concatenated with the name of its parent node on the right, separated by a dot.

The tree sub-divides into "zones" beginning at the root zone. A DNS zone may consist of only one domain, or may consist of many domains and sub-domains, depending on the administrative choices of the zone manager. DNS can also be partitioned according to "class" where the separate classes can be thought of as an array of parallel namespace trees.

Administrative responsibility for any zone may be divided by creating additional zones. Authority over the new zone is said to be "delegated" to a designated name server. The parent zone ceases to be authoritative for the new zone.

The definitive descriptions of the rules for forming domain names appear in RFC 1035, RFC 1123, and RFC 2181.
A domain name consists of one or more parts, technically called "labels", that are conventionally concatenated, and delimited by dots, such as example.com.

The right-most label conveys the top-level domain; for example, the domain name www.example.com belongs to the top-level domain "com".

The hierarchy of domains descends from right to left; each label to the left specifies a subdivision, or subdomain of the domain to the right. For example, the label "example" specifies a subdomain of the "com" domain, and "www" is a subdomain of example.com. This tree of subdivisions may have up to 127 levels.

A label may contain zero to 63 characters. The null label, of length zero, is reserved for the root zone. The full domain name may not exceed the length of 253 characters in its textual representation. In the internal binary representation of the DNS the maximum length requires 255 octets of storage, as it also stores the length of the name.

Although no technical limitation exists to use any character in domain name labels which are representable by an octet, hostnames use a preferred format and character set. The characters allowed in labels are a subset of the ASCII character set, consisting of characters "a" through "z", "A" through "Z", digits "0" through "9", and hyphen. This rule is known as the "LDH rule" (letters, digits, hyphen). Domain names are interpreted in case-independent manner. Labels may not start or end with a hyphen. An additional rule requires that top-level domain names should not be all-numeric.

The limited set of ASCII characters permitted in the DNS prevented the representation of names and words of many languages in their native alphabets or scripts. To make this possible, ICANN approved the Internationalizing Domain Names in Applications (IDNA) system, by which user applications, such as web browsers, map Unicode strings into the valid DNS character set using Punycode. In 2009 ICANN approved the installation of internationalized domain name country code top-level domains ("ccTLD"s). In addition, many registries of the existing top-level domain names ("TLD"s) have adopted the IDNA system.

The Domain Name System is maintained by a distributed database system, which uses the client–server model. The nodes of this database are the name servers. Each domain has at least one authoritative DNS server that publishes information about that domain and the name servers of any domains subordinate to it. The top of the hierarchy is served by the root name servers, the servers to query when looking up ("resolving") a TLD.

An "authoritative" name server is a name server that only gives answers to DNS queries from data that has been configured by an original source, for example, the domain administrator or by dynamic DNS methods, in contrast to answers obtained via a query to another name server that only maintains a cache of data.

An authoritative name server can either be a "master" server or a "slave" server. A master server is a server that stores the original ("master") copies of all zone records. A slave server uses a special automatic updating mechanism in the DNS protocol in communication with its master to maintain an identical copy of the master records.

Every DNS zone must be assigned a set of authoritative name servers. This set of servers is stored in the parent domain zone with name server (NS) records.

An authoritative server indicates its status of supplying definitive answers, deemed "authoritative", by setting a protocol flag, called the ""Authoritative Answer"" ("AA") bit in its responses. This flag is usually reproduced prominently in the output of DNS administration query tools, such as dig, to indicate "that the responding name server is an authority for the domain name in question."

Domain name resolvers determine the domain name servers responsible for the domain name in question by a sequence of queries starting with the right-most (top-level) domain label.

For proper operation of its domain name resolver, a network host is configured with an initial cache ("hints") of the known addresses of the root name servers. The hints are updated periodically by an administrator by retrieving a dataset from a reliable source.

Assuming the resolver has no cached records to accelerate the process, the resolution process starts with a query to one of the root servers. In typical operation, the root servers do not answer directly, but respond with a referral to more authoritative servers, e.g., a query for "www.wikipedia.org" is referred to the "org" servers. The resolver now queries the servers referred to, and iteratively repeats this process until it receives an authoritative answer. The diagram illustrates this process for the host that is named by the fully qualified domain name "www.wikipedia.org".

This mechanism would place a large traffic burden on the root servers, if every resolution on the Internet required starting at the root. In practice caching is used in DNS servers to off-load the root servers, and as a result, root name servers actually are involved in only a relatively small fraction of all requests.

In theory, authoritative name servers are sufficient for the operation of the Internet. However, with only authoritative name servers operating, every DNS query must start with recursive queries at the root zone of the Domain Name System and each user system would have to implement resolver software capable of recursive operation.

To improve efficiency, reduce DNS traffic across the Internet, and increase performance in end-user applications, the Domain Name System supports DNS cache servers which store DNS query results for a period of time determined in the configuration ("time-to-live") of the domain name record in question.
Typically, such caching DNS servers also implement the recursive algorithm necessary to resolve a given name starting with the DNS root through to the authoritative name servers of the queried domain. With this function implemented in the name server, user applications gain efficiency in design and operation.

The combination of DNS caching and recursive functions in a name server is not mandatory; the functions can be implemented independently in servers for special purposes.

Internet service providers typically provide recursive and caching name servers for their customers. In addition, many home networking routers implement DNS caches and recursors to improve efficiency in the local network.

The client side of the DNS is called a DNS resolver. A resolver is responsible for initiating and sequencing the queries that ultimately lead to a full resolution (translation) of the resource sought, e.g., translation of a domain name into an IP address. DNS resolvers are classified by a variety of query methods, such as "recursive", "non-recursive", and "iterative". A resolution process may use a combination of these methods.

In a "non-recursive query", a DNS resolver queries a DNS server that provides a record either for which the server is authoritative, or it provides a partial result without querying other servers. In case of a caching DNS resolver, the non-recursive query of its local DNS cache delivers a result and reduces the load on upstream DNS servers by caching DNS request records for a period of time after an initial response from upstream DNS servers.

In a "recursive query", a DNS resolver queries a single DNS server, which may in turn query other DNS servers on behalf of the requester. For example, a simple stub resolver running on a home router typically makes a recursive query to the DNS server run by the user's ISP. A recursive query is one for which the DNS server answers the query completely by querying other name servers as needed. In typical operation, a client issues a recursive query to a caching recursive DNS server, which subsequently issues non-recursive queries to determine the answer and send a single answer back to the client. The resolver, or another DNS server acting recursively on behalf of the resolver, negotiates use of recursive service using bits in the query headers. DNS servers are not required to support recursive queries.

The "iterative query" procedure is a process in which a DNS resolver queries a chain of one or more DNS servers. Each server refers the client to the next server in the chain, until the current server can fully resolve the request. For example, a possible resolution of www.example.com would query a global root server, then a "com" server, and finally an "example.com" server.

Name servers in delegations are identified by name, rather than by IP address. This means that a resolving name server must issue another DNS request to find out the IP address of the server to which it has been referred. If the name given in the delegation is a subdomain of the domain for which the delegation is being provided, there is a circular dependency.

In this case, the name server providing the delegation must also provide one or more IP addresses for the authoritative name server mentioned in the delegation. This information is called "glue". The delegating name server provides this glue in the form of records in the "additional section" of the DNS response, and provides the delegation in the "authority section" of the response. A glue record is a combination of the name server and IP address.

For example, if the authoritative name server for example.org is ns1.example.org, a computer trying to resolve www.example.org first resolves ns1.example.org. As ns1 is contained in example.org, this requires resolving example.org first, which presents a circular dependency. To break the dependency, the name server for the top level domain org includes glue along with the delegation for example.org. The glue records are address records that provide IP addresses for ns1.example.org. The resolver uses one or more of these IP addresses to query one of the domain's authoritative servers, which allows it to complete the DNS query.

A standard practice in implementing name resolution in applications is to reduce the load on the Domain Name System servers by caching results locally, or in intermediate resolver hosts. Results obtained from a DNS request are always associated with the time to live (TTL), an expiration time after which the results must be discarded or refreshed. The TTL is set by the administrator of the authoritative DNS server. The period of validity may vary from a few seconds to days or even weeks.

As a result of this distributed caching architecture, changes to DNS records do not propagate throughout the network immediately, but require all caches to expire and to be refreshed after the TTL. RFC 1912 conveys basic rules for determining appropriate TTL values.

Some resolvers may override TTL values, as the protocol supports caching for up to sixty-eight years or no caching at all. Negative caching, i.e. the caching of the fact of non-existence of a record, is determined by name servers authoritative for a zone which must include the Start of Authority (SOA) record when reporting no data of the requested type exists. The value of the "minimum" field of the SOA record and the TTL of the SOA itself is used to establish the TTL for the negative answer.

A reverse lookup is a query of the DNS for domain names when the IP address is known. Multiple domain names may be associated with an IP address. The DNS stores IP addresses in the form of domain names as specially formatted names in pointer (PTR) records within the infrastructure top-level domain arpa. For IPv4, the domain is in-addr.arpa. For IPv6, the reverse lookup domain is ip6.arpa. The IP address is represented as a name in reverse-ordered octet representation for IPv4, and reverse-ordered nibble representation for IPv6.

When performing a reverse lookup, the DNS client converts the address into these formats before querying the name for a PTR record following the delegation chain as for any DNS query. For example, assuming the IPv4 address 208.80.152.2 is assigned to Wikimedia, it is represented as a DNS name in reverse order: 2.152.80.208.in-addr.arpa. When the DNS resolver gets a pointer (PTR) request, it begins by querying the root servers, which point to the servers of American Registry for Internet Numbers (ARIN) for the 208.in-addr.arpa zone. ARIN's servers delegate 152.80.208.in-addr.arpa to Wikimedia to which the resolver sends another query for 2.152.80.208.in-addr.arpa, which results in an authoritative response.

Users generally do not communicate directly with a DNS resolver. Instead DNS resolution takes place transparently in applications such as web browsers, e-mail clients, and other Internet applications. When an application makes a request that requires a domain name lookup, such programs send a resolution request to the DNS resolver in the local operating system, which in turn handles the communications required.

The DNS resolver will almost invariably have a cache (see above) containing recent lookups. If the cache can provide the answer to the request, the resolver will return the value in the cache to the program that made the request. If the cache does not contain the answer, the resolver will send the request to one or more designated DNS servers. In the case of most home users, the Internet service provider to which the machine connects will usually supply this DNS server: such a user will either have configured that server's address manually or allowed DHCP to set it; however, where systems administrators have configured systems to use their own DNS servers, their DNS resolvers point to separately maintained name servers of the organization. In any event, the name server thus queried will follow the process outlined above, until it either successfully finds a result or does not. It then returns its results to the DNS resolver; assuming it has found a result, the resolver duly caches that result for future use, and hands the result back to the software which initiated the request.

Some large ISPs have configured their DNS servers to violate rules, such as by disobeying TTLs, or by indicating that a domain name does not exist just because one of its name servers does not respond.

Some applications, such as web browsers, maintain an internal DNS cache to avoid repeated lookups via the network. This practice can add extra difficulty when debugging DNS issues, as it obscures the history of such data. These caches typically use very short caching times – in the order of one minute.

Internet Explorer represents a notable exception: versions up to IE 3.x cache DNS records for 24 hours by default. Internet Explorer 4.x and later versions (up to IE 8) decrease the default time out value to half an hour, which may be changed by modifying default configuration.

Google Chrome triggers a specific error message for DNS issues. When the DNS server is down or broken, Google Chrome returns an error message.
The Domain Name System includes several other functions and features.

Hostnames and IP addresses are not required to match in a one-to-one relationship. Multiple hostnames may correspond to a single IP address, which is useful in virtual hosting, in which many web sites are served from a single host. Alternatively, a single hostname may resolve to many IP addresses to facilitate fault tolerance and load distribution to multiple server instances across an enterprise or the global Internet.

DNS serves other purposes in addition to translating names to IP addresses. For instance, mail transfer agents use DNS to find the best mail server to deliver e-mail: An MX record provides a mapping between a domain and a mail exchanger; this can provide an additional layer of fault tolerance and load distribution.

The DNS is used for efficient storage and distribution of IP addresses of blacklisted email hosts. A common method is to place the IP address of the subject host into the sub-domain of a higher level domain name, and to resolve that name to a record that indicates a positive or a negative indication.

For example:
E-mail servers can query blacklist.example to find out if a specific host connecting to them is in the blacklist. Many of such blacklists, either subscription-based or free of cost, are available for use by email administrators and anti-spam software.

The Sender Policy Framework and DomainKeys were designed to take advantage of another DNS record type, the TXT record, but have since been assigned specific record types.

To provide resilience in the event of computer or network failure, multiple DNS servers are usually provided for coverage of each domain. At the top level of global DNS, thirteen groups of root name servers exist, with additional "copies" of them distributed worldwide via anycast addressing.

Dynamic DNS (DDNS) updates a DNS server with a client IP address on-the-fly, for example, when moving between ISPs or mobile hot spots, or when the IP address changes administratively.

The DNS protocol uses two types of DNS messages, queries and replies, and they both have the same format. Each message consists of a header and four sections: question, answer, authority, and an additional space. A header field ("flags") controls the content of these four sections.

The header section contains the following fields: "Identification", "Flags", "Number of questions", "Number of answers", "Number of authority resource records" (RRs), and "Number of additional RRs". The identification field can be used to match responses with queries. The flag field consists of several sub-fields. The first is a single bit which indicates if the message is a query (0) or a reply (1). The second sub-field consists of four bits indicating the type of query, or the type of query this message is a response to. 0 is a standard query, 1 an inverse query, 2 is a server status request. A single-bit sub-field indicates if the DNS server is authoritative for the queried hostname. Another single-bit sub-field indicates if the client wants to send a recursive query ("RD"). The next single-bit sub-field indicates if the replying DNS server supports recursion ("RA"), as not all DNS servers are configured to do this task. Another sub-field indicates if the message was truncated for some reason ("TC"), and a four-bit sub-field is used for error codes. The "question" section contains the domain name and type of record (A, AAAA, MX, TXT, etc.) being resolved. The domain name is broken into discrete labels which are concatenated; each label is prefixed by the length of that label. The "answer" section has the resource records of the queried name. A domain name may occur in multiple records if it has multiple IP addresses associated.

DNS primarily uses the User Datagram Protocol (UDP) on port number 53 to serve requests. DNS queries consist of a single UDP request from the client followed by a single UDP reply from the server. When the length of the answer exceeds 512 bytes and both client and server support EDNS, larger UDP packets are used. Otherwise, the query is sent again using the Transmission Control Protocol (TCP). TCP is also used for tasks such as zone transfers. Some resolver implementations use TCP for all queries.

The Domain Name System specifies a set of various types of resource records (RRs), which are the basic information elements of the domain name system. Each record has a type (name and number), an expiration time (time to live), a class, and type-specific data. Resource records of the same type are described as a "resource record set" (RRset). The order of resource records in a set, which is returned by a resolver to an application, is undefined, but often servers implement round-robin ordering to achieve load balancing. The Domain Name System Security Extensions (DNSSEC), however, work on the complete set of resource record in canonical order.

When sent over an Internet Protocol network, all records use the common format specified in RFC 1035:

"NAME" is the fully qualified domain name of the node in the tree . On the wire, the name may be shortened using label compression where ends of domain names mentioned earlier in the packet can be substituted for the end of the current domain name. A free standing "@" is used to denote the current origin.

"TYPE" is the record type. It indicates the format of the data and it gives a hint of its intended use. For example, the "A" record is used to translate from a domain name to an IPv4 address, the "NS" record lists which name servers can answer lookups on a DNS zone, and the "MX" record specifies the mail server used to handle mail for a domain specified in an e-mail address.

"RDATA" is data of type-specific relevance, such as the IP address for address records, or the priority and hostname for MX records. Well known record types may use label compression in the RDATA field, but "unknown" record types must not (RFC 3597).

The "CLASS" of a record is set to IN (for "Internet") for common DNS records involving Internet hostnames, servers, or IP addresses. In addition, the classes Chaos (CH) and Hesiod (HS) exist. Each class is an independent name space with potentially different delegations of DNS zones.

In addition to resource records defined in a zone file, the domain name system also defines several request types that are used only in communication with other DNS nodes ("on the wire"), such as when performing zone transfers (AXFR/IXFR) or for EDNS (OPT).

The domain name system supports wildcard DNS records which specify names that start with the "asterisk label", '*', e.g., *.example. DNS records belonging to wildcard domain names specify rules for generating resource records within a single DNS zone by substituting whole labels with matching components of the query name, including any specified descendants. For example, in the following configuration, the DNS zone "x.example" specifies that all subdomains, including subdomains of subdomains, of "x.example" use the mail exchanger (MX) "a.x.example". The A record for "a.x.example" is needed to specify the mail exchanger IP address. As this has the result of excluding this domain name and its subdomains from the wildcard matches, an additional MX record for the subdomain "a.x.example", as well as a wildcarded MX record for all of its subdomains, must also be defined in the DNS zone.

The role of wildcard records was refined in RFC 4592, because the original definition in RFC 1034 was incomplete and resulted in misinterpretations by implementers.

The original DNS protocol had limited provisions for extension with new features. In 1999, Paul Vixie published in RFC 2671 (superseded by RFC 6891) an extension mechanism, called Extension mechanisms for DNS (EDNS) that introduced optional protocol elements without increasing overhead when not in use. This was accomplished through the OPT pseudo-resource record that only exists in wire transmissions of the protocol, but not in any zone files. Initial extensions were also suggested (EDNS0), such as increasing the DNS message size in UDP datagrams.

Dynamic DNS updates use the UPDATE DNS opcode to add or remove resource records dynamically from a zone database maintained on an authoritative DNS server. The feature is described in RFC 2136. This facility is useful to register network clients into the DNS when they boot or become otherwise available on the network. As a booting client may be assigned a different IP address each time from a DHCP server, it is not possible to provide static DNS assignments for such clients.

Originally, security concerns were not major design considerations for DNS software or any software for deployment on the early Internet, as the network was not open for participation by the general public. However, the expansion of the Internet into the commercial sector in the 1990s changed the requirements for security measures to protect data integrity and user authentication.

Several vulnerability issues were discovered and exploited by malicious users. One such issue is DNS cache poisoning, in which data is distributed to caching resolvers under the pretense of being an authoritative origin server, thereby polluting the data store with potentially false information and long expiration times (time-to-live). Subsequently, legitimate application requests may be redirected to network hosts operated with malicious intent.

DNS responses traditionally do not have a cryptographic signature, leading to many attack possibilities; the Domain Name System Security Extensions (DNSSEC) modify DNS to add support for cryptographically signed responses. DNSCurve has been proposed as an alternative to DNSSEC. Other extensions, such as TSIG, add support for cryptographic authentication between trusted peers and are commonly used to authorize zone transfer or dynamic update operations.

Some domain names may be used to achieve spoofing effects. For example, and paypa1.com are different names, yet users may be unable to distinguish them in a graphical user interface depending on the user's chosen typeface. In many fonts the letter "l" and the numeral "1" look very similar or even identical. This problem is acute in systems that support internationalized domain names, as many character codes in ISO 10646 may appear identical on typical computer screens. This vulnerability is occasionally exploited in phishing.

Techniques such as forward-confirmed reverse DNS can also be used to help validate DNS results.

A device looking up a DNS record must communicate with a DNS server to do so. Considerable attention has been given to the adverse privacy implications. Even if DNS records cannot easily be read, modified or spoofed due to security extensions, a person with access to the DNS server or the traffic stream "on the wire" may have little difficulty in matching the IP address of the device (which often identifies the user), to the websites, email or other domains they visit, and track how often and when these records are queried, since DNS records typically expire and must be requeried regularly.

DNS can also "leak" from otherwise secure or private connections, if attention is not paid to their configuration, and at times DNS has been used to bypass firewalls by malicious persons, and exfiltrate data, since it is often seen as innocuous.

Two main approaches are in use to counter privacy issues with DNS:

The right to use a domain name is delegated by domain name registrars which are accredited by the Internet Corporation for Assigned Names and Numbers (ICANN) or other organizations such as OpenNIC, that are charged with overseeing the name and number systems of the Internet. In addition to ICANN, each top-level domain (TLD) is maintained and serviced technically by an administrative organization, operating a registry. A "registry" is responsible for operating the database of names within its authoritative zone, although the term is most often used for TLDs. A "registrant" is a person or organization who asked for domain registration. The registry receives registration information from each domain name "registrar", which is authorized (accredited) to assign names in the corresponding zone and publishes the information using the WHOIS protocol. As of 2015, usage of RDAP is being considered.

ICANN publishes the complete list of TLDs, TLD registries, and domain name registrars. Registrant information associated with domain names is maintained in an online database accessible with the WHOIS service. For most of the more than 290 country code top-level domains (ccTLDs), the domain registries maintain the WHOIS (Registrant, name servers, expiration dates, etc.) information. For instance, DENIC, Germany NIC, holds the DE domain data. From about 2001, most Generic top-level domain (gTLD) registries have adopted this so-called "thick" registry approach, i.e. keeping the WHOIS data in central registries instead of registrar databases.

For top-level domains on COM and NET, a "thin" registry model is used. The domain registry (e.g., GoDaddy, BigRock and PDR, VeriSign, etc, etc) holds basic WHOIS data (i.e., registrar and name servers, etc.). Organizations, or registrants using ORG on the other hand, are on the Public Interest Registry exclusively.

Some domain name registries, often called "network information centers" (NIC), also function as registrars to end-users, in addition to providing access to the WHOIS datasets. The top-level domain registries, such as for the domains COM, NET, and ORG use a registry-registrar model consisting of many domain name registrars. In this method of management, the registry only manages the domain name database and the relationship with the registrars. The "registrants" (users of a domain name) are customers of the registrar, in some cases through additional subcontracting of resellers.

The Domain Name System is defined by Request for Comments (RFC) documents published by the Internet Engineering Task Force (Internet standards). The following is a list of RFCs that define the DNS protocol.





These RFCs are advisory in nature, but may provide useful information despite defining neither a standard or BCP. (RFC 1796)


These RFCs have an official status of Unknown, but due to their age are not clearly labeled as such.






</doc>
<doc id="8340" url="https://en.wikipedia.org/wiki?curid=8340" title="David Letterman">
David Letterman

David Michael Letterman (born April 12, 1947) is an American television host, comedian, writer, and producer. He hosted late night television talk shows for 33 years, beginning with the February 1, 1982, debut of "Late Night with David Letterman" on NBC, and ending with the May 20, 2015, broadcast of "Late Show with David Letterman" on CBS. In total, Letterman hosted 6,028 episodes of "Late Night" and "Late Show", surpassing friend and mentor Johnny Carson as the longest-serving late night talk show host in American television history. In 1996 Letterman was ranked 45th on "TV Guide"s 50 Greatest TV Stars of All Time. In 2002, "The Late Show with David Letterman" was ranked seventh on TV Guide's 50 Greatest TV Shows of All Time.

Letterman currently hosts the Netflix series "My Next Guest Needs No Introduction with David Letterman". 

Letterman is also a television and film producer. His company, Worldwide Pants, produced his shows as well as "The Late Late Show with Craig Ferguson" and several prime-time comedies, the most successful of which was "Everybody Loves Raymond", now in syndication.

Several late-night hosts have cited Letterman's influence, including Conan O'Brien (his successor on "Late Night"), Stephen Colbert (his successor on "The Late Show"), Jimmy Fallon, Jimmy Kimmel, and Seth Meyers.

Letterman was born in Indianapolis, Indiana. His father, Harry Joseph Letterman (April 15, 1915 – February 13, 1973), was a florist. His mother, Dorothy Marie Letterman Mengering (née Hofert; July 18, 1921 – April 11, 2017), a church secretary for the Second Presbyterian Church of Indianapolis, was an occasional figure on Letterman's show, usually at holidays and birthdays.

He lived on the north side of Indianapolis (Broad Ripple area), about 12 miles from the Indianapolis Motor Speedway and he enjoyed collecting model cars, including racers. In 2000, he told an interviewer for "Esquire" that, while growing up, he admired his father's ability to tell jokes and be the life of the party. Harry Joseph Letterman survived a heart attack at age 36, when David was a young boy. The fear of losing his father was constantly with Letterman as he grew up. The elder Letterman died of a second heart attack at age 57.
Letterman attended his hometown's Broad Ripple High School and worked as a stock boy at the local Atlas Supermarket. According to the "Ball State Daily News", he originally had wanted to attend Indiana University, but his grades were not good enough, so he instead attended Ball State University, in Muncie, Indiana. He is a member of the Sigma Chi fraternity, and he graduated in 1969 from what was then the Department of Radio and Television. A self-described average student, Letterman later endowed a scholarship for what he called "C students" at Ball State.

Though he registered for the draft and passed his physical after graduating from college, he was not drafted for service in Vietnam because of receiving a draft lottery number of 346 (out of 366).

Letterman began his broadcasting career as an announcer and newscaster at the college's student-run radio station—WBST—a 10-watt campus station which now is part of Indiana Public Radio. He was fired for treating classical music with irreverence. He then became involved with the founding of another campus station—WAGO-AM 570 (now WWHI, 91.3).

He credits Paul Dixon, host of the "Paul Dixon Show", a Cincinnati-based talk show also shown in Indianapolis while he was growing up, for inspiring his choice of career:
I was just out of college [in 1969], and I really didn't know what I wanted to do. And then all of a sudden I saw him doing it [on TV]. And I thought: That's really what I want to do!

Letterman began his career as a radio talk show host on WNTS (AM) and on Indianapolis television station WLWI (which changed its call sign to WTHR in 1976) as an anchor and weatherman. He received some attention for his unpredictable on-air behavior, which included congratulating a tropical storm for being upgraded to a hurricane and predicting hail stones "the size of canned hams." He would also occasionally report the weather and the day's very high and low temps for fictitious cities ("Eight inches of snow in Bingree and surrounding areas") while on another occasion saying that a state border had been erased when a satellite map accidentally omitted the state border between Indiana and Ohio, attributing it to dirty political dealings. ("The higher-ups have removed the border between Indiana and Ohio making it one giant state. Personally, I'm against it. I don't know what to do about it.") He also starred in a local kiddie show, made wisecracks as host of a late night TV show called "Freeze-Dried Movies" (he once acted out a scene from "Godzilla" using plastic dinosaurs), and hosted a talk show that aired early on Saturday mornings called "Clover Power", in which he interviewed 4-H members about their projects.

In 1971 Letterman appeared as a pit road reporter for ABC Sports' tape-delayed coverage of the Indianapolis 500 (his first nationally telecast appearance; WLWI was the local ABC affiliate at the time). Letterman was initially introduced as Chris Economaki, although this was corrected at the end of the interview (Jim McKay announced his name as Dave Letterman). Letterman interviewed Mario Andretti, who had just crashed out of the race.

In 1975, encouraged by his then-wife Michelle and several of his Sigma Chi fraternity brothers, Letterman moved to Los Angeles, with hope of becoming a comedy writer. He and Michelle packed their belongings in his pickup truck and headed west. As of 2012, he still owned the truck. In Los Angeles, he began performing comedy at The Comedy Store. Jimmie Walker saw him on stage; with an endorsement from George Miller, Letterman joined a group of comedians whom Walker hired to write jokes for his stand-up act, a group that at various times would also include Jay Leno, Paul Mooney, Robert Schimmel, Richard Jeni, Louie Anderson, Elayne Boosler, Byron Allen, Jack Handey, and Steve Oedekerk.

By the summer of 1977, Letterman was a writer and regular on the six-week summer series "The Starland Vocal Band Show", broadcast on CBS. He hosted a 1977 pilot for a game show entitled "The Riddlers" (that was never picked up), and co-starred in the Barry Levinson-produced comedy special "Peeping Times" that aired in January 1978. Later that year, Letterman was a cast member on Mary Tyler Moore's variety show, "Mary". Letterman made a guest appearance on "Mork & Mindy" (as a parody of EST leader Werner Erhard) and appearances on game shows such as "The $20,000 Pyramid", "The Gong Show", "Hollywood Squares", "Password Plus" and "Liar's Club", as well as the Canadian cooking show "Celebrity Cooks" (November 1977), talk shows such as "90 Minutes Live" (February 24 and April 14, 1978), and "The Mike Douglas Show" (April 3, 1979 and February 7, 1980). He was also screen tested for the lead role in the 1980 film "Airplane!", a role that eventually went to Robert Hays.

His dry, sarcastic humor caught the attention of scouts for "The Tonight Show Starring Johnny Carson", and Letterman was soon a regular guest on the show. Letterman became a favorite of Carson and was a regular guest host for the show beginning in 1978. Letterman credits Carson as the person who influenced his career the most.

On June 23, 1980, Letterman was given his own morning comedy show on NBC, "The David Letterman Show". It was originally 90 minutes long, but was shortened to 60 minutes in August 1980. The show was a critical success, winning two Emmy Awards, but was a ratings disappointment and was canceled in October 1980.

NBC kept Letterman under contract (paying him) to be able to try him in a different time slot. "Late Night with David Letterman" debuted February 1, 1982; the first guest on the first show was Bill Murray. Murray later went on to become one of Letterman's most recurrent guests, guesting on the show's 30th anniversary episode, which aired January 31, 2012 and on the very last show, which aired May 20, 2015. The show ran Monday through Thursday at 12:30 a.m. Eastern Time, immediately following "The Tonight Show Starring Johnny Carson" (a Friday night broadcast was added in June 1987). It was seen as being edgy and unpredictable, and soon developed a cult following (particularly among college students). Letterman's reputation as an acerbic interviewer was borne out in verbal sparring matches with Cher (who even called him an asshole on the show), Shirley MacLaine, Charles Grodin, and Madonna. The show also featured comedy segments and running characters, in a style heavily influenced by the 1950s and 1960s programs of Steve Allen.

The show often featured quirky, genre-mocking regular features, including "Stupid Pet Tricks" (which had its origins on Letterman's morning show), Stupid Human Tricks, dropping various objects off the roof of a five-story building, demonstrations of unorthodox clothing (such as suits made of Alka-Seltzer, Velcro and suet), a recurring Top 10 list, the Monkey-Cam (and the Audience Cam), a facetious letter-answering segment, several "Film[s] by My Dog Bob" in which a camera was mounted on Letterman's own dog (often with comic results) and Small Town News, all of which would eventually move with Letterman to CBS.

Other memorable moments included Letterman using a bullhorn to interrupt a live interview on "The Today Show", announcing that he was the NBC News president and that he was not wearing any pants; walking across the hall to Studio 6B, at the time the news studio for WNBC-TV, and interrupting Al Roker's weather segments during "Live at Five"; and staging "elevator races", complete with commentary by NBC Sports' Bob Costas. In one infamous appearance, in 1982, Andy Kaufman (who was already wearing a neck brace) appeared with professional wrestler Jerry Lawler, who slapped and knocked the comedian to the ground (though Lawler and Kaufman's friend Bob Zmuda later revealed that the event was staged).

In 1992, Johnny Carson retired, and many fans believed that Letterman would become host of "The Tonight Show". When NBC instead gave the job to Jay Leno, Letterman departed NBC to host his own late-night show on CBS, opposite "The Tonight Show" at 11:30 p.m., called the "Late Show with David Letterman". The new show debuted on August 30, 1993, and was taped at the historic Ed Sullivan Theater, where Ed Sullivan broadcast his eponymous variety series from 1948 to 1971. For Letterman's arrival, CBS spent  million in renovations. In addition to that cost, CBS also signed Letterman to a lucrative three-year,  million/year contract, doubling his "Late Night" salary. The total cost for everything (renovations, negotiation right paid to NBC, signing Letterman, announcer Bill Wendell, Paul Shaffer, the writers and the band) was over  million.

But while the expectation was that Letterman would retain his unique style and sense of humor with the move, "Late Show" was not an exact replica of his old NBC program. Recognizing the more formal mood (and wider audience) of his new time slot and studio, Letterman eschewed his trademark blazer with khaki pants and white wrestling shoes wardrobe combination in favor of expensive shoes, tailored suits and light-colored socks. The monologue was lengthened. Paul Shaffer and the World's Most Dangerous Band followed Letterman to CBS, but they added a brass section and were rebranded the CBS Orchestra (Shaffer's request); a small band had been mandated by Carson while Letterman occupied the 12:30 slot. Additionally, because of intellectual property disagreements, Letterman was unable to import many of his "Late Night" segments verbatim, but he sidestepped this problem by simply renaming them (the "Top Ten List" became the "Late Show Top Ten", "Viewer Mail" became the "CBS Mailbag", etc.) "Time" magazine stated that "Letterman's innovation ... gained power from its rigorous formalism", as his biographer Jason Zinoman puts it, he was "a fascinatingly disgruntled eccentric trapped inside a more traditional talk show."

The main competitor of the "Late Show" was NBC's "The Tonight Show", which was hosted by Jay Leno for 22 years, but from June 1, 2009, to January 22, 2010, was hosted by Conan O'Brien. In 1993 and 1994, the "Late Show" consistently gained higher ratings than "The Tonight Show". But in 1995, ratings dipped and Leno's show consistently beat Letterman's in the ratings from the time that Hugh Grant came on Leno's show after Grant's arrest for soliciting a prostitute;

Leno typically attracted about five million nightly viewers between 1999 and 2009. The "Late Show" lost nearly half its audience during its competition with Leno, attracting 7.1 million viewers nightly in its 1993–94 season and about 3.8 million per night as of Leno's departure in 2009. In the final months of his first stint as host of "The Tonight Show", Leno beat Letterman in the ratings by a 1.3 million viewer margin (5.2 million to 3.9 million), and "Nightline" and the "Late Show" were virtually tied. Once O'Brien took over "Tonight", however, Letterman closed the gap in the ratings. O'Brien initially drove the median age of "Tonight Show" viewers from 55 to 45, with most older viewers opting to watch the "Late Show" instead. Following Leno's return to "The Tonight Show", however, Leno regained his lead.

Letterman's shows have garnered both critical and industry praise, receiving 67 Emmy Award nominations, winning 12 times in his first 20 years in late night television. From 1993 to 2009, Letterman ranked higher than Leno in the annual Harris Poll of "Nation's Favorite TV Personality" 12 times. For example, in 2003 and 2004 Letterman ranked second in that poll, behind only Oprah Winfrey, a year that Leno was ranked fifth. Leno was higher than Letterman on that poll three times during the same period, in 1998, 2007, and 2008.

On March 27, 1995, Letterman acted as the host for the 67th Academy Awards ceremony. Critics blasted Letterman for what they deemed a poor hosting of the Oscars, noting that his irreverent style undermined the traditional importance and glamor of the event. In a joke about their unusual names (inspired by a celebrated comic essay in "The New Yorker", "Yma Dream" by Thomas Meehan), he started off by introducing Uma Thurman to Oprah Winfrey, and then both of them to Keanu Reeves: "Oprah...Uma. Uma...Oprah," "Have you kids met Keanu?" This and many of his other jokes fell flat. Although Letterman attracted the highest ratings to the annual telecast since 1983, many felt that the bad publicity garnered by Letterman's hosting caused a decline in the "Late Show"'s ratings.

Letterman recycled the apparent debacle into a long-running gag. On his first show after the Oscars, he joked, "Looking back, I had no idea that thing was being televised." He lampooned his stint two years later, during Billy Crystal's opening Oscar skit, which also parodied the plane-crashing scenes from that year's chief nominated film, "The English Patient".

For years afterward, Letterman recounted his hosting the Oscars, although the Academy of Motion Picture Arts and Sciences continued to hold Letterman in high regard and they had invited him to host the Oscars again. On September 7, 2010, he made an appearance on the premiere of the 14th season of "The View", and confirmed that he had been considered for hosting again.

On January 14, 2000, a routine check-up revealed that an artery in Letterman's heart was severely obstructed. He was rushed to emergency surgery for a quintuple bypass.

During the initial weeks of his recovery, reruns of the "Late Show" were shown and introduced by friends of Letterman including Drew Barrymore, Ray Romano, Robin Williams, Bonnie Hunt, Megan Mullally, Bill Murray, Regis Philbin, Charles Grodin, Nathan Lane, Julia Roberts, Bruce Willis, Jerry Seinfeld, Martin Short, Steven Seagal, Hillary Clinton, Danny DeVito, Steve Martin, and Sarah Jessica Parker.

Subsequently, while still recovering from surgery, Letterman revived the late night tradition that had virtually disappeared on network television during the 1990s of 'guest hosts' by allowing Bill Cosby, Kathie Lee Gifford, Dana Carvey, Janeane Garofalo, and others to host new episodes of the "Late Show".

Upon his return to the show on February 21, 2000, Letterman brought all but one of the doctors and nurses on stage who had participated in his surgery and recovery (with extra teasing of a nurse who had given him bed baths—"This woman has seen me naked!"), including Dr. O. Wayne Isom and physician Louis Aronne, who frequently appeared on the show. In a show of emotion, Letterman was nearly in tears as he thanked the health care team with the words "These are the people who saved my life!" The episode earned an Emmy nomination.

For a number of episodes, Letterman continued to crack jokes about his bypass, including saying, "Bypass surgery: it's when doctors surgically create new blood flow to your heart. A bypass is what happened to me when I didn't get "The Tonight Show!" It's a whole different thing." In a later running gag he lobbied his home state of Indiana to rename the freeway circling Indianapolis (I-465) "The David Letterman Bypass." He also featured a montage of faux news coverage of his bypass surgery, which included a clip of Letterman's heart for sale on the Home Shopping Network. Letterman became friends with his doctors and nurses. In 2008, a "Rolling Stone" interview stated he hosted a doctor and nurse who'd helped perform the emergency quintuple-bypass heart surgery that saved his life in 2000. 'These are people who were complete strangers when they opened my chest,' he says. 'And now, eight years later, they're among my best friends.'

Additionally, Letterman invited the band Foo Fighters to play "Everlong", introducing them as "my favorite band, playing my favorite song." During a later Foo Fighters appearance, Letterman said that Foo Fighters had been in the middle of a South American tour which they canceled to come play on his comeback episode.

Letterman again handed over the reins of the show to several guest hosts (including Bill Cosby, Brad Garrett, Whoopi Goldberg, Elvis Costello, John McEnroe, Vince Vaughn, Will Ferrell, Bonnie Hunt, Luke Wilson and bandleader Paul Shaffer) in February 2003, when he was diagnosed with a severe case of shingles. Later that year, Letterman made regular use of guest hosts—including Tom Arnold and Kelsey Grammer—for new shows broadcast on Fridays. In March 2007, Adam Sandler—who had been scheduled to be the lead guest—served as a guest host while Letterman was ill with a stomach virus.

In March 2002, as Letterman's contract with CBS neared expiration, ABC offered him the time slot for long-running news program "Nightline" with Ted Koppel. Letterman was interested as he believed he could never match Leno's ratings at CBS due to Letterman's complaint of weaker lead-ins from the network's late local news programs, but was reluctant to replace Koppel. Letterman addressed his decision to re-sign on the air, stating that he was content at CBS and that he had great respect for Koppel.

On December 4, 2006, CBS revealed that Letterman signed a new contract to host "Late Show with David Letterman" through the fall of 2010. "I'm thrilled to be continuing on at CBS," said Letterman. "At my age you really don't want to have to learn a new commute." Letterman further joked about the subject by pulling up his right pants leg, revealing a tattoo, presumably temporary, of the ABC logo.

"Thirteen years ago, David Letterman put CBS late night on the map and in the process became one of the defining icons of our network," said Leslie Moonves, president and CEO of CBS Corporation. His presence on our air is an ongoing source of pride, and the creativity and imagination that the "Late Show" puts forth every night is an ongoing display of the highest quality entertainment. We are truly honored that one of the most revered and talented entertainers of our time will continue to call CBS 'home.'

According to a 2007 article in "Forbes" magazine, Letterman earned  million a year. A 2009 article in "The New York Times", however, said his salary was estimated at  million per year. In June 2009, Letterman's Worldwide Pants and CBS reached agreement to continue the "Late Show" until at least August 2012. The previous contract had been set to expire in 2010, and the two-year extension is shorter than the typical three-year contract period negotiated in the past. Worldwide Pants agreed to lower its fee for the show, though it had remained a "solid moneymaker for CBS" under the previous contract.

On the February 3, 2011, edition of the "Late Show", during an interview with Howard Stern, Letterman said he would continue to do his talk show for "maybe two years, I think."

In April 2012, CBS announced it had extended its contract with Letterman through 2014. His contract was subsequently extended to 2015.

During the taping of his April 3, 2014, show, Letterman announced that he had informed CBS president Leslie Moonves that he would retire from hosting "Late Show" by May 20, 2015. It was announced soon after that comedian and political satirist Stephen Colbert would succeed Letterman. Letterman's last episode aired on May 20, 2015, and opened with a presidential send off featuring four of the five living American presidents, George H. W. Bush, Bill Clinton, George W. Bush and Barack Obama, each mimicking the late president Gerald Ford's statement that "Our long national nightmare is over." It also featured cameos from "The Simpsons" and "Wheel of Fortune" (the latter with a puzzle saying "Good riddance to David Letterman"), a Top Ten List of "things I wish I could have said to David Letterman" performed by regular guests including Alec Baldwin, Barbara Walters, Steve Martin, Jerry Seinfeld, Jim Carrey, Chris Rock, Julia Louis-Dreyfus, Peyton Manning, Tina Fey, and Bill Murray, and closed with a montage of scenes from both his CBS and NBC series set to a live performance of "Everlong" by Foo Fighters.

The final episode of "Late Show with David Letterman" was watched by 13.76 million viewers in the United States with an audience share of 9.3/24, earning the show its highest ratings since following the 1994 Olympics on February 25, 1994, and the show's highest demo numbers (4.1 in adults 25–54 and 3.1 in adults 18–49) since Oprah Winfrey's first "Late Show" appearance following the ending of her feud with Letterman on December 1, 2005. Bill Murray, who had been his first guest on "Late Night", was his final guest on "Late Show". In a rarity for a late-night show, it was also the highest-rated program on network television that night, beating out all prime-time shows. In total, Letterman hosted 6,028 episodes of "Late Night" and "Late Show", surpassing friend and mentor Johnny Carson as the longest-serving late night talk show host in American television history.

In the months following the end of "Late Show" Letterman has been seen occasionally at sports events such as the Indianapolis 500, during which he submitted to an interview with a local publication. He made a surprise appearance on stage in San Antonio, Texas when he was invited up for an extended segment during Steve Martin and Martin Short's "A Very Stupid Conversation" show saying "I retired, and...I have no regrets," Letterman told the crowd after walking on stage. "I was happy. I'll make actual friends. I was complacent. I was satisfied. I was content, and then a couple of days ago Donald Trump said he was running for president. I have made the biggest mistake of my life, ladies and gentlemen" and then delivering a Top Ten List roasting Donald Trump's presidential campaign followed by an on-stage conversation with Martin and Short. Cell phone recordings of the appearance were posted on YouTube by audience members and were widely reported in the media.

In 2016, Letterman joined the climate change documentary show "Years of Living Dangerously" as one of the show's celebrity correspondents. In season two's premiere episode, Letterman traveled to India to investigate the country's efforts to expand its inadequate energy grid, power its booming economy and bring electricity for the first time to 300 million citizens. He also interviewed Indian Prime Minister Narendra Modi, and traveled to rural villages where power is a scarce luxury and explored the United States' role in India's energy future.

On April 7, 2017, Letterman gave the induction speech for the band Pearl Jam into the Rock & Roll Hall Of Fame at a ceremony held at the Barclays Center in Brooklyn, New York City. Also in 2017, Letterman and Alec Baldwin co-hosted "The Essentials" on Turner Classic Movies. Letterman and Baldwin introduced seven films for the series.

In 2018, Letterman has been hosting a six-episode monthly series of hour-long programs on Netflix consisting of long-form interviews and field segments. The show, "My Next Guest Needs No Introduction with David Letterman", premiered January 12, 2018, featuring Barack Obama.

In spite of Johnny Carson's clear intention to pass his title to Letterman, NBC selected Jay Leno to host "The Tonight Show" after Carson's departure. Letterman maintained a close relationship with Carson through his break with NBC. Three years after he left for CBS, HBO produced a made-for-television movie called "The Late Shift", based on a book by "The New York Times" reporter Bill Carter, chronicling the battle between Letterman and Leno for the coveted "Tonight Show" hosting spot.

Carson later made a few cameo appearances as a guest on Letterman's show. Carson's final television appearance came May 13, 1994, on a "Late Show" episode taped in Los Angeles, when he made a surprise appearance during a 'Top 10 list' segment. In early 2005, it was revealed that Carson occasionally sent jokes to Letterman, who used these jokes in his monologue; according to CBS senior vice president Peter Lassally (a one-time producer for both men), Carson got "a big kick out of it." Letterman would do a characteristic Johnny Carson golf swing after delivering one of Carson's jokes. In a tribute to Carson, all of the opening monologue jokes during the first show following Carson's death were written by Carson.

Lassally also claimed that Carson had always believed Letterman, not Leno, to be his "rightful successor." During the early years of the "Late Show"s run, Letterman occasionally used some of Carson's trademark bits, including "Carnac the Magnificent" (with Paul Shaffer as Carnac), "Stump the Band", and the "Week in Review."

Oprah Winfrey appeared on Letterman's show when he was hosting NBC's "Late Night" on May 2, 1989. Following that appearance, the two had a 16-year feud which arose, as Winfrey explained to Letterman after the feud had been resolved, as a result of the acerbic tone of their 1989 interview of which she said that it "felt so uncomfortable to me that I didn't want to have that experience again".

The feud apparently ended in 2005 when Winfrey appeared on CBS's "Late Show with David Letterman" on December 2, in an event Letterman jokingly referred to as "the Super Bowl of Love".

Winfrey and Letterman also appeared together in a "Late Show" promo that aired during CBS's coverage of Super Bowl XLI in February 2007, with the two sitting next to each other on the couch watching the game. Since the game was played between the Indianapolis Colts and Chicago Bears, the Indianapolis-born Letterman wears a Peyton Manning jersey, while Winfrey—whose show was taped in Chicago—wears a Brian Urlacher jersey. On September 10, 2007, Letterman made his first appearance on "The Oprah Winfrey Show" at Madison Square Garden in New York City.

Three years later, during CBS's coverage of Super Bowl XLIV, the two appeared again in a "Late Show" promo, this time with Winfrey sitting on a couch between Letterman and Jay Leno. This time Letterman was wearing the retired 70 jersey of Colts' Hall of Fame and Letterman regular guest, Art Donovan (the Colts faced the New Orleans Saints in this Super Bowl). The appearance was Letterman's idea: Leno flew to New York City on an NBC corporate jet, sneaking into the Ed Sullivan Theater during the "Late Show"'s February 4 taping wearing a disguise, meeting Winfrey and Letterman at a living room set created in the theater's balcony where they taped their promo.

Winfrey interviewed Letterman in January 2013 on "Oprah's Next Chapter". Winfrey and Letterman discussed their feud during the interview and Winfrey revealed that she had had a "terrible experience" while appearing on Letterman's show years earlier. Letterman could not recall the incident but apologized.

"Late Show" went off air for eight weeks during the months of November and December because of the Writers Guild of America strike. Letterman's production company, Worldwide Pants, was the first company to make an individual agreement with the WGA, thus allowing his show to come back on air on January 2, 2008. On his first episode since being off air, he surprised the viewing audience with his newly grown beard, which signified solidarity with the strike. His beard was shaved off during the show on January 7, 2008.

On June 8 and 9, 2009, Letterman told two sexually themed jokes about a daughter (never named) of Sarah Palin on his TV show. Palin was in New York City at the time with her then fourteen-year-old daughter, Willow, and some contemporaries thought the jokes to be aimed at Willow, which caused some small amount of controversy.

In a statement posted on the Internet, Palin said, "I doubt [Letterman would] ever dare make such comments about anyone else's daughter" and that "laughter incited by sexually perverted comments made by a 62-year-old male celebrity aimed at a 14-year-old girl is disgusting." On his show of June 10, Letterman responded to the controversy, saying the jokes were meant to be about Palin's eighteen-year-old daughter, Bristol, whose pregnancy as an unmarried teenager had caused some controversy during the United States presidential election of 2008. "These are not jokes made about (Palin's) 14-year-old daughter ... I would never, never make jokes about raping or having sex of any description with a 14-year-old girl."

His remarks did not put an end to public criticism, however. The National Organization for Women (NOW) released a statement supporting Palin, noting that Letterman had made "[only] something of an apology." When the controversy failed to subside, Letterman addressed the issue again on his show of June 15, faulting himself for the error and apologizing "especially to the two daughters involved, Bristol and Willow, and also to the governor and her family and everybody else who was outraged by the joke."

On August 17, 2011, it was reported that an Islamist militant had posted a death threat against Letterman on a website frequented by Al-Qaeda supporters, calling on American Muslims to kill Letterman for making a joke about the death of an Al-Qaeda leader, killed in a drone strike in Pakistan in June 2011, Ilyas Kashmiri. In his show on August 22, Letterman joked about the threat, saying "State Department authorities are looking into this. They're not taking this lightly. They're looking into it. They're questioning, they're interrogating, there's an electronic trail—but everybody knows it's Leno."

Letterman was the focus of "The Avengers on "Late Night with David Letterman"", issue 239 (January 1984) of the Marvel comic book series "The Avengers", in which the title characters (specifically Hawkeye, Wonder Man, Black Widow, Beast and Black Panther) are guests on "Late Night". A parody of Letterman, named "David Endochrine", is gassed to death along with his bandleader named "Paul" and their audience in Frank Miller's "The Dark Knight Returns".

Letterman appeared in the pilot episode of the short-lived 1986 series "Coach Toast", and he appears with a bag over his head as a guest on Bonnie Hunt's 1990s sitcom, "The Building". He appeared in "The Simpsons" as himself in a couch gag when the Simpsons find themselves (and the couch) in "Late Night with David Letterman". He had a cameo in the feature film "Cabin Boy", with Chris Elliott, who worked as a writer on Letterman's show. In this and other appearances, Letterman is listed in the credits as "Earl Hofert", the name of Letterman's maternal grandfather. He also appeared as himself in the Howard Stern biographical film "Private Parts" as well as the 1999 Andy Kaufman biopic "Man on the Moon", in a few episodes of Garry Shandling's 1990s TV series "The Larry Sanders Show" and in "The Abstinence", a 1996 episode of the sitcom "Seinfeld".

Letterman provided vocals for the Warren Zevon song "Hit Somebody" from "My Ride's Here", and provided the voice for Butt-head's father in the 1996 animated film "Beavis and Butt-Head Do America", once again credited as Earl Hofert.

In 2010, a documentary "Dying to do Letterman" was released directed by Joke Fincioen and Biagio Messina featuring Steve Mazan, a stand-up comic, who has cancer and wants to appear on the Letterman show. The film won best documentary and jury awards at the Cinequest Film Festival. Steve Mazan published a same-titled book (full title, "Dying to Do Letterman: Turning Someday into Today") about his own saga.

Letterman appeared as a guest on CNN's "Piers Morgan Tonight" on May 29, 2012, when he was interviewed by Regis Philbin, the guest host and long-time friend. Philbin again interviewed Letterman (and Shaffer) while guest-hosting CBS' "The Late Late Show" (between the tenures of Craig Ferguson and James Corden) on January 27, 2015.

In June 2013, he appeared in the second episode of season two of "Comedians in Cars Getting Coffee".

On November 5, 2013, Letterman and Bruce McCall published a fiction satire book titled "This Land Was Made for You and Me (But Mostly Me)". 

Letterman started his production company — Worldwide Pants Incorporated — which produced his show and several others, including "Everybody Loves Raymond"; "The Late Late Show" and two television series for Bonnie Hunt. Worldwide Pants also produced the dramedy program "Ed" which aired on NBC from 2000–2004. It was Letterman's first association with NBC since he left the network in 1993. During the run of "Ed," the star, Tom Cavanagh, appeared as a guest on the "Late Show" several times.

In 2005, Worldwide Pants produced its first feature film, "Strangers with Candy", which was a prequel to the Comedy Central TV series of the same title. In 2007, Worldwide Pants produced the ABC comedy series, "Knights of Prosperity".

Worldwide Pants made significant news in December 2007 when it was announced that Letterman's company had independently negotiated its own contract with the Writers Guild of America, East, thus allowing Letterman, Craig Ferguson, and their writers to return to work, while the union continued its strike against production companies, networks and studios who had not reached an agreement.

In late April 2010, several music industry websites reported that Letterman started a record label named Clear Entertainment/C.E. Music and signed his first artist, Runner Runner. Lucy Walsh announced on her MySpace page that she has been signed by Letterman and Clear Entertainment/C.E. Music and is working on her album.

Rahal Letterman Lanigan Racing (RLLR) is an auto racing team that currently races in the United SportsCar Championship (formerly the American Le Mans Series), and full-time in the Verizon IndyCar Series. It is co-owned by 1986 Indianapolis 500 winner Bobby Rahal, businessman Mike Lanigan, and Letterman himself, and is based in Hilliard, Ohio. The team won the 2004 Indianapolis 500 with driver Buddy Rice.

The Letterman Foundation for Courtesy and Grooming is a private foundation through which Letterman has donated millions of dollars to charities and other non-profits in Indiana and Montana, celebrity-affiliated organizations such as Paul Newman's Hole in the Wall Gang Camp, universities such as Ball State, and other organizations such as the American Cancer Society, Salvation Army, and Doctors Without Borders.

Letterman's biggest influence and his mentor was Johnny Carson. Other comedians that influenced Letterman were Paul Dixon, Steve Allen, Jonathan Winters, Garry Moore, Jack Paar, Don Rickles, and David Brenner. Although Ernie Kovacs has also been mentioned as an influence, Letterman has denied this.

Comedians that were influenced by Letterman include: Stephen Colbert, Ray Romano, Jimmy Kimmel, Jay Leno, Conan O'Brien, Jon Stewart, Arsenio Hall, Larry Wilmore, Seth Meyers, Jimmy Fallon, John Oliver, and James Corden.

In 2015, Forbes estimated that Letterman's annual income was $35 million.

On July 2, 1968, Letterman married his college sweetheart, Michelle Cook (born July 2, 1946), in Muncie, Indiana; their marriage ended in divorce by October 1977. He also had a long-term live-in relationship with the former head writer and producer on "Late Night", Merrill Markoe (born August 13, 1948), from 1978 to 1988. Markoe was the mind behind several "Late Night" staples, such as "Stupid Pet/Human Tricks". "Time" magazine stated that theirs was the defining relationship of Letterman's career with Merrill also acting as his writing partner. It was she "who put the surrealism in Letterman's comedy."

Letterman and Regina Lasko (born November 20, 1960) started dating in February 1986, while he was still living with Markoe. He has a son, Harry Joseph Letterman (born November 3, 2003), with her. Harry is named after Letterman's father. In 2005, police discovered a plot to kidnap Harry Letterman and demand a ransom of  million. Kelly Frank, a house painter who had worked for Letterman, was charged in the conspiracy.

Letterman and Lasko wed on March 19, 2009, during a quiet courthouse civil ceremony in Choteau, Montana, where he had purchased a ranch in 1999. Letterman announced the marriage during the taping of his show of March 23, shortly after congratulating Bruce Willis for his marriage the week before. Letterman told the audience he nearly missed the ceremony because his truck became stuck in mud two miles from their house. The family resides in North Salem, New York, on a estate.

Letterman suffers from tinnitus (ringing in the ears), which is a symptom of hearing loss. On the "Late Show" in 1996, Letterman talked about his tinnitus in an interview he did with actor William Shatner, who has severe tinnitus himself, caused from an on-set explosion. Letterman said at first he could not figure out where the noise in his head was coming from and that he hears constant noises and ringing in his ears 24 hours a day.

Letterman no longer drinks alcohol. On more than one occasion, he said that he had once been a "horrible alcoholic" and had begun drinking around the age of 13 and continued until 1981 when he was 34. He recounts in 1981, "I was drunk 80% of the time. ... I loved it. I was one of those guys, I looked around, and everyone else had stopped drinking and I couldn't understand why." When he is shown drinking on the "Late Show" (or, before that, on "Late Night") what appears to be alcohol, it is actually replaced with apple juice by the crew. In 2015, he said that "For years and years and years – 30, 40 years – I was anxious and hypochondriacal and an alcoholic, and many, many other things that made me different from other people." He became calmer through a combination of transcendental meditation and low doses of medication.

He stated in 2017 that he is a Presbyterian, a religious tradition he was originally brought up in by his mother. However, he once said he is motivated by "Lutheran, Midwestern guilt".

Letterman's sister is a journalist, as is her husband. Their son, Liam Letterman Shelton, attended Letterman's "alma mater", Ball State University in Muncie, Indiana, where Letterman funded the journalism school, and studied a four-year double major in journalism news/telecommunications news.

Beginning in May 1988, Letterman was stalked by Margaret Mary Ray, a woman suffering from schizophrenia. She stole his Porsche, camped out on his tennis court, and repeatedly broke into his house. Her exploits drew national attention, with Letterman occasionally joking about her on his show, although he never referred to her by name. After she committed suicide in October 1998, Letterman told "The New York Times" that he had great compassion for her. A spokesperson for Letterman said: "This is a sad ending to a confused life."

On October 1, 2009, Letterman announced on his show that he had been the victim of a blackmail attempt by someone threatening to reveal that he'd had sex with several of his female employees, and at the same time, he confirmed that he had such relationships. He stated that three weeks earlier (on September 9, 2009) someone had left a package in his car with material he said he would write into a screenplay and a book if Letterman did not pay him  million. Letterman said that he contacted the Manhattan District Attorney's office, ultimately cooperating with them to conduct a sting operation involving giving the man a phony check. Subsequently, Robert J. "Joe" Halderman, a producer of the CBS true crime journalism series "48 Hours", was arrested after trying to deposit the check. He was indicted by a Manhattan grand jury and pleaded not guilty to a charge of attempted grand larceny on October 2, 2009. Eventually, on March 9, 2010, he pleaded guilty to this same felony and served a six-month jail sentence, followed by probation and community service.

A central figure in the case and one of the women with whom Letterman had had a sexual relationship was his longtime personal assistant Stephanie Birkitt, who often appeared with him on his show. She had also worked for "48 Hours". Until a month prior to the revelations, she had shared a residence with Halderman, who allegedly had copied her personal diary and used it, along with private emails, in the blackmail package.

In the days following the initial announcement of the affairs and the arrest, several prominent women, including Kathie Lee Gifford, co-host of NBC's "Today Show", and NBC news anchor Ann Curry questioned whether Letterman's affairs with subordinates created an unfair working environment. A spokesman for Worldwide Pants said that the company's sexual harassment policy did not prohibit sexual relationships between managers and employees. According to business news reporter Eve Tahmincioglu, "CBS suppliers are supposed to follow the company's business conduct policies" and the CBS 2008 Business Conduct Statement states that "If a consenting romantic or sexual relationship between a supervisor and a direct or indirect subordinate should develop, CBS requires the supervisor to disclose this information to his or her Company's Human Resources Department...".

On October 3, 2009, a former CBS employee, Holly Hester, announced that she and Letterman had engaged in a year-long "secret" affair in the early 1990s while she was his intern and a student at New York University.

On October 5, 2009, Letterman devoted a segment of his show to a public apology to his wife and staff. Three days later, Worldwide Pants announced that Birkitt had been placed on a "paid leave of absence" from the "Late Show". On October 15, CBS News announced that the company's Chief Investigative Correspondent, Armen Keteyian, had been assigned to conduct an "in-depth investigation" into Letterman.

Letterman is a car enthusiast, and owns an extensive collection. In 2012, it was reported that the collection consisted of ten Ferraris, eight Porsches, four Austin Healeys, two Honda motorcycles, a Chevy pickup and one car each from automakers Mercedes-Benz, Jaguar, MG, Volvo, and Pontiac.

In his 2013 appearance on "Comedians in Cars Getting Coffee", part of Jerry Seinfeld's conversation with Letterman was filmed in Letterman's outwardly unassuming 1995 Volvo 960 station wagon that is powered by a 380-horsepower racing engine. Paul Newman had the car built for Letterman.

Letterman shares a close relationship with the rock and roll band Foo Fighters since their appearance on his first show upon his return from heart surgery (see section "Heart surgery hiatus" for more information). The band appeared many times on the "Late Show" (see section "Retirement from Late Show" for more information), including a week-long stint in October 2014.

While introducing the band's performance of "Miracle" on the show of October 17, 2014, Letterman told the story of how a souvenir video of himself and his four-year-old son learning to ski used the song as background music, unbeknownst to Letterman until he saw it. He said "This is the second song of theirs that will always have great, great meaning for me for the rest of my life". This was the first time the band had heard this story.

Worldwide Pants co-produced Dave Grohl's "" TV series. "Letterman was the first person to get behind this project," Grohl admitted.

On September 7, 2007, Letterman visited his "alma mater", Ball State University in Muncie, Indiana, for the dedication of a communications facility named in his honor for his dedication to the university. The  million, David Letterman Communication and Media Building opened for the 2007 fall semester. Thousands of Ball State students, faculty, and local residents welcomed Letterman back to Indiana. Letterman's emotional speech touched on his struggles as a college student and his late father, and also included the "top ten good things about having your name on a building", finishing with "if reasonable people can put my name on a  million building, anything is possible." Over many years Letterman "has provided substantial assistance to [Ball State's] Department of Telecommunications, including an annual scholarship that bears his name."

At the same time, Letterman received a Sagamore of the Wabash award given by Indiana Governor Mitch Daniels, which recognizes distinguished service to the state of Indiana.

In his capacities as either a performer, producer, or as part of a writing team, Letterman is among the most nominated people in the history of the Emmy Awards with 52 nominations, winning two Daytime Emmys and ten Primetime Emmys since 1981. He won four American Comedy Awards and in 2011 became the first recipient of the Johnny Carson Award for Comedic Excellence at The Comedy Awards.

Letterman was a recipient of the 2012 Kennedy Center Honors, where he was called "one of the most influential personalities in the history of television, entertaining an entire generation of late-night viewers with his unconventional wit and charm." On May 16, 2017, Letterman was named the next recipient of the Mark Twain Prize for American Humor, the award granted annually by the John F. Kennedy Center for the Performing Arts. He was scheduled to receive the prize in a ceremony slated for October 22.




</doc>
<doc id="8341" url="https://en.wikipedia.org/wiki?curid=8341" title="Delroy Lindo">
Delroy Lindo

Delroy George Lindo (born 18 November 1952) is a British actor and theatre director. Lindo has been nominated for Tony and Screen Actors Guild awards and has won a Satellite Award. He is perhaps best known for his roles in three Spike Lee films, having portrayed West Indian Archie in Lee's "Malcolm X" (1992), Woody Carmichael in "Crooklyn" (1994), and Rodney Little in Clockers (1995). Lindo also played Catlett in "Get Shorty", Arthur Rose in "The Cider House Rules", and Detective Castlebeck in "Gone in 60 Seconds" (2000). Lindo starred as Alderman Ronin Gibbons in the TV series "The Chicago Code" (2011) and as Winter on the series "Believe," which premiered in 2014.

Delroy Lindo was born in 1952 in Lewisham, south east London, the son of Jamaican parents who had emigrated to Britain. Lindo got interested in acting as a child in a Nativity play. His mother was a nurse and his father worked in various jobs. As a teenager, he and his mother moved to Toronto, Ontario, Canada. When he was sixteen, they moved to San Francisco. At the age of 24, Lindo started acting studies at the American Conservatory Theater, graduating in 1979.

Lindo's film debut came in 1976 with the British comedy "Find the Lady", followed by two other roles in films, including an Army Sergeant in "More American Graffiti" (1979).

He quit film for 10 years to concentrate on theatre acting. In 1982 he debuted on Broadway in ""Master Harold"...and the Boys," directed by the play's South African author Athol Fugard. By 1988 Lindo had earned a Tony nomination for his portrayal of Herald Loomis in August Wilson's "Joe Turner's Come and Gone".

Lindo returned to film in the 1990s, acting alongside Rutger Hauer and Joan Chen in the science fiction film "Salute of the Jugger" (1990), which has become a cult classic. Although he had turned down Spike Lee for a role in his debut "Do the Right Thing", Lee cast him as Woody Carmichael in the drama "Crooklyn" (1994), which brought him notice. Together with his other roles with Lee - as the West Indian Archie, a psychotic gangster, in "Malcolm X", and a starring role as a neighbourhood drug dealer in "Clockers" - he became established in his film career.

Other films in which he has starring roles are Barry Sonnenfeld's "Get Shorty" (1995), Ron Howard's "Ransom" (1996) and "Soul of the Game" (1996), as the baseball player Satchel Paige. As a character actor, Lindo has readily taken on roles as treacherous bad guys as well as those of trustworthy professionals.

In 1998 Lindo co-starred as African-American explorer Matthew Henson, in the TV film "Glory & Honor", directed by Kevin Hooks. It portrayed his nearly 20-year partnership with Commander Robert Peary in Arctic exploration and their effort to find the Geographic North Pole in 1909. He received a Satellite Award as best actor. Lindo continues to work in television and was most recently seen on the short-lived NBC drama "Kidnapped".

Lindo played an angel in the comedy film "A Life Less Ordinary" (1997), in which Dan Hedaya played the angel Gabriel, and Lindo's boss. He guest-starred on "The Simpsons" in the episode "Brawl in the Family", playing a similar character named Gabriel.

Lindo had a small role in the 1995 science fiction/action film "Congo," playing the corrupt Captain Wanta. Lindo was not credited for the role, but one of his lines in the film, ""Stop eating my sesame cake!"", has become an internet meme.

In the British film, "Wondrous Oblivion" (2003), directed by Paul Morrison, he starred as Dennis Samuels, the father of a Jamaican immigrant family in London in the 1950s; he coaches his children and the son of a neighbour Jewish family in cricket, earning their admiration in a time of strained social relations. Lindo said he made the film in honour of his parents, who had similarly moved to London in those years.

In 2007, Lindo began an association with Berkeley Repertory Theatre in Berkeley, California, when he directed Tanya Barfield's play "The Blue Door". In the autumn of 2008, Lindo revisited August Wilson's play, "Joe Turner's Come and Gone", directing a production at the Berkeley Rep. In 2010, he played the role of elderly seer Bynum in David Lan's production of "Joe Turner" at the Young Vic Theatre in London.

Lindo is poised to play Marcus Garvey in an upcoming biopic of the black nationalist historical figure.



</doc>
<doc id="8343" url="https://en.wikipedia.org/wiki?curid=8343" title="David Janssen">
David Janssen

David Janssen (born David Harold Meyer, March 27, 1931 – February 13, 1980) was an American film and television actor who is best known for his starring role as Richard Kimble in the television series "The Fugitive" (1963–1967). Janssen also had the title roles in three other series: "Richard Diamond, Private Detective"; "Harry O"; and "O'Hara, U.S. Treasury".

In 1996 "TV Guide" ranked him number 36 on its "50 Greatest TV Stars of All Time" list.

Janssen was born in 1931 in Naponee, a village in Franklin County in southern Nebraska, to Harold Edward Meyer, a banker (May 12, 1906 – November 4, 1990) and Berniece Graf (May 11, 1910 – November 26, 1995). Janssen was of Irish and Jewish descent. Following his parents' divorce in 1935, his mother moved with five-year-old David to Los Angeles, California, and later married Eugene Janssen (February 18, 1918 – March 30, 1996) in 1940 in Los Angeles. Young David used his stepfather's name after he entered show business as a child.

He attended Fairfax High School in Los Angeles, where he excelled on the basketball court, setting a school-scoring record that lasted over 20 years. His first film part was at the age of thirteen, and by the age of twenty-five he had appeared in twenty films and served two years as an enlisted man in the United States Army. During his Army days, Janssen became friends with fellow enlistees Martin Milner and Clint Eastwood while posted at Fort Ord, California.

Janssen appeared in many television series before he landed programs of his own. In 1956, he and Peter Breck appeared in John Bromfield's syndicated series "Sheriff of Cochise" in the episode "The Turkey Farmers". Later, he guest-starred on NBC's medical drama "The Eleventh Hour" in the role of Hal Kincaid in the 1962 episode "Make Me a Place", with series co-stars Wendell Corey and Jack Ging. He joined friend Martin Milner in a 1962 episode of "Route 66" as the character Kamo in the episode "One Tiger to a Hill."

Janssen starred in four television series of his own:

At the time, the final episode of "The Fugitive" held the record for the greatest number of American homes with television sets to watch a series finale, at 72 percent in August 1967. David Janssen was well liked by everyone, but more loved by his fans. He fit the perfect role of “why me?” One could not help feeling sorry for his Fugitive character of being quiet, unassuming, afraid, but mainly caring for others. On an episode, he stopped to help an old lady cross the street, on another; he helped a young, attractive woman start up her stalled car in the middle of nowhere.

His films include "To Hell and Back", the biography of Audie Murphy, who was the most decorated American soldier of World War II; John Wayne's Vietnam war film "The Green Berets"; opposite Gregory Peck in the space story "Marooned", in which Janssen played an astronaut sent to rescue three stranded men in space, and "The Shoes of the Fisherman", as a television journalist in Rome reporting on the election of a new Pope (Anthony Quinn).

He starred as a Los Angeles police detective trying to clear himself in the killing of an apparently innocent doctor in the 1967 film "Warning Shot". The film was shot during a break in the spring and summer of 1966 between the third and fourth seasons of "The Fugitive."

Janssen played an alcoholic in the 1977 TV movie "A Sensitive, Passionate Man", which co-starred Angie Dickinson, and an engineer who devises an unbeatable system for blackjack in the 1978 made-for-TV movie "Nowhere to Run", co-starring Stefanie Powers and Linda Evans. Janssen's impressively husky voice was used to good effect as the narrator for the TV mini-series "Centennial" (1978–79); he also appeared in the final episode. He starred in the made for tv mini series “S.O.S. Titanic” as John Jacob Astor, playing opposite Beverly Ross as his wife, Madeleine, in 1979.

Though Janssen's scenes were cut from the final release, he also appeared as a journalist in the film "Inchon", which he accepted to work with Laurence Olivier who played General Douglas MacArthur. At the time of his death, Janssen had just begun filming a television movie playing the part of Father Damien, the priest who dedicated himself to the leper colony on the island of Molokai, Hawaii. The part was eventually reassigned to actor Ken Howard of the CBS series "The White Shadow".

In 1996 "TV Guide" ranked him number 36 on its 50 Greatest TV Stars of All Time list.

Janssen was married twice. His first marriage was to model and interior decorator Ellie Graham, whom he married in Las Vegas on August 25, 1958. They divorced in 1968. In 1975, he married actress and model Dani Crayne Greco. They remained married until Janssen's death.

A heavy drinker and a four-pack-a-day smoker, Janssen died of a heart attack in the early morning of February 13, 1980, at his home in Malibu, California at the age of 48. At the time of his death, Janssen was filming the television movie "Father Damien". Janssen was buried at the Hillside Memorial Park Cemetery in Culver City, California. A non-denominational funeral was held at the Jewish chapel of the cemetery on February 17. Suzanne Pleshette delivered the eulogy at the request of Janssen's widow. Milton Berle, Johnny Carson, Tommy Gallagher, Richard Harris, Stan Herman, Rod Stewart and Gregory Peck were among Janssen's pallbearers. Honorary pallbearers included Jack Lemmon, George Peppard, James Stewart and Danny Thomas.

For his contribution to the television industry, David Janssen has a star on the Hollywood Walk of Fame located on the 7700 block of Hollywood Boulevard.



</doc>
<doc id="8344" url="https://en.wikipedia.org/wiki?curid=8344" title="Docetism">
Docetism

In Christianity, docetism (from the Greek "dokeĩn" (to seem) "dókēsis" (apparition, phantom), is the doctrine that the phenomenon of Christ, his historical and bodily existence, and above all the human form of Jesus, was mere semblance without any true reality. Broadly it is taken as the belief that Jesus only seemed to be human, and that his human form was an illusion. The word "Dokētaí" (illusionists) referring to early groups who denied Jesus' humanity, first occurred in a letter by Bishop Serapion of Antioch (197–203), who discovered the doctrine in the Gospel of Peter, during a pastoral visit to a Christian community using it in Rhosus, and later condemned it as a forgery. It appears to have arisen over theological contentions concerning the meaning, figurative or literal, of a sentence from the Gospel of John: "the Word was made Flesh".

Docetism was unequivocally rejected at the First Council of Nicaea in 325 and is regarded as heretical by the Catholic Church, Orthodox Church, Coptic Church and many other Christian denominations that accept and hold to the statements of these early church councils.

Docetism is broadly defined as any teaching that claims that Jesus' body was either absent or illusory. The term 'docetic' is rather nebulous. For Robert Price "docetism", together with "encratism", "Gnosticism" and "adoptionism", has been employed "far beyond what historically descriptive usage would allow". Two varieties were widely known. In one version, as in Marcionism, Christ was so divine that he could not have been human, since God lacked a material body, which therefore could not physically suffer. Jesus only "appeared" to be a flesh-and-blood man; his body was a phantasm. Other groups who were accused of docetism held that Jesus was a man in the flesh, but Christ was a separate entity who entered Jesus's body in the form of a dove at his baptism, empowered him to perform miracles, and abandoned him upon his death on the cross.

Docetism's origin within Christianity is obscure. Ernst Käsemann controversially defined the Christology of St John’s Gospel as "naïve docetism" in 1968. The ensuing debate reached an impasse as awareness grew that the very term "docetism", like "gnosticism", was difficult to define within the religio-historical framework of the debate. It has occasionally been argued that its origins were in heterodox Judaism or Oriental and Grecian philosophies. The alleged connection with Jewish Christianity would have reflected Jewish Christian concerns with the inviolability of (Jewish) monotheism. Docetic opinions seem to have circulated from very early times, 1 John 4:2 appearing explicitly to reject them. Some 1stcentury Christian groups developed docetic interpretations partly as a way to make Christian teachings more acceptable to pagan ways of thinking about divinity.

In his critique of the theology of Clement of Alexandria, Photius in his Myriobiblon held that Clement's views reflected a quasi-docetic view of the nature of Christ, writing that "[Clement] hallucinates that the Word was not incarnate but "only seems to be"." (ὀνειροπολεῖ καὶ μὴ σαρκωθῆναι τὸν λόγον ἀλλὰ "δόξαι".) In Clement’s time, some disputes contended over whether Christ assumed the "psychic" flesh of mankind as heirs to Adam, or the "spiritual" flesh of the resurrection. Docetism largely died out during the first millennium AD.

The opponents against whom Ignatius of Antioch inveighs are often taken to be Monophysite docetists. In his letter to the Smyrnaeans, 7:1, written around 110AD, he writes:
While these characteristics fit a Monophysite framework, a slight majority of scholars consider that Ignatius was waging a polemic on two distinct fronts, one Jewish, the other docetic; a minority holds that he was concerned with a group that commingled Judaism and docetism. Others, however, doubt that there was actual docetism threatening the churches, arguing that he was merely criticizing Christians who lived Jewishly or that his critical remarks were directed at an Ebionite or Cerinthian possessionist Christology, according to which Christ was a heavenly spirit that temporarily possessed Jesus.

The Qur'an has a docetic Christology, viewing Jesus as a divine illuminator rather than the redeemer (as he is viewed in Christianity). However, the Islamic docetism is not focused on the general life and person of Jesus or the Christ. In Islam "the Christ" ("al-masīḥ") is not generally viewed as distinct from humanity nor a special spirit being as in docetism or some gnosticisms. Islamic docetism focuses on a denial of the crucifixion of Jesus. Sura 4:157–58 reads:
Since Arthur Drews published his "The Christ Myth" (Die Christusmythe) in 1909, occasional connections have been drawn between docetist theories and the modern idea that Christ was a myth. Shailer Mathews called Drews' theory a "modern docetism". Frederick Cornwallis Conybeare thought any connection to be based on a misunderstanding of docetism. The idea recurred in classicist Michael Grant's 1977 review of the evidence for Jesus, who compared modern scepticism about a historical Jesus to the ancient docetic idea that Jesus only "seemed" to come into the world "in the flesh". Modern theories did away with "seeming".






</doc>
<doc id="8347" url="https://en.wikipedia.org/wiki?curid=8347" title="Greek drachma">
Greek drachma

Drachma ( , ; pl. "drachmae" or "drachmas") was the currency used in Greece during several periods in its history:

It was also a small unit of weight.

The name "drachma" is derived from the verb (, "(I) grasp"). It is believed that the same word with the meaning of "handful" or "handle" is found in Linear B tablets of the Mycenean Pylos. Initially a drachma was a fistful (a "grasp") of six "oboloí" or "obeloí" (metal sticks, literally "spits") used as a form of currency as early as 1100 BC and being a form of "bullion": bronze, copper, or iron ingots denominated by weight. A hoard of over 150 rod-shaped obeloi was uncovered at Heraion of Argos in Peloponnese. Six of them are displayed at the Numismatic Museum of Athens.

It was the standard unit of silver coinage at most ancient Greek mints, and the name "obol" was used to describe a coin that was one-sixth of a drachma. The notion that "drachma" derived from the word for fistful was recorded by Herakleides of Pontos (387–312 BC) who was informed by the priests of Heraion that Pheidon, king of Argos, dedicated rod-shaped obeloi to Heraion. Similar information about Pheidon's obeloi was also recorded at the Parian Chronicle.

Ancient Greek coins normally had distinctive names in daily use. The Athenian tetradrachm was called owl, the Aeginetic stater was called chelone, the Corinthian stater was called "hippos" (horse) and so on. Each city would mint its own and have them stamped with recognizable symbols of the city, known as badge in numismatics, along with suitable inscriptions, and they would often be referred to either by the name of the city or of the image depicted. The exact exchange value of each was determined by the quantity and quality of the metal, which reflected on the reputation of each mint.

Among the Greek cities that used the drachma were: Abdera, Abydos, Alexandria, Aetna, Antioch, Athens, Chios, Cyzicus, Corinth, Ephesus, Eretria, Gela, Catana, Kos, Maronia, Naxos, Pella, Pergamum, Rhegion, Salamis, Smyrni, Sparta, Syracuse, Tarsus, Thasos, Tenedos, Troy and more.

The 5th century BC Athenian "tetradrachm" ("four drachmae") coin was perhaps the most widely used coin in the Greek world prior to the time of Alexander the Great (along with the Corinthian stater). It featured the helmeted profile bust of Athena on the obverse (front) and an owl on the reverse (back). In daily use they were called "glaukes" (owls), hence the proverb , 'an owl to Athens', referring to something that was in plentiful supply, like 'coals to Newcastle'. The reverse is featured on the national side of the modern Greek 1 euro coin.

Drachmae were minted on different weight standards at different Greek mints. The standard that came to be most commonly used was the Athenian or Attic one, which weighed a little over 4.3 grams.

After Alexander the Great's conquests, the name "drachma" was used in many of the Hellenistic kingdoms in the Middle East, including the Ptolemaic kingdom in Alexandria and the Parthian Empire based in what is modern-day Iran. The Arabic unit of currency known as "dirham" (), known from pre-Islamic times and afterwards, inherited its name from the drachma or didrachm (, 2 drachmae); the dirham is still the name of the official currencies of Morocco and the United Arab Emirates. The Armenian dram () also derives its name from the drachma.

It is difficult to estimate comparative exchange rates with modern currency because the range of products produced by economies of centuries gone by were different from today, which makes purchasing power parity (PPP) calculations very difficult; however, some historians and economists have estimated that in the 5th century BC a drachma had a rough value of 25 U.S. dollars (in the year 1990 – equivalent to 46.50 USD in 2015), whereas classical historians regularly say that in the heyday of ancient Greece (the fifth and fourth centuries) the daily wage for a skilled worker or a hoplite was one drachma, and for a heliast (juror) half a drachma since 425 BC.

Modern commentators derived from Xenophon that half a drachma per day (360 days per year) would provide "a comfortable subsistence" for "the poor citizens" (for the head of a household in 355 BC). Earlier in 422 BC, we also see in Aristophanes ("Wasps", line 300–302) that the daily half-drachma of a juror is just enough for the daily subsistence of a family of three.

A modern person might think of one drachma as the rough equivalent of a skilled worker's daily pay in the place where they live, which could be as low as $1 USD, or as high as $100 USD, depending on the country.

Fractions and multiples of the drachma were minted by many states, most notably in Ptolemaic Egypt, which minted large coins in gold, silver and bronze.

Notable Ptolemaic coins included the gold "pentadrachm" and "octadrachm", and silver "tetradrachm", "decadrachm" and "pentakaidecadrachm". This was especially noteworthy as it would not be until the introduction of the Guldengroschen in 1486 that coins of substantial size (particularly in silver) would be minted in significant quantities.

For the Roman successors of the drachma, see Roman provincial coins.

The weight of the silver drachma was approximately 4.3 grams or 0.15 ounces, although weights varied significantly from one city-state to another. It was divided into six obols of 0.72 grams, which were subdivided into four tetartemoria of 0.18 grams, one of the smallest coins ever struck, approximately 5–7 mm in diameter.

Minae and talents were never actually minted: they represented weight measures used for commodities (e.g. grain) as well as metals like silver or gold. The New Testament mentions both didrachma and, by implication, tetradrachma in context of the Temple tax. Luke's Gospel includes a parable told by Jesus of a woman with 10 drachmae, who lost one and searched her home until she found it.

The drachma was reintroduced in May 1832, shortly before the establishment of the modern state of Greece (with the exception of the subdivision Taurus). It replaced the "phoenix" at par. The drachma was subdivided into 100 lepta.

The first coinage consisted of copper denominations of 1, 2, 5 and 10 lepta, silver denominations of , , 1 and 5 drachmae and a gold coin of 20 drachmae. The drachma coin weighed 4.5 g and contained 90% silver, with the 20-drachma coin containing 5.8 g of gold.

In 1868, Greece joined the Latin Monetary Union and the drachma became equal in weight and value to the French franc. The new coinage issued consisted of copper coins of 1, 2, 5 and 10 lepta, with the 5- and 10-lepta coins bearing the names "obolos" () and "diobolon" (), respectively; silver coins of 20 and 50 lepta, 1, 2 and 5 drachmae and gold coins of 5, 10 and 20 drachmae. (Very small numbers of 50- and 100-drachma coins in gold were also issued.)

In 1894, cupro-nickel 5-, 10- and 20-lepta coins were introduced. No 1-lepton or 2-lepta coin had been issued since the late 1870s. Silver coins of 1 and 2 drachmae were last issued in 1911, and no coins were issued between 1912 and 1922, during which time the Latin Monetary Union collapsed due to World War I.

Between 1926 and 1930, a new coinage was introduced for the new Hellenic Republic, consisting of cupro-nickel coins in denominations of 20 lepta, 50 lepta, 1 drachma, and 2 drachmae; nickel coins of 5 drachmae; and silver coins of 10 and 20 drachmae. These were the last coins issued for the first modern drachma, and none were issued for the second.

Notes were issued by the National Bank of Greece from 1841 until 2001 when Greece joined the Euro. Early denominations ranged from 10 to 500 drachmae. Smaller denominations (1, 2, 3 and 5 drachmae) were issued from 1885, with the first 5-drachma notes being made by cutting 10-drachma notes in half.

When Greece finally achieved its independence from the Ottoman Empire in 1828, the phoenix was introduced as the monetary unit; its use was short-lived, however, and in 1832 the phoenix was replaced by the drachma, adorned with the image of King Otto of Greece, who reigned as modern Greece’s first king from 1832 to 1862. The drachma was divided into 100 lepta. In 2002 the drachma ceased to be legal tender after the euro, the monetary unit of the European Union, became Greece’s sole currency.

Between 1917 and 1920, the Greek government issued paper money in denominations of 10 lepta, 50 lepta, 1 drachma, 2 drachmae, and 5 drachmae. The National Bank of Greece introduced 1000-drachma notes in 1901, and the Bank of Greece introduced 5000-drachma notes in 1928. The Greek government again issued notes between 1940 and 1944, in denominations ranging from 50 lepta to 20 drachmae.

During the German-Italian occupation of Greece from 1941 to 1944, catastrophic hyperinflation and Nazi looting of the Greek treasury caused much higher denominations to be issued, culminating in 100,000,000,000-drachma notes in 1944.

In November 1944, after Greece was liberated from Germany, old drachmae were exchanged for new ones at the rate of 50,000,000,000 to 1. Only paper money was issued. The government issued notes of 1, 5, 10 and 20 drachmae, with the Bank of Greece issuing 50-, 100-, 500-, 1000-, 5000-, and 10,000-drachma notes. This drachma also suffered from high inflation. The government later issued 100-, 500-, and 1000-drachma notes, and the Bank of Greece issued 20,000-and 50,000-drachma notes.

In 1953, in an effort to halt inflation, Greece joined the Bretton Woods system. In 1954, the drachma was revalued at a rate of 1000 to 1. The new currency was pegged at 30 drachmae = 1 United States dollar. In 1973, the Bretton Woods System was abolished; over the next 25 years the official exchange rate gradually declined, reaching 400 drachmae to 1 U. S. dollar. On 1 January 2002, the Greek drachma was officially replaced as the circulating currency by the euro, and it has not been legal tender since 1 March 2002.

The first issue of coins minted in 1954 consisted of holed aluminium 5-, 10- and 20-lepton pieces, with 50-lepton, 1-, 2-, 5- and 10-drachma pieces in cupro-nickel. A silver 20-drachma piece was issued in 1960, replacing the 20-drachma banknote, and also minted only in collector sets in 1965. Coins in denominations from 50 lepta to 20 drachmae carried a portrait of King Paul (1947–1964). New coins were introduced in 1966, ranging from 50 lepta to 10 drachmae, depicting King Constantine II (1964–1974). A silver 30 drachma coin for the centennial of Greece's royal dynasty was minted in 1963. The following year a non-circulating coin of this value was produced to commemorate the royal wedding. The reverse of all coins was altered in 1971 to reflect the military junta which was in power from 1967 to 1974. This design included a soldier standing in front of the flames of the rising phoenix.

A 20-drachmae coin in cupro-nickel with an image of Europa on the obverse was issued in 1973. In the latter part of 1973, several new coin types were introduced: unholed aluminium (10 and 20 lepta), nickel-brass (50 lepta, 1 drachma, and 2 drachmae) and cupro-nickel (5, 10, and 20 drachmae). These provisional coins carried the design of the phoenix rising from the flame on the obverse, and used the country's new designation as the "Hellenic Republic", replacing the coins also issued in 1973 as the Kingdom of Greece with King Constantine II's portrait. A new series of all 8 denominations was introduced in 1976 carrying images of early national heroes on the smaller values.

Cupro-nickel 50-drachmae coins were introduced in 1980. In 1986, nickel-brass 50-drachma coins were introduced, followed by copper 1- and 2-drachma pieces in 1988 and nickel-brass coins of 20 and 100 drachmae in 1990. In 2000, a set of 6 themed 500-drachma coins was issued to commemorate the 2004 Athens Olympic Games.

Coins in circulation at the time of the adoption of the euro were

The first issues of banknotes were in denominations of 10, 20 and 50 drachmae, soon followed by 100, 500 and 1000 drachmae by 1956. 5000-drachma notes were introduced in 1984, followed by 10,000-drachma notes in 1995 and 200-drachma notes in 1997.

Banknotes in circulation at the time of the adoption of the euro were

In Unicode, the currency symbol is . There is a special Attic numeral, for the value of one drachma but it fails to render in most browsers.

The Drachmi Greek Democratic Movement Five Stars which was founded in 2013, aims to restore the Drachma, as Greece's currency.



 


</doc>
<doc id="8349" url="https://en.wikipedia.org/wiki?curid=8349" title="Denarius">
Denarius

The denarius (, dēnāriī, ) was the standard Roman silver coin from its introduction in the Second Punic War c. 211 BC to the reign of Gordian III (AD 238-244), when it was gradually replaced by the Antoninianus. It continued to be minted in very small quantities, likely for ceremonial purposes, until and through the tetrarchy (293-313).

The word "dēnārius" is derived from the Latin "dēnī" "containing ten", as its value was originally of 10 assēs. The word for "money" descends from it in Italian ("denaro"), Slovene ("denar"), Portuguese ("dinheiro"), and Spanish ("dinero"). Its name also survives in the dinar currency.

Its symbol is represented in Unicode as 𐆖 (U+10196), however it can also be represented as X̶ (capital letter X with combining long stroke overlay).

A predecessor of the "denarius" was first struck in 267 BC, five years before the First Punic War, with an average weight of 6.81 grams, or of a Roman pound. Contact with the Greeks prompted a need for silver coinage in addition to the bronze currency that the Romans were using at that time. The predecessor of the "denarius" was a Greek-styled silver coin called the "didrachm" which was struck in Neapolis and other Greek cities in southern Italy. These coins were inscribed for Rome but closely resemble their Greek counterparts. They were most likely used for trade purposes and were seldom used in Rome.

The first distinctively Roman silver coin appeared around 226 BC. Classic historians sometimes called these coins "denarii", but they are classified by modern numismatists as "quadrigati", which is derived from the quadriga, or four-horse chariot, on the reverse, and which with a two-horse chariot or "biga" was the prototype for the most common designs used on Roman silver coins for the next 150 years.

Rome overhauled its coinage around 211 BC and introduced the denarius alongside a short-lived denomination called the victoriatus. This denarius contained an average 4.5 grams, or of a Roman pound, of silver. It formed the backbone of Roman currency throughout the Roman republic.

The denarius began to undergo slow debasement toward the end of the republican period. Under the rule of Augustus (31 BC-AD 14) its silver content fell to 3.9 grams (a theoretical weight of of a Roman pound). It remained at nearly this weight until the time of Nero (AD 37-68), when it was reduced to of a pound, or 3.4 grams. Debasement of the coin's silver content continued after Nero. Later Roman emperors reduced its content to 3 grams around the late 3rd century.

The value at its introduction was 10 asses, giving the denarius its name, which translates as "containing ten". In about 141 BC, it was re-tariffed at 16 asses, to reflect the decrease in weight of the as. The denarius continued to be the main coin of the Roman Empire until it was replaced by the antoninianus in the middle of the 3rd century. The coin was last issued, in bronze, under Aurelian between AD 270 and 275, and in the first years of the reign of Diocletian. ('Denarius', in "A Dictionary of Ancient Roman Coins", by John R. Melville-Jones (1990)).

It is difficult to give even rough comparative values for money from before the 20th century, as the range of products and services available for purchase was so different. Classical historians often say that in the late Roman Republic and early Roman Empire (~27 BC) the daily wage for an unskilled laborer and common soldier was 1 denarius (with no tax deductions) or about US$2.80 in bread. During the republic (509–27 BC), legionary pay was 112.5 denarii per year (0.3 per day), later doubled by Julius Caesar to 225 denarii (0.6 per day), with soldiers having to pay for their own food and arms. Centurions received considerably higher pay: under Augustus, the lowest rank of centurion was paid 3,750 denarii per year, and the highest rank, 15,000 denarii.

The silver content of the denarius under the Roman Empire (after Nero) was about 50 grains, 3.24 grams, or (0.105ozt) troy ounce. On June 6, 2011, this was about US$3.62 in value if the silver were 0.999 pure.

The fineness of the silver content varied with political and economic circumstances. From a purity of greater than 90% silver in the 1st century AD, the denarius fell to under 60% purity by the year 200, and plummeted to 5% purity by the year 300. By the reign of Gallienus, the "antoninianus" was a copper coin with a thin silver wash.

By comparison, a laborer earning the minimum wage in the United States in January 2014 made US$58 for an 8-hour day, before taxes (based on the mode value of $7.25 per hour, which was true then in 20 states) and an employee earning the minimum wage in the United Kingdom in 2014 made £52 for an 8-hour day, before taxes.

In the final years of the 1st century BC Tincomarus, a local ruler in southern Britain, started issuing coins that appear to have been made from melted down "denarii". The coins of Eppillus, issued around Calleva Atrebatum around the same time, appear to have derived design elements from various "denarii" such as those of Augustus and M. Volteius.

Even after the "denarius" was no longer regularly issued, it continued to be used as a unit of account, and the name was applied to later Roman coins in a way that is not understood. The Arabs who conquered large parts of the land that once belonged to the Eastern Roman Empire issued their own gold dinar. The lasting legacy of the "denarius" can be seen in the use of "d" as the abbreviation for the British penny until 1971. It also survived in France as the name of a coin, the denier. The denarius also survives in the common Arabic name for a currency unit, the "dinar" used from pre-Islamic times, and still used in several modern Arab nations. The major currency unit in former Principality of Serbia, Kingdom of Serbia and former Yugoslavia was "dinar", and it is still used in present-day Serbia. The Macedonian currency "denar" is also derived from the Roman denarius. The Italian word "denaro", the Spanish word "dinero", the Portuguese word "dinheiro", and the Slovene word "", all meaning money, are also derived from Latin "denarius".

1 gold aureus = 2 gold quinarii = 25 silver denarii = 50 silver quinarii = 100 bronze sestertii = 200 bronze dupondii = 400 copper asses = 800 copper semisses = 1600 copper quadrantes

The denarius has been commonly identified as the tribute penny held by Jesus in the Render unto Caesar passage Matthew 22:15-22 and Mark 12:13-17.

In the New Testament, the gospels refer to the denarius as a day's wage for a common laborer (Matthew 20:2, John 12:5). In the Book of Revelation, during the Third Seal: Black Horse, a choinix (or quart) of wheat and three quarts of barley were each valued at one denarius. Bible scholar Robert H. Mounce says the price of the wheat and barley as described in the vision appears to be ten to twelve times their normal cost in ancient times. Revelation describes a condition where basic goods are sold at greatly inflated prices. Thus, the black horse rider depicts times of deep scarcity or famine but not of starvation. The English word "quart" translates choinix. Apparently, a choinix of wheat was the daily ration of one adult. Thus, in the conditions pictured by Revelation 6 the normal income for a working-class family would buy enough food for only one person. The less costly barley would feed three people for one day's wages.




</doc>
<doc id="8350" url="https://en.wikipedia.org/wiki?curid=8350" title="Della Rovere">
Della Rovere

|styles = 

The Della Rovere family (; literally "of the oak tree") was a noble family of Italy. It had humble origins in Savona, in Liguria, and acquired power and influence through nepotism and ambitious marriages arranged by two Della Rovere popes: Francesco Della Rovere, who ruled as Sixtus IV from 1471 to 1484) and his nephew Giuliano, who became Julius II in 1503. Sixtus IV built the Sistine Chapel, which is named for him. The Basilica of San Pietro in Vincoli in Rome is the family church of the Della Rovere. Members of the family were influential in the Church of Rome, and as dukes of Urbino; that title was extinguished with the death of Francesco Maria II in 1631, and the family died out with the death of his grand-daughter Vittoria, Grand Duchess of Tuscany.

Francesco Della Rovere was born into a poor family in Liguria in north-west Italy in 1414, the son of Leonardo della Rovere of Savona. He was elected pope in 1471. As Sixtus IV he was both wealthy and powerful, and at once set about giving power and wealth to his nephews of the Della Rovere and Riario families. Within months of his election, he had made Giuliano della Rovere (the future pope Julius II) and Pietro Riario both cardinals and bishops; four other nephews were also made cardinals. He made Giovanni Della Rovere, who was not a priest, prefect of Rome, and arranged for him to marry into the da Montefeltro family, dukes of Urbino. Sixtus claimed descent from a noble Della Rovere family, the counts of Vinovo in Piemonte, and adopted their coat-of-arms.

Guidobaldo da Montefeltro adopted Francesco Maria I della Rovere, his sister's child and nephew of Pope Julius II. Guidobaldo I, who was heirless, called Francesco Maria at his court, and named him as heir of the Duchy of Urbino in 1504, this through the intercession of Julius II. In 1508, Francesco Maria inherited the duchy thereby starting the line of Rovere Dukes of Urbino. That dynasty ended in 1626 when Pope Urban VIII incorporated Urbino into the papal dominions. As compensation to the last sovereign duke, the title only could be continued by Francesco Maria II, and after his death by his heir, Federico Ubaldo.

Vittoria, last descendant of the della Rovere family (she was the only child of Federico Ubaldo), married Ferdinando II de' Medici, Grand Duke of Tuscany. They had two children: Cosimo III, Tuscany's longest reigning monarch, and Francesco Maria de' Medici, a prince of the Church.


Among the many people who did not belong to this family, but bore the same name, are:
and various artists, including:



</doc>
<doc id="8351" url="https://en.wikipedia.org/wiki?curid=8351" title="David Mamet">
David Mamet

David Alan Mamet (; born November 30, 1947) is an American playwright, film director, screenwriter and author. He won a Pulitzer Prize and received Tony nominations for his plays "Glengarry Glen Ross" (1984) and "Speed-the-Plow" (1988). He first gained critical acclaim for a trio of off-Broadway plays in 1976: "The Duck Variations," "Sexual Perversity in Chicago," and "American Buffalo." His plays "Race" and "The Penitent", respectively, opened on Broadway in 2009 and previewed off-Broadway in 2017.

Feature films that Mamet both wrote and directed include "House of Games" (1987), "Homicide" (1991), "The Spanish Prisoner" (1997), "Heist" (2001), and "Redbelt" (2008). His screenwriting credits include "The Postman Always Rings Twice" (1981), "The Verdict" (1982), "The Untouchables" (1987), "Hoffa" (1992), "Wag the Dog" (1997), and "Hannibal" (2001). Mamet himself wrote the screenplay for the 1992 adaptation of "Glengarry Glen Ross", and wrote and directed the 1994 adaptation of his play "Oleanna" (1992). He was the executive producer and frequent writer for the TV show "The Unit" (2006–2009).

Mamet's books include: "The Old Religion" (1997), a novel about the lynching of Leo Frank; "Five Cities of Refuge: Weekly Reflections on Genesis, Exodus, Leviticus, Numbers and Deuteronomy" (2004), a Torah commentary with Rabbi Lawrence Kushner; "The Wicked Son" (2006), a study of Jewish self-hatred and antisemitism; "Bambi vs. Godzilla", a commentary on the movie business; "The Secret Knowledge: On the Dismantling of American Culture" (2011), a commentary on cultural and political issues; and "Three War Stories" (2013), a trio of novellas about the physical and psychological effects of war.

Mamet was born in 1947 in Chicago to Lenore June (née Silver), a teacher, and Bernard Morris Mamet, a labor attorney. One of his first jobs was as a busboy at Chicago's London House and The Second City. He also worked as an actor, editor for "Oui" magazine and cab-driver. He was educated at the progressive Francis W. Parker School and at Goddard College in Plainfield, Vermont. At the Chicago Public Library Foundation 20th anniversary fundraiser in 2006, though, Mamet announced "My alma mater is the Chicago Public Library. I got what little educational foundation I got in the third-floor reading room, under the tutelage of a Coca-Cola sign".

After a move to Chicago's North Side neighborhood, Mamet encountered theater director Robert Sickinger, and began to work occasionally at Sickinger's Hull House Theatre. This represented the start of Mamet's lifelong involvement with the theater.

Mamet is a founding member of the Atlantic Theater Company; he first gained acclaim for a trio of off-Broadway plays in 1976, "The Duck Variations," "Sexual Perversity in Chicago," and "American Buffalo." He was awarded the Pulitzer Prize in 1984 for "Glengarry Glen Ross," which received its first Broadway revival in the summer of 2005. His play "Race", which opened on Broadway on December 6, 2009 and featured James Spader, David Alan Grier, Kerry Washington, and Richard Thomas in the cast, received mixed reviews. His play "The Anarchist", starring Patti LuPone and Debra Winger, in her Broadway debut, opened on Broadway on November 13, 2012 in previews and was scheduled to close on December 16, 2012. His 2017 play "The Penitent" previewed off-Broadway on February 8, 2017.

In 2002, Mamet was inducted into the American Theatre Hall of Fame. Mamet later received the PEN/Laura Pels International Foundation for Theater Award for Grand Master of American Theater in 2010.

In 2017, Mamet released an online class for writers entitled "David Mamet teaches dramatic writing".

Mamet first entered film work as a screenwriter, later directing his own scripts.

Mamet's first produced screenplay was the 1981 production of "The Postman Always Rings Twice", based upon James M. Cain's novel. He received an Academy Award nomination one year later for his first original script, "The Verdict", written in the late 1970s. He also wrote the screenplay for "The Untouchables" (1987), "Hoffa" (1992), "The Edge" (1997), "Wag the Dog" (1997), "Ronin" (1998), and "Hannibal" (2001).

In 1987, Mamet made his film directing debut with his screenplay "House of Games", which won Best Film and Best Screenplay awards at the 1987 Venice Film Festival and "Film of the Year" for the 1989 London Critics Circle Film Awards. The film starred his then-wife, Lindsay Crouse, and a host of longtime stage associates and friends, including fellow Goddard College graduates. Mamet was quoted as saying, "It was my first film as a director and I needed support, so I stacked the deck." After "House of Games", Mamet later wrote and directed two more films focusing on the world of con artists, "The Spanish Prisoner" (1997) and "Heist" (2001).

Other films that Mamet both wrote and directed include: "Things Change" (1988), "Homicide" (1991) (nominated for the Palme d'Or at 1991 Cannes Film Festival and won a "Screenwriter of the Year" award for Mamet from the London Critics Circle Film Awards), "Oleanna" (1994), "The Winslow Boy" (1999), "State and Main" (2000), "Spartan" (2004), "Redbelt" (2008), and the 2013 bio-pic TV movie "Phil Spector".

His latest feature-length film, a thriller titled "Blackbird", was slated for release in 2015, but is still in development.
When Mamet adapted his play for the 1992 film "Glengarry Glen Ross", he wrote an additional part (including the monologue "Coffee's for closers") for Alec Baldwin.

Mamet continues to work with an informal repertory company for his films, including Crouse, William H. Macy, Joe Mantegna, Rebecca Pidgeon, and Ricky Jay, as well as the aforementioned school friends. 

Mamet has funded his own films with payments he receives for credited and uncredited rewrites of typically big-budget films. For instance, Mamet did a rewrite of the script for "Ronin" under the pseudonym “Richard Weisz” and turned in an early version of a script for "Malcolm X" that director Spike Lee rejected. In 2000, Mamet directed a film version of "Catastrophe," a one-act play by Samuel Beckett featuring Harold Pinter and John Gielgud (in his final screen performance). In 2008, he directed and wrote the mixed martial arts movie "Redbelt," about a martial arts instructor tricked into fighting in a professional bout.

In "On Directing Film," Mamet asserts that directors should focus on getting the point of a scene across, rather than simply following a protagonist, or adding visually beautiful or intriguing shots. Films should create order from disorder in search of the objective.

In 1990 Mamet published "The Hero Pony", a 55-page collection of poetry. He has also published a series of short plays, monologues and four novels, "The Village" (1994), "The Old Religion" (1997), "Wilson: A Consideration of the Sources" (2000), and "Chicago" (2018). He has written several non-fiction texts, and children's stories, including "True and False: Heresy and Common Sense for the Actor"(1997). In 2004 he published a lauded version of the classical Faust story, "Faustus", however, when the play was staged in San Francisco during the spring of 2004, it was not well received by critics. On May 1, 2010, Mamet released a graphic novel "The Trials of Roderick Spode (The Human Ant)".

On June 2, 2011, "The Secret Knowledge: On the Dismantling of American Culture", Mamet's book detailing his conversion from modern liberalism to "a reformed liberal" was released.

Mamet published "Three War Stories", a collection of novellas, on November 11, 2013. In an interview with Newsmax TV, Mamet said he wanted to write about war, despite never having served. Moreover, the book allowed Mamet to free characters that had occupied his mind for years. On the subject of characters as a reason for writing, Mamet told the host, “You want to get these guys out of your head. You just want them to stop talking to you."

Mamet wrote the "Wasted Weekend" episode of "Hill Street Blues" that aired in 1987. His then-wife, Lindsay Crouse, appeared in numerous episodes (including that one) as Officer McBride. Mamet is also the creator, producer and frequent writer of the television series "The Unit", where he wrote a well-circulated memo to the writing staff. He directed a third-season episode of "The Shield" with Shawn Ryan. In 2007, Mamet directed two television commercials for Ford Motor Company. The two 30-second ads featured the Ford Edge and were filmed in Mamet's signature style of fast-paced dialogue and clear, simple imagery. Mamet's sister, Lynn, is a producer and writer for television shows, such as "The Unit" and "Law & Order".

Mamet has contributed several dramas to BBC Radio through Jarvis & Ayres Productions, including an adaptation of "Glengarry Glen Ross" for BBC Radio 3 and new dramas for BBC Radio 4. The comedy "Keep Your Pantheon (or On the Whole I'd Rather Be in Mesopotamia)" was aired in 2007.

Since May 2005 he has been a contributing blogger at "The Huffington Post", drawing satirical cartoons with themes including political strife in Israel. In a 2008 essay at "The Village Voice" titled "Why I Am No Longer a 'Brain-Dead Liberal"' he revealed that he had gradually rejected political correctness and progressivism and embraced conservatism. Mamet has spoken in interviews of changes in his views, highlighting his agreement with free market theorists such as Friedrich Hayek the historian Paul Johnson, and economist Thomas Sowell, whom Mamet called "one of our greatest minds".

During promotion of a book, Mamet was criticized for claiming that the British people had "a taint of anti-semitism," claiming they "want to give [Israel] away." In the same interview, Mamet went on to say that "there are famous dramatists and novelists [in the UK] whose works are full of anti-Semitic filth," but that he could not specify to whom he was referring for fear of litigation. He is known for his pro-Israel positions; in his book "The Secret Knowledge" he claimed that "Israelis would like to live in peace within their borders; the Arabs would like to kill them all."

In November 2012 Mamet penned an article for "The Jewish Journal of Greater Los Angeles" imploring fellow Jewish Americans to vote for Republican nominee Mitt Romney.

In an essay for "Newsweek", published on 29 January 2013, Mamet argued against gun control laws: "It was intended to guard us against this inevitable decay of government that the Constitution was written. Its purpose was and is not to enthrone a Government superior to an imperfect and confused electorate, but to protect us from such a government."

Mamet's style of writing dialogue, marked by a cynical, street-smart edge, precisely crafted for effect, is so distinctive that it has come to be called "Mamet speak." Mamet has recognized an association of his edgy narrative style by noting his debt to Harold Pinter, to whom he dedicated "Glengarry Glen Ross". He often uses italics and quotation marks to highlight particular words and to draw attention to his characters' frequent manipulation and deceitful use of language. His characters frequently interrupt one another, their sentences trail off unfinished, and their dialogue overlaps. Moreover, certain expressions and figures of speech are deliberately misrepresented to show that the character is not paying close attention to every detail of his dialogue (e.g., "or so forth" instead of "and so forth"). Mamet himself has criticized his (and other writers') tendency to write "pretty" at the expense of sound, logical plots.

When asked how he developed his style for writing dialogue, Mamet said, "In my family, in the days prior to television, we liked to while away the evenings by making ourselves miserable, based solely on our ability to speak the language viciously. That's probably where my ability was honed."

One instance of Mamet's dialogue style can be found in "Glengarry Glen Ross", in which two down-on-their-luck real estate salesmen are considering stealing from their employer's office. George Aaronow and Dave Moss equivocate on the meaning of "talk" and "speak", turning language and meaning to deceptive purposes:

Mamet dedicated "Glengarry Glen Ross" to Harold Pinter, who was instrumental in its being first staged at the Royal National Theatre, (London) in 1983, and whom Mamet has acknowledged as an influence on its success, and on his other work.

Mamet's plays have frequently sparked debate and controversy. During a staging of "Oleanna" in 1992, in which a post-secondary student accuses her professor of sexual harassment, a critic reported that the play divided the audience by gender and recounted "couples emerged screaming at each other".

Arthur Holmberg in his 2014 book "David Mamet and Male Friendship", has reconsidered the gender issue in many of Mamet's plays throughout his career by asserting a prominent and recurrent reversed sexual orientation of portrayed male gender preferences.

Mamet and actress Lindsay Crouse were married in 1977 and divorced in 1990. He and Crouse have two children, Willa and Zosia. Willa is a professional photographer and Zosia is an actress. Mamet has been married to actress and singer-songwriter Rebecca Pidgeon since 1991. They have two children, Clara and Noah.

The papers of David Mamet were sold to the Harry Ransom Center at the University of Texas at Austin in 2007 and first opened for research in 2009. The growing collection consists mainly of manuscripts and related production materials for most of his plays, films, and other writings, but also includes his personal journals from 1966 to 2005. In 2015, the Ransom Center secured a second major addition to Mamet's papers that include more recent works. Additional materials relating to Mamet and his career can be found in the Ransom Center's collections of Robert De Niro, Mel Gussow, Tom Stoppard, Sam Shepard, Paul Schrader, Don DeLillo, and John Russell Brown.

Mamet is credited as writer of these works except where noted. Credits in addition to writer also noted.


</doc>
<doc id="8352" url="https://en.wikipedia.org/wiki?curid=8352" title="December 6">
December 6





</doc>
<doc id="8353" url="https://en.wikipedia.org/wiki?curid=8353" title="December 5">
December 5





</doc>
<doc id="8354" url="https://en.wikipedia.org/wiki?curid=8354" title="December 4">
December 4






</doc>
<doc id="8355" url="https://en.wikipedia.org/wiki?curid=8355" title="December 3">
December 3





</doc>
<doc id="8356" url="https://en.wikipedia.org/wiki?curid=8356" title="December 2">
December 2






</doc>
<doc id="8357" url="https://en.wikipedia.org/wiki?curid=8357" title="December 1">
December 1






</doc>
<doc id="8359" url="https://en.wikipedia.org/wiki?curid=8359" title="December 24">
December 24





</doc>
<doc id="8360" url="https://en.wikipedia.org/wiki?curid=8360" title="December 26">
December 26







</doc>
<doc id="8361" url="https://en.wikipedia.org/wiki?curid=8361" title="Definable real number">
Definable real number

Informally, a definable real number is a real number that can be uniquely specified by its description. The description may be expressed as a construction or as a formula of a formal language. For example, the positive square root of 2, formula_1, can be defined as the unique positive solution to the equation formula_2, and it can be constructed with a compass and straightedge.

Different choices of a formal language or its interpretation can give rise to different notions of definability. Specific varieties of definable numbers include the constructible numbers of geometry, the algebraic numbers, and the computable numbers.

One way of specifying a real number uses geometric techniques. A real number "r" is a constructible number if there is a method to construct a line segment of length "r" using a compass and straightedge, beginning with a fixed line segment of length 1.

Each positive integer, and each positive rational number, is constructible. The positive square root of 2 is constructible. However, the cube root of 2 is not constructible; this is related to the impossibility of doubling the cube.

A real number "r" is called an algebraic number if there is a polynomial "p"("x"), with only integer coefficients, so that "r" is a root of "p", that is, "p"("r")=0. 
Each algebraic number can be defined individually using the order relation on the reals. For example, if a polynomial "q"("x") has 5 roots, the third one can be defined as the unique "r" such that "q"("r") = 0 and such that there are two distinct numbers less than "r" for which "q" is zero.

All rational numbers are algebraic, and all constructible numbers are algebraic. There are numbers such as the cube root of 2 which are algebraic but not constructible.

The algebraic numbers form a subfield of the real numbers. This means that 0 and 1 are algebraic numbers and, moreover, if "a" and "b" are algebraic numbers, then so are "a"+"b", "a"−"b", "ab" and, if "b" is nonzero, "a"/"b".

The algebraic numbers also have the property, which goes beyond being a subfield of the reals, that for each positive integer "n" and each algebraic number "a", all of the "n"th roots of "a" that are real numbers are also algebraic.

There are only countably many algebraic numbers, but there are uncountably many real numbers, so in the sense of cardinality most real numbers are not algebraic. This nonconstructive proof that not all real numbers are algebraic was first published by
Georg Cantor in his 1874 paper "On a Property of the Collection of All Real Algebraic Numbers".

Non-algebraic numbers are called transcendental numbers. Specific examples of transcendental numbers include π and Euler's number "e".

A real number is a computable number if there is an algorithm that, given a natural number "n", produces a decimal expansion for the number accurate to "n" decimal places. This notion was introduced by Alan Turing in 1936.

The computable numbers include the algebraic numbers along with many transcendental numbers including π and "e". Like the algebraic numbers, the computable numbers also form a subfield of the real numbers, and the positive computable numbers are closed under taking "n"th roots for each positive "n".

Not all real numbers are computable. The entire set of computable numbers is countable, so most reals are not computable. Specific examples of noncomputable real numbers include the limits of Specker sequences, and algorithmically random real numbers such as Chaitin's Ω numbers.

Another notion of definability comes from the formal theories of arithmetic, such as Peano arithmetic. The language of arithmetic has symbols for 0, 1, the successor operation, addition, and multiplication, intended to be interpreted in the usual way over the natural numbers. Because no variables of this language range over the real numbers, a different sort of definability is needed to refer to real numbers. A real number "a" is "definable in the language of arithmetic" (or "arithmetical") if its Dedekind cut can be defined as a predicate in that language; that is, if there is a first-order formula "φ" in the language of arithmetic, with three free variables, such that

A real number "a" is first-order definable in the language of set theory, without parameters, if there is a formula "φ" in the language of set theory, with one free variable, such that "a" is the unique real number such that "φ"("a") holds (see ). This notion cannot be expressed as a formula in the language of set theory.

All analytical numbers, and in particular all computable numbers, are definable in the language of set theory. Thus the real numbers definable in the language of set theory include all familiar real numbers such as 0, 1, π, "e", et cetera, along with all algebraic numbers. Assuming that they form a set in the model, the real numbers definable in the language of set theory over a particular model of ZFC form a field. 
Each set model "M" of ZFC set theory that contains uncountably many real numbers must contain real numbers that are not definable within "M" (without parameters). This follows from the fact that there are only countably many formulas, and so only countably many elements of "M" can be definable over "M". Thus, if "M" has uncountably many real numbers, we can prove from "outside" "M" that not every real number of "M" is definable over "M". 
This argument becomes more problematic if it is applied to class models of ZFC, such as the von Neumann universe . The argument that applies to set models cannot be directly generalized to class models in ZFC because the property "the real number "x" is definable over the class model "N"" cannot be expressed as a formula of ZFC. Similarly, the question whether the von Neumann universe contains real numbers that it cannot define cannot be expressed as a sentence in the language of ZFC. Moreover, there are countable models of ZFC in which all real numbers, all sets of real numbers, functions on the reals, etc. are definable .




</doc>
<doc id="8362" url="https://en.wikipedia.org/wiki?curid=8362" title="Diego de Almagro">
Diego de Almagro

Diego de Almagro, (; – July 8, 1538), also known as El Adelantado and El Viejo, was a Spanish conquistador and a companion. Later he was a rival of Francisco Pizarro. He participated in the Spanish conquest of Peru and was credited as the first European discoverer of Chile. Later, Almagro lost his left eye battling with coastal natives in the New World. In 1525 he joined the Pizarro brothers and Hernándo de Luque at Panama for the conquest of Peru.

The origins of Diego de Almagro remain obscure. He was born in 1475 in the village of Almagro, 1 in Ciudad Real, where he took the surname for being the illegitimate son of Juan de Montenegro and Elvira Gutiérrez. In order to save the honor of the mother, her relatives took her infant and moved him to the nearby town of Bolaños de Calatrava, being raised in this town and in Aldea del Rey, run by Sancha López del Peral.

When he turned 4 he returned to Almagro, being under the tutelage of an uncle named Hernán Gutiérrez until he was 15 years old, when due to his uncle's hardness he ran away from home. He went to the home of his mother, who was now living with her new husband, to tell her what had happened and that she was going to travel the world, asking for some bread to help her live in her misery. His mother, anguished, gave him a piece of bread and some coins and said: ""Take, son, and do not give me more pressure, and go, and God help in your adventure.""

He went to Seville and after probably stealing to survive the boy becomes a "criado" or servant and raised by Don Luis de Polanco, one of the four mayors of the Catholic Kings and later his counselor, and who was mayor of that city. While performing his duties as a servant, Almagro stabbed another servant for certain differences, leaving him with injuries so serious that they motivated that a trial against him be promoted.

Being wanted for justice, Don Luis de Polanco, making use of his influence, got Don Pedro Arias de Avila to allow him to embark in one of the ships that would go to the New World from the port of Sanlucar de Barrameda. The Casa de Contratacion demanded that the men who crossed the Indies carry their own weapons, clothes, and farming tools, which Don Polanco provided to his servant.
Diego de Almagro arrived in the New World on June 30, 1514, under the expedition that Ferdinand II of Aragon had sent under the guidance of Pedrarias Dávila. The expedition had landed in the city of Santa María la Antigua del Darién, Panama, where many other future conquistadors had already arrived, among them Francisco Pizarro.

There are not many details of Almagro's activities during this period, but it is known that he accompanied various sailors who departed from the city of Darien between 1514 and 1515. De Almagro eventually returned and settled in Darien, where he was granted an encomienda. He built a house and made a living from agriculture.

De Almagro undertook his first conquest on November 1515, commanding 260 men as he founded Villa del Acla, named after the Indian place. Due to illness he had to leave behind this mission to the licenciate Gaspar de Espinosa.

Espinosa decided to undertake a new expedition, which departed in December 1515 with 200 men, including De Almagro and Francisco Pizarro, who for the first time was designated as a captain. During this expedition, which lasted 14 months, De Almagro, Pizarro and Hernando de Luque became close friends.

Also during this time De Almagro established a friendship with Vasco Núñez de Balboa, who was in charge of Acla. De Almagro wanted to have a ship built with the remaining materials of the Espinosa expedition, to be finished on the coast of the "Great South Sea", as the Pacific Ocean was first called by the Spanish. Current historians do not believe that De Almagro was expected to participate in Balboa's expedition and probably returned to Darien.

De Almagro took part in the various expeditions that took place in the Gulf of Panama, taking part again in Espinosa's parties. Espinosa was supported by using Balboa's ships. De Almagro was recorded as a witness on the lists of natives whom Espinosa ordered to be carried. De Almagro remained as an early settler in the newly founded city of Panama. For four years he stayed there, working at the management of his properties and those of Pizarro. He took Ana Martínez, an indigenous woman, as a common-law wife. In this period, his first son, el "Mozo", was born to them.

By 1524 an association of conquest regarding South America was formalized among Almagro, Pizarro and Luque. By the beginning of August 1524, they had received the requisite permission to discover and conquer lands further south. De Almagro would remain in Panama to recruit men and gather supplies for the expeditions led by Pizarro.

After several expeditions to South America, Pizarro secured his stay in Peru with the "Capitulation" on 6 July 1529. During Pizarro's continued exploration of Incan territory, he and his men succeeded in defeating the Inca army under Emperor Atahualpa during the Battle of Cajamarca in 1532. De Almagro joined Pizarro soon afterward, bringing more men and arms.

After Peru fell to the Spanish, both Pizarro and De Almagro initially worked together in the founding of new cities to consolidate their dominions. As such, Pizarro dispatched De Almagro to pursue Quizquiz, fleeing to the Inca Empire's northern city of Quito. Their fellow conquistador Sebastián de Belalcázar, who had gone forth without Pizarro's approval, had already reached Quito and witnessed the destruction of the city by Inca general Rumiñawi. The Inca warrior had ordered the city to be burned and its gold to be buried at an undisclosed location where the Spanish could never find it. The arrival of Pedro de Alvarado from Guatemala, in search of Inca gold further complicated the situation for Almagro and Belalcázar. Alvarado's presence, however, did not last long as he left South America in exchange for monetary compensation from Pizarro.

In an attempt to claim Quito ahead of Belalcázar, in August 1534 De Almagro founded a city on the shores of Laguna de Colta (Colta Lake) in the foothills of Chimborazo, some south of present-day Quito, and named it "Santiago de Quito." Four months later would come the foundation of the Peruvian city of Trujillo, which Almagro named as "Villa Trujillo de Nueva Castilla" (the Village of Trujillo in New Castille) in honor of Francisco Pizarro's birthplace, Trujillo in Extremadura, Spain. These events were the height of the Pizarro-Almagro friendship, which historians describe as one of the last events in which their friendship soon faded and entered a period of turmoil for the control of the Incan capital of Cuzco.

After splitting the treasure of Inca emperor Atahualpa, both Pizarro and Almagro left towards Cuzco and took the city in 1533. However, De Almagro's friendship with Pizarro showed signs of deterioration in 1526 when Pizarro, in the name of the rest of the conquistadors, called forth the "Capitulacion de Toledo" law in which King Charles I of Spain had laid out his authorization for the conquest of Peru and the awards every conquistador would receive from it. Long before, however, each conquistador had promised to equally split the benefits. Pizarro managed to have a larger stake and awards for himself. Despite this, De Almagro still obtained an important fortune for his services, and the King awarded him in November 1532 the noble title of "Don" and he was assigned a personal coat of arms.

Although by this time Diego de Almagro had already acquired sufficient wealth in the conquest of Peru and was living a luxurious life in Cuzco, the prospect of conquering the lands further south was very attractive to him. Given that the dispute with Pizarro over Cuzco had kept intensifying, Almagro spent a great deal of time and money equipping a company of 500 men for a new exploration south of Peru.

By 1534 the Spanish crown had determined to split the region in two parallel lines, forming the governorship of "Nueva Castilla" (from the 1° to the 14° latitude, close to Pisco), and that of "Nueva Toledo" (from the 14° to the 25° latitude, in Taltal, Chile), assigning the first to Francisco Pizarro and the second to Diego de Almagro. The crown had previously assigned Almagro the governorship of Cuzco, and as such De Almagro was heading there when Charles V divided the territory between Nueva Castilla and Nuevo Toledo. This might have been the reason why Almagro did not immediately confront Pizarro for Cuzco, and promptly decided to embark on his new quest for the discovery of the riches of Chile.

Charles V had given Diego a grant extending two hundred leagues south of Francisco Pizarro's. Francisco and Diego concluded a new contract on 12 June 1535, in which they agreed to share future discoveries equally. Diego raised an expedition for Chile, expecting it "would lead to even greater riches than they had found in Peru." Almagro prepared the way by sending ahead three of his Spanish soldiers, the religious chief of the Inca empire, Willaq Umu, and Paullo Topa, brother of Manco Inca Yupanqui. De Almagro sent Juan de Saavedra forward with one hundred and fifty men, and soon followed them with additional forces. Saavedra established on January 23, 1535 the first Spanish settlement in Bolivia near the Inca regional capital of Paria.

Almagro left Cuzco on July 3, 1535 with his supporters and stopped at Moina until the 20th of that month. Meanwhile, Francisco Pizarro's brother, Juan Pizarro, had arrested Inca Manco Inca Yupanqui, further complicating De Almagro's plans as it heavily increased the dissatisfaction of the Indians submitted to Spanish rule. Not having formally been appointed governor of any territories in the Capitulation of Toledo in 1528, however, forcing him to declare himself "adelantado" (governor) of Nueva Toledo, or southern Peru and present-day Chile. Some sources suggest Almagro received such a requirement in 1534 by the Spanish king and was officially declared governor of New Toledo.

Once he left Moina, De Almagro followed the Inca trail followed by 750 Spaniards deciding to join him in quest for the gold lost in the ransom of Atahualpa, which had mainly benefited the Pizarro brothers and their supporters. After crossing the Bolivian mountain range and traveling past Lake Titicaca, Almagro arrived on the shores of the Desaguadero River and finally set up camp in Tupiza. From there, the expedition stopped at Chicoana and then turned to the southeast to cross the Andes mountains.

The expedition turned out to be a difficult and exhausting endeavor. The hardest phase was the crossing of the Andean cordilleras: the cold, hunger and tiredness meant the death of various Spanish and natives, but mainly slaves who were not accustomed to such rigorous climate.

Upon this point, De Almagro determined everything was a failure. He ordered a small group under Rodrigo Orgonez on a reconnaissance of the country to the south.

By luck, these men found the Valley of Copiapó, where Gonzalo Calvo Barrientos, a Spanish soldier whom Pizarro had expelled from Peru for stealing objects the Inca had offered for his ransom, had already established a friendship with the local natives. There, in the valley of the river Copiapó, Almagro took official possession of Chile and claimed it in the name of King Charles V.

De Almagro promptly initiated the exploration of the new territory, starting up the valley the Aconcagua River, where he was well received by the natives. However, the intrigues of his interpreter, Felipillo, who had previously helped Pizarro in dealing with "Atahualpa", almost thwarted De Almagro's efforts. Felipillo had secretly urged the local natives to attack the Spanish, but they desisted, not understanding the dangers that they posed. De Almagro directed Gómez de Alvarado along with 100 horsemen and 100 foot to continue the exploration, which ended in the confluence of the Ñuble and Itata rivers. The Battle of Reinohuelén between the Spanish and hostile Mapuche Indians forced the explorers to return to the north.

De Almagro's own reconnaissance of the land and the bad news of Gómez de Alvarado's encounter with the fierce Mapuche, along with the bitter cold winter that settled ferociously upon them, only served to confirm that everything had failed. He never found gold or the cities which Incan scouts had told him lay ahead, only communities of the indigenous population who lived from subsistence agriculture. Local tribes put up fierce resistance to the Spanish forces. The exploration of the territories of Nueva Toledo, which lasted 2 years, was marked by a complete failure for De Almagro. Despite this, at first he thought staying and founding a city would serve well for his honor. The initial optimism that led Almagro to bring his son he had with the indigenous Panamanian Ana Martínez to Chile had faded.

Some historians have suggested that, but for the urging of his senior explorers, De Almagro would probably have stayed permanently in Chile. He was urged to return to Peru and this time take definitive possession of Cuzco, so as to consolidate an inheritance for his son. Dismayed with his experience in the south, Almagro made plans of return to Peru. He never officially founded a city in the territory of what is now Chile.

The withdrawal of the Spanish from valleys of Chile was violent: Almagro authorized his soldiers to ransack the natives' properties, leaving their soil desolate. In addition, the Spanish soldiers took natives captive to serve as slaves. The locals were captured, tied together, and forced to carry the heavy loads belonging to the conquistadors.

After the exhausting crossing of the Atacama Desert, mainly due to the weather conditions, Almagro finally reached Cuzco, Peru, in 1537. According to some authors, it was during this time that the Spanish term ""roto"" (torn), used by Peruvians to refer to Chileans, was first coined. De Almagro's disappointed troops returned to Cuzco with their "torn clothes" due to the extensive and laborious passage on foot by the Atacama Desert.

After his return, De Almagro was surprised to learn of the Inca Manco's rebellion. Diego de Almagro sent an embassy to the Inca, but they mistrusted all of the Spaniards by this time. Hernando Pizarro's men formed an uneasy truce with De Almagro's men, surveying to determine the boundaries of their leaders' royal grants. They needed to determine in which portion the city of Cuzco was located. However, De Almagro's troops quickly took the city and imprisoned the Pizarro brothers, Hernando and Gonzalo, on the night of 8 April 1537.

After occupying Cuzco, De Almagro confronted an army sent by Francisco Pizarro to liberate his brothers. Alonso de Alvarado commanded it and was defeated during the Battle of Abancay on July 12, 1537. He and some of his men were imprisoned. Later, Gonzalo Pizarro and De Alvarado escaped prison. Subsequent negotiations between Francisco Pizarro and De Almagro concluded with the liberation of Hernando, the third Pizarro brother, in return for conceding control and administration of Cuzco to De Almagro. Pizarro never intended to give up the city permanently, but was buying time to organize an army strong enough to defeat Almagro's troops.

During this time Almagro fell ill, and Pizarro and his brothers grabbed the opportunity to defeat him and his followers. The Almagristas were defeated at Las Salinas in April 1538, with Orgóñez being killed on the field of battle. De Almagro fled to Cuzco, still in the hands of his loyal supporters, but found only temporary refuge; the forces of the Pizarro brothers entered the city without resistance. Once captured, Almagro was humiliated by Hernando Pizarro and his requests to appeal to the King were ignored.

When Diego de Almagro begged for his life, Hernando responded:

"-he was surprised to see Almagro demean himself in a manner so unbecoming a brave cavalier, that his fate was no worse than had befallen many a soldier before him; and that, since God had given him the grace to be a Christian, he should employ his remaining moments in making up his account with Heaven!"

Almagro was condemned to death and executed by "garrote" in his dungeon, and then decapitated, on July 8, 1538. His corpse was taken to the public Plaza Mayor of Cuzco, where a herald proclaimed his crimes. Hernan Ponce de Leon took his body and buried him in the church of Our Lady of Mercy in Cuzco.

Diego de Almagro II (1520–1542), known as "El Mozo" (The Lad), son of Diego de Almagro I, whose mother was an Indian girl of Panama, became the foil of the conspirators who had put Pizarro to the sword. Pizarro was murdered on June 26, 1541; the conspirators promptly proclaimed the lad De Almagro Governor of Peru. From various causes, all of the conspirators either died or were killed except for one, who was executed after the lad Almagro gave an order. The lad De Almagro fought the desperate battle of Chupas on September 16, 1542, escaped to Cuzco, but was arrested, immediately condemned to death, and executed in the great square of the city.





</doc>
<doc id="8363" url="https://en.wikipedia.org/wiki?curid=8363" title="Divinity">
Divinity

In religion, divinity or godhead is the state of things that are believed to come from a supernatural power or deity, such as a god, supreme being, creator deity, or spirits, and are therefore regarded as sacred and holy.
Such things are regarded as divine due to their transcendental origins or because their attributes or qualities are superior or supreme relative to things of the Earth. Divine things are regarded as eternal and based in truth, while material things are regarded as ephemeral and based in illusion. Such things that may qualify as divine are apparitions, visions, prophecies, miracles, and in some views also the soul, or more general things like resurrection, immortality, grace, and salvation. Otherwise what is or is not divine may be loosely defined, as it is used by different belief systems.

The root of the word "divine" is literally "godly" (from the Latin "deus", cf. "Dyaus", closely related to Greek "zeus", "div" in Persian and "deva" in Sanskrit), but the use varies significantly depending on which deity is being discussed. This article outlines the major distinctions in the conventional use of the terms.

For specific related academic terms, see Divinity (academic discipline), or Divine (Anglican).

Divinity as a quality has two distinct usages:
Overlap occurs between these usages because deities or godly entities are often identical with or identified by the powers and forces that are credited to them — in many cases a deity is merely a power or force personified — and these powers and forces may then be extended or granted to mortal individuals. For instance, Jehovah is closely associated with storms and thunder throughout much of the Old Testament. He is said to speak in thunder, and thunder is seen as a token of his anger. This power was then extended to prophets like Moses and Samuel, who caused thunderous storms to rain down on their enemies. (See and 1 Samuel 12:18.)

Divinity always carries connotations of goodness, beauty, beneficence, justice, and other positive, pro-social attributes. In monotheistic faiths there is an equivalent cohort of malefic supernatural beings and powers, such as demons, devils, afreet, etc., which are not conventionally referred to as divine; "demonic" is often used instead. Pantheistic and polytheistic faiths make no such distinction; gods and other beings of transcendent power often have complex, ignoble, or even irrational motivations for their acts. Note that while the terms "demon" and "demonic" are used in monotheistic faiths as antonyms to "divine", they are in fact derived from the Greek word "daimón" (δαίμων), which itself translates as "divinity".

There are three distinct usages of "divinity" and "divine" in religious discourse:

In monotheistic faiths, the word "divinity" is often used to refer to the singular God central to that faith. Often the word takes the definite article and is capitalized — ""the Divinity"" — as though it were a proper name or definitive honorific. 
"Divine" — capitalized — may be used as an adjective to refer to the manifestations of such a Divinity or its powers: e.g. "basking in the Divine presence..."

The terms "divinity" and "divine" — uncapitalized, and lacking the definite article — are sometimes used as to denote 'god(s) or certain other beings and entities which fall short of absolute Godhood but lie outside the human realm. These include (by no means an exhaustive list):

As previously noted, divinities are closely related to the transcendent force(s) or power(s) credited to them, so much so that in some cases the powers or forces may themselves be invoked independently. This leads to the second usage of the word "divine" (and a less common usage of "divinity"): to refer to the operation of transcendent power in the world.

In its most direct form, the operation of transcendent power implies some form of divine intervention. For pan- and polytheistic faiths this usually implies the direct action of one god or another on the course of human events. In Greek legend, for instance, it was Poseidon (god of the sea) who raised the storms which blew Odysseus' craft off course on his return journey, and Japanese tradition holds that a god-sent wind saved them from Mongol invasion. Prayers or propitiations are often offered to specific gods of pantheisms to garner favorable interventions in particular enterprises: e.g. safe journeys, success in war, or a season of bountiful crops. Many faiths around the world — from Japanese Shinto and Chinese traditional religion, to certain African practices and the faiths derived from those in the Caribbean, to Native American beliefs — hold that ancestral or household deities offer daily protection and blessings. In monotheistic religions, divine intervention may take very direct forms: miracles, visions, or intercessions by blessed figures.

Transcendent force or power may also operate through more subtle and indirect paths. Monotheistic faiths generally support some version of divine providence, which acknowledges that the divinity of the faith has a profound but unknowable plan always unfolding in the world. Unforeseeable, overwhelming, or seemingly unjust events are often thrown on 'the will of the Divine', in deferences like the Muslim "inshallah" ('as God wills it') and Christian 'God works in mysterious ways'. Often such faiths hold out the possibility of divine retribution as well, where the divinity will unexpectedly bring evil-doers to justice through the conventional workings of the world; from the subtle redressing of minor personal wrongs, to such large-scale havoc as the destruction of Sodom and Gomorrah or the biblical Great Flood. Other faiths are even more subtle: the doctrine of "karma" shared by Buddhism and Hinduism is a divine law similar to divine retribution but without the connotation of punishment: our acts, good or bad, intentional or unintentional, reflect back on us as part of the natural working of the universe. Philosophical Taoism also proposes a transcendent operant principle — transliterated in English as "tao" or "dao", meaning 'the way' — which is neither an entity or a being per se, but reflects the natural ongoing process of the world. Modern western mysticism and new age philosophy often use the term 'the Divine' as a noun in this latter sense: a non-specific principle or being that gives rise to the world, and acts as the source or wellspring of life. In these latter cases the faiths do not promote deference, as happens in monotheisms; rather each suggests a path of action that will bring the practitioner into conformance with the divine law: "ahimsa" — 'no harm' — for Buddhist and Hindu faiths; "de" or "te" — 'virtuous action' — in Taoism; and any of numerous practices of peace and love in new age thinking.

In the third usage, extensions of divinity and divine power are credited to living, mortal individuals. Political leaders are known to have claimed actual divinity in certain early societies — the ancient Egyptian Pharaohs being the premier case — taking a role as objects of worship and being credited with superhuman status and powers. More commonly, and more pertinent to recent history, leaders merely claim some form of divine mandate, suggesting that their rule is in accordance with the will of God. The doctrine of the divine right of kings was introduced as late as the 17th century, proposing that kings rule by divine decree; Japanese Emperors ruled by divine mandate until the inception of the Japanese constitution after World War II.

Less politically, most faiths have any number of people that are believed to have been touched by divine forces: saints, prophets, heroes, oracles, martyrs, and enlightened beings, among others. Saint Francis of Assisi, in Catholicism, is said to have received instruction directly from God and it is believed that he grants plenary indulgence to all who confess their sins and visit his chapel on the appropriate day. In Greek mythology, Achilles' mother bathed him in the river Styx to give him immortality, and Hercules — as the son of Zeus — inherited near-godly powers. In religious Taoism, Lao Tsu is venerated as a saint with his own powers. Various individuals in the Buddhist faith, beginning with Siddhartha, are considered to be enlightened, and in religious forms of Buddhism they are credited with divine powers. Christ is said to have performed divine miracles.

In general, mortals with divine qualities are carefully distinguished from the deity or deities in their religion's main pantheon. Even the Christian faith, which generally holds Christ to be identical to God, distinguishes between God the Father and Christ the begotten Son. There are, however, certain esoteric and mystical schools of thought, present in many faiths — Sufis in Islam, Gnostics in Christianity, Advaitan Hindus, Zen Buddhists, as well as several non-specific perspectives developed in new age philosophy — which hold that all humans are in essence divine, or unified with the Divine in a non-trivial way. Such divinity, in these faiths, would express itself naturally if it were not obscured by the social and physical worlds we live in; it needs to be brought to the fore through appropriate spiritual practices.

In traditional Christian theology, the concept and nature of divinity always has its source ultimately from God himself. It's the state or quality of being divine, and the term can denote Godly nature or character. In Hebrew, the terms would usually be "el", "elohim", and in Greek usually "theos", or "theias". The divinity in the Bible is considered the Godhead itself, or God in general. Or it may have reference to a deity. Even angels in the Psalms are considered divine or "elohim", as spirit beings, in God's form. Redeemed Christians, when taken to heaven as immortalized born-again believers, according to Biblical verses, are said to partake of the "divine nature". (Psalm 8:5; Hebrews 2:9; 2 Peter 1:4)

In the Christian Greek Scriptures of the Bible, the Greek word θεῖον ("theion") in the Douay Version, is translated as "divinity". Examples are below:

The word translated as either "deity", "Godhead", or "divinity" in the Greek New Testament is also the Greek word θεότητος ("theotētos"), and the one Verse that contains it is this:
Colossians 2:9

The word "divine" in the New Testament is the Greek word θείας ("theias"), and is the adjective form of "divinity". Biblical examples from the King James Bible are below:

The most prominent conception of divine entities in The Church of Jesus Christ of Latter-day Saints (LDS Church) is the Godhead, a divine council of three distinct beings: Elohim (the Father), Jehovah (the Son, or Jesus), and the Holy Spirit. Joseph Smith described a nontrinitarian Godhead, with God the Father and Jesus Christ each having individual physical bodies, and the Holy Spirit as a distinct personage with a spirit body. Smith also introduced the existence of a Heavenly Mother in the King Follett Discourse, but very little is acknowledged or known beyond her existence.

Mormons hold a belief in the divine potential of humanity; Smith taught a form of divinization where mortal men and women can become like god through salvation and exaltation. Lorenzo Snow succinctly summarized this using a couplet, which is often repeated within the LDS Church: "As man now is, God once was: As God now is, man may be."



</doc>
<doc id="8367" url="https://en.wikipedia.org/wiki?curid=8367" title="Depth of field">
Depth of field

In optics, particularly as it relates to film and photography, the optical phenomenon known as depth of field (DOF), is the distance about the plane of focus (POF) where objects appear acceptably sharp in an image. Although an optical imaging system can precisely focus on only one plane at a time, the decrease in sharpness is gradual on each side of the POF, so that within the DOF the unsharpness is imperceptible under normal viewing conditions.

In some cases, it may be desirable to have the entire image sharp, and a large DOF is appropriate. In other cases, a small DOF may be more effective, emphasizing the subject while de-emphasizing the foreground and background. In cinematography, a large DOF is often called deep focus, and a small DOF is often called shallow focus.

Precise focus is possible in only one two-dimensional plane; in that plane, a point object will produce a point image. In any other plane, a point object is "defocused", and will produce a blur spot shaped like the aperture, which for the purpose of analysis is usually assumed to be circular. When this circular spot is sufficiently small, it is indistinguishable from a point, and appears to be in focus; it is rendered as "acceptably sharp". The diameter of the circle increases with distance from the plane of focus; the largest circle that is indistinguishable from a point is known as the "acceptable circle of confusion", or informally, simply as the "circle of confusion". The acceptable circle of confusion is influenced by visual acuity, viewing conditions, and the amount by which the image is enlarged (Ray 2000, 52–53). The increase of the circle diameter with defocus is gradual, so the limits of depth of field are not hard boundaries between sharp and unsharp.

For 35 mm motion pictures, the image area on the film is roughly 22 mm by 16 mm. The limit of tolerable error was traditionally set at 0.05 mm (0.002 in) diameter, while for 16 mm film, where the size is about half as large, the tolerance is stricter, 0.025 mm (0.001 in). More modern practice for 35 mm productions set the circle of confusion limit at 0.025 mm (0.001 in).

For full-frame 35mm still photography, the circle of confusion is usually chosen to be about 1/30 mm. Because the human eye is capable of resolving a spot with diameter about 1/4 mm at 25 cm distance from the viewing eye, and the 35 mm negative needs about an 8X enlargement to make an 8x10 inch print, it is sometimes argued that the criterion should be about 1/32 mm on the 35mm negative, but 1/30 mm is close enough.

For 6x6 cm format enlarged to 8x8 inches and viewed at 25 cm, the enlargement is 3.4X, hence the circle of confusion criterion is about 1/(3.4 x 4) = 0.07 mm.

Similarly, for subminiature photography (for example the Tessina) with a frame format of 14x21mm, 8x12 inches corresponds to 14.5X enlargement, hence circle of confusion limit about 0.017 mm.

Many sources propose CoC limits as a fraction of the film format diagonal, typically 1/1000 in the early twentieth century to 1/1500 more recently. The three formats above at fraction 1/1500 would use 0.029 (about 1/32), 0.056, and 0.017 mm.

Traditional depth-of-field formulas and tables assume equal circles of confusion for near and far objects. Some authors, such as Merklinger (1992), have suggested that distant objects often need to be much sharper to be clearly recognizable, whereas closer objects, being larger on the film, do not need to be so sharp. The loss of detail in distant objects may be particularly noticeable with extreme enlargements. Achieving this additional sharpness in distant objects usually requires focusing beyond the hyperfocal distance, sometimes almost at infinity. For example, if photographing a cityscape with a traffic bollard in the foreground, this approach, termed the "object field method" by Merklinger, would recommend focusing very close to infinity, and stopping down to make the bollard sharp enough. With this approach, foreground objects cannot always be made perfectly sharp, but the loss of sharpness in near objects may be acceptable if recognizability of distant objects is paramount.

Other authors (Adams 1980, 51) have taken the opposite position, maintaining that slight unsharpness in foreground objects is usually more disturbing than slight unsharpness in distant parts of a scene.

Moritz von Rohr also used an object field method, but unlike Merklinger, he used the conventional criterion of a maximum circle of confusion diameter in the image plane, leading to unequal front and rear depths of field.

Several other factors, such as subject matter, movement, camera-to-subject distance, lens focal length, selected lens "f"-number, format size, and circle of confusion criteria also influence when a given defocus becomes noticeable. The combination of focal length, subject distance, and format size defines magnification at the film / sensor plane.

DOF is determined by subject magnification at the film / sensor plane and the selected lens aperture or "f"-number. For a given "f"-number, increasing the magnification, either by moving closer to the subject or using a lens of greater focal length, decreases the DOF; decreasing magnification increases DOF. For a given subject magnification, increasing the "f"-number (decreasing the aperture diameter) increases the DOF; decreasing "f"-number decreases DOF.

If the original image is enlarged to make the final image, the circle of confusion in the original image must be smaller than that in the final image by the ratio of enlargement. Cropping an image and enlarging to the same size final image as an uncropped image taken under the same conditions is equivalent to using a smaller format under the same conditions, so the cropped image has less DOF. (Stroebel 1976, 134, 136–37).

When focus is set to the hyperfocal distance, the DOF extends from half the hyperfocal distance to infinity, and the DOF is the largest possible for a given "f"-number.

The comparative DOFs of two different format sizes depend on the conditions of the comparison. The DOF for the smaller format can be either more than or less than that for the larger format. In the discussion that follows, it is assumed that the final images from both formats are the same size, are viewed from the same distance, and are judged with the same circle of confusion criterion. (Derivations of the effects of format size are given under Derivation of the DOF formulae.)

When the "same picture" is taken in two different format sizes from the same distance at the same "f"-number with lenses that give the same angle of view, and the final images (e.g., in prints, or on a projection screen or electronic display) are the same size, DOF is, to a first approximation, inversely proportional to format size (Stroebel 1976, 139). Though commonly used when comparing formats, the approximation is valid only when the subject distance is large in comparison with the focal length of the larger format and small in comparison with the hyperfocal distance of the smaller format.

Moreover, the larger the format size, the longer a lens will need to be to capture the same framing as a smaller format. In motion pictures, for example, a frame with a 12 degree horizontal field of view will require a 50 mm lens on 16 mm film, a 100 mm lens on 35 mm film, and a 250 mm lens on 65 mm film. Conversely, using the same focal length lens with each of these formats will yield a progressively wider image as the film format gets larger: a 50 mm lens has a horizontal field of view of 12 degrees on 16 mm film, 23.6 degrees on 35 mm film, and 55.6 degrees on 65 mm film. Therefore, because the larger formats require longer lenses than the smaller ones, they will accordingly have a smaller depth of field. Compensations in exposure, framing, or subject distance need to be made in order to make one format look like it was filmed in another format.

Many small-format digital SLR camera systems allow using many of the same lenses on both full-frame and "cropped format" cameras. If, for the same focal length setting, the subject distance is adjusted to provide the "same field of view" at the subject, at the same "f"-number and final-image size, the smaller format has "greater" DOF, as with the "same picture" comparison above. If pictures are taken from the "same distance" using the same "f"-number, same focal length, and the final images are the same size, the smaller format has "less" DOF. If pictures taken from the same subject distance using the same focal length, are given the "same enlargement", both final images will have the "same" DOF. The pictures from the two formats will differ because of the different angles of view. If the larger format is cropped to the captured area of the smaller format, the final images will have the same angle of view, have been given the same enlargement, and have the same DOF.

In many cases, the DOF is fixed by the requirements of the desired image. For a given DOF and field of view, the required "f"-number is proportional to the format size. For example, if a 35 mm camera required 11, a 4×5 camera would require 45 to give the same DOF. For the same ISO speed, the exposure time on the 4×5 would be sixteen times as long; if the 35 camera required 1/250 second, the 4×5 camera would require 1/15 second. The longer exposure time with the larger camera might result in motion blur, especially with windy conditions, a moving subject, or an unsteady camera.

Adjusting the "f"-number to the camera format is equivalent to maintaining the same absolute aperture diameter; when set to the same absolute aperture diameters, both formats have the same DOF.

Comparison of fast standard lenses in the four main formats when used for portraiture with appropriate circles of confusion to produce an uncropped image at 10x8 inches to be viewed at 25 cm show that the following settings with similar aperture diameters produce similar DoF:
For any of these, doubling the f-number will approximately double the depth of field.

When the lens axis is perpendicular to the image plane, as is normally the case, the plane of focus (POF) is parallel to the image plane, and the DOF extends between parallel planes on either side of the POF. When the lens axis is not perpendicular to the image plane, the POF is no longer parallel to the image plane; the ability to rotate the POF is known as the Scheimpflug principle. Rotation of the POF is accomplished with camera movements (tilt, a rotation of the lens about a horizontal axis, or swing, a rotation about a vertical axis). Tilt and swing are available on most view cameras, and are also available with specific lenses on some small- and medium-format cameras.

When the POF is rotated, the near and far limits of DOF are no longer parallel; the DOF becomes wedge-shaped, with the apex of the wedge nearest the camera (Merklinger 1993, 31–32; Tillmanns 1997, 71). With tilt, the height of the DOF increases with distance from the camera; with swing, the width of the DOF increases with distance.

In some cases, rotating the POF can better fit the DOF to the scene, and achieve the required sharpness at a smaller f-number. Alternatively, rotating the POF, in combination with a small f-number, can minimize the part of an image that is within the DOF.

For a given subject framing and camera position, the DOF is controlled by the lens aperture diameter, which is usually specified as the f-number, the ratio of lens focal length to aperture diameter. Reducing the aperture diameter (increasing the f-number) increases the DOF because the circle of confusion is shrunk directly and indirectly by reducing the light hitting the outside of the lens which is focused to a different point than light hitting the inside of the lens due to spherical aberration caused by the construction of the lens; however, it also reduces the amount of light transmitted, and increases diffraction, placing a practical limit on the extent to which DOF can be increased by reducing the aperture diameter.

Motion pictures make only limited use of this control; to produce a consistent image quality from shot to shot, cinematographers usually choose a single aperture setting for interiors and another for exteriors, and adjust exposure through the use of camera filters or light levels. Aperture settings are adjusted more frequently in still photography, where variations in depth of field are used to produce a variety of special effects.

The advent of digital technology in photography has provided additional means of controlling the extent of image sharpness; some methods allow extended DOF that would be impossible with traditional techniques, and some allow the DOF to be determined after the image is made.

Focus stacking is a digital image processing technique which combines multiple images taken at different focal distances to give a resulting image with a greater depth of field than any of the individual source images. Available programs for multi-shot DOF enhancement include Adobe Photoshop, Syncroscopy AutoMontage, PhotoAcute Studio, Helicon Focus and CombineZ. Getting sufficient depth of field can be particularly challenging in macro photography. The images to the right illustrate the extended DOF that can be achieved by combining multiple images.

Wavefront coding is a method that convolves rays in such a way that it provides an image where fields are in focus simultaneously with all planes out of focus by a constant amount.

A plenoptic camera uses a microlens array to capture 4D light field information about a scene.

Colour apodization is a technique combining a modified lens design with image processing to achieve an increased depth of field. The lens is modified such that each colour channel has a different lens aperture. For example, the red channel may be "f"/2.4, green may be "f"/2.4, whilst the blue channel may be "f"/5.6. Therefore, the blue channel will have a greater depth of field than the other colours. The image processing identifies blurred regions in the red and green channels and in these regions copies the sharper edge data from the blue channel. The result is an image that combines the best features from the different "f"-numbers, (Kay 2011).

In 2013, Nokia implemented DOF control in some of its high-end smartphones, called Refocus, which can change a picture's depth of field after the picture is taken. It works best when there are close-up and distant objects in the frame.

If the camera position and image framing (i.e., angle of view) have been chosen, the only means of controlling DOF is the lens aperture. Most DOF formulas imply that any arbitrary DOF can be achieved by using a sufficiently large f-number. Because of diffraction, however, this isn't really true. Once a lens is stopped down to where most aberrations are well corrected, stopping down further will decrease sharpness in the plane of focus. At the DOF limits, however, further stopping down decreases the size of the defocus blur spot, and the overall sharpness may still increase. Eventually, the defocus blur spot becomes negligibly small, and further stopping down serves only to decrease sharpness even at DOF limits (Gibson 1975, 64). There is thus a tradeoff between sharpness in the POF and sharpness at the DOF limits. But the sharpness in the POF is always greater than that at the DOF limits; if the blur at the DOF limits is imperceptible, the blur in the POF is imperceptible as well.

For general photography, diffraction at DOF limits typically becomes significant only at fairly large f-numbers; because large f-numbers typically require long exposure times, motion blur may cause greater loss of sharpness than the loss from diffraction. The size of the diffraction blur spot depends on the effective f-number formula_1, however, so diffraction is a greater issue in close-up photography, and the tradeoff between DOF and overall sharpness can become quite noticeable (Gibson 1975, 53; Lefkowitz 1979, 84).

Many lenses for small- and medium-format cameras include scales that indicate the DOF for a given focus distance and f-number; the 35 mm lens in the image is typical. That lens includes distance scales in feet and meters; when a marked distance is set opposite the large white index mark, the focus is set to that distance. The DOF scale below the distance scales includes markings on either side of the index that correspond to f-numbers. When the lens is set to a given f-number, the DOF extends between the distances that align with the f-number markings.

Some cameras have the DOF scale not on lens barrel, but on focusing knob or dial; for example, the Rolleiflex TLR has its DOF scale on the focusing knob; the subminiature camera Tessina has DOF a scale on the focusing dial.

When the 35 mm lens above is set to f/11 and focused at approximately 1.3 m, the DOF (a "zone" of acceptable sharpness) extends from 1 m to 2 m. Conversely, the required focus and f-number can be determined from the desired DOF limits by locating the near and far DOF limits on the lens distance scale and setting focus so that the index mark is centered between the near and far distance marks. The required f-number is determined by finding the markings on the DOF scale that are closest to the near and far distance marks (Ray 1994, 315). For the 35 mm lens above, if it were desired for the DOF to extend from 1 m to 2 m, focus would be set so that index mark was centered between the marks for those distances, and the aperture would be set to f/11.

The focus so determined would be about 1.3 m, the approximate harmonic mean of the near and far distances. See the section Focus and "f"-number from DOF limits for additional discussion.

If the marks for the near and far distances fall outside the marks for the largest f-number on the DOF scale, the desired DOF cannot be obtained; for example, with the 35 mm lens above, it is not possible to have the DOF extend from 0.7 m to infinity. The DOF limits can be determined visually, by focusing on the farthest object to be within the DOF and noting the distance mark on the lens distance scale, and repeating the process for the nearest object to be within the DOF.

Some distance scales have markings for only a few distances; for example, the 35 mm lens above shows only 3 ft and 5 ft on its upper scale. Using other distances for DOF limits requires visual interpolation between marked distances. Since the distance scale is nonlinear, accurate interpolation can be difficult. In most cases, English and metric distance markings are not coincident, so using both scales to note focused distances can sometimes lessen the need for interpolation. Many autofocus lenses have smaller distance and DOF scales and fewer markings than do comparable manual-focus lenses, so that determining focus and f-number from the scales on an autofocus lens may be more difficult than with a comparable manual-focus lens. In most cases, determining these settings using the lens DOF scales on an autofocus lens requires that the lens or camera body be set to manual focus.

On a view camera, the focus and f-number can be obtained by measuring the "focus spread" and performing simple calculations. The procedure is described in more detail in the section Focus and f-number from DOF limits. Some view cameras include DOF calculators that indicate focus and f-number without the need for any calculations by the photographer (Tillmanns 1997, 67–68; Ray 2002, 230–31).

The hyperfocal distance is the nearest focal distance at which the DOF extends to infinity; focusing the camera at the hyperfocal distance results in the largest possible depth of field for a given f-number (Ray 2000, 55). Focusing "beyond" the hyperfocal distance does not increase the far DOF (which already extends to infinity), but it does decrease the DOF in front of the subject, decreasing the total DOF. Some photographers consider this wasting DOF; however, see Object field methods above for a rationale for doing so. Focusing on the hyperfocal distance is a special case of zone focusing in which the far limit of DOF is at infinity.

If the lens includes a DOF scale, the hyperfocal distance can be set by aligning the infinity mark on the distance scale with the mark on the DOF scale corresponding to the f-number to which the lens is set. For example, with the 35 mm lens shown above set to f/11, aligning the infinity mark with the "11" to the left of the index mark on the DOF scale would set the focus to the hyperfocal distance.

Some cameras have their hyperfocal distance marked on the focus dial. For example, on the Minox LX focusing dial there is a red dot between 2 m and infinity; when the lens is set at the red dot, that is, focused at the hyperfocal distance, the depth of field stretches from 2 m to infinity.

The Zeiss Ikon Contessa camera has 20 ft marked in red and aperture 8 marked in red; this is the snapshot hyperfocal setting.

Depth of field can be anywhere from a fraction of a millimeter to virtually infinite. In some cases, such as landscapes, it may be desirable to have the entire image sharp, and a large DOF is appropriate. In other cases, artistic considerations may dictate that only a part of the image be in focus, emphasizing the subject while de-emphasizing the background, perhaps giving only a suggestion of the environment (Langford 1973, 81). For example, a common technique in melodramas and horror films is a closeup of a person's face, with someone just behind that person visible but out of focus. A portrait or close-up still photograph might use a small DOF to isolate the subject from a distracting background. The use of limited DOF to emphasize one part of an image is known as "selective focus", "differential focus" or "shallow focus".

Although a small DOF implies that other parts of the image will be unsharp, it does not, by itself, determine "how" unsharp those parts will be. The amount of background (or foreground) blur depends on the distance from the plane of focus, so if a background is close to the subject, it may be difficult to blur sufficiently even with a small DOF. In practice, the lens f-number is usually adjusted until the background or foreground is acceptably blurred, often without direct concern for the DOF.

Sometimes, however, it is desirable to have the entire subject sharp while ensuring that the background is sufficiently unsharp. When the distance between subject and background is fixed, as is the case with many scenes, the DOF and the amount of background blur are not independent. Although it is not always possible to achieve both the desired subject sharpness and the desired background unsharpness, several techniques can be used to increase the separation of subject and background.

For a given scene and subject magnification, the background blur increases with lens focal length. If it is not important that background objects be unrecognizable, background de-emphasis can be increased by using a lens of longer focal length and increasing the subject distance to maintain the same magnification. This technique requires that sufficient space in front of the subject be available; moreover, the perspective of the scene changes because of the different camera position, and this may or may not be acceptable.

The situation is not as simple if it is important that a background object, such as a sign, be unrecognizable. The magnification of background objects also increases with focal length, so with the technique just described, there is little change in the recognizability of background objects. However, a lens of longer focal length may still be of some help; because of the narrower angle of view, a slight change of camera position may suffice to eliminate the distracting object from the field of view.

Although tilt and swing are normally used to maximize the part of the image that is within the DOF, they also can be used, in combination with a small f-number, to give selective focus to a plane that isn't perpendicular to the lens axis. With this technique, it is possible to have objects at greatly different distances from the camera in sharp focus and yet have a very shallow DOF. The effect can be interesting because it differs from what most viewers are accustomed to seeing.

The DOF beyond the subject is always greater than the DOF in front of the subject. When the subject is at the hyperfocal distance or beyond, the far DOF is infinite, so the ratio is 1:∞; as the subject distance decreases, near:far DOF ratio increases, approaching unity at high magnification. For large apertures at typical portrait distances, the ratio is still close to 1:1. The oft-cited rule that 1/3 of the DOF is in front of the subject and 2/3 is beyond (a 1:2 ratio) is true only when the subject distance is 1/3 the hyperfocal distance.

As a lens is stopped down, the defocus blur at the DOF limits decreases but diffraction blur increases. The presence of these two opposing factors implies a point at which the combined blur spot is minimized (Gibson 1975, 64); at that point, the f-number is optimal for image sharpness.
If the final image is viewed under normal conditions (e.g., an 8″×10″ image viewed at 10″), it may suffice to determine the f-number using criteria for minimum required sharpness, and there may be no practical benefit from further reducing the size of the blur spot. But this may not be true if the final image is viewed under more demanding conditions, e.g., a very large final image viewed at normal distance, or a portion of an image enlarged to normal size (Hansma 1996). Hansma also suggests that the final-image size may not be known when a photograph is taken, and obtaining the maximum practicable sharpness allows the decision to make a large final image to be made at a later time.

Hansma (1996) and Peterson (1996) have discussed determining the combined effects of defocus and diffraction using a root-square combination of the individual blur spots. Hansma's approach determines the f-number that will give the maximum possible sharpness; Peterson's approach determines the minimum f-number that will give the desired sharpness in the final image, and yields a maximum focus spread for which the desired sharpness can be achieved. In combination, the two methods can be regarded as giving a maximum and minimum f-number for a given situation, with the photographer free to choose any value within the range, as conditions (e.g., potential motion blur) permit. Gibson (1975), 64) gives a similar discussion, additionally considering blurring effects of camera lens aberrations, enlarging lens diffraction and aberrations, the negative emulsion, and the printing paper. Couzin (1982) gave a formula essentially the same as Hansma's for optimal "f"-number, but did not discuss its derivation.

Hopkins (1955), Stokseth (1969), and Williams and Becklund (1989) have discussed the combined effects using the modulation transfer function.

In semiconductor photolithography applications, depth of field is extremely important as integrated circuit layout features must be printed with high accuracy at extremely small size. The difficulty is that the wafer surface is not perfectly flat, but may vary by several micrometres. Even this small variation causes some distortion in the projected image, and results in unwanted variations in the resulting pattern. Thus photolithography engineers take extreme measures to maximize the optical depth of field of the photolithography equipment. To minimize this distortion further, semiconductor manufacturers may use chemical mechanical polishing to make the wafer surface even flatter before lithographic patterning.

A person may sometimes experience better vision in daylight than at night because of an increased depth of field due to constriction of the pupil (i.e., miosis).

The basis of these formulas is given in the section Derivation of the DOF formulae; refer to the diagram in that section for illustration of the quantities discussed below.

Let formula_2 be the lens focal length, formula_3 be the lens f-number, and formula_4 be the circle of confusion for a given image format. The hyperfocal distance formula_5 is given by

Let formula_7 be the distance at which the camera is focused (the "subject distance"). When formula_7 is large in comparison with the lens focal length, the distance formula_9 from the camera to the near limit of DOF and the distance formula_10 from the camera to the far limit of DOF are

and

The depth of field formula_13 is

Substituting for formula_5 and rearranging, DOF can be expressed as

Thus, for a given image format, depth of field is determined by three factors: the focal length of the lens, the f-number of the lens opening (the aperture), and the camera-to-subject distance.

When the subject distance is the hyperfocal distance,

and

For formula_19, the far limit of DOF is at infinity and the DOF is infinite; of course, only objects at or beyond the near limit of DOF will be recorded with acceptable sharpness.

When the subject distance formula_7 approaches the focal length, using the formulas given above can result in significant errors. For close-up work, the hyperfocal distance has little applicability, and it usually is more convenient to express DOF in terms of image magnification. Let formula_21 be the magnification; when the subject distance is small in comparison with the hyperfocal distance,

so that for a given magnification, DOF is independent of focal length. In other words, for the same subject magnification, at the same "f"-number, all focal lengths used on a given image format give approximately the same DOF.

The discussion thus far has assumed a symmetrical lens for which the entrance and exit pupils coincide with the front and rear nodal planes, and for which the pupil magnification (the ratio of exit pupil diameter to that of the entrance pupil) is unity. Although this assumption usually is reasonable for large-format lenses, it often is invalid for medium- and small-format lenses.

When formula_23, the DOF for an asymmetrical lens is

where formula_25 is the pupil magnification. When the pupil magnification is unity, this equation reduces to that for a symmetrical lens.

Except for close-up and macro photography, the effect of lens asymmetry is minimal. At unity magnification, however, the errors from neglecting the pupil magnification can be significant. Consider a telephoto lens with formula_26 and a retrofocus wide-angle lens with formula_27, at formula_28. The asymmetrical-lens formula gives formula_29 and formula_30 respectively. The symmetrical lens formula gives formula_31 in either case. The errors are −33% and 33% respectively.

If only working f-number is directly available, the following formula can be used instead:

For given near and far DOF limits formula_9 and formula_10, the required f-number is smallest when focus is set to

the harmonic mean of the near and far distances. When the subject distance is large in comparison with the lens focal length, the required f-number is

When the far limit of DOF is at infinity,

and

In practice, these settings usually are determined on the image side of the lens, using measurements on the bed or rail with a view camera, or using lens DOF scales on manual-focus lenses for small- and medium-format cameras. If formula_39 and formula_40 are the image distances that correspond to the near and far limits of DOF, the required f-number is minimized when the image distance
formula_41 is

In practical terms, focus is set to halfway between the near and far image distances. The required f-number is

The image distances are measured from the camera's image plane to the lens's image nodal plane, which is not always easy to locate. In most cases, focus and f-number can be determined with sufficient accuracy using the approximate formulas above, which require only the difference between the near and far image distances; view camera users sometimes refer to the difference formula_44 as the "focus spread" (Hansma 1996, 55). Most lens DOF scales are based on the same concept.

The focus spread is related to the depth of focus. Ray (2000, 56) gives two definitions of the latter. The first is the tolerance of the position of the image plane for which an object remains acceptably sharp; the second is that the limits of depth of focus are the image-side conjugates of the near and far limits of DOF. With the first definition, focus spread and depth of focus are usually close in value though conceptually different. With the second definition, focus spread and depth of focus are the same.

If a subject is at distance formula_7 and the foreground or background is at distance formula_46, let the distance between the subject and the foreground or background be indicated by

The blur disk diameter formula_48 of a detail at distance formula_49 from the subject can be expressed as a function of the subject magnification formula_50, focal length formula_2, f-number formula_3 or alternatively the diameter of the entrance pupil formula_53 (often called the aperture) according to

The minus sign applies to a foreground object, and the plus sign applies to a background object.

The blur increases with the distance from the subject; when formula_55, the detail is within the depth of field, and the blur is imperceptible. If the detail is only slightly outside the DOF, the blur may be only barely perceptible.

For a given subject magnification, f-number, and distance from the subject of the foreground or background detail, the degree of detail blur varies with the lens focal length. For a background detail, the blur increases with focal length; for a foreground detail, the blur decreases with focal length. For a given scene, the positions of the subject, foreground, and background usually are fixed, and the distance between subject and the foreground or background remains constant regardless of the camera position; however, to maintain constant magnification, the subject distance must vary if the focal length is changed. For small distance between the foreground or background detail, the effect of focal length is small; for large distance, the effect can be significant. For a reasonably distant background detail, the blur disk diameter is

depending only on focal length.

The blur diameter of foreground details is very large if the details are close to the lens.

The magnification of the detail also varies with focal length; for a given detail, the ratio of the blur disk diameter to imaged size of the detail is independent of focal length, depending only on the detail size and its distance from the subject. This ratio can be useful when it is important that the background be recognizable (as usually is the case in evidence or surveillance photography), or unrecognizable (as might be the case for a pictorial photographer using selective focus to isolate the subject from a distracting background). As a general rule, an object is recognizable if the blur disk diameter is one-tenth to one-fifth the size of the object or smaller (Williams 1990, 205), and unrecognizable when the blur disk diameter is the object size or greater.

The effect of focal length on background blur is illustrated in van Walree's article on Depth of field.

The distance scales on most medium- and small-format lenses indicate distance from the camera's image plane. Most DOF formulas, including those in this article, use the object distance formula_7 from the lens's front nodal plane, which often is not easy to locate. Moreover, for many zoom lenses and internal-focusing non-zoom lenses, the location of the front nodal plane, as well as focal length, changes with subject distance. When the subject distance is large in comparison with the lens focal length, the exact location of the front nodal plane is not critical; the distance is essentially the same whether measured from the front of the lens, the image plane, or the actual nodal plane. The same is not true for close-up photography; at unity magnification, a slight error in the location of the front nodal plane can result in a DOF error greater than the errors from any approximations in the DOF equations.

The asymmetrical lens formulas require knowledge of the pupil magnification, which usually is not specified for medium- and small-format lenses. The pupil magnification can be estimated by looking into the front and rear of the lens and measuring the diameters of the apparent apertures, and computing the ratio of rear diameter to front diameter (Shipman 1977, 144). However, for many zoom lenses and internal-focusing non-zoom lenses, the pupil magnification changes with subject distance, and several measurements may be required.

Most DOF formulas, including those discussed in this article, employ several simplifications:

The lens designer cannot restrict analysis to Gaussian optics and cannot ignore lens aberrations. However, the requirements of practical photography are less demanding than those of lens design, and despite the simplifications employed in development of most DOF formulas, these formulas have proven useful in determining camera settings that result in acceptably sharp pictures. It should be recognized that DOF limits are not hard boundaries between sharp and unsharp, and that there is little point in determining DOF limits to a precision of many significant figures.

A symmetrical lens is illustrated at right. The subject, at distance formula_7, is in focus at image distance formula_41. Point objects 
at distances formula_60 and formula_61 would be in focus at image distances formula_62 and formula_63, respectively; at image distance formula_41, they are imaged as blur spots. The depth of field is controlled by the aperture stop diameter formula_53; when the blur spot diameter is equal to the acceptable circle of confusion formula_4, the near and far limits of DOF are at formula_61 and formula_60. From similar triangles,

and

It usually is more convenient to work with the lens f-number than the aperture diameter; the f-number formula_3 is related to the lens focal length formula_2 and the aperture diameter formula_53 by

The image distance formula_41 is related to an object distance formula_7 by the thin lens equation

Solve the equations set (1) to (6) and obtain the exact solutions without any simplification

and

Solving equation (8) for the focus distance formula_7 and setting the far limit of DOF formula_10 to infinity gives

where formula_5 is the hyperfocal distance. Setting the subject distance to the hyperfocal distance and solving for the near limit of DOF gives

Substituting the expression for hyperfocal distance into equations (7) and (8) for the near and far limits of DOF gives

For any practical value of formula_5, the focal length is negligible in comparison, so that

Substituting the approximate expression for hyperfocal distance into the formulas for the near and far limits of DOF gives

and

However, if one states by definition that formula_81, then coming

and

Combining, the depth of field formula_13 is

Magnification formula_21 can be expressed as

at the hyperfocal distance, the magnification formula_88 then is

Substituting formula_90 for formula_5 and simplifying gives

It is sometimes convenient to express DOF in terms of magnification formula_21. Substituting

and

into the formula for DOF and rearranging gives

after Larmore (1965), 163).

Multiplying the numerator and denominator of the exact formula above by

gives

If the "f"-number and circle of confusion are constant, decreasing the focal length formula_2 increases the second term in the denominator, decreasing the denominator and increasing the value of the right-hand side, so that a shorter focal length gives greater DOF.

The term in parentheses in the denominator is the hyperfocal magnification formula_88, so that

As subject distance is decreased, the subject magnification increases, and eventually becomes large in comparison with the hyperfocal magnification. Thus the effect of focal length is greatest near the hyperfocal distance, and decreases as subject distance is decreased. However, the near/far perspective will differ for different focal lengths, so the difference in DOF may not be readily apparent.

When formula_23, formula_103, and

so that for a given magnification, DOF is essentially independent of focal length. Stated otherwise, for the same subject magnification and the same "f"-number, all focal lengths for a given image format give approximately the same DOF. This statement is true only when the subject distance is small in comparison with the hyperfocal distance, however.

When the subject distance is large in comparison with the lens focal length,

and

so that

For formula_19, the far limit of DOF is at infinity and the DOF is infinite; of course, only objects at or beyond the near limit of DOF will be recorded with acceptable sharpness.

When the subject distance formula_7 approaches the lens focal length, the focal length no longer is negligible, and the approximate formulas (11),(12) above cannot be used without introducing significant error. Use formulas (9) and (10) instead.

It usually is more convenient to express DOF in terms of magnification. The distance is small in comparison with the hyperfocal distance, so the simplified formula

can be used with good accuracy (formula_110is the working f-number). For a given magnification, DOF is independent of focal length.

From the "exact" equations for near and far limits of DOF, the DOF in front of the subject is

and the DOF beyond the subject is

The near:far DOF ratio is

This ratio is always less than unity; at moderate-to-large subject distances, formula_114, and

When the subject is at the hyperfocal distance or beyond, the far DOF is infinite, and the near:far ratio is zero. It's commonly stated that approximately 1/3 of the DOF is in front of the subject and approximately 2/3 is beyond; however, this is true only when formula_116.

At closer subject distances, it's often more convenient to express the DOF ratio in terms of the magnification

substitution into the "exact" equation for DOF ratio gives

As magnification increases, the near:far ratio approaches a limiting value of unity.

When the subject distance is much less than hyperfocal, the total DOF is given to good approximation by

When additionally the magnification is small compared to unity, the value of formula_21 in the numerator can be neglected, and the formula further simplifies to

The DOF ratio for two different formats is then

Essentially the same approach is described in Stroebel (1976), 136–39).

The results of the comparison depend on what is assumed. One approach is to assume that essentially the same picture is taken with each format and enlarged to produce the same size final image, so the subject distance remains the same, the focal length is adjusted to maintain the same angle of view, and to a first approximation, magnification is in direct proportion to some characteristic dimension of each format. If both pictures are enlarged to give the same size final images with the same sharpness criteria, the circle of confusion is also in direct proportion to the format size. Thus if formula_123 is the characteristic dimension of the format,

With the same "f"-number, the DOF ratio is then

so the DOF ratio is in inverse proportion to the format size. This ratio is approximate, and breaks down in the macro range of the larger format (the value of formula_21 in the numerator is no longer negligible) or as distance approaches the hyperfocal distance for the smaller format (the DOF of the smaller format approaches infinity).

If the formats have approximately the same aspect ratios, the characteristic dimensions can be the format diagonals; if the aspect ratios differ considerably (e.g., 4×5 vs. 6×17), the dimensions must be chosen more carefully, and the DOF comparison may not even be meaningful.

If the DOF is to be the same for both formats the required "f"-number is in direct proportion to the format size:

Adjusting the "f"-number in proportion to format size is equivalent to using the same absolute aperture diameter for both formats, discussed in detail below in Use of absolute aperture diameter.

If the same lens focal length is used in both formats, magnifications can be maintained in the ratio of the format sizes by adjusting subject distances; the DOF ratio is the same as that given above, but the images differ because of the different perspectives and angles of view.

If the same DOF is required for each format, an analysis similar to that above shows that the required "f"-number is in direct proportion to the format size.

Another approach is to use the same focal length with both formats at the same subject distance, so the magnification is the same, and with the same "f"-number,

so the DOF ratio is in "direct" proportion to the format size, due to the smaller format size having a smaller circle of confusion when the final image size is the same. The perspective is the same for both formats, but because of the different angles of view, the pictures are not the same.

Cropping an image and enlarging to the same size final image as an uncropped image taken under the same conditions is equivalent to using a smaller format; the cropped image requires greater enlargement and consequently has a smaller circle of confusion. A cropped then enlarged image has less DOF than the uncropped image.

The aperture diameter is normally given in terms of the "f"-number because all lenses set to the same "f"-number give approximately the same image illuminance (Ray 2002, 130), simplifying exposure settings. In deriving the basic DOF equations, formula_129 can be substituted for the absolute aperture diameter formula_53, giving the DOF in terms of the absolute aperture diameter:

after Larmore (1965), 163). When the subject distance formula_7 is small in comparison with the hyperfocal distance, the second term in the denominator can be neglected, leading to

With the same subject distance and angle of view for both formats, formula_134, and

so the DOFs are in inverse proportion to the absolute aperture diameters. When the diameters are the same, the two formats have the same DOF. Von Rohr (1906) made this same observation, saying "At this point it will be sufficient to note that all these formulae involve quantities relating exclusively to the entrance-pupil and its position with respect to the object-point, whereas the focal length of the transforming system does not enter into them." Lyon's Depth of Field Outside the Box describes an approach very similar to that of von Rohr.

Using the same absolute aperture diameter for both formats with the "same picture" criterion is equivalent to adjusting the "f"-number in proportion to the format sizes, discussed above under "Same picture" for both formats

The equations for
the DOF limits can be combined to eliminate formula_136 and solve for the subject distance. For given near and far DOF limits formula_9 and formula_10, the subject distance is

the harmonic mean of the near and far distances. The equations for DOF limits also can be combined to eliminate
formula_7 and solve for the required f-number, giving

When the subject distance is large in comparison with the lens focal
length, this simplifies to

When the far limit of DOF is at infinity, the equations for formula_7 and formula_3 give indeterminate results. But if all terms in the numerator and denominator on the right-hand side of the equation for formula_7 are divided by formula_146, it is seen that when formula_146 is at infinity,

Similarly, if all terms in the numerator and denominator on the right-hand side of the equation for formula_3 are divided by formula_146, it is seen that when formula_146 is at infinity,

Most discussions of DOF concentrate on the object side of the lens,
but the formulas are simpler and the measurements usually easier to make on the
image side. If the basic image-side equations

and

are combined and solved for the image distance formula_41, the result is

the harmonic mean of the near and far image distances. The basic image-side equations can also be combined and solved for formula_3, giving

The image distances are measured from the camera's image plane to the lens's image nodal plane, which is not always easy to locate. The harmonic mean is always less than the arithmentic mean, but when the difference between the near and far image distances is reasonably small, the two means are close to equal, and focus can be set with sufficient accuracy using

This formula requires only the "difference"
formula_160 between the near and far image distances.
View camera users often refer to this difference as the "focus spread";
it usually is measured on the bed or focusing rail.
Focus is simply set to halfway between the near and far image distances.

Substituting formula_161 into the equation for formula_3 and rearranging gives

One variant of the thin-lens equation is formula_164, where formula_21 is the magnification; substituting this into the equation for formula_3 gives

At moderate-to-large subject distances, formula_21 is small compared to unity, and the
f-number can often be determined with sufficient accuracy using

For close-up photography, the magnification cannot be ignored, and the f-number should be determined using the first approximate formula.

As with the approximate formula for formula_41, the approximate formulas for formula_3 require only the focus spread formula_44 rather than the absolute image distances.

When the far limit of DOF is at infinity, formula_173.

On manual-focus small- and medium-format lenses, the focus and f-number usually are determined using the lens DOF scales, which often are based on the approximate equations above.

If the equation for the far limit of DOF is solved for formula_4, and the far distance replaced by an arbitrary distance formula_46, the blur disk diameter formula_48 at that distance is

When the background is at the far limit of DOF, the blur disk diameter is equal to the circle of confusion formula_4, and the blur is just imperceptible. The diameter of the background blur disk increases with the distance to the background. A similar relationship holds for the foreground; the general expression for a defocused object at distance formula_46 is

For a given scene, the distance between the subject and a foreground or background object is usually
fixed; let that distance be represented by

then

or, in terms of subject distance,

with the minus sign used for foreground objects and the plus sign used for background objects. For a relatively distant background object,

In terms of subject magnification, the subject distance is

so that, for a given f-number and subject magnification,

Differentiating formula_48 with respect to formula_2 gives

With the plus sign, the derivative is everywhere positive, so that for a background object, the blur disk size increases with focal length. With the minus sign, the derivative is everywhere negative, so that for a foreground object, the blur disk size decreases with focal length.

The magnification of the defocused object also varies with focal length; the magnification of the
defocused object is

where formula_191 is the image distance of the subject. For a defocused object with some characteristic dimension formula_192, the imaged size of that object is

The ratio of the blur disk size to the imaged size of that object then is

so for a given defocused object, the ratio of the blur disk diameter to object size is independent of focal length, and depends only on the object size and its distance from the subject.

This discussion thus far has assumed a symmetrical lens for which the entrance and exit pupils coincide with the object and image nodal planes, and for which the pupil magnification is unity. Although this assumption usually is reasonable for large-format lenses, it often is invalid for medium- and small-format lenses.

For an asymmetrical lens, the DOF ahead of the subject distance and the DOF beyond the subject distance are given by

and

where formula_25 is the pupil magnification.

Combining gives the total DOF:

When formula_23, the second term in the denominator becomes small in comparison with the first, and (Shipman 1977, 147)

When the pupil magnification is unity, the equations for asymmetrical lenses reduce to those given earlier for symmetrical lenses.

Except for close-up and macro photography, the effect of lens asymmetry is minimal. A slight rearrangement of the last equation gives

As magnification decreases, the formula_202 term becomes smaller in comparison with the formula_203 term, and eventually the effect of pupil magnification becomes negligible.





</doc>
<doc id="8368" url="https://en.wikipedia.org/wiki?curid=8368" title="Dumnonii">
Dumnonii

The Dumnonii or Dumnones were a British tribe who inhabited Dumnonia, the area now known as Devon and Cornwall (and some areas of present-day Dorset and Somerset) in the further parts of the South West peninsula of Britain, from at least the Iron Age up to the early Saxon period. They were bordered to the east by the Durotriges tribe.

William Camden, in his 1607 edition of "Britannia", describes Cornwall and Devon as being two parts of the same 'country' which:
Camden had learnt some Welsh during the course of his studies and it would appear that he is the origin of the interpretation of Dumnonii as "deep valley dwellers" from his understanding of the Welsh of his time. John Rhys later theorized that the tribal name was derived from the name of a goddess, "Domnu", probably meaning "the goddess of the deep". The proto-Celtic root *dubno- or *dumno- meaning "the deep" or "the earth" (or alternatively meaning "dark" or "gloomy") appears in personal names such as Dumnorix and Dubnovellaunus. Another group with a similar name but with no known links were the Fir Domnann of Connacht.

The Roman name of the town of Exeter, "Isca Dumnoniorum" ("Isca of the Dumnonii"), contains the root "*iska-" "water" for "Water of the Dumnonii". The Latin name suggests that the city was already an "oppidum", or walled town, on the banks on the River Exe before the foundation of the Roman city, in about AD 50. The Dumnonii gave their name to the English county of Devon, and their name is represented in Britain's two extant Brythonic languages as "Dewnans" in Cornish and "Dyfnaint" in Welsh. Amédée Thierry ("Histoire des Gaulois", 1828), one of the inventors of the "historic race" of Gauls, could confidently equate them with the Cornish ("les Cornouailles").

Victorian historians often referred to the tribe as the Damnonii, which is also the name of another people from lowland Scotland, although there are no known links between the two populations.

The people of Dumnonia spoke a Southwestern Brythonic dialect similar to the forerunner of more recent Cornish and Breton. Irish immigrants, the Déisi, are evidenced by the Ogham-inscribed stones they have left behind, confirmed and supplemented by toponymical studies. The stones are sometimes inscribed in Latin, sometimes in both scripts. Tristram Risdon suggested the continuance of a Brythonic dialect in the South Hams, Devon, as late as the 14th century, in addition to its use in Cornwall.

Ptolemy's 2nd century "Geography" places the Dumnonii to the west of the Durotriges. The name "purocoronavium" that appears in the Ravenna Cosmography implies the existence of a sub-tribe called the Cornavii or Cornovii, perhaps the ancestors of the Cornish people.

In the sub-Roman period a Brythonic kingdom called Dumnonia emerged, covering the entire peninsula, although it is believed by some to have effectively been a collection of sub-kingdoms.

A kingdom of Domnonée (and of Cornouaille alongside) was established in the province of Armorica directly across the English Channel, and has apparent links with the British population, suggesting an ancient connection of peoples along the western Atlantic seaboard.

The Latin name for Exeter is Isca Dumnoniorum ("Water of the Dumnonii"). This oppidum (a Latin term meaning an important town) on the banks of I River Exe certainly existed prior to the foundation of the Roman city in about AD 50. "Isca" is derived from the Brythonic word for flowing water, which was given to the River Exe. This is reflected in the Welsh name for Exeter: "Caerwysg" meaning "fortified settlement on the river Uisc".

Isca Dumnoniorum originated with a settlement that developed around the Roman fortress of the Legio II Augusta and is one of the four "poleis" (cities) attributed to the tribe by Ptolemy. It is also listed in two routes of the late 2nd century Antonine Itinerary.

A legionary bath-house was built inside the fortress sometime between 55 and 60 and underwent renovation shortly afterwards (c. 60-65) but by c. 68 (perhaps even 66) the legion had transferred to a newer fortress at Gloucester. This saw the dismantling of the Isca fortress, and the site was then abandoned. Around AD 75, work on the "civitas forum" and "basilica" had commenced on the site of the former "principia" and by the late 2nd century the "civitas" walls had been completed. They were 3 metres thick and 6 metres high and enclosed exactly the same area as the earlier fortress. However, by the late 4th century the "civitas" was in decline.

As well as Isca Dumnoniorum, Ptolemy's 2nd century "Geography" names three other towns:

The Ravenna Cosmography includes the last two names (in slightly different forms, as "Tamaris" and "Uxelis"), and adds several more names which may be settlements in the territory. These include:

Other Romano-British sites in Dumnonia include:

New settlements continued to be built throughout the Roman period, including sites at Chysauster and Trevelgue Head. The style is native in form with no Romanised features. Near Padstow, a Roman site of some importance now lies buried under the sands on the opposite side of the Camel estuary near St. Enodoc's Church, and may have been a western coastal equivalent of a Saxon Shore Fort. At Magor Farm in Illogan, near Camborne, an archaeological site has been identified as being a villa.

The Dumnonii are thought to have occupied relatively isolated territory in Cornwall, Devon, Somerset and possibly part of Dorset. Their cultural connections, as expressed in their ceramics, were with the peninsula of Armorica across the Channel, rather than with the southeast of Britain. They do not seem to have been politically centralised: coins are relatively rare, none of them locally minted, and the structure, distribution and construction of Bronze Age and Iron Age hill forts, "rounds" and defensible farmsteads in the south west point to a number of smaller tribal groups living alongside each other.

Dumnonia is noteworthy for its many settlements that have survived from the Romano-British period, but also for its lack of a villa system. Local archaeology has revealed instead the isolated enclosed farmsteads known locally as "rounds". These seem to have survived the Roman abandonment of Britain, but were subsequently replaced, in the 6th and 7th centuries, by the unenclosed farms taking the Brythonic toponymic "tre-".

As in most other Brythonic areas, Iron Age hill forts, such as Hembury Castle, were refortified for the use of chieftains or kings. Other high-status settlements such as Tintagel seem to have been reconstructed during this period. Post-Roman imported pottery has been excavated from many sites across the region, and the apparent surge in late 5th century Mediterranean and/or Byzantine imports is yet to be explained satisfactorily.

Apart from fishing and agriculture, the main economic resource of the Dumnonii was tin mining. The area of Dumnonia had been mined since ancient times, and the tin was exported from the ancient trading port of Ictis (St Michael's Mount). Tin extraction (mainly by streaming) had existed here from the early Bronze Age around the 22nd century BC. West Cornwall, around Mount's Bay, was traditionally thought to have been visited by metal traders from the eastern Mediterranean

During the first millennium BC trade became more organised, first with the Phoenicians, who settled Gades (Cadiz) around 1100 BC, and later with the Greeks, who had settled Massilia (Marseilles) and Narbo (Narbonne) around 600 BC. Smelted Cornish tin was collected at Ictis whence it was conveyed across the Bay of Biscay to the mouth of the Loire and then to Gades via the Loire and Rhone valleys. It went then through the Mediterranean Sea in ships to Gades.

During the period c. 500-450 BC, the tin deposits seem to have become more important, and fortified settlements appear such as at Chun Castle and Kenidjack Castle, to protect both the tin smelters and mines.

The earliest account of Cornish tin mining was written by Pytheas of Massilia late in the 4th century BC after his circumnavigation of the British Isles. Underground mining was described in this account, although it cannot be determined when it had started. Pytheas's account was noted later by other writers including Pliny the Elder and Diodorus Siculus.

It is likely that tin trade with the Mediterranean was later on under the control of the Veneti. Britain was one of the places proposed for the "Cassiterides", that is Tin Islands. Tin working continued throughout Roman occupation although it appears that output declined because of new supplies brought in from the deposits discovered in Iberia (Spain and Portugal). However, when these supplies diminished, production in Dumnonia increased and appears to have reached a peak during the 3rd century AD.

The Sub-Roman or Post-Roman history of Dumnonia comes from a variety of sources and is considered exceedingly difficult to interpret given that historical fact, legend and confused pseudo-history are compounded by a variety of sources in Middle Welsh and Latin. The main sources available for discussion of this period include Gildas's "De Excidio Britanniae" and Nennius's "Historia Brittonum", the "Annales Cambriae", "Anglo-Saxon Chronicle", William of Malmesbury's "Gesta Regum Anglorum" and "De Antiquitate Glastoniensis Ecclesiae", along with texts from the "Black Book of Carmarthen" and the "Red Book of Hergest", and Bede's "Historia ecclesiastica gentis Anglorum" as well as "The Descent of the Men of the North" ("Bonedd Gwŷr y Gogledd", in Peniarth MS 45 and elsewhere) and the "Book of Baglan".






</doc>
<doc id="8372" url="https://en.wikipedia.org/wiki?curid=8372" title="Declaration of independence">
Declaration of independence

A declaration of independence or declaration of statehood is an assertion by a defined territory that it is independent and constitutes a state. Such places are usually declared from part or all of the of another nation or failed nation, or are breakaway territories from within the larger state. In 2010, the UN's International Court of Justice ruled in an advisory opinion in Kosovo that "International law contains no prohibition on declarations of independence", though the state from which the territory wishes to secede may regard the declaration as rebellion, which may lead to a war of independence or a constitutional settlement to resolve the crisis.



</doc>
<doc id="8373" url="https://en.wikipedia.org/wiki?curid=8373" title="Drag racing">
Drag racing

Drag racing is a type of motor racing in which automobiles or motorcycles (usually specially prepared for the purpose) compete, usually two at a time, to be first to cross a set finish line. The race follows a short, straight course from a standing start over a measured distance, most commonly ¼ mile (), with a shorter () becoming increasingly popular, as it has become the standard for Top Fuel dragsters and funny cars, where some major bracket races and other sanctioning bodies have adopted it as the standard, while the ⅛ mi ( is also popular in some circles. Electronic timing and speed sensing systems have been used to record race results since the 1960s.

The history of automobiles and motorcycles being used for drag racing is nearly as long as the history of motorized vehicles themselves, and has taken the form of both illegal street racing, and as an organized and regulated motorsport. This article covers the legal sport.

Before each race (commonly known as a pass), each driver is allowed to perform a burnout, which heats the driving tires and lays rubber down at the beginning of the track, improving traction. Each driver then lines up (or stages) at the starting line.

Modern professional races are started electronically by a system known as a "Christmas tree", which consists of a column of lights for each driver/lane, and two light beam sensors per lane on the track at the starting line. Current National Hot Rod Association (NHRA) trees, for example, feature one blue light (split into halves), then three amber, one green, and one red. When the first light beam is broken by a vehicle's front tire(s), the vehicle is "pre-staged" (approximately from the starting line), and the pre-stage indicator on the tree is lit. When the second light beam is broken, the vehicle is "staged", and the stage indicator on the tree is lit. Vehicles may then leave the pre-stage beam, but must remain in the stage beam until the race starts.

Once one competitor is staged, their opponent has a set amount of time to stage or they will be instantly disqualified, indicated by a red light on the tree. Otherwise, once both drivers are staged, the system chooses a short delay at random (to prevent a driver being able to anticipate the start), then starts the race. The light sequence at this point varies slightly. For example, in NHRA Professional classes, three amber lights on the tree flash simultaneously, followed 0.4 seconds later by a green light (this is also known as a "pro tree"). In NHRA Sportsman classes, the amber lights illuminate in sequence from top to bottom, 0.5 seconds apart, followed 0.5 seconds later by the green light (this is also known as a "sportsman tree" or "full tree"). If a vehicle leaves the starting line before the green light illuminates, the red light for that lane illuminates instead, and the driver is disqualified (also known as "red lighting"). In a handicap start, the green light automatically lights up for the first driver, and the red light is only lit in the proper lane after both cars have launched if one driver leaves early, or if both drivers left early, the driver whose reaction time is worse (if one lane has a -.015 and the other lane has a -.022, the lane of the driver who committed a 0.022 is given the red light after both cars have left)., as a red light infraction is only assessed to the driver with the worse infraction, if both drivers leave early. Even if both drivers leave early, the green light is automatically lit for the driver that left last, and they still may win the pass (as in the 2014 NHRA Auto Club Pro Stock final, Erica Enders-Stevens and Jason Line both committed red light infractions; only Line was assessed with a red light, as he was -.011 versus Enders-Stevens' -.002).

Several measurements are taken for each race: reaction time, elapsed time, and speed. Reaction time is the period from the green light illuminating to the vehicle leaving the starting line. Elapsed time is the period from the vehicle leaving the starting line to crossing the finish line. Speed is measured through a speed trap covering the final to the finish line, indicating average speed of the vehicle in that distance.

Except where a breakout rule is in place, the winner is the first vehicle to cross the finish line, and therefore the driver with the lowest combined reaction time and elapsed time. Because these times are measured separately, a driver with a slower elapsed time can actually win if that driver's advantage in reaction time exceeds the elapsed time difference. In heads-up racing, this is known as a "holeshot win". In categories where a breakout rule is in effect (for example, NHRA Junior Dragster, Super Comp, Super Gas, Super Stock, and Stock classes, as well as some dial-in classes), if a competitor is faster than his or her predetermined time (a "breakout"), that competitor loses. If both competitors are faster than their predetermined times, the competitor who breaks out by less time wins. Regardless, a red light foul is worse than a breakout, except in Junior Dragster where exceeding the absolute limit is a cause for disqualification.

Most race events use a traditional bracket system, where the losing car and driver are eliminated from the event while the winner advances to the next round, until a champion is crowned. Events can range from 16 to over 100 car brackets. Drivers are typically seeded by elapsed times in qualifying. In bracket racing without a breakout (such as NHRA Competition Eliminator), pairings are based on times compared to their index (faster than index for class is better). In bracket racing with a breakout (Stock, Super Stock, but also the NHRA's Super classes), the closest to the index is favourable.

A popular alternative to the standard eliminations format is the Chicago Style format (also called the Three Round format in Australia), named for the US 30 Dragstrip in suburban Gary, Indiana where a midweek meet featured this format. All entered cars participate in one qualifying round, and then are paired for the elimination round. The two fastest times among winners from this round participate in the championship round. Depending on the organisation, the next two fastest times may play for third, then fifth, and so forth, in consolation rounds. Currently, an IHRA 400 Thunder championship race in Australia uses the format.

The standard distance of a drag race is 1,320 feet, 402 m, or 1/4 mile. However, due to safety concerns, certain sanctioning bodies (notably the NHRA for its Top Fuel and Funny Car classes) have shortened races to 1,000 feet. Some drag strips are even shorter and run 660 feet, 201 m, or 1/8 mile. The 1,000 foot distance is now also popular with bracket racing, especially in meets where there are 1/8 mile cars and 1/4 mile cars racing together, and is used by the revived American Drag Racing League for its primary classes (not Jr Dragster). Some organisations that deal with Pro Modified and "Mountain Motor" Pro Stock cars (Professional Drag Racers Association) use the 1/8 mile distance, even if the tracks are 1/4 mile tracks.

The National Hot Rod Association (NHRA) oversees the majority of drag racing events in North America. The next largest organization is the International Hot Rod Association (IHRA). Nearly all drag strips are associated with one sanctioning body or the other.

Besides NHRA and IHRA, there are niche organizations for muscle cars and nostalgia vehicles. The Nostalgia Drag Racing League (NDRL) based in Brownsburg, IN, runs a series of 1/4 mile (402m) drag races in the Midwest for 1979 and older nostalgic appearing cars, with four classes of competition running in an index system. Pro 7.0 and Pro 7.50 run heads up 200 mile per hour (320 kilometre per hour) passes, while Pro Comp and Pro Gas run 8.0 to 10.0 indices. NDRL competition vehicles typically include Front Engine Dragsters, Altereds, Funny Cars, early Pro Stock clones, Super Stocks and Gassers.

The National Electric Drag Racing Association (NEDRA) races electric vehicles against high performance gasoline-powered vehicles such as Dodge Vipers or classic muscle cars in 1/4 and 1/8 mile (402m & 201m) races. The current electric drag racing record is 6.940 seconds at 201.37 mph (324.0736 kph) for a quarter mile (420m). Another niche organization is the VWDRC which run a VW-only championship with vehicles running under 7 seconds.

Prior to the founding of the NHRA and IHRA, smaller organizations sanctioned drag racing in the early years, which included the competing AHRA in the United States from 1955 to 2005.

The first Australian Nationals event was run in 1965 at Riverside raceway, near Melbourne. The Australian National Drag Racing Association (ANDRA) was established in 1973, and today they claim they are the "best in the world outside the United States". ANDRA sanctions races throughout Australia and throughout the year at all levels, from Junior Dragster to Top Fuel.

The ANDRA Pro Series is for professional drivers and riders and includes Top Fuel, Top Alcohol, Top Doorslammer (similar to the USA Pro Modified class), Pro Stock (using 400 cubic inch engines (6.5 litres)), Top Bike and Pro Stock Motorcycle.

The Rocket Allstars Racing Series is for ANDRA sportsman drivers and riders and includes Competition, Super Stock, Super Compact, Competition Bike, Supercharged Outlaws, Modified, Super Sedan, Modified Bike, Super Street and Junior Dragster.

Broadcasting is provided on SBS Speedweek.

In 2015, after a dispute with ANDRA, Sydney Dragway, Willowbank Raceway and the Perth Motorplex invited the International Hot Rod Association (IHRA) to sanction events at their tracks. Since then the Perth Motorplex has reverted back to an ANDRA sanction and Springmount Raceway has embraced the IHRA umbrella. The 400 Thunder Series now attracts professional racers to its races at Sydney Dragway and Willowbank Raceway and is the premiere series in Australia.

Communications Provider OVO Mobile provides a live stream of all 400 Thunder Australian Professional Drag Racing Series events to fans globally.
The 400 Thunder Series is aired on Fox Sports with each professional bracket having its own half hour program from each 400 Thunder Series event.

Drag racing was imported to Europe by American NATO troops during the Cold War. Races were held in West Germany beginning in the 1960s at the airbases at Ramstein and Sembach and in the UK at various airstrips and racing circuits before the opening of Europe's first permanent drag strip at Santa Pod Raceway in 1966.

The FIA organises a Europe-wide four wheeled championship for the Top Fuel, Top Methanol Dragster, Top Methanol Funny Car, Pro Modified and Pro Stock classes. FIM Europe organises a similar championship for bike classes. In addition, championships are run for sportsman classes in many countries throughout Europe by the various national motorsport governing bodies.

Drag racing in New Zealand started in the 1960s. The New Zealand Hot Rod Association (NZHRA) sanctioned what is believed to have been the first drag meeting at an open cut coal mine at Kopuku, south of Auckland, sometime in 1966. In 1973, the first and only purpose built drag strip opened in Meremere by the Pukekohe Hot Rod Club. In April 1993 the governance of drag racing was separated from the NZHRA and the New Zealand Drag Racing Association (NZDRA) was formed. In 2014, New Zealand's second purpose built drag strip - Masterton Motorplex - opened.

The first New Zealand Drag Racing Nationals was held in the 1966/67 season at Kopuku, near Auckland.

There are now two governing bodies operating drag racing in New Zealand with the Florida-based International Hot Rod Association sanctioning both of New Zealands major tracks at Ruapuna (Pegasus Bay Drag Racing Association) on the South Island and Meremere Dragway Inc in the North Island. However, the official ASN of the sport, per FIA regulations, is the New Zealand Drag Racing Association.

A lot of countries in South America race 200 meters, unlike the United States and places like Australia, which race 400 meters or 1/4 mile.

Organized drag racing in Colombia is Club G3's responsibility, which is a private organization. The events take place at Autódromo de Tocancipá.

Curaçao

On the island of Curaçao, organization of drag racing events is handled by the Curaçao Autosport Foundation (FAC)
All racing events, including street legal competitions, happen at the Curaçao International Raceway.

'Aruba'

On the island of Aruba all racing events, including street legal competitions, happen at Palomarga international raceway.

Barbados

On the island of Barbados, organization of drag racing events is done by the Barbados Association of Dragsters and Drifters. Currently the drag racing is done at Bushy Park racing circuit over 1/8 mile, while "acceleration tests" of 1/4 mile are done at the Paragon military base.

Saint Lucia

On the Island of Saint Lucia, organization of drag racing events is done by Time Line Events, currently races are held at the US Old military base also known as the "Ca Ca Beff", "The Base" near the Hewanorra International Airport in Vieux Fort.

Dominican Republic

On Santo Domingo, organization of drag racing events is done by Autodromo Sunix and they happen at the Autodromo Sunix, close to the Airport SDQ.

Organized drag racing is rapidly growing in India. "Autocar India" organised the country's first drag race meet in Mumbai in 2002.

Drag racing is also gaining popularity in Pakistan, with private organizations organizing such events. The Bahria Town housing project recently organized a drag racing event in Rawalpindi, with the help of some of the country's best drivers.

Sri Lanka has seen an immense growth in Drag racing through legal meets held by the Ceylon Motor Sports Club, an FiA sanctioned body. In recent years, exotic cars and Japanese power houses have been taking part in these popular events.

Drag racing is an established sport in South Africa, with a number of strips around the country including Tarlton International Raceway and ODI Raceway. Drag racing is controlled by Motorsport South Africa and all drivers are required to hold a valid Motorsport South Africa license. Drivers can compete in a number of categories including Top Eliminator, Senior Eliminator, Super Competition Eliminator, Competition Eliminator, Pro Street Bikes, Superbike Eliminator, Supersport Shootout (motorcycle), Street Modified, and Factory Stock.

There are hundreds of classes in drag racing, each with different requirements and restrictions on things such as weight, engine size, body style, modifications, and many others. NHRA and IHRA share some of these classes, but many are solely used by one sanctioning body or the other. The NHRA boasts over 200 classes, while the IHRA has fewer. Some IHRA classes have multiple sub-classes in them to differentiate by engine components and other features. There is even a class for aspiring youngsters, Junior Dragster, which typically uses an eighth-mile track, also favored by VW racers.

In 1997, the FIA (cars) and UEM (bikes) began sanctioning drag racing in Europe with a fully established European Drag Racing Championship, in cooperation (and rules compliance) with NHRA. The major European drag strips include Santa Pod Raceway in Podington, England; Alastaro Circuit, Finland; Mantorp Park, Sweden; Gardermoen Raceway, Norway and the Hockenheimring in Germany. 

There is a somewhat arbitrary definition of what constitutes a "professional" class. The NHRA includes 5 pro classes; Top Fuel, Funny Car, Pro Stock, Pro Modified and Pro Stock Motorcycle. The FIA features a different set of 5 pro classes; Top Fuel, Top Methanol Dragster, Top Methanol Funny Car, Pro Modified and Pro Stock. Other sanctioning bodies have similarly different definitions. A partial list of classes includes:
A complete listing of all classes can be found on the respective NHRA and IHRA official websites.

The UEM also has a different structure of professional categories with Top Fuel Bike, Super Twin Top Fuel Bike, and Pro Stock Bike contested, leaving the entire European series with a total of 8 professional categories.

To allow different cars to compete against each other, some competitions are raced on a handicap basis, with faster cars delayed on the starting line enough to theoretically even things up with the slower car. This may be based on rule differences between the cars in stock, super stock, and modified classes, or on a competitor's chosen "dial-in" in bracket racing.

For a list of drag racing world records in each class, see Dragstrip#Quarter mile times.

A "dial-in" is a time the driver estimates it will take his or her car to cross the finish line, and is generally displayed on one or more windows so the starter can adjust the starting lights on the tree accordingly. The slower car will then get a head start equal to the difference in the two dial-ins, so if both cars perform perfectly, they would cross the finish line dead even. If either car goes faster than its dial-in (called breaking out), it is disqualified regardless of who has the lower elapsed time; if both cars break out, the one who breaks out by the smallest amount wins. However, if a driver had jump-started (red light) or crossed a boundary line, both violations override any break out (except in some classes with an absolute break out rule such as Junior classes). This eliminates any advantage from putting a slower time on the windshield to get a head start. 

The effect of the bracket racing rules is to place a premium on consistency of performance of the driver and car rather than on raw speed, in that victory goes to the driver able to precisely predict elapsed time, whether it is fast or slow. This in turn makes victory much less dependent on large infusions of money, and more dependent on skill. Therefore, bracket racing is popular with casual weekend racers. Many of these recreational racers will drive their vehicles to the track, race them, and then simply drive them home. As most tracks host only one NHRA national event, and two or three regional events (smaller tours, car shows, "etc.") annually, on most weekends these tracks host local casual and weekend racers. Organizationally, however, the tracks are run according to the rules of either the NHRA or the IHRA with regional points and a championship on the line. Even street vehicles must pass a safety inspection prior to being allowed to race.

The National Hot Rod Association (NHRA) was founded in 1951, to take illegal racing off the street.

The organization banned the use of nitromethane in 1957, calling it unsafe, in part through the efforts of C. J. Hart; the ban would be lifted in 1963.







</doc>
<doc id="8375" url="https://en.wikipedia.org/wiki?curid=8375" title="Draugr">
Draugr

The draugr or draug (, plural ; modern , and Danish, Swedish, and ), also called , literally "again-walker" () is an undead creature from Norse mythology.

The word "draugr" can be traced to a Proto-Indo European stem "*" "phantom", from "*" "deceive".
The Old Norse meaning of the word is a revenant.

The will appears to be strong, strong enough to draw the "hugr" [animate will] back to one's body. These reanimated individuals were known as "draugar". However, though the dead might live again, they could also die again. "Draugar" die a "second death" as Chester Gould calls it, when their bodies decay, are burned, dismembered or otherwise destroyed.

Draugar live in their graves, often guarding treasure buried with them in their burial mound. They are animated corpses—unlike ghosts they have a corporeal body with similar physical abilities as in life. Older literature makes clear distinctions between sea-draugar and land-draugar.

Draugar possess superhuman strength, can increase their size at will, and carry the unmistakable stench of decay. "The appearance of a "draugr" was that of a dead body: swollen, blackened and generally hideous to look at." They are undead figures from Norse and Icelandic mythology that appear to retain some semblance of intelligence. They exist either to guard their treasure, wreak havoc on living beings, or torment those who had wronged them in life. The draugr's ability to increase its size also increased its weight, and the body of the draugr was described as being extremely heavy. Thorolf of Eyrbyggja saga was "uncorrupted, and with an ugly look about him... swollen to the size of an ox," and his body was so heavy that it could not be raised without levers. They are also noted for the ability to rise from the grave as wisps of smoke and "swim" through solid rock, which would be useful as a means of exiting their graves.

In folklore, draugar slay their victims through various methods including crushing them with their enlarged forms, devouring their flesh, devouring them whole in their enlarged forms, indirectly killing them by driving them mad, and by drinking their blood. Animals feeding near the grave of a draugr may be driven mad by the creature's influence. They may also die from being driven mad. Thorolf, for example, caused birds that flew over his bowl barrow to drop dead. Draugar are also noted as being able to drive living people insane.

The draugr's victims were not limited to trespassers in its home. The roaming undead decimated livestock by running the animals to death while either riding them or pursuing them in some hideous, half-flayed form. Shepherds, whose duties to their flocks left them out of doors at night time, were also particular targets for the hunger and hatred of the undead:
Draugar are noted for having numerous magical abilities (referred to as "trollskap") resembling those of living witches and wizards such as shape-shifting, controlling the weather and seeing into the future. Among the creatures that a draugr may turn into are a seal, a great flayed bull, a grey horse with a broken back but no ears or tail and a cat that would sit upon a sleeper's chest and grow steadily heavier until the victim suffocated. The draugr Þráinn (Thrain) shape-shifted into a "cat-like creature" ("kattakyn") in "Hrómundar saga Gripssonar":

Draugar have the ability to enter into the dreams of the living, "but it generally happens even so that they leave beside the living person some gift, by which, on awakening, the living person may be assured of the tangible nature of the visit." Draugar also have the ability to curse a victim, as shown in the Grettis saga, where Grettir is cursed to be unable to become any stronger. Draugar also brought disease to a village and could create temporary darkness in daylight hours. While the draugr certainly preferred to be active during the night, it did not appear to be vulnerable to sunlight like some other revenants. Draugr can also kill people with bad luck.

A draugr's presence may be shown by a great light that glowed from the mound like foxfire. This fire would form a barrier between the land of the living and the land of the dead. The draugr could also move magically through the earth, swimming through solid stone as does Killer-Hrapp:

Some draugar are immune to weapons, and only a hero has the strength and courage needed to stand up to so formidable an opponent. In legends the hero would often have to wrestle the draugr back to his grave, thereby defeating him, since weapons would do no good. A good example of this kind of fight is found in "Hrómundar saga Gripssonar". Although iron could injure a draugr, as is the case with many supernatural creatures, it would not be sufficient to stop it. Sometimes the hero is required to dispose of the body in unconventional ways. The preferred method is to cut off the draugr's head, burn the body, and dump the ashes in the sea; the emphasis being on making absolutely sure the draugr was dead and gone.

The draugar were said to be either "hel-blár" ("death-blue") or, conversely, "nár-fölr" ("corpse-pale"). The "death-blue" color was not actually grey but was a dark blue or maroon hue that covered the entire body. Glámr, the undead shepherd of "Grettis saga", was reported to be dark blue in color and in Laxdæla saga, the bones of a dead sorceress who had appeared in dreams were dug up and found to be "blue and evil looking."

The resting place of the draugr was a tomb that served much as a workable home for the creature. Draugar are able to leave this dwelling place and visit the living during the night. Such visits are supposed to be universally horrible events that often end in death for one or more of the living, which would then warrant the exhumation of the draugr by a hero.

The motivation of the actions of a draugr was primarily jealousy and greed. The greed of a draugr causes it to viciously attack any would-be grave robbers, but the draugr also expresses an innate jealousy of the living, stemming from a longing for the things of the life it once had. This idea is clearly expressed in "Friðþjófs saga", where a dying king declared:

This desire for the friendship experienced in life is one example of the manifestation of this aspect of the draugr. Draugar also exhibit an immense and nearly insatiable appetite, as shown in the encounter of Aran and Asmund, sword brothers who made an oath that if one should die, the other would sit vigil with him for three days inside the burial mound. When Aran died, Asmund brought his own possessions into the barrow: banners, armor, hawk, hound, and horse. Then Asmund set himself to wait the agreed upon three days:

After a person’s death, the main indication that the person will become a draugr is that the corpse is not in a horizontal position. In most cases, the corpse is found in an upright or sitting position, and this is an indication that the dead might return. Any mean, nasty, or greedy person can become a draugr. As noted by Ármann, “most medieval Icelandic ghosts are evil or marginal people. If not dissatisfied or evil, they are unpopular”. This is the prime way that draugar share characteristics with ghosts, since any person can become a ghost.

In many Western mythologies, ghosts are generally people with unfinished business or those who are so evil their spirit makes an impact on the place they lived. Ghosts and draugar refuse to follow the prescribed path of death, selfishly staying on Earth when they are supposed to move on. This is easily understandable because, “selfishness is an important attribute of every ghost, and therefore it is no wonder that ghosts tend to be people who were troublesome during their lifetime”.

However, unlike ghosts, draugar can also come about through infection by another draugr such as in the story of Glámr. When Glámr arrives in the haunted valley in "Grettis saga", "the previous evil spirits are relegated to the sidelines and, when Glámr is found dead, they disappear, whereas he takes over their role as ghost of the valley." Although Glámr is an arguably marginal character to begin with, it is only after his fight with the first malignant spirit that the first spirit leaves the valley, and Glámr takes its place wreaking havoc. Similarly, in "Eyrbyggja saga", a shepherd is killed by a draugr and rises the next night as one himself.

Traditionally, a pair of open iron scissors were placed on the chest of the recently deceased, and straws or twigs might be hidden among their clothes. The big toes were tied together or needles were driven through the soles of the feet in order to keep the dead from being able to walk. Tradition also held that the coffin should be lifted and lowered in three different directions as it was carried from the house to confuse a possible draugr's sense of direction.

The most effective means of preventing the return of the dead was believed to be the corpse door. A special door was built, through which the corpse was carried feet-first with people surrounding it so the corpse couldn't see where it was going. The door was then bricked up to prevent a return. It is speculated that this belief began in Denmark and spread throughout the Norse culture. The belief was founded on the idea that the dead could only leave through the way they entered.

In "Eyrbyggja saga", the draugar infesting the home of the Icelander Kiartan were driven off by holding a "door-doom". One by one the draugar were summoned to the door-doom and given judgment and were forced out of the home by this legal method. The home was then purified with holy water to ensure they never came back.

As Draugr are immune to weapons they can only be defeated by a hero's strength and courage. But, a draugr can truly be killed when a hero decapitates the draugr then quickly burn it to ash, then they have throw the ashes into the sea. By doing this a draugr can truly be killed. (From the Prose Edda)

A variation of the draugr is the "haugbui". The haugbui (from Old Norse "haugr"' "howe, barrow, tumulus") was a mound-dweller, the dead body living on within its tomb. The notable difference between the two was that the haugbui is unable to leave its grave site and only attacks those that trespass upon their territory.

The haugbui was rarely found far from its burial place and is a type of undead commonly found in Norse saga material. The creature is said to either swim alongside boats or sail around them in a partially submerged vessel, always on their own. In some accounts, witnesses portray them as shapeshifters who take on the appearance of seaweed or moss-covered stones on the shoreline.

The words "dragon" and "draugr" are not linguistically related. However, both the serpent and the spirit serve as jealous guardians of the graves of kings or ancient civilizations. Dragons that act as draugar appear in "Beowulf" as well as in some of the heroic lays of the "Poetic Edda" (in the form of Fafnir).

One of the best-known draugar is Glámr, who is defeated by the hero in "Grettis saga". After Glámr dies on Christmas Eve, "people became aware that Glámr was not resting in peace. He wrought such havoc that some people fainted at the sight of him, while others went out of their minds". After a mundane battle, Grettir eventually gets Glámr on his back. Just before Grettir kills him, Glámr curses Grettir because "Glámr was endowed with more evil force than most other ghosts", and thus he was able to speak and leave Grettir with his curse after his death.

A somewhat ambivalent, alternative view of the draugr is presented by the example of Gunnar Hámundarson in "Njáls saga": "It seemed as though the howe was agape, and that Gunnar had turned within the howe to look upwards at the moon. They thought that they saw four lights within the howe, but not a shadow to be seen. Then they saw that Gunnar was merry, with a joyful face."

In the "Eyrbyggja saga", a shepherd is assaulted by a blue-black draugr. The shepherd's neck is broken during the ensuing scuffle. The shepherd rises the next night as a draugr.

In more recent Scandinavian folklore, the draug (the modern spelling used in Denmark, Norway, and Sweden) is often identified with the spirits of mariners drowned at sea. The creature is said to possess a distinctly human form, with the exception that its head is composed entirely of seaweed. In other tellings, the draug is described as being a headless fisherman, dressed in oilskin and sailing in half a boat (the Norwegian municipality of Bø, Nordland has the half-boat in its coat-of-arms). This trait is common in the northernmost part of Norway, where life and culture was based on fishing more than anywhere else. The reason for this may be that the fishermen often drowned in great numbers, and the stories of restless dead coming in from sea were more common up north than anywhere else in the country.

A recorded legend from Trøndelag tells how a cadaver lying on a beach became the object of a quarrel between the two types of draug (headless and seaweed-headed). A similar source even tells of a third type, the "gleip", known to hitch themselves to sailors walking ashore and make them slip on the wet rocks.

But, though the draug usually presages death, there is an amusing account in Northern Norway of a northerner who managed to outwit him:

The modern and popular connection between the draug and the sea can be traced back to the author Jonas Lie and the story-teller Regine Nordmann, as well as the drawings of Theodor Kittelsen, who spent some years living in Svolvær. Up north, the tradition of sea-draugs is especially vivid.

Arne Garborg describes land-draugs coming fresh from the graveyards, and the term "draug" is even used of vampires. The notion of draugs who live in the mountains is present in the poetic works of Henrik Ibsen ("Peer Gynt"), and Aasmund Olavsson Vinje. The Nynorsk translation of "The Lord of the Rings" used the term for both Nazgûl and the dead men of Dunharrow. Tolkien's Barrow-Wights bear obvious similarity to, and were inspired by the haugbui.

The term "draug" has come to be used to describe any type of revenant in Nordic folklore.





</doc>
<doc id="8376" url="https://en.wikipedia.org/wiki?curid=8376" title="Day">
Day

A day, a unit of time, is approximately the period of time during which the Earth completes one rotation with respect to the Sun ("solar day"). In 1960, the second was redefined in terms of the orbital motion of the Earth in year 1900, and was designated the SI base unit of time. The unit of measurement "day", was redefined as 86 400 SI seconds and symbolized "d". In 1967, the second and so the day were redefined by atomic electron transition. A civil day is usually 86 400 seconds, plus or minus a possible leap second in Coordinated Universal Time (UTC), and occasionally plus or minus an hour in those locations that change from or to daylight saving time. 

Day can be defined as each of the twenty-four-hour periods, reckoned from one midnight to the next, into which a week, month, or year is divided, and corresponding to a rotation of the earth on its axis. However its use depends on its context, for example when people say 'day and night', 'day' will have a different meaning. It will mean the interval of light between two successive nights; the time between sunrise and sunset. People tend to sleep during the night and are awake at a day, in this instance 'day' will mean time of light between one night and the next.However, in order to be clear when using 'day' in that sense, "daytime" should be used distinguish it from "day" referring to a 24-hour period ; this is since daytime typically always means 'the time of the day between sunrise and sunset. The word "day" may also refer to a day of the week or to a calendar date, as in answer to the question, "On which day?" The life patterns (circadian rhythms) of humans and many other species are related to Earth's solar day and the day-night cycle.

Several definitions of this universal human concept are used according to context, need and convenience. Besides the day of 24 hours (86 400 seconds), the word "day" is used for several different spans of time based on the rotation of the Earth around its axis. An important one is the solar day, defined as the time it takes for the Sun to return to its culmination point (its highest point in the sky). Because celestial orbits are not perfectly circular, and thus objects travel at different speeds at various positions in their orbit, a solar day is not the same length of time throughout the orbital year. Because the Earth orbits the Sun elliptically as the Earth spins on an inclined axis, this period can be up to 7.9 seconds more than (or less than) 24 hours. In recent decades, the average length of a solar day on Earth has been about 86 400.002 seconds (24.000 000 6 hours) and there are about 365.2422 solar days in one mean tropical year. 

Ancient custom has a new day start at either the rising or setting of the Sun on the local horizon (Italian reckoning, for example, being 24 hours from sunset, oldstyle). The exact moment of, and the interval between, two sunrises or sunsets depends on the geographical position (longitude as well as latitude), and the time of year (as indicated by ancient hemispherical sundials).

A more constant day can be defined by the Sun passing through the local meridian, which happens at local noon (upper culmination) or midnight (lower culmination). The exact moment is dependent on the geographical longitude, and to a lesser extent on the time of the year. The length of such a day is nearly constant (24 hours ± 30 seconds). This is the time as indicated by modern sundials.

A further improvement defines a fictitious mean Sun that moves with constant speed along the celestial equator; the speed is the same as the average speed of the real Sun, but this removes the variation over a year as the Earth moves along its orbit around the Sun (due to both its velocity and its axial tilt).

A "day", understood as the span of time it takes for the Earth to make one entire rotationwith respect to the celestial background or a distant star (assumed to be fixed), is called a "stellar day". This period of rotation is about 4 minutes less than 24 hours (23 hours 56 minutes and 4.1 seconds) and there are about 366.2422 stellar days in one mean tropical year (one stellar day more than the number of solar days). Other planets and moons have stellar and solar days of different lengths from Earth's.

A day, in the sense of daytime that is distinguished from night-time, is commonly defined as the period during which sunlight directly reaches the ground, assuming that there are no local obstacles. The length of daytime averages slightly more than half of the 24-hour day. Two effects make daytime on average longer than nights. The Sun is not a point, but has an apparent size of about 32 minutes of arc. Additionally, the atmosphere refracts sunlight in such a way that some of it reaches the ground even when the Sun is below the horizon by about 34 minutes of arc. So the first light reaches the ground when the centre of the Sun is still below the horizon by about 50 minutes of arc. Thus, daytime is on average around 7 minutes longer than 12 hours.

The term comes from the Old English "dæg", with its cognates such as "dagur" in Icelandic, "Tag" in German, and "dag" in Norwegian, Danish, Swedish and Dutch. All of them from the Indo-European root dyau which explains the similarity with Latin dies though the word is known to come from the Germanic branch. , "day" is the 205th most common word in US English, and the 210th most common in UK English.

A day, symbol "d", defined as 86 400 seconds, is not an SI unit, but is accepted for use with SI. The Second is the base unit of time in SI units.

In 1967–68, during the 13th CGPM (Resolution 1), the International Bureau of Weights and Measures (BIPM) redefined a second as … the duration of 9 192 631 770 periods of the radiation corresponding to the transition between two hyperfine levels of the ground state of the caesium 133 atom.
This makes the SI-based day last exactly 794 243 384 928 000 of those periods.

Mainly due to tidal effects, the Earth's rotational period is not constant, resulting in minor variations for both solar days and stellar "days". The Earth's day has increased in length over time. This phenomenon is due to tides raised by the Moon which slow Earth's rotation. Because of the way the second is defined, the mean length of a day is now about 86 400.002 seconds, and is increasing by about 1.7 milliseconds per century (an average over the last 2 700 years). (See tidal acceleration for details.) The length of a day circa 620 million years ago has been estimated from rhythmites (alternating layers in sandstone) as having been about 21.9 hours. The length of day for the Earth before the moon was created is still unknown.

In order to keep the civil day aligned with the apparent movement of the Sun, a day according to Coordinated Universal Time (UTC) can include a negative or positive leap second. Therefore, although typically 86 400 SI seconds in duration, a civil day can be either 86 401 or 86 399 SI seconds long on such a day.

Leap seconds are announced in advance by the International Earth Rotation and Reference Systems Service (IERS), which measures the Earth's rotation and determines whether a leap second is necessary. Leap seconds occur only at the end of a UTC-calculated month, and have only ever been inserted at the end of June 30 or December 31.

For civil purposes, a common clock time is typically defined for an entire region based on the local mean solar time at a central meridian. Such "time zones" began to be adopted about the middle of the 19th century when railroads with regularly occurring schedules came into use, with most major countries having adopted them by 1929. As of 2015, throughout the world, 40 such zones are now in use: the central zone, from which all others are defined as offsets, is known as , which uses Coordinated Universal Time (UTC).

The most common convention starts the civil day at midnight: this is near the time of the lower culmination of the Sun on the central meridian of the time zone. Such a day may be referred to as a calendar day.
A day is commonly divided into 24 hours of 60 minutes, with each minute composed of 60 seconds.

In the 19th century, an idea circulated to make a decimal fraction ( or ) of an astronomical day the base unit of time. This was an afterglow of the short-lived movement toward a decimalisation of timekeeping and the calendar, which had been given up already due to its difficulty in transitioning from traditional, more familiar units. The most successful alternative is the "centiday", equal to 14.4 minutes (864 seconds), being not only a shorter multiple of an hour (0.24 vs 2.4) but also closer to the SI multiple "kilosecond" (1 000 seconds) and equal to the traditional Chinese unit, "kè".

The word refers to various similarly defined ideas, such as:

For most diurnal animals, the day naturally begins at dawn and ends at sunset. Humans, with their cultural norms and scientific knowledge, have employed several different conceptions of the day's boundaries. Common convention among the ancient Romans, ancient Chinese and in modern times is for the civil day to begin at midnight, i.e. 00:00, and last a full 24 hours until 24:00 (i.e. 00:00 of the next day). 
In ancient Egypt, the day was reckoned from sunrise to sunrise.

The Jewish day begins at either sunset or nightfall (when three second-magnitude stars appear). The "Damascus Document", copies of which were also found among the Dead Sea scrolls, states regarding the observance of the Sabbath that "No one is to do any work on Friday "from the moment that the Sun's disk stands distant from the horizon by the length of its own diameter"," presumably indicating that the monastic community responsible for producing this work counted the day as ending shortly before the Sun had begun to set. 

Medieval Europe also followed this tradition, known as Florentine reckoning: in this system, a reference like "two hours into the day" meant "two hours after sunset" and thus times during the evening need to be shifted back one calendar day in modern reckoning. Days such as Christmas Eve, Halloween, and the Eve of Saint Agnes are remnants of the older pattern when holidays began during the prior evening. Prior to 1926, Turkey had two time systems: Turkish (counting the hours from sunset) and French (counting the hours from midnight).

In many cultures, nights are named after the previous day. For example,"Friday night" usually means the entire night between Friday and Saturday. This difference from the civil day often leads to confusion. Events starting at midnight are often announced as occurring the day before. TV-guides tend to list nightly programs at the previous day, although programming a VCR requires the strict logic of starting the new day at 00:00 (to further confuse the issue, VCRs set to the 12-hour clock notation will label this "12:00 AM"). Expressions like "today", "yesterday" and "tomorrow" become ambiguous during the night. Because Jews and Muslims begin their days at nightfall, "Saturday" night, for example, is what most people would call Friday night.

Validity of tickets, passes, etc., for a day or a number of days may end at midnight, or closing time, when that is earlier. However, if a service (e.g., public transport) operates from for example, 6:00 to 1:00 the next day (which may be noted as 25:00), the last hour may well count as being part of the previous day. For services depending on the day ("closed on Sundays", "does not run on Fridays", and so on) there is a risk of ambiguity. For example, a day ticket on the Nederlandse Spoorwegen (Dutch Railways) is valid for 28 hours, from 0:00 to 28:00 (that is, 4:00 the next day); the validity of a pass on Transport for London (TfL) services is until the end of the "transport day"—that is to say, until 4:30 am on the day after the "expiry" date stamped on the pass.

In places which experience the midnight sun (polar day), daytime may extend beyond one 24 hour period and could even extend to months




</doc>
<doc id="8377" url="https://en.wikipedia.org/wiki?curid=8377" title="Database">
Database

A database is an organized collection of data, stored and accessed electronically. Database designers typically organize the data to model aspects of reality in a way that supports processes requiring information, such as (for example) modelling the availability of rooms in hotels in a way that supports finding a hotel with vacancies.

A database-management system (DBMS) is a computer-software application that interacts with end-users, other applications, and the database itself to capture and analyze data. (Sometimes a DBMS is loosely referred to as a "database".) A general-purpose DBMS allows the definition, creation, querying, update, and administration of databases. A database is generally stored in a DBMS-specific format which is not portable, but different DBMSs can share data by using standards such as SQL and ODBC or JDBC.

Computer scientists may classify database-management systems according to the database models that they support. Relational databases became dominant in the 1980s. These model data as rows and columns in a series of tables, and the vast majority use SQL for writing and querying data. In the 2000s, non-relational databases became popular, referred to as NoSQL because they use different query languages.

Formally, a "database" refers to a set of related data and the way it is organized. Access to this data is usually provided by a "database management system" (DBMS) consisting of an integrated set of computer software that allows users to interact with one or more databases and provides access to all of the data contained in the database (although restrictions may exist that limit access to particular data). The DBMS provides various functions that allow entry, storage and retrieval of large quantities of information and provides ways to manage how that information is organized.

Because of the close relationship between them, the term "database" is often used casually to refer to both a database and the DBMS used to manipulate it.

Outside the world of professional information technology, the term "database" is often used to refer to any collection of related data (such as a spreadsheet or a card index). This article is concerned only with databases where the size and usage requirements necessitate use of a database management system.

Existing DBMSs provide various functions that allow management of a database and its data which can be classified into four main functional groups:


Both a database and its DBMS conform to the principles of a particular database model. "Database system" refers collectively to the database model, database management system, and database.

Physically, database servers are dedicated computers that hold the actual databases and run only the DBMS and related software. Database servers are usually multiprocessor computers, with generous memory and RAID disk arrays used for stable storage. RAID is used for recovery of data if any of the disks fail. Hardware database accelerators, connected to one or more servers via a high-speed channel, are also used in large volume transaction processing environments. DBMSs are found at the heart of most database applications. DBMSs may be built around a custom multitasking kernel with built-in networking support, but modern DBMSs typically rely on a standard operating system to provide these functions.

Since DBMSs comprise a significant market, computer and storage vendors often take into account DBMS requirements in their own development plans.

Databases and DBMSs can be categorized according to the database model(s) that they support (such as relational or XML), the type(s) of computer they run on (from a server cluster to a mobile phone), the query language(s) used to access the database (such as SQL or XQuery), and their internal engineering, which affects performance, scalability, resilience, and security.

Databases are used to support internal operations of organizations and to underpin online interactions with customers and suppliers (see Enterprise software).

Databases are used to hold administrative information and more specialized data, such as engineering data or economic models. Examples of database applications include computerized library systems, flight reservation systems, computerized parts inventory systems, and many content management systems that store websites as collections of webpages in a database.

DBMS may become a complex software system and its development typically requires thousands of human years of development effort. Some general-purpose DBMSs such as Adabas, Oracle and DB2 have been upgraded since the 1970s. General-purpose DBMSs aim to meet the needs of as many applications as possible, which adds to the complexity. However, since their development cost can be spread over a large number of users, they are often the most cost-effective approach. On the other hand, a general-purpose DBMS may introduce unnecessary overhead. Therefore, many systems use a special-purpose DBMS. A common example is an email system that performs many of the functions of a general-purpose DBMS such as the insertion and deletion of messages composed of various items of data or associating messages with a particular email address; but these functions are limited to what is required to handle email and don't provide the user with all of the functionality that would be available using a general-purpose DBMS.
Application software can often access a database on behalf of end-users, without exposing the DBMS interface directly. Application programmers may use a wire protocol directly, or more likely through an application programming interface. Database designers and database administrators interact with the DBMS through dedicated interfaces to build and maintain the applications' databases, and thus need some more knowledge and understanding about how DBMSs operate and the DBMSs' external interfaces and tuning parameters.

The sizes, capabilities, and performance of databases and their respective DBMSs have grown in orders of magnitude. These performance increases were enabled by the technology progress in the areas of processors, computer memory, computer storage, and computer networks. The development of database technology can be divided into three eras based on data model or structure: navigational, SQL/relational, and post-relational.

The two main early navigational data models were the hierarchical model, epitomized by IBM's IMS system, and the CODASYL model (network model), implemented in a number of products such as IDMS.

The relational model, first proposed in 1970 by Edgar F. Codd, departed from this tradition by insisting that applications should search for data by content, rather than by following links. The relational model employs sets of ledger-style tables, each used for a different type of entity. Only in the mid-1980s did computing hardware become powerful enough to allow the wide deployment of relational systems (DBMSs plus applications). By the early 1990s, however, relational systems dominated in all large-scale data processing applications, and they remain dominant: IBM DB2, Oracle, MySQL, and Microsoft SQL Server are the top DBMS. The dominant database language, standardised SQL for the relational model, has influenced database languages for other data models.

Object databases were developed in the 1980s to overcome the inconvenience of object-relational impedance mismatch, which led to the coining of the term "post-relational" and also the development of hybrid object-relational databases.

The next generation of post-relational databases in the late 2000s became known as NoSQL databases, introducing fast key-value stores and document-oriented databases. A competing "next generation" known as NewSQL databases attempted new implementations that retained the relational/SQL model while aiming to match the high performance of NoSQL compared to commercially available relational DBMSs.

The introduction of the term "database" coincided with the availability of direct-access storage (disks and drums) from the mid-1960s onwards. The term represented a contrast with the tape-based systems of the past, allowing shared interactive use rather than daily batch processing. The Oxford English Dictionary cites a 1962 report by the System Development Corporation of California as the first to use the term "data-base" in a specific technical sense.

As computers grew in speed and capability, a number of general-purpose database systems emerged; by the mid-1960s a number of such systems had come into commercial use. Interest in a standard began to grow, and Charles Bachman, author of one such product, the Integrated Data Store (IDS), founded the "Database Task Group" within CODASYL, the group responsible for the creation and standardization of COBOL. In 1971, the Database Task Group delivered their standard, which generally became known as the "CODASYL approach", and soon a number of commercial products based on this approach entered the market.

The CODASYL approach relied on the "manual" navigation of a linked data set which was formed into a large network. Applications could find records by one of three methods:

Later systems added B-trees to provide alternate access paths. Many CODASYL databases also added a very straightforward query language. However, in the final tally, CODASYL was very complex and required significant training and effort to produce useful applications.

IBM also had their own DBMS in 1966, known as Information Management System (IMS). IMS was a development of software written for the Apollo program on the System/360. IMS was generally similar in concept to CODASYL, but used a strict hierarchy for its model of data navigation instead of CODASYL's network model. Both concepts later became known as navigational databases due to the way data was accessed, and Bachman's 1973 Turing Award presentation was "The Programmer as Navigator". IMS is classified as a hierarchical database. IDMS and Cincom Systems' TOTAL database are classified as network databases. IMS remains in use .

Edgar Codd worked at IBM in San Jose, California, in one of their offshoot offices that was primarily involved in the development of hard disk systems. He was unhappy with the navigational model of the CODASYL approach, notably the lack of a "search" facility. In 1970, he wrote a number of papers that outlined a new approach to database construction that eventually culminated in the groundbreaking "A Relational Model of Data for Large Shared Data Banks".

In this paper, he described a new system for storing and working with large databases. Instead of records being stored in some sort of linked list of free-form records as in CODASYL, Codd's idea was to use a "table" of fixed-length records, with each table used for a different type of entity. A linked-list system would be very inefficient when storing "sparse" databases where some of the data for any one record could be left empty. The relational model solved this by splitting the data into a series of normalized tables (or "relations"), with optional elements being moved out of the main table to where they would take up room only if needed. Data may be freely inserted, deleted and edited in these tables, with the DBMS doing whatever maintenance needed to present a table view to the application/user.
The relational model also allowed the content of the database to evolve without constant rewriting of links and pointers. The relational part comes from entities referencing other entities in what is known as one-to-many relationship, like a traditional hierarchical model, and many-to-many relationship, like a navigational (network) model. Thus, a relational model can express both hierarchical and navigational models, as well as its native tabular model, allowing for pure or combined modeling in terms of these three models, as the application requires.

For instance, a common use of a database system is to track information about users, their name, login information, various addresses and phone numbers. In the navigational approach, all of this data would be placed in a single record, and unused items would simply not be placed in the database. In the relational approach, the data would be "normalized" into a user table, an address table and a phone number table (for instance). Records would be created in these optional tables only if the address or phone numbers were actually provided.

Linking the information back together is the key to this system. In the relational model, some bit of information was used as a "key", uniquely defining a particular record. When information was being collected about a user, information stored in the optional tables would be found by searching for this key. For instance, if the login name of a user is unique, addresses and phone numbers for that user would be recorded with the login name as its key. This simple "re-linking" of related data back into a single collection is something that traditional computer languages are not designed for.

Just as the navigational approach would require programs to loop in order to collect records, the relational approach would require loops to collect information about any "one" record. Codd's suggestions was a set-oriented language, that would later spawn the ubiquitous SQL. Using a branch of mathematics known as tuple calculus, he demonstrated that such a system could support all the operations of normal databases (inserting, updating etc.) as well as providing a simple system for finding and returning "sets" of data in a single operation.

Codd's paper was picked up by two people at Berkeley, Eugene Wong and Michael Stonebraker. They started a project known as INGRES using funding that had already been allocated for a geographical database project and student programmers to produce code. Beginning in 1973, INGRES delivered its first test products which were generally ready for widespread use in 1979. INGRES was similar to System R in a number of ways, including the use of a "language" for data access, known as QUEL. Over time, INGRES moved to the emerging SQL standard.

IBM itself did one test implementation of the relational model, PRTV, and a production one, Business System 12, both now discontinued. Honeywell wrote MRDS for Multics, and now there are two new implementations: Alphora Dataphor and Rel. Most other DBMS implementations usually called "relational" are actually SQL DBMSs.

In 1970, the University of Michigan began development of the MICRO Information Management System based on D.L. Childs' Set-Theoretic Data model. MICRO was used to manage very large data sets by the US Department of Labor, the U.S. Environmental Protection Agency, and researchers from the University of Alberta, the University of Michigan, and Wayne State University. It ran on IBM mainframe computers using the Michigan Terminal System. The system remained in production until 1998.

In the 1970s and 1980s, attempts were made to build database systems with integrated hardware and software. The underlying philosophy was that such integration would provide higher performance at lower cost. Examples were IBM System/38, the early offering of Teradata, and the Britton Lee, Inc. database machine.

Another approach to hardware support for database management was ICL's CAFS accelerator, a hardware disk controller with programmable search capabilities. In the long term, these efforts were generally unsuccessful because specialized database machines could not keep pace with the rapid development and progress of general-purpose computers. Thus most database systems nowadays are software systems running on general-purpose hardware, using general-purpose computer data storage. However this idea is still pursued for certain applications by some companies like Netezza and Oracle (Exadata).

IBM started working on a prototype system loosely based on Codd's concepts as "System R" in the early 1970s. The first version was ready in 1974/5, and work then started on multi-table systems in which the data could be split so that all of the data for a record (some of which is optional) did not have to be stored in a single large "chunk". Subsequent multi-user versions were tested by customers in 1978 and 1979, by which time a standardized query language – SQL – had been added. Codd's ideas were establishing themselves as both workable and superior to CODASYL, pushing IBM to develop a true production version of System R, known as "SQL/DS", and, later, "Database 2" (DB2).

Larry Ellison's Oracle Database (or more simply, Oracle) started from a different chain, based on IBM's papers on System R. Though Oracle V1 implementations were completed in 1978, it wasn't until Oracle Version 2 when Ellison beat IBM to market in 1979.

Stonebraker went on to apply the lessons from INGRES to develop a new database, Postgres, which is now known as PostgreSQL. PostgreSQL is often used for global mission critical applications (the .org and .info domain name registries use it as their primary data store, as do many large companies and financial institutions).

In Sweden, Codd's paper was also read and Mimer SQL was developed from the mid-1970s at Uppsala University. In 1984, this project was consolidated into an independent enterprise. In the early 1980s, Mimer introduced transaction handling for high robustness in applications, an idea that was subsequently implemented on most other DBMSs.

Another data model, the entity–relationship model, emerged in 1976 and gained popularity for database design as it emphasized a more familiar description than the earlier relational model. Later on, entity–relationship constructs were retrofitted as a data modeling construct for the relational model, and the difference between the two have become irrelevant.

The 1980s ushered in the age of desktop computing. The new computers empowered their users with spreadsheets like Lotus 1-2-3 and database software like dBASE. The dBASE product was lightweight and easy for any computer user to understand out of the box. C. Wayne Ratliff, the creator of dBASE, stated: "dBASE was different from programs like BASIC, C, FORTRAN, and COBOL in that a lot of the dirty work had already been done. The data manipulation is done by dBASE instead of by the user, so the user can concentrate on what he is doing, rather than having to mess with the dirty details of opening, reading, and closing files, and managing space allocation." dBASE was one of the top selling software titles in the 1980s and early 1990s.

The 1990s, along with a rise in object-oriented programming, saw a growth in how data in various databases were handled. Programmers and designers began to treat the data in their databases as objects. That is to say that if a person's data were in a database, that person's attributes, such as their address, phone number, and age, were now considered to belong to that person instead of being extraneous data. This allows for relations between data to be relations to objects and their attributes and not to individual fields. The term "object-relational impedance mismatch" described the inconvenience of translating between programmed objects and database tables. Object databases and object-relational databases attempt to solve this problem by providing an object-oriented language (sometimes as extensions to SQL) that programmers can use as alternative to purely relational SQL. On the programming side, libraries known as object-relational mappings (ORMs) attempt to solve the same problem.

XML databases are a type of structured document-oriented database that allows querying based on XML document attributes. XML databases are mostly used in enterprise database management, where XML is being used as the machine-to-machine data interoperability standard. XML database management systems include commercial software MarkLogic and Oracle Berkeley DB XML, and a free use software Clusterpoint Distributed XML/JSON Database. All are enterprise software database platforms and support industry standard ACID-compliant transaction processing with strong database consistency characteristics and high level of database security.

NoSQL databases are often very fast, do not require fixed table schemas, avoid join operations by storing denormalized data, and are designed to scale horizontally. The most popular NoSQL systems include MongoDB, Couchbase, Riak, Memcached, Redis, CouchDB, Hazelcast, Apache Cassandra, and HBase, which are all open-source software products.

In recent years, there has been a strong demand for massively distributed databases with high partition tolerance, but according to the CAP theorem it is impossible for a distributed system to simultaneously provide consistency, availability, and partition tolerance guarantees. A distributed system can satisfy any two of these guarantees at the same time, but not all three. For that reason, many NoSQL databases are using what is called eventual consistency to provide both availability and partition tolerance guarantees with a reduced level of data consistency.

NewSQL is a class of modern relational databases that aims to provide the same scalable performance of NoSQL systems for online transaction processing (read-write) workloads while still using SQL and maintaining the ACID guarantees of a traditional database system. Such databases include Google F1/Spanner, Citus, CockroachDB, TiDB, ScaleBase, MemSQL, NuoDB, and VoltDB.

Database technology has been an active research topic since the 1960s, both in academia and in the research and development groups of companies (for example IBM Research). Research activity includes theory and development of prototypes. Notable research topics have included models, the atomic transaction concept, and related concurrency control techniques, query languages and query optimization methods, RAID, and more.

The database research area has several dedicated academic journals (for example, "ACM Transactions on Database Systems"-TODS, "Data and Knowledge Engineering"-DKE) and annual conferences (e.g., ACM SIGMOD, ACM PODS, VLDB, IEEE ICDE).

One way to classify databases involves the type of their contents, for example: bibliographic, document-text, statistical, or multimedia objects. Another way is by their application area, for example: accounting, music compositions, movies, banking, manufacturing, or insurance. A third way is by some technical aspect, such as the database structure or interface type. This section lists a few of the adjectives used to characterize different kinds of databases.




The first task of a database designer is to produce a conceptual data model that reflects the structure of the information to be held in the database. A common approach to this is to develop an entity-relationship model, often with the aid of drawing tools. Another popular approach is the Unified Modeling Language. A successful data model will accurately reflect the possible state of the external world being modeled: for example, if people can have more than one phone number, it will allow this information to be captured. Designing a good conceptual data model requires a good understanding of the application domain; it typically involves asking deep questions about the things of interest to an organization, like "can a customer also be a supplier?", or "if a product is sold with two different forms of packaging, are those the same product or different products?", or "if a plane flies from New York to Dubai via Frankfurt, is that one flight or two (or maybe even three)?". The answers to these questions establish definitions of the terminology used for entities (customers, products, flights, flight segments) and their relationships and attributes.

Producing the conceptual data model sometimes involves input from business processes, or the analysis of workflow in the organization. This can help to establish what information is needed in the database, and what can be left out. For example, it can help when deciding whether the database needs to hold historic data as well as current data.

Having produced a conceptual data model that users are happy with, the next stage is to translate this into a schema that implements the relevant data structures within the database. This process is often called logical database design, and the output is a logical data model expressed in the form of a schema. Whereas the conceptual data model is (in theory at least) independent of the choice of database technology, the logical data model will be expressed in terms of a particular database model supported by the chosen DBMS. (The terms "data model" and "database model" are often used interchangeably, but in this article we use "data model" for the design of a specific database, and "database model" for the modelling notation used to express that design.)

The most popular database model for general-purpose databases is the relational model, or more precisely, the relational model as represented by the SQL language. The process of creating a logical database design using this model uses a methodical approach known as normalization. The goal of normalization is to ensure that each elementary "fact" is only recorded in one place, so that insertions, updates, and deletions automatically maintain consistency.

The final stage of database design is to make the decisions that affect performance, scalability, recovery, security, and the like, which depend on the particular DBMS. This is often called "physical database design", and the output is the physical data model. A key goal during this stage is data independence, meaning that the decisions made for performance optimization purposes should be invisible to end-users and applications. There are two types of data independence: Physical data independence and logical data independence. Physical design is driven mainly by performance requirements, and requires a good knowledge of the expected workload and access patterns, and a deep understanding of the features offered by the chosen DBMS.

Another aspect of physical database design is security. It involves both defining access control to database objects as well as defining security levels and methods for the data itself.

A database model is a type of data model that determines the logical structure of a database and fundamentally determines in which manner data can be stored, organized, and manipulated. The most popular example of a database model is the relational model (or the SQL approximation of relational), which uses a table-based format.

Common logical data models for databases include:

An object-relational database combines the two related structures.

Physical data models include:

Other models include:

Specialized models are optimized for particular types of data:

A database management system provides three views of the database data:


While there is typically only one conceptual (or logical) and physical (or internal) view of the data, there can be any number of different external views. This allows users to see database information in a more business-related way rather than from a technical, processing viewpoint. For example, a financial department of a company needs the payment details of all employees as part of the company's expenses, but does not need details about employees that are the interest of the human resources department. Thus different departments need different "views" of the company's database.

The three-level database architecture relates to the concept of "data independence" which was one of the major initial driving forces of the relational model. The idea is that changes made at a certain level do not affect the view at a higher level. For example, changes in the internal level do not affect application programs written using conceptual level interfaces, which reduces the impact of making physical changes to improve performance.

The conceptual view provides a level of indirection between internal and external. On one hand it provides a common view of the database, independent of different external view structures, and on the other hand it abstracts away details of how the data are stored or managed (internal level). In principle every level, and even every external view, can be presented by a different data model. In practice usually a given DBMS uses the same data model for both the external and the conceptual levels (e.g., relational model). The internal level, which is hidden inside the DBMS and depends on its implementation, requires a different level of detail and uses its own types of data structure types.

Separating the "external", "conceptual" and "internal" levels was a major feature of the relational database model implementations that dominate 21st century databases.

Database languages are special-purpose languages, which allow one or more of the following tasks, sometimes distinguished as sublanguages:


Database languages are specific to a particular data model. Notable examples include:


A database language may also incorporate features like:

Because of the critical importance of database technology to the smooth running of an enterprise, database systems include complex mechanisms to deliver the required performance, security, and availability, and allow database administrators to control the use of these features.

Database storage is the container of the physical materialization of a database. It comprises the "internal" (physical) "level" in the database architecture. It also contains all the information needed (e.g., metadata, "data about the data", and internal data structures) to reconstruct the "conceptual level" and "external level" from the internal level when needed. Putting data into permanent storage is generally the responsibility of the database engine a.k.a. "storage engine". Though typically accessed by a DBMS through the underlying operating system (and often utilizing the operating systems' file systems as intermediates for storage layout), storage properties and configuration setting are extremely important for the efficient operation of the DBMS, and thus are closely maintained by database administrators. A DBMS, while in operation, always has its database residing in several types of storage (e.g., memory and external storage). The database data and the additional needed information, possibly in very large amounts, are coded into bits. Data typically reside in the storage in structures that look completely different from the way the data look in the conceptual and external levels, but in ways that attempt to optimize (the best possible) these levels' reconstruction when needed by users and programs, as well as for computing additional types of needed information from the data (e.g., when querying the database).

Some DBMSs support specifying which character encoding was used to store data, so multiple encodings can be used in the same database.

Various low-level database storage structures are used by the storage engine to serialize the data model so it can be written to the medium of choice. Techniques such as indexing may be used to improve performance. Conventional storage is row-oriented, but there are also column-oriented and correlation databases.

Often storage redundancy is employed to increase performance. A common example is storing "materialized views", which consist of frequently needed "external views" or query results. Storing such views saves the expensive computing of them each time they are needed. The downsides of materialized views are the overhead incurred when updating them to keep them synchronized with their original updated database data, and the cost of storage redundancy.

Occasionally a database employs storage redundancy by database objects replication (with one or more copies) to increase data availability (both to improve performance of simultaneous multiple end-user accesses to a same database object, and to provide resiliency in a case of partial failure of a distributed database). Updates of a replicated object need to be synchronized across the object copies. In many cases, the entire database is replicated.

Database security deals with all various aspects of protecting the database content, its owners, and its users. It ranges from protection from intentional unauthorized database uses to unintentional database accesses by unauthorized entities (e.g., a person or a computer program).

Database access control deals with controlling who (a person or a certain computer program) is allowed to access what information in the database. The information may comprise specific database objects (e.g., record types, specific records, data structures), certain computations over certain objects (e.g., query types, or specific queries), or utilizing specific access paths to the former (e.g., using specific indexes or other data structures to access information). Database access controls are set by special authorized (by the database owner) personnel that uses dedicated protected security DBMS interfaces.

This may be managed directly on an individual basis, or by the assignment of individuals and privileges to groups, or (in the most elaborate models) through the assignment of individuals and groups to roles which are then granted entitlements. Data security prevents unauthorized users from viewing or updating the database. Using passwords, users are allowed access to the entire database or subsets of it called "subschemas". For example, an employee database can contain all the data about an individual employee, but one group of users may be authorized to view only payroll data, while others are allowed access to only work history and medical data. If the DBMS provides a way to interactively enter and update the database, as well as interrogate it, this capability allows for managing personal databases.

Data security in general deals with protecting specific chunks of data, both physically (i.e., from corruption, or destruction, or removal; e.g., see physical security), or the interpretation of them, or parts of them to meaningful information (e.g., by looking at the strings of bits that they comprise, concluding specific valid credit-card numbers; e.g., see data encryption).

Change and access logging records who accessed which attributes, what was changed, and when it was changed. Logging services allow for a forensic database audit later by keeping a record of access occurrences and changes. Sometimes application-level code is used to record changes rather than leaving this to the database. Monitoring can be set up to attempt to detect security breaches.

Database transactions can be used to introduce some level of fault tolerance and data integrity after recovery from a crash. A database transaction is a unit of work, typically encapsulating a number of operations over a database (e.g., reading a database object, writing, acquiring lock, etc.), an abstraction supported in database and also other systems. Each transaction has well defined boundaries in terms of which program/code executions are included in that transaction (determined by the transaction's programmer via special transaction commands).

The acronym ACID describes some ideal properties of a database transaction: atomicity, consistency, isolation, and durability.

A database built with one DBMS is not portable to another DBMS (i.e., the other DBMS cannot run it). However, in some situations, it is desirable to move, migrate a database from one DBMS to another. The reasons are primarily economical (different DBMSs may have different total costs of ownership or TCOs), functional, and operational (different DBMSs may have different capabilities). The migration involves the database's transformation from one DBMS type to another. The transformation should maintain (if possible) the database related application (i.e., all related application programs) intact. Thus, the database's conceptual and external architectural levels should be maintained in the transformation. It may be desired that also some aspects of the architecture internal level are maintained. A complex or large database migration may be a complicated and costly (one-time) project by itself, which should be factored into the decision to migrate. This in spite of the fact that tools may exist to help migration between specific DBMSs. Typically, a DBMS vendor provides tools to help importing databases from other popular DBMSs.

After designing a database for an application, the next stage is building the database. Typically, an appropriate general-purpose DBMS can be selected to be utilized for this purpose. A DBMS provides the needed user interfaces to be utilized by database administrators to define the needed application's data structures within the DBMS's respective data model. Other user interfaces are used to select needed DBMS parameters (like security related, storage allocation parameters, etc.).

When the database is ready (all its data structures and other needed components are defined), it is typically populated with initial application's data (database initialization, which is typically a distinct project; in many cases using specialized DBMS interfaces that support bulk insertion) before making it operational. In some cases, the database becomes operational while empty of application data, and data are accumulated during its operation.

After the database is created, initialised and populated it needs to be maintained. Various database parameters may need changing and the database may need to be tuned (tuning) for better performance; application's data structures may be changed or added, new related application programs may be written to add to the application's functionality, etc.

Sometimes it is desired to bring a database back to a previous state (for many reasons, e.g., cases when the database is found corrupted due to a software error, or if it has been updated with erroneous data). To achieve this, a backup operation is done occasionally or continuously, where each desired database state (i.e., the values of its data and their embedding in database's data structures) is kept within dedicated backup files (many techniques exist to do this effectively). When this state is needed, i.e., when it is decided by a database administrator to bring the database back to this state (e.g., by specifying this state by a desired point in time when the database was in this state), these files are utilized to restore that state.

Static analysis techniques for software verification can be applied also in the scenario of query languages. In particular, the *Abstract interpretation framework has been extended to the field of query languages for relational databases as a way to support sound approximation techniques. The semantics of query languages can be tuned according to suitable abstractions of the concrete domain of data. The abstraction of relational database system has many interesting applications, in particular, for security purposes, such as fine grained access control, watermarking, etc.

Other DBMS features might include:




</doc>
<doc id="8378" url="https://en.wikipedia.org/wiki?curid=8378" title="Dipole">
Dipole

In electromagnetism, there are two kinds of dipoles:

Dipoles can be characterized by their dipole moment, a vector quantity. For the simple electric dipole given above, the electric dipole moment points from the negative charge towards the positive charge, and has a magnitude equal to the strength of each charge times the separation between the charges. (To be precise: for the definition of the dipole moment, one should always consider the "dipole limit", where, for example, the distance of the generating charges should "converge" to 0 while simultaneously, the charge strength should "diverge" to infinity in such a way that the product remains a positive constant.)

For the current loop, the magnetic dipole moment points through the loop (according to the right hand grip rule), with a magnitude equal to the current in the loop times the area of the loop.

In addition to current loops, the electron, among other fundamental particles, has a magnetic dipole moment. That is because it generates a magnetic field that is identical to that generated by a very small current loop. However, the electron's magnetic moment is not due to a current loop, but is instead an intrinsic property of the electron. It is also possible that the electron has an "electric" dipole moment although it has not yet been observed (see electron electric dipole moment for more information).
A permanent magnet, such as a bar magnet, owes its magnetism to the intrinsic magnetic dipole moment of the electron. The two ends of a bar magnet are referred to as poles (not to be confused with monopoles), and may be labeled "north" and "south". In terms of the Earth's magnetic field, they are respectively "north-seeking" and "south-seeking" poles: if the magnet were freely suspended in the Earth's magnetic field, the north-seeking pole would point towards the north and the south-seeking pole would point towards the south. The dipole moment of the bar magnet points from its magnetic south to its magnetic north pole. The north pole of a bar magnet in a compass points north. However, that means that Earth's geomagnetic north pole is the "south" pole (south-seeking pole) of its dipole moment and vice versa.

The only known mechanisms for the creation of magnetic dipoles are by current loops or quantum-mechanical spin since the existence of magnetic monopoles has never been experimentally demonstrated.

The term comes from the Greek ("dis"), "twice" and ("polos"), "axis".

A "physical dipole" consists of two equal and opposite point charges: in the literal sense, two poles. Its field at large distances (i.e., distances large in comparison to the separation of the poles) depends almost entirely on the dipole moment as defined above. A "point (electric) dipole" is the limit obtained by letting the separation tend to 0 while keeping the dipole moment fixed. The field of a point dipole has a particularly simple form, and the order-1 term in the multipole expansion is precisely the point dipole field.

Although there are no known magnetic monopoles in nature, there are magnetic dipoles in the form of the quantum-mechanical spin associated with particles such as electrons (although the accurate description of such effects falls outside of classical electromagnetism). A theoretical magnetic "point dipole" has a magnetic field of exactly the same form as the electric field of an electric point dipole. A very small current-carrying loop is approximately a magnetic point dipole; the magnetic dipole moment of such a loop is the product of the current flowing in the loop and the (vector) area of the loop.

Any configuration of charges or currents has a 'dipole moment', which describes the dipole whose field is the best approximation, at large distances, to that of the given configuration. This is simply one term in the multipole expansion when the total charge ("monopole moment") is 0—as it "always" is for the magnetic case, since there are no magnetic monopoles. The dipole term is the dominant one at large distances: Its field falls off in proportion to , as compared to for the next (quadrupole) term and higher powers of for higher terms, or for the monopole term.

Many molecules have such dipole moments due to non-uniform distributions of positive and negative charges on the various atoms. Such is the case with polar compounds like hydrogen fluoride (HF), where electron density is shared unequally between atoms. Therefore, a molecule's dipole is an electric dipole with an inherent electric field which should not be confused with a magnetic dipole which generates a magnetic field.

The physical chemist Peter J. W. Debye was the first scientist to study molecular dipoles extensively, and, as a consequence, dipole moments are measured in units named "debye" in his honor.

For molecules there are three types of dipoles:

More generally, an induced dipole of "any" polarizable charge distribution "ρ" (remember that a molecule has a charge distribution) is caused by an electric field external to "ρ". This field may, for instance, originate from an ion or polar molecule in the vicinity of "ρ" or may be macroscopic (e.g., a molecule between the plates of a charged capacitor). The size of the induced dipole moment is equal to the product of the strength of the external field and the dipole polarizability of "ρ".

Dipole moment values can be obtained from measurement of the dielectric constant. Some typical gas phase values in debye units are:

Potassium bromide (KBr) has one of the highest dipole moments because it is an ionic compound that exists as a molecule in the gas phase.
The overall dipole moment of a molecule may be approximated as a vector sum of bond dipole moments. As a vector sum it depends on the relative orientation of the bonds, so that from the dipole moment information can be deduced about the molecular geometry.

For example, the zero dipole of CO implies that the two C=O bond dipole moments cancel so that the molecule must be linear. For HO the O−H bond moments do not cancel because the molecule is bent. For ozone (O) which is also a bent molecule, the bond dipole moments are not zero even though the O−O bonds are between similar atoms. This agrees with the Lewis structures for the resonance forms of ozone which show a positive charge on the central oxygen atom. 
An example in organic chemistry of the role of geometry in determining dipole moment is the "cis" and "trans" isomers of 1,2-dichloroethene. In the "cis" isomer the two polar C−Cl bonds are on the same side of the C=C double bond and the molecular dipole moment is 1.90 D. In the "trans" isomer, the dipole moment is zero because the two C−Cl bonds are on opposite sides of the C=C and cancel (and the two bond moments for the much less polar C−H bonds also cancel).

Another example of the role of molecular geometry is boron trifluoride, which has three polar bonds with a difference in electronegativity greater than the traditionally cited threshold of 1.7 for ionic bonding. However, due to the equilateral triangular distribution of the fluoride ions about the boron cation center, the molecule as a whole does not exhibit any identifiable pole: one cannot construct a plane that divides the molecule into a net negative part and a net positive part.

Consider a collection of "N" particles with charges "q" and position vectors r. For instance, this collection may be a molecule consisting of electrons, all with charge −"e", and nuclei with charge "eZ", where "Z" is the atomic number of the "i" th nucleus.
The dipole observable (physical quantity) has the quantum mechanical dipole operator:
Notice that this definition is valid only for non-charged dipoles, i.e. total charge equal to zero. To a charged dipole we have the next equation:
where formula_3 is the center of mass of the molecule/group of particles.

A non-degenerate ("S"-state) atom can have only a zero permanent dipole. This fact follows quantum mechanically from the inversion symmetry of atoms. All 3 components of the dipole operator are antisymmetric under inversion with respect to the nucleus,

where formula_5 is the dipole operator and formula_6 is the inversion operator.

The permanent dipole moment of an atom in a non-degenerate state (see degenerate energy level) is given as the expectation (average) value of the dipole operator,

where formula_8 is an "S"-state, non-degenerate, wavefunction, which is symmetric or antisymmetric under inversion: formula_9. Since the product of the wavefunction (in the ket) and its complex conjugate (in the bra) is always symmetric under inversion and its inverse,

it follows that the expectation value changes sign under inversion. We used here the fact that formula_11, being a symmetry operator, is unitary: formula_12 and by definition the Hermitian adjoint formula_13 may be moved from bra to ket and then becomes formula_14. Since the only quantity that is equal to minus itself is the zero, the expectation value vanishes,

In the case of open-shell atoms with degenerate energy levels, one could define a dipole moment by the aid of the first-order Stark effect. This gives a non-vanishing dipole (by definition proportional to a non-vanishing first-order Stark shift) only if some of the wavefunctions belonging to the degenerate energies have opposite parity; i.e., have different behavior under inversion. This is a rare occurrence, but happens for the excited H-atom, where 2s and 2p states are "accidentally" degenerate (see article Laplace–Runge–Lenz vector for the origin of this degeneracy) and have opposite parity (2s is even and 2p is odd).

The far-field strength, "B", of a dipole magnetic field is given by

where

Conversion to cylindrical coordinates is achieved using and

where "ρ" is the perpendicular distance from the "z"-axis. Then,

The field itself is a vector quantity:

where

This is "exactly" the field of a point dipole, "exactly" the dipole term in the multipole expansion of an arbitrary field, and "approximately" the field of any dipole-like configuration at large distances.

The vector potential A of a magnetic dipole is

with the same definitions as above.

The electrostatic potential at position r due to an electric dipole at the origin is given by:

where

This term appears as the second term in the multipole expansion of an arbitrary electrostatic potential Φ(r). If the source of Φ(r) is a dipole, as it is assumed here, this term is the only non-vanishing term in the multipole expansion of Φ(r). The electric field from a dipole can be found from the gradient of this potential:

where E is the electric field and "δ" is the 3-dimensional delta function. This is formally identical to the magnetic H field of a point magnetic dipole with only a few names changed.

Since the direction of an electric field is defined as the direction of the force on a positive charge, electric field lines point away from a positive charge and toward a negative charge.

When placed in an electric or magnetic field, equal but opposite forces arise on each side of the dipole creating a torque :

for an electric dipole moment p (in coulomb-meters), or

for a magnetic dipole moment m (in ampere-square meters).

The resulting torque will tend to align the dipole with the applied field, which in the case of an electric dipole, yields a potential energy of

The energy of a magnetic dipole is similarly

In addition to dipoles in electrostatics, it is also common to consider an electric or magnetic dipole that is oscillating in time. It is an extension, or a more physical next-step, to spherical wave radiation.

In particular, consider a harmonically oscillating electric dipole, with angular frequency "ω" and a dipole moment "p" along the ẑ direction of the form

In vacuum, the exact field produced by this oscillating dipole can be derived using the retarded potential formulation as:

For  ≫ 1, the far-field takes the simpler form of a radiating "spherical" wave, but with angular dependence embedded in the cross-product:

The time-averaged Poynting vector

is not distributed isotropically, but concentrated around the directions lying perpendicular to the dipole moment, as a result of the non-spherical electric and magnetic waves. In fact, the spherical harmonic function (sin "θ") responsible for such toroidal angular distribution is precisely the "l" = 1 "p" wave.

The total time-average power radiated by the field can then be derived from the Poynting vector as

Notice that the dependence of the power on the fourth power of the frequency of the radiation is in accordance with the Rayleigh scattering, and the underlying effects why the sky consists of mainly blue colour.

A circular polarized dipole is described as a superposition of two linear dipoles.




</doc>
<doc id="8386" url="https://en.wikipedia.org/wiki?curid=8386" title="Dynamics">
Dynamics

Dynamics (from Greek δυναμικός "dynamikos" "powerful", from δύναμις "dynamis" "power") or dynamic may refer to:







</doc>
<doc id="8387" url="https://en.wikipedia.org/wiki?curid=8387" title="Draught beer">
Draught beer

Draught beer, also spelt draft, is beer served from a cask or keg rather than from a bottle or can. Draught beer served from a pressurised keg is also known as 

Until Joseph Bramah patented the beer engine in 1785, beer was served directly from the barrel and carried to the customer. The Old English "" ("carry; pull") developed into a series of related words including "drag", "draw", and "draught". By the time Bramah's beer pumps became popular, the use of the term "draught" to refer to the acts of serving or drinking beer was well established and transferred easily to beer served via the hand pumps. In time, the word came to be restricted to only such beer. The usual spelling is now "draught" in the United Kingdom, Ireland, Australia, and New Zealand and more commonly "draft" in North America, although it can be spelt either way. Regardless of spelling, the word is pronounced or depending on the region the speaker is from.

Canned draught is beer served from a pressurised container featuring a widget. Smooth flow (also known as cream flow, nitrokeg, or smooth) is the name brewers give to draught beers pressurised with a partial nitrogen gas blend.

In 1691, an article in the "London Gazette" mentioned John Lofting, who held a patent for a fire engine: "The said patentee has also projected a very useful engine for starting of beer, and other liquors which will draw from 20 to 30 barrels an hour, which are completely fixed with brass joints and screws at reasonable rates".

In the early 20th century, draught beer started to be served from pressurised containers. Artificial carbonation was introduced in the United Kingdom in 1936, with Watney’s experimental pasteurised beer Red Barrel. Though this method of serving beer did not take hold in the U.K. until the late 1950s, it did become the favored method in the rest of Europe, where it is known by such terms as "en pression". The carbonation method of serving beer subsequently spread to the rest of the world; by the early 1970s the term "draught beer" almost exclusively referred to beer served under pressure as opposed to the traditional cask or barrel beer.

In Britain, the Campaign for Real Ale (CAMRA) was founded in 1971 to protect traditional - unpressurised beer and brewing methods. The group devised the term "real ale" to differentiate between beer served from the cask and beer served under pressure. The term "real ale" has since been expanded to include bottle-conditioned beer.

Keg beer is often filtered and/or pasteurised, both of which are processes that render the yeast inactive.

In brewing parlance, a keg is different from a cask. A cask has a tap hole near the edge of the top, and a spile hole on the side used for conditioning the unfiltered and unpasteurised beer. A keg has a single opening in the centre of the top to which a flow pipe is attached. Kegs are artificially pressurised after fermentation with carbon dioxide or a mixture of carbon dioxide and nitrogen gas.

"Keg" has become a term of contempt used by some, particularly in Britain, since the 1960s when pasteurised draught beers started replacing traditional cask beers.

Keg beer was replacing traditional cask ale in all parts of the UK, primarily because it requires less care to handle. Since 1971, CAMRA has conducted a consumer campaign on behalf of those who prefer traditional cask beer. CAMRA has lobbied the British Parliament to ensure support for cask ale and microbreweries have sprung up to serve those consumers who prefer traditional cask beer.

Pressurised CO in the keg's headspace maintains carbonation in the beer. The CO pressure varies depending on the amount of CO already in the beer and the keg storage temperature. Occasionally the CO gas is blended with nitrogen gas. CO / nitrogen blends are used to allow a higher operating pressure in complex dispensing systems.

Nitrogen is used under high pressure when dispensing dry stouts (such as Guinness) and other creamy beers because it displaces CO to (artificially) form a rich tight head and a less carbonated taste. This makes the beer feel smooth on the palate and gives a foamy appearance. Premixed bottled gas for creamy beers is usually 75% nitrogen and 25% CO. This premixed gas which only works well with creamy beers is often referred to as Guinness Gas, Beer Gas, or Aligal. Using "Beer Gas" with other beer styles can cause the last 5% to 10% of the beer in each keg to taste very flat and lifeless. In the UK, the term "keg beer" would imply the beer is pasteurised, in contrast to unpasteurised cask ale. Some of the newer microbreweries may offer a nitro keg stout which is filtered but not pasteurized.

Cask beer should be stored and served at a cellar temperature of . Once a cask is opened, it should be consumed within three days. Keg beer is given additional cooling just prior to being served either by flash coolers or a remote cooler in the cellar. This chills the beer down to temperatures between .

The words "draft" and "draught" have been used as marketing terms to describe canned or bottled beers, implying that they taste and appear like beers from a cask or keg. Commercial brewers use this as a marketing tool although it is incorrect to call any beer not drawn from a cask or keg "draught". Two examples are Miller Genuine Draft, a pale lager which is produced using a cold filtering system, and Guinness stout in patented "Draught-flow" cans and bottles. Guinness is an example of beers that use a nitrogen widget to create a smooth beer with a very dense head. Guinness has recently replaced the widget system from their bottled "draught" beer with a coating of cellulose fibres on the inside of the bottle. Statements indicate a new development in bottling technology that enables the mixture of nitrogen and carbon dioxide to be present in the beer without using a widget, making it according to Guinness "more drinkable" from the bottle.

In some countries such as Japan, the term "draft" applied to canned or bottled beer indicates that the beer is not pasteurized (though it may be filtered), giving it a fresher taste but shorter shelf life than conventional packaged beers.




</doc>
<doc id="8388" url="https://en.wikipedia.org/wiki?curid=8388" title="Director">
Director

Director may refer to:










</doc>
<doc id="8389" url="https://en.wikipedia.org/wiki?curid=8389" title="Major depressive disorder">
Major depressive disorder

Major depressive disorder (MDD), also known simply as depression, is a mental disorder characterized by at least two weeks of low mood that is present across most situations. It is often accompanied by low self-esteem, loss of interest in normally enjoyable activities, low energy, and pain without a clear cause. People may also occasionally have false beliefs or see or hear things that others cannot. Some people have periods of depression separated by years in which they are normal, while others nearly always have symptoms present. Major depressive disorder can negatively affect a person's personal, work, or school life as well as sleeping, eating habits, and general health. Between 2–7% of adults with major depression die by suicide, and up to 60% of people who die by suicide had depression or another mood disorder.
The cause is believed to be a combination of genetic, environmental, and psychological factors. Risk factors include a family history of the condition, major life changes, certain medications, chronic health problems, and substance abuse. About 40% of the risk appears to be related to genetics. The diagnosis of major depressive disorder is based on the person's reported experiences and a mental status examination. There is no laboratory test for major depression. Testing, however, may be done to rule out physical conditions that can cause similar symptoms. Major depression should be differentiated from sadness, which is a normal part of life and is less severe. The United States Preventive Services Task Force (USPSTF) recommends screening for depression among those over the age 12, while a prior Cochrane review found that the routine use of screening questionnaires have little effect on detection or treatment.
Typically, people are treated with counseling and antidepressant medication. Medication appears to be effective, but the effect may only be significant in the most severely depressed. It is unclear whether medications affect the risk of suicide. Types of counseling used include cognitive behavioral therapy (CBT) and interpersonal therapy. If other measures are not effective, electroconvulsive therapy (ECT) may be tried. Hospitalization may be necessary in cases with a risk of harm to self and may occasionally occur against a person's wishes.
Major depressive disorder affected approximately 216 million people (3% of the world's population) in 2015. The percentage of people who are affected at one point in their life varies from 7% in Japan to 21% in France. Lifetime rates are higher in the developed world (15%) compared to the developing world (11%). It causes the second most years lived with disability, after low back pain. The most common time of onset is in a person's 20s and 30s. Females are affected about twice as often as males. The American Psychiatric Association added "major depressive disorder" to the "Diagnostic and Statistical Manual of Mental Disorders" (DSM-III) in 1980. It was a split of the previous depressive neurosis in the DSM-II, which also encompassed the conditions now known as dysthymia and adjustment disorder with depressed mood. Those currently or previously affected may be stigmatized.

Major depression significantly affects a person's family and personal relationships, work or school life, sleeping and eating habits, and general health. Its impact on functioning and well-being has been compared to that of other chronic medical conditions such as diabetes.

A person having a major depressive episode usually exhibits a very low mood, which pervades all aspects of life, and an inability to experience pleasure in activities that were formerly enjoyed. Depressed people may be preoccupied with, or ruminate over, thoughts and feelings of worthlessness, inappropriate guilt or regret, helplessness, hopelessness, and self-hatred. In severe cases, depressed people may have symptoms of psychosis. These symptoms include delusions or, less commonly, hallucinations, usually unpleasant. Other symptoms of depression include poor concentration and memory (especially in those with melancholic or psychotic features), withdrawal from social situations and activities, reduced sex drive, irritability, and thoughts of death or suicide. Insomnia is common among the depressed. In the typical pattern, a person wakes very early and cannot get back to sleep. Hypersomnia, or oversleeping, can also happen. Some antidepressants may also cause insomnia due to their stimulating effect.

A depressed person may report multiple physical symptoms such as fatigue, headaches, or digestive problems; physical complaints are the most common presenting problem in developing countries, according to the World Health Organization's criteria for depression. Appetite often decreases, with resulting weight loss, although increased appetite and weight gain occasionally occur. Family and friends may notice that the person's behavior is either agitated or lethargic. Older depressed people may have cognitive symptoms of recent onset, such as forgetfulness, and a more noticeable slowing of movements. Depression often coexists with physical disorders common among the elderly, such as stroke, other cardiovascular diseases, Parkinson's disease, and chronic obstructive pulmonary disease.

Depressed children may often display an irritable mood rather than a depressed mood, and show varying symptoms depending on age and situation. Most lose interest in school and show a decline in academic performance. They may be described as clingy, demanding, dependent, or insecure. Diagnosis may be delayed or missed when symptoms are interpreted as normal moodiness.

Major depression frequently co-occurs with other psychiatric problems. The 1990–92 "National Comorbidity Survey" (US) reports that half of those with major depression also have lifetime anxiety and its associated disorders such as generalized anxiety disorder. Anxiety symptoms can have a major impact on the course of a depressive illness, with delayed recovery, increased risk of relapse, greater disability and increased suicide attempts. There are increased rates of alcohol and drug abuse and particularly dependence, and around a third of individuals diagnosed with ADHD develop comorbid depression. Post-traumatic stress disorder and depression often co-occur. Depression may also coexist with attention deficit hyperactivity disorder (ADHD), complicating the diagnosis and treatment of both. Depression is also frequently comorbid with alcohol abuse and personality disorders.

Depression and pain often co-occur. One or more pain symptoms are present in 65% of depressed patients, and anywhere from 5 to 85% of patients with pain will be suffering from depression, depending on the setting; there is a lower prevalence in general practice, and higher in specialty clinics. The diagnosis of depression is often delayed or missed, and the outcome can worsen if the depression is noticed but completely misunderstood.

Depression is also associated with a 1.5- to 2-fold increased risk of cardiovascular disease, independent of other known risk factors, and is itself linked directly or indirectly to risk factors such as smoking and obesity. People with major depression are less likely to follow medical recommendations for treating and preventing cardiovascular disorders, which further increases their risk of medical complications. In addition, cardiologists may not recognize underlying depression that complicates a cardiovascular problem under their care.

The cause of major depressive disorder is unknown. The biopsychosocial model proposes that biological, psychological, and social factors all play a role in causing depression. The diathesis–stress model specifies that depression results when a preexisting vulnerability, or diathesis, is activated by stressful life events. The preexisting vulnerability can be either genetic, implying an interaction between nature and nurture, or schematic, resulting from views of the world learned in childhood.

Childhood abuse, either physical, sexual or psychological, are all risk factors for depression, among other psychiatric issues that co-occur such as anxiety and drug abuse. Childhood trauma also correlates with severity of depression, lack of response to treatment and length of illness. However, some are more susceptible to developing mental illness such as depression after trauma, and various genes have been suggested to control susceptibility.

The 5-HTTLPR, or serotonin transporter promoter gene's short allele has been associated with increased risk of depression. However, since the 1990s results have been inconsistent, with three recent reviews finding an effect and two finding none. Other genes that have been linked to a gene-environment interaction include CRHR1, FKBP5 and BDNF, the first two of which are related to the stress reaction of the HPA axis, and the latter of which is involved in neurogenesis. A 2018 study found 44 areas within the chromosomes that were linked to MDD.

Depression may also come secondary to a chronic or terminal medical condition such as HIV/AIDS, or asthma and may be labeled "secondary depression". It is unknown if the underlying diseases induce depression through effect on quality of life, of through shared etiologies (such as degeneration of the basal ganglia in parkinson's disease or immune dysregulation in asthma). Depression may also be iatrogenic (the result of healthcare), such as drug induced depression. Therapies associated with depression include interferon therapy, beta-blockers, Isotretinoin, contraceptives, cardiac agents, anticonvulsants, antimigraine drugs, antipsychotics, and hormonal agents agents such as gonadotropin-releasing hormone agonist. Drug abuse in early age is also associated with increased risk of developing depression later in life. Depression that occurs as a result of pregnancy is called postpartum depression, and is thought to be the result of hormonal changes associated with pregnancy. Seasonal affective disorder, a type of depression associated with seasonal changes in sunlight, is thought to be the result of decreased sunlight.

The pathophysiology of depression is not yet understood, but the current theories center around monoaminergic systems, the circadian rhythm, immunological dysfunction, HPA axis dysfunction and structural or functional abnormalities of emotional circuits.

The monoamine theory, derived from the efficacy of monoaminergic drugs in treating depression, was the dominant theory until recently. The theory postulates that insufficient activity of monoamine neurotransmitters is the primary cause of depression. Evidence for the monoamine theory comes from multiple areas. Firstly, acute depletion of tryptophan, a necessary precursor of serotonin, a monoamine, can cause depression in those in remission or relatives of depressed patients; this suggests that decreased serotonergic neurotransmission is important in depression. Secondly, the correlation between depression risk and polymorphisms in the 5-HTTLPR gene, which codes for serotonin receptors, suggests a link. Third, decreased size of the locus coeruleus, decreased activity of tyrosine hydroxylase, increased density of alpha-2 adrenergic receptor, and evidence from rat models suggest decreased adrenergic neurotransmission in depression. Furthermore, decreased levels of homovanillic acid, altered response to dextroamphetamine, responses of depressive symptoms to dopamine receptor agonists, decreased dopamine receptor D1 binding in the striatum, and polymorphism of dopamine receptor genes implicate dopamine in depression. Lastly, increased activity of monoamine oxidase, which degrades monoamines, has been associated with depression. However, this theory is inconsistent with the fact that serotonin depletion does not cause depression in healthy persons, the fact that antidepressants instantly increase levels of monoamines but take weeks to work, and the existence of atypical antidepressants which can be effective despite not targeting this pathway. One proposed explanation for the therapeutic lag, and further support for the deficiency of monoamines, is a desensitization of self-inhibition in raphe nuclei by the increased serotonin mediated by antidepressants. However, disinhibition of the dorsal raphe has been proposed to occur as a result of "decreased" serotonergic activity in tryptophan depletion, resulting in a depressed state mediated by increased serotonin. Further countering the monoamine hypothesis is the fact that rats with lesions of the dorsal raphe are not more depressive that controls, the finding of increased jugular 5-HIAA in depressed patients that normalized with SSRI treatment, and the preference for carbohydrates in depressed patients. Already limited, the monoamine hypothesis has been further oversimplified when presented to the general public.

Immune system abnormalities have been observed, including increased levels of cytokines involved in generating sickness behavior (which shares overlap with depression). The effectiveness of nonsteroidal anti-inflammatory drugs (NSAIDs) and cytokine inhibitors in treating depression, and normalization of cytokine levels after successful treatment further suggest immune system abnormalities in depression.

HPA axis abnormalities have been suggested in depression given the association of CRHR1 with depression and the increased frequency of dexamethasone test non-suppression in depressed patients. However, this abnormality is not adequate as a diagnosis tool, because its sensitivity is only 44%. These stress-related abnormalities have been hypothesized to be the cause of hippocampal volume reductions seen in depressed patients. Furthermore, a meta-analysis yielded decreased dexamethasone suppression, and increased response to psychological stressors. Further abnormal results have been obscured with the cortisol awakening response, with increased response being associated with depression.

Theories unifying neuroimaging findings have been proposed. The first model proposed is the "Limbic Cortical Model", which involves hyperactivity of the ventral paralimbic regions and hypoactivity of frontal regulatory regions in emotional processing. Another model, the "Corito-Striatal model", suggests that abnormalities of the prefrontal cortex in regulating striatal and subcortical structures results in depression. Another model proposes hyperactivity of salience structures in identifying negative stimuli, and hypoactivity of cortical regulatory structures resulting in a negative emotional bias and depression, consistent with emotional bias studies.

A diagnostic assessment may be conducted by a suitably trained general practitioner, or by a psychiatrist or psychologist, who records the person's current circumstances, biographical history, current symptoms, and family history. The broad clinical aim is to formulate the relevant biological, psychological, and social factors that may be impacting on the individual's mood. The assessor may also discuss the person's current ways of regulating mood (healthy or otherwise) such as alcohol and drug use. The assessment also includes a mental state examination, which is an assessment of the person's current mood and thought content, in particular the presence of themes of hopelessness or pessimism, self-harm or suicide, and an absence of positive thoughts or plans. Specialist mental health services are rare in rural areas, and thus diagnosis and management is left largely to primary-care clinicians. This issue is even more marked in developing countries. The mental health examination may include the use of a rating scale such as the Hamilton Rating Scale for Depression or the Beck Depression Inventory or the Suicide Behaviors Questionnaire-Revised. The score on a rating scale alone is insufficient to diagnose depression to the satisfaction of the DSM or ICD, but it provides an indication of the severity of symptoms for a time period, so a person who scores above a given cut-off point can be more thoroughly evaluated for a depressive disorder diagnosis. Several rating scales are used for this purpose.

Primary-care physicians and other non-psychiatrist physicians have more difficulty with underrecognition and undertreatment of depression compared to psychiatric physicians, in part because of the physical symptoms that often accompany depression, in addition to the many potential patient, provider, and system barriers that the authors describe. A review found that non-psychiatrist physicians miss about two-thirds of cases, though this has improved somewhat in more recent studies.

Before diagnosing a major depressive disorder, in general a doctor performs a medical examination and selected investigations to rule out other causes of symptoms. These include blood tests measuring TSH and thyroxine to exclude hypothyroidism; basic electrolytes and serum calcium to rule out a metabolic disturbance; and a full blood count including ESR to rule out a systemic infection or chronic disease. Adverse affective reactions to medications or alcohol misuse are often ruled out, as well. Testosterone levels may be evaluated to diagnose hypogonadism, a cause of depression in men. Vitamin D levels might be evaluated, as low levels of vitamin D have been associated with greater risk for depression.

Subjective cognitive complaints appear in older depressed people, but they can also be indicative of the onset of a dementing disorder, such as Alzheimer's disease. Cognitive testing and brain imaging can help distinguish depression from dementia. A CT scan can exclude brain pathology in those with psychotic, rapid-onset or otherwise unusual symptoms. In general, investigations are not repeated for a subsequent episode unless there is a medical indication.

No biological tests confirm major depression. Biomarkers of depression have been sought to provide an objective method of diagnosis. There are several potential biomarkers, including Brain-Derived Neurotrophic Factor and various functional MRI techniques. One study developed a decision tree model of interpreting a series of fMRI scans taken during various activities. In their subjects, the authors of that study were able to achieve a sensitivity of 80% and a specificity of 87%, corresponding to a negative predictive value of 98% and a positive predictive value of 32% (positive and negative likelihood ratios were 6.15, 0.23, respectively). However, much more research is needed before these tests could be used clinically.

The most widely used criteria for diagnosing depressive conditions are found in the American Psychiatric Association's revised fourth edition of the "Diagnostic and Statistical Manual of Mental Disorders" (DSM-IV-TR), and the World Health Organization's "International Statistical Classification of Diseases and Related Health Problems" (ICD-10), which uses the name "depressive episode" for a single episode and "recurrent depressive disorder" for repeated episodes. The latter system is typically used in European countries, while the former is used in the US and many other non-European nations, and the authors of both have worked towards conforming one with the other.

Both DSM-IV-TR and ICD-10 mark out typical (main) depressive symptoms. ICD-10 defines three typical depressive symptoms (depressed mood, anhedonia, and reduced energy), two of which should be present to determine depressive disorder diagnosis. According to DSM-IV-TR, there are two main depressive symptoms—depressed mood and anhedonia. At least one of these must be present to make a diagnosis of major depressive episode.

Major depressive disorder is classified as a mood disorder in DSM-IV-TR. The diagnosis hinges on the presence of single or recurrent major depressive episodes. Further qualifiers are used to classify both the episode itself and the course of the disorder. The category Depressive Disorder Not Otherwise Specified is diagnosed if the depressive episode's manifestation does not meet the criteria for a major depressive episode. The ICD-10 system does not use the term "major depressive disorder" but lists very similar criteria for the diagnosis of a depressive episode (mild, moderate or severe); the term "recurrent" may be added if there have been multiple episodes without mania.

A major depressive episode is characterized by the presence of a severely depressed mood that persists for at least two weeks. Episodes may be isolated or recurrent and are categorized as mild (few symptoms in excess of minimum criteria), moderate, or severe (marked impact on social or occupational functioning). An episode with psychotic features—commonly referred to as "psychotic depression"—is automatically rated as severe. If the patient has had an episode of mania or markedly elevated mood, a diagnosis of bipolar disorder is made instead. Depression without mania is sometimes referred to as "unipolar" because the mood remains at one emotional state or "pole".

DSM-IV-TR excludes cases where the symptoms are a result of bereavement, although it is possible for normal bereavement to evolve into a depressive episode if the mood persists and the characteristic features of a major depressive episode develop. The criteria have been criticized because they do not take into account any other aspects of the personal and social context in which depression can occur. In addition, some studies have found little empirical support for the DSM-IV cut-off criteria, indicating they are a diagnostic convention imposed on a continuum of depressive symptoms of varying severity and duration: Excluded are a range of related diagnoses, including dysthymia, which involves a chronic but milder mood disturbance; recurrent brief depression, consisting of briefer depressive episodes; minor depressive disorder, whereby only some symptoms of major depression are present; and adjustment disorder with depressed mood, which denotes low mood resulting from a psychological response to an identifiable event or stressor.

The DSM-IV-TR recognizes five further subtypes of MDD, called "specifiers", in addition to noting the length, severity and presence of psychotic features:

In 2016, the United States Preventive Services Task Force (USPSTF) recommended screening in the adult populations with evidence that it increases the detection of people with depression and with proper treatment improves outcomes. They recommend screening in those between the age of 12 to 18 as well.

A Cochrane review from 2005 found screening programs do not significantly improve detection rates, treatment, or outcome.

To confirm major depressive disorder as the most likely diagnosis, other potential diagnoses must be considered, including dysthymia, adjustment disorder with depressed mood, or bipolar disorder. Dysthymia is a chronic, milder mood disturbance in which a person reports a low mood almost daily over a span of at least two years. The symptoms are not as severe as those for major depression, although people with dysthymia are vulnerable to secondary episodes of major depression (sometimes referred to as "double depression"). Adjustment disorder with depressed mood is a mood disturbance appearing as a psychological response to an identifiable event or stressor, in which the resulting emotional or behavioral symptoms are significant but do not meet the criteria for a major depressive episode. Bipolar disorder, also known as "manic–depressive disorder", is a condition in which depressive phases alternate with periods of mania or hypomania. Although depression is currently categorized as a separate disorder, there is ongoing debate because individuals diagnosed with major depression often experience some hypomanic symptoms, indicating a mood disorder continuum. Further differential diagnoses involve chronic fatigue syndrome.

Other disorders need to be ruled out before diagnosing major depressive disorder. They include depressions due to physical illness, medications, and substance abuse. Depression due to physical illness is diagnosed as a Mood disorder due to a general medical condition. This condition is determined based on history, laboratory findings, or physical examination. When the depression is caused by a medication, drug of abuse, or exposure to a toxin, it is then diagnosed as a specific mood disorder (previously called "Substance-induced mood disorder" in the DSM-IV-TR).

Preventative efforts may result in decreases in rates of the condition of between 22 and 38%. Eating large amounts of fish may also reduce the risk.

Behavioral interventions, such as interpersonal therapy and cognitive-behavioral therapy, are effective at preventing new onset depression. Because such interventions appear to be most effective when delivered to individuals or small groups, it has been suggested that they may be able to reach their large target audience most efficiently through the Internet.

However, an earlier meta-analysis found preventive programs with a competence-enhancing component to be superior to behavior-oriented programs overall, and found behavioral programs to be particularly unhelpful for older people, for whom social support programs were uniquely beneficial. In addition, the programs that best prevented depression comprised more than eight sessions, each lasting between 60 and 90 minutes, were provided by a combination of lay and professional workers, had a high-quality research design, reported attrition rates, and had a well-defined intervention.

The Netherlands mental health care system provides preventive interventions, such as the "Coping with Depression" course (CWD) for people with sub-threshold depression. The course is claimed to be the most successful of psychoeducational interventions for the treatment and prevention of depression (both for its adaptability to various populations and its results), with a risk reduction of 38% in major depression and an efficacy as a treatment comparing favorably to other psychotherapies.

The three most common treatments for depression are psychotherapy, medication, and electroconvulsive therapy. Psychotherapy is the treatment of choice (over medication) for people under 18. The UK National Institute for Health and Care Excellence (NICE) 2004 guidelines indicate that antidepressants should not be used for the initial treatment of mild depression, because the risk-benefit ratio is poor. The guidelines recommend that antidepressants treatment in combination with psychosocial interventions should be considered for:

The guidelines further note that antidepressant treatment should be continued for at least six months to reduce the risk of relapse, and that SSRIs are better tolerated than tricyclic antidepressants.

American Psychiatric Association treatment guidelines recommend that initial treatment should be individually tailored based on factors including severity of symptoms, co-existing disorders, prior treatment experience, and patient preference. Options may include pharmacotherapy, psychotherapy, exercise, electroconvulsive therapy (ECT), transcranial magnetic stimulation (TMS) or light therapy. Antidepressant medication is recommended as an initial treatment choice in people with mild, moderate, or severe major depression, and should be given to all patients with severe depression unless ECT is planned. There is evidence that collaborative care by a team of health care practitioners produces better results than routine single-practitioner care.

Treatment options are much more limited in developing countries, where access to mental health staff, medication, and psychotherapy is often difficult. Development of mental health services is minimal in many countries; depression is viewed as a phenomenon of the developed world despite evidence to the contrary, and not as an inherently life-threatening condition. A 2014 Cochrane review found insufficient evidence to determine the effectiveness of psychological versus medical therapy in children.

Physical exercise is recommended for management of mild depression, and has a moderate effect on symptoms. Exercise has also been found to be effective for (unipolar) major depression. It is equivalent to the use of medications or psychological therapies in most people. In older people it does appear to decrease depression. Exercise may be recommended to people who are willing, motivated, and physically healthy enough to participate in an exercise program as treatment.

There is a small amount of evidence that skipping a night's sleep may improve depressive symptoms, with the effects usually showing up within a day. This effect is usually temporary. Besides sleepiness, this method can cause a side effect of mania or hypomania.

In observational studies smoking cessation has benefits in depression as large as or larger than those of medications.

Besides exercise, sleep and diet may play a role in depression, and interventions in these areas may be an effective add-on to conventional methods.

Psychotherapy can be delivered to individuals, groups, or families by mental health professionals. A 2015 review found that cognitive behavioral therapy appears to be similar to antidepressant medication in terms of effect. A 2012 review found psychotherapy to be better than no treatment but not other treatments. With more complex and chronic forms of depression, a combination of medication and psychotherapy may be used. A 2014 Cochrane review found that work-directed interventions combined with clinical interventions helped to reduce sick days taken by people with depression. There is moderate-quality evidence that psychological therapies are a useful addition to standard antidepressant treatment of treatment-resistant depression in the short term.

Psychotherapy has been shown to be effective in older people. Successful psychotherapy appears to reduce the recurrence of depression even after it has been terminated or replaced by occasional booster sessions.

Cognitive behavioral therapy (CBT) currently has the most research evidence for the treatment of depression in children and adolescents, and CBT and interpersonal psychotherapy (IPT) are preferred therapies for adolescent depression. In people under 18, according to the National Institute for Health and Clinical Excellence, medication should be offered only in conjunction with a psychological therapy, such as CBT, interpersonal therapy, or family therapy. Cognitive behavioral therapy has also been shown to reduce the number of sick days taken by people with depression, when used in conjunction with primary care.

The most-studied form of psychotherapy for depression is CBT, which teaches clients to challenge self-defeating, but enduring ways of thinking (cognitions) and change counter-productive behaviors. Research beginning in the mid-1990s suggested that CBT could perform as well as or better than antidepressants in patients with moderate to severe depression. CBT may be effective in depressed adolescents, although its effects on severe episodes are not definitively known. Several variables predict success for cognitive behavioral therapy in adolescents: higher levels of rational thoughts, less hopelessness, fewer negative thoughts, and fewer cognitive distortions. CBT is particularly beneficial in preventing relapse.

Cognitive behavioral therapy and occupational programs (including modification of work activities and assistance) have been shown to be effective in reducing sick days taken by workers with depression.

Several variants of cognitive behavior therapy have been used in those with depression, the most notable being rational emotive behavior therapy, and mindfulness-based cognitive therapy. Mindfulness based stress reduction programs may reduce depression symptoms. Mindfulness programs also appear to be a promising intervention in youth.

Psychoanalysis is a school of thought, founded by Sigmund Freud, which emphasizes the resolution of unconscious mental conflicts. Psychoanalytic techniques are used by some practitioners to treat clients presenting with major depression. A more widely practiced therapy, called psychodynamic psychotherapy, is in the tradition of psychoanalysis but less intensive, meeting once or twice a week. It also tends to focus more on the person's immediate problems, and has an additional social and interpersonal focus. In a meta-analysis of three controlled trials of Short Psychodynamic Supportive Psychotherapy, this modification was found to be as effective as medication for mild to moderate depression.

Conflicting results have arisen from studies that look at the effectiveness of antidepressants in people with acute, mild to moderate depression. Stronger evidence supports the usefulness of antidepressants in the treatment of depression that is chronic (dysthymia) or severe.

While small benefits were found, researchers Irving Kirsch and Thomas Moore state they may be due to issues with the trials rather than a true effect of the medication. In a later publication, Kirsch concluded that the overall effect of new-generation antidepressant medication is below recommended criteria for clinical significance. Similar results were obtained in a meta analysis by Fornier.

A review commissioned by the National Institute for Health and Care Excellence concluded that there is strong evidence that SSRIs have greater efficacy than placebo on achieving a 50% reduction in depression scores in moderate and severe major depression, and that there is some evidence for a similar effect in mild depression. Similarly, a Cochrane systematic review of clinical trials of the generic tricyclic antidepressant amitriptyline concluded that there is strong evidence that its efficacy is superior to placebo.

In 2014 the U.S. FDA published a systematic review of all antidepressant maintenance trials submitted to the agency between 1985 and 2012. The authors concluded that maintenance treatment reduced the risk of relapse by 52% compared to placebo, and that this effect was primarily due to recurrent depression in the placebo group rather than a drug withdrawal effect.

To find the most effective antidepressant medication with minimal side-effects, the dosages can be adjusted, and if necessary, combinations of different classes of antidepressants can be tried. Response rates to the first antidepressant administered range from 50–75%, and it can take at least six to eight weeks from the start of medication to remission. Antidepressant medication treatment is usually continued for 16 to 20 weeks after remission, to minimize the chance of recurrence, and even up to one year of continuation is recommended. People with chronic depression may need to take medication indefinitely to avoid relapse.

Selective serotonin reuptake inhibitors (SSRIs) are the primary medications prescribed, owing to their relatively mild side-effects, and because they are less toxic in overdose than other antidepressants. People who do not respond to one SSRI can be switched to another antidepressant, and this results in improvement in almost 50% of cases. Another option is to switch to the atypical antidepressant bupropion. Venlafaxine, an antidepressant with a different mechanism of action, may be modestly more effective than SSRIs. However, venlafaxine is not recommended in the UK as a first-line treatment because of evidence suggesting its risks may outweigh benefits, and it is specifically discouraged in children and adolescents.
For child and adolescent depression, fluoxetine is recommended if medication are used. Fluoxetine; however, appears to have only slight benefit in children, while other antidepressants have not been shown to be effective. Medications are not recommended in children with mild disease. There is also insufficient evidence to determine effectiveness in those with depression complicated by dementia. Any antidepressant can cause low blood sodium levels ; nevertheless, it has been reported more often with SSRIs. It is not uncommon for SSRIs to cause or worsen insomnia; the sedating antidepressant mirtazapine can be used in such cases.

Irreversible monoamine oxidase inhibitors, an older class of antidepressants, have been plagued by potentially life-threatening dietary and drug interactions. They are still used only rarely, although newer and better-tolerated agents of this class have been developed. The safety profile is different with reversible monoamine oxidase inhibitors such as moclobemide where the risk of serious dietary interactions is negligible and dietary restrictions are less strict.
For children, adolescents, and probably young adults between 18 and 24 years old, there is a higher risk of both suicidal ideations and suicidal behavior in those treated with SSRIs. For adults, it is unclear whether SSRIs affect the risk of suicidality. One review found no connection; another an increased risk; and a third no risk in those 25–65 years old and a decrease risk in those more than 65. A black box warning was introduced in the United States in 2007 on SSRI and other antidepressant medications due to increased risk of suicide in patients younger than 24 years old. Similar precautionary notice revisions were implemented by the Japanese Ministry of Health.

There is some evidence that omega-3 fatty acids fish oil supplements containing high levels of eicosapentaenoic acid (EPA) to docosahexaenoic acid (DHA) are effective in the treatment of, but not the prevention of major depression. However, a Cochrane review determined there was insufficient high quality evidence to suggest Omega-3 fatty acids were effective in depression. There is limited evidence that vitamin D supplementation is of value in alleviating the symptoms of depression in individuals who are vitamin D deficient. There is some preliminary evidence that COX-2 inhibitors have a beneficial effect on major depression. Lithium appears effective at lowering the risk of suicide in those with bipolar disorder and unipolar depression to nearly the same levels as the general population. There is a narrow range of effective and safe dosages of lithium thus close monitoring may be needed. Low-dose thyroid hormone may be added to existing antidepressants to treat persistent depression symptoms in people who have tried multiple courses of medication. Limited evidence suggests stimulants such as amphetamine and modafinil may be effective in the short term, or as add on therapy. Also, it is suggested that folate supplement may have a role in depression management.

Electroconvulsive therapy (ECT) is a standard psychiatric treatment in which seizures are electrically induced in patients to provide relief from psychiatric illnesses. ECT is used with informed consent as a last line of intervention for major depressive disorder.

A round of ECT is effective for about 50% of people with treatment-resistant major depressive disorder, whether it is unipolar or bipolar. Follow-up treatment is still poorly studied, but about half of people who respond relapse within twelve months.

Aside from effects in the brain, the general physical risks of ECT are similar to those of brief general anesthesia. Immediately following treatment, the most common adverse effects are confusion and memory loss. ECT is considered one of the least harmful treatment options available for severely depressed pregnant women.

A usual course of ECT involves multiple administrations, typically given two or three times per week until the patient is no longer suffering symptoms. ECT is administered under anesthetic with a muscle relaxant. Electroconvulsive therapy can differ in its application in three ways: electrode placement, frequency of treatments, and the electrical waveform of the stimulus. These three forms of application have significant differences in both adverse side effects and symptom remission. After treatment, drug therapy is usually continued, and some patients receive maintenance ECT.

ECT appears to work in the short term via an anticonvulsant effect mostly in the frontal lobes, and longer term via neurotrophic effects primarily in the medial temporal lobe.

Transcranial magnetic stimulation (TMS) or deep transcranial magnetic stimulation is a noninvasive method used to stimulate small regions of the brain. TMS was approved by the FDA for treatment-resistant major depressive disorder in 2008 and as of 2014 evidence supports that it is probably effective. The American Psychiatric Association the Canadian Network for Mood and Anxiety Disorders, and the Royal Australia and New Zealand College of Psychiatrists have endorsed rTMS for trMDD.

Bright light therapy reduces depression symptom severity, with benefit was found for both seasonal affective disorder and for nonseasonal depression, and an effect similar to those for conventional antidepressants. For non-seasonal depression, adding light therapy to the standard antidepressant treatment was not effective. For non-seasonal depression where light was used mostly in combination with antidepressants or wake therapy a moderate effect was found, with response better than control treatment in high-quality studies, in studies that applied morning light treatment, and with people who respond to total or partial sleep deprivation. Both analyses noted poor quality, short duration, and small size of most of the reviewed studies. There is insufficient evidence for Reiki and dance movement therapy in depression.

Major depressive episodes often resolve over time whether or not they are treated. Outpatients on a waiting list show a 10–15% reduction in symptoms within a few months, with approximately 20% no longer meeting the full criteria for a depressive disorder. The median duration of an episode has been estimated to be 23 weeks, with the highest rate of recovery in the first three months.

Studies have shown that 80% of those suffering from their first major depressive episode will suffer from at least 1 more during their life, with a lifetime average of 4 episodes. Other general population studies indicate that around half those who have an episode recover (whether treated or not) and remain well, while the other half will have at least one more, and around 15% of those experience chronic recurrence. Studies recruiting from selective inpatient sources suggest lower recovery and higher chronicity, while studies of mostly outpatients show that nearly all recover, with a median episode duration of 11 months. Around 90% of those with severe or psychotic depression, most of whom also meet criteria for other mental disorders, experience recurrence.

Recurrence is more likely if symptoms have not fully resolved with treatment. Current guidelines recommend continuing antidepressants for four to six months after remission to prevent relapse. Evidence from many randomized controlled trials indicates continuing antidepressant medications after recovery can reduce the chance of relapse by 70% (41% on placebo vs. 18% on antidepressant). The preventive effect probably lasts for at least the first 36 months of use.

Those people experiencing repeated episodes of depression require ongoing treatment in order to prevent more severe, long-term depression. In some cases, people must take medications for long periods of time or for the rest of their lives.

Cases when outcome is poor are associated with inappropriate treatment, severe initial symptoms that may include psychosis, early age of onset, more previous episodes, incomplete recovery after 1 year, pre-existing severe mental or medical disorder, and family dysfunction as well.

Depressed individuals have a shorter life expectancy than those without depression, in part because depressed patients are at risk of dying by suicide. However, they also have a higher rate of dying from other causes, being more susceptible to medical conditions such as heart disease. Up to 60% of people who die by suicide have a mood disorder such as major depression, and the risk is especially high if a person has a marked sense of hopelessness or has both depression and borderline personality disorder. The lifetime risk of suicide associated with a diagnosis of major depression in the US is estimated at 3.4%, which averages two highly disparate figures of almost 7% for men and 1% for women (although suicide attempts are more frequent in women). The estimate is substantially lower than a previously accepted figure of 15%, which had been derived from older studies of hospitalized patients.

Depression is often associated with unemployment and poverty. Major depression is currently the leading cause of disease burden in North America and other high-income countries, and the fourth-leading cause worldwide. In the year 2030, it is predicted to be the second-leading cause of disease burden worldwide after HIV, according to the World Health Organization. Delay or failure in seeking treatment after relapse and the failure of health professionals to provide treatment are two barriers to reducing disability.

Major depressive disorder affects approximately 216 million people in 2015 (3% of the global population). The percentage of people who are affected at one point in their life varies from 7% in Japan to 21% in France. In most countries the number of people who have depression during their lives falls within an 8–18% range. In North America, the probability of having a major depressive episode within a year-long period is 3–5% for males and 8–10% for females. Major depression to be about twice as common in women as in men, although it is unclear why this is so, and whether factors unaccounted for are contributing to this. The relative increase in occurrence is related to pubertal development rather than chronological age, reaches adult ratios between the ages of 15 and 18, and appears associated with psychosocial more than hormonal factors. Depression is a major cause of disability worldwide.

People are most likely to develop their first depressive episode between the ages of 30 and 40, and there is a second, smaller peak of incidence between ages 50 and 60. The risk of major depression is increased with neurological conditions such as stroke, Parkinson's disease, or multiple sclerosis, and during the first year after childbirth. It is also more common after cardiovascular illnesses, and is related more to a poor outcome than to a better one. Studies conflict on the prevalence of depression in the elderly, but most data suggest there is a reduction in this age group. Depressive disorders are more common to observe in urban than in rural population and the prevalence is in groups with stronger socioeconomic factors i.e. homelessness.

The Ancient Greek physician Hippocrates described a syndrome of melancholia as a distinct disease with particular mental and physical symptoms; he characterized all "fears and despondencies, if they last a long time" as being symptomatic of the ailment. It was a similar but far broader concept than today's depression; prominence was given to a clustering of the symptoms of sadness, dejection, and despondency, and often fear, anger, delusions and obsessions were included.

The term "depression" itself was derived from the Latin verb "deprimere", "to press down". From the 14th century, "to depress" meant to subjugate or to bring down in spirits. It was used in 1665 in English author Richard Baker's "Chronicle" to refer to someone having "a great depression of spirit", and by English author Samuel Johnson in a similar sense in 1753. The term also came into use in physiology and economics. An early usage referring to a psychiatric symptom was by French psychiatrist Louis Delasiauve in 1856, and by the 1860s it was appearing in medical dictionaries to refer to a physiological and metaphorical lowering of emotional function. Since Aristotle, melancholia had been associated with men of learning and intellectual brilliance, a hazard of contemplation and creativity. The newer concept abandoned these associations and through the 19th century, became more associated with women.
Although "melancholia" remained the dominant diagnostic term, "depression" gained increasing currency in medical treatises and was a synonym by the end of the century; German psychiatrist Emil Kraepelin may have been the first to use it as the overarching term, referring to different kinds of melancholia as "depressive states".

Sigmund Freud likened the state of melancholia to mourning in his 1917 paper "Mourning and Melancholia". He theorized that objective loss, such as the loss of a valued relationship through death or a romantic break-up, results in subjective loss as well; the depressed individual has identified with the object of affection through an unconscious, narcissistic process called the "libidinal cathexis" of the ego. Such loss results in severe melancholic symptoms more profound than mourning; not only is the outside world viewed negatively but the ego itself is compromised. The patient's decline of self-perception is revealed in his belief of his own blame, inferiority, and unworthiness. He also emphasized early life experiences as a predisposing factor. Adolf Meyer put forward a mixed social and biological framework emphasizing "reactions" in the context of an individual's life, and argued that the term "depression" should be used instead of "melancholia". The first version of the DSM (DSM-I, 1952) contained "depressive reaction" and the DSM-II (1968) "depressive neurosis", defined as an excessive reaction to internal conflict or an identifiable event, and also included a depressive type of manic-depressive psychosis within Major affective disorders.

In the mid-20th century, researchers theorized that depression was caused by a chemical imbalance in neurotransmitters in the brain, a theory based on observations made in the 1950s of the effects of reserpine and isoniazid in altering monoamine neurotransmitter levels and affecting depressive symptoms. The chemical imbalance theory has never been proven.

The term "unipolar" (along with the related term "bipolar") was coined by the neurologist and psychiatrist Karl Kleist, and subsequently used by his disciples Edda Neele and Karl Leonhard.

The term "Major depressive disorder" was introduced by a group of US clinicians in the mid-1970s as part of proposals for diagnostic criteria based on patterns of symptoms (called the "Research Diagnostic Criteria", building on earlier Feighner Criteria), and was incorporated into the DSM-III in 1980. To maintain consistency the ICD-10 used the same criteria, with only minor alterations, but using the DSM diagnostic threshold to mark a "mild depressive episode", adding higher threshold categories for moderate and severe episodes. The ancient idea of "melancholia" still survives in the notion of a melancholic subtype.

The new definitions of depression were widely accepted, albeit with some conflicting findings and views. There have been some continued empirically based arguments for a return to the diagnosis of melancholia. There has been some criticism of the expansion of coverage of the diagnosis, related to the development and promotion of antidepressants and the biological model since the late 1950s.

The term "depression" is used in a number of different ways. It is often used to mean this syndrome but may refer to other mood disorders or simply to a low mood. People's conceptualizations of depression vary widely, both within and among cultures. "Because of the lack of scientific certainty," one commentator has observed, "the debate over depression turns on questions of language. What we call it—'disease,' 'disorder,' 'state of mind'—affects how we view, diagnose, and treat it." There are cultural differences in the extent to which serious depression is considered an illness requiring personal professional treatment, or is an indicator of something else, such as the need to address social or moral problems, the result of biological imbalances, or a reflection of individual differences in the understanding of distress that may reinforce feelings of powerlessness, and emotional struggle.

The diagnosis is less common in some countries, such as China. It has been argued that the Chinese traditionally deny or somatize emotional depression (although since the early 1980s, the Chinese denial of depression may have modified). Alternatively, it may be that Western cultures reframe and elevate some expressions of human distress to disorder status. Australian professor Gordon Parker and others have argued that the Western concept of depression "medicalizes" sadness or misery. Similarly, Hungarian-American psychiatrist Thomas Szasz and others argue that depression is a metaphorical illness that is inappropriately regarded as an actual disease. There has also been concern that the DSM, as well as the field of descriptive psychiatry that employs it, tends to reify abstract phenomena such as depression, which may in fact be social constructs. American archetypal psychologist James Hillman writes that depression can be healthy for the soul, insofar as "it brings refuge, limitation, focus, gravity, weight, and humble powerlessness." Hillman argues that therapeutic attempts to eliminate depression echo the Christian theme of resurrection, but have the unfortunate effect of demonizing a soulful state of being.

Historical figures were often reluctant to discuss or seek treatment for depression due to social stigma about the condition, or due to ignorance of diagnosis or treatments. Nevertheless, analysis or interpretation of letters, journals, artwork, writings, or statements of family and friends of some historical personalities has led to the presumption that they may have had some form of depression. People who may have had depression include English author Mary Shelley, American-British writer Henry James, and American president Abraham Lincoln. Some well-known contemporary people with possible depression include Canadian songwriter Leonard Cohen and American playwright and novelist Tennessee Williams. Some pioneering psychologists, such as Americans William James and John B. Watson, dealt with their own depression.

There has been a continuing discussion of whether neurological disorders and mood disorders may be linked to creativity, a discussion that goes back to Aristotelian times. British literature gives many examples of reflections on depression. English philosopher John Stuart Mill experienced a several-months-long period of what he called "a dull state of nerves", when one is "unsusceptible to enjoyment or pleasurable excitement; one of those moods when what is pleasure at other times, becomes insipid or indifferent". He quoted English poet Samuel Taylor Coleridge's "Dejection" as a perfect description of his case: "A grief without a pang, void, dark and drear, / A drowsy, stifled, unimpassioned grief, / Which finds no natural outlet or relief / In word, or sigh, or tear." English writer Samuel Johnson used the term "the black dog" in the 1780s to describe his own depression, and it was subsequently popularized by depression sufferer former British Prime Minister Sir Winston Churchill.

Social stigma of major depression is widespread, and contact with mental health services reduces this only slightly. Public opinions on treatment differ markedly to those of health professionals; alternative treatments are held to be more helpful than pharmacological ones, which are viewed poorly. In the UK, the Royal College of Psychiatrists and the Royal College of General Practitioners conducted a joint Five-year Defeat Depression campaign to educate and reduce stigma from 1992 to 1996; a MORI study conducted afterwards showed a small positive change in public attitudes to depression and treatment.

Trials are looking at the effects of botulinum toxins on depression. The idea is that the drug is used to make the person look less frowning and that this stops the negative facial feedback from the face. In 2015 it turned out, however, that the partly positive effects that had been observed until then could have been placebo effects.

MDD has been studied by taking MRI scans of patients with depression have revealed a number of differences in brain structure compared to those who are not depressed. Meta-analyses of neuroimaging studies in major depression reported that, compared to controls, depressed patients had increased volume of the lateral ventricles and adrenal gland and smaller volumes of the basal ganglia, thalamus, hippocampus, and frontal lobe (including the orbitofrontal cortex and gyrus rectus). Hyperintensities have been associated with patients with a late age of onset, and have led to the development of the theory of vascular depression.

Depression is especially common among those over 65 years of age and increases in frequency with age beyond this age. In addition the risk of depression increases in relation to the age and frailty of the individual. Depression is one the most important factors which negatively impact quality of life in adults as well as the elderly. Both symptoms and treatment among the elderly differ from those of the rest of the adult populations.

As with many other diseases it is common among the elderly not to present classical depressive symptoms. Diagnosis and treatment is further complicated in that the elderly are often simultaneously treated with a number of other drugs, and often have other concurrent diseases. Treatment differs in that studies of SSRI-drugs have shown lesser and often inadequate effect among the elderly, while other drugs with more clear effects have adverse effects which can be especially difficult to handle among the elderly. Duloxetine is an SNRI-drug with documented effect on recurring depression among the elderly, but has adverse effects in form of dizziness, dryness of the mouth, diarrhea, and constipation.

Problem solving therapy was as of 2015 the only psychological therapy with proven effect, and can be likened to a simpler form of cognitive behavioral therapy. However, elderly with depression are seldom offered any psychological treatment, and the evidence surrounding which other treatments are effective is incomplete. Electroconvulsive therapy (ECT or electric-shock therapy) has been used as treatment of the elderly, and register-studies suggest it is effective although less so among the elderly than among the rest of the adult population.

The risks involved with treatment of depression among the elderly as opposed to benefits is not entirely clear. Awaiting more evidence on how depression-treatment among the elderly is best designed it is important to follow up treatment results, and to reconsider changing treatments if it does not help.

Models of depression in animals for the purpose of study include iatrogenic depression models (such as drug induced), forced swim tests, tail suspension test, and learned helplessness models. Criteria frequently used to assess depression in animals include expression of despair, neurovegetative changes, and anhedonia, as many other depressive criteria are untestable in animals such as guilt and suicidality.


</doc>
<doc id="8391" url="https://en.wikipedia.org/wiki?curid=8391" title="Diana (mythology)">
Diana (mythology)

Diana (Classical Latin: ) was the goddess of the hunt, the moon, and nature in Roman mythology, associated with wild animals and woodland, and having the power to talk to and control animals. She was equated with the Greek goddess Artemis, though she had an independent origin in Italy.

Diana was known as the virgin goddess of childbirth and women. She was one of the three maiden goddesses, along with Minerva and Vesta, who swore never to marry. Oak groves and deer were especially sacred to her. Diana was born with her twin brother, Apollo, on the island of Delos, daughter of Jupiter and Latona. She made up a triad with two other Roman deities; Egeria the water nymph, her servant and assistant midwife; and Virbius, the woodland god.

Diana is revered in Roman Neopaganism and Stregheria.

Diana (pronounced with long 'ī' and 'ā') is an adjectival form developed from an ancient *"divios", corresponding to later 'divus', 'dius', as in Dius Fidius, Dea Dia and in the neuter form "dium" meaning the sky.
It is derived from Proto-Indo-European "*d(e)y(e)w", meaning "bright sky" or "daylight"; the same word is also the root behind the name of the Aryan Vedic sky god Dyaus, as well as the Latin words deus (god), "dies" (day, daylight), and "diurnal" (daytime).

On the Tablets of Pylos a theonym διϝια ("diwia") is supposed as referring to a deity precursor of Artemis. Modern scholars mostly accept the identification.

The ancient Latin writers Varro and Cicero considered the etymology of Dīāna as allied to that of "dies" and connected to the shine of the Moon.

The persona of Diana is complex and contains a number of archaic features. According to Georges Dumézil it falls into a particular subset of celestial gods, referred to in histories of religion as "frame gods". Such gods, while keeping the original features of celestial divinities, i.e. transcendent heavenly power and abstention from direct rule in worldly matters, did not share the fate of other celestial gods in Indoeuropean religions—that of becoming "dei otiosi" or gods without practical purpose, since they did retain a particular sort of influence over the world and mankind.

The celestial character of Diana is reflected in her connection with inaccessibility, virginity, light, and her preference for dwelling on high mountains and in sacred woods.
Diana, therefore, reflects the heavenly world ("diuum" means sky or open air) in its sovereignty, supremacy, impassibility, and indifference towards such secular matters as the fates of mortals and states. At the same time, however, she is seen as active in ensuring the succession of kings and in the preservation of humankind through the protection of childbirth.

These functions are apparent in the traditional institutions and cults related to the goddess.

According to Dumezil the forerunner of all "frame gods" is an Indian epic hero who was the image (avatar) of the Vedic god Dyaus. Having renounced the world, in his roles of father and king, he attained the status of an immortal being while retaining the duty of ensuring that his dynasty is preserved and that there is always a new king for each generation.

The Scandinavian god Heimdallr performs an analogous function: he is born first and will die last. He too gives origin to kingship and the first king, bestowing on him regal prerogatives.
Diana, although a female deity, has exactly the same functions, preserving mankind through childbirth and royal succession.

F. H. Pairault in her essay on Diana qualifies Dumézil's theory as ""impossible to verify"".

Dumezil's interpretation appears deliberately to ignore that of James G. Frazer, who links Diana with the male god Janus as a divine couple. This looks odd as Dumézil's definition of the concept of "frame god" would fit well the figure of Janus. Frazer identifies the two with the supreme heavenly couple Jupiter-Juno and additionally ties in these figures to the overarching Indoeuropean religious complex. This regality is also linked to the cult of trees, particularly oaks.
In this interpretative schema, the institution of the Rex Nemorensis and related ritual should be seen as related to the theme of the dying god and the kings of May.

As a goddess of hunting, Diana often wears a short tunic and hunting boots. She is often portrayed holding a bow, and carrying a quiver on her shoulder, accompanied by a deer or hunting dogs. Like Venus, she was portrayed as beautiful and youthful. The crescent moon, sometimes worn as a diadem, is a major attribute of the goddess.

Diana was initially just the hunting goddess, associated with wild animals and woodlands. She also later became a moon goddess, supplanting Titan goddess Luna. She also became the goddess of childbirth and ruled over the countryside. Catullus wrote a poem to Diana in which she has more than one alias: Latonia, Lucina, Iuno, Trivia, Luna.

In Rome, the cult of Diana should have been almost as old as the city itself as Varro mentions her in the list of deities to whom king Titus Tatius vowed a shrine. It is noteworthy that the list includes Luna and Diana Lucina as separate entities.
Another testimony to the high antiquity of her cult is to be found in the "lex regia" of King Tullus Hostilius that condemns those guilty of incest to the "sacratio" to the goddess.
Diana was worshipped at a festival on August 13, when King Servius Tullius, himself born a slave, dedicated her temple on the Aventine Hill in the mid-6th century BC. Being placed on the Aventine, and thus outside the "pomerium", meant that Diana's cult essentially remained a "foreign" one, like that of Bacchus; she was never officially "transferred" to Rome as Juno was after the sack of Veii. It seems that her cult originated in Aricia, where her priest, the Rex Nemorensis remained. There the simple open-air fane was held in common by the Latin tribes, which Rome aspired to weld into a league and direct. Diana of the wood was soon thoroughly Hellenized, "a process which culminated with the appearance of Diana beside Apollo in the first "lectisternium" at Rome". Diana was regarded with great reverence and was a patroness of lower-class citizens, called plebeians, and slaves; slaves could receive asylum in her temples. This fact is of difficult interpretation. Georg Wissowa proposed the explanation that it might be because the first slaves of the Romans must have been Latins of the neighbouring tribes. However, in Ephesus too there was the same custom of the asylum (ασυλιον).

According to Françoise Hélène Pairault's study, historical and archaeological evidence point to the fact that both Diana of the Aventine and Diana Nemorensis were the product of the direct or indirect influence of the cult of Artemis spread by the Phoceans among the Greek towns of Campania Cuma and Capua, which in turn passed it over to the Etruscans and the Latins by the 6th and 5th centuries BC.

The origin of the ritual of the rex Nemorensis should have to be traced to the legend of Orestes and Iphigenia more than that of Hippolitos. The formation of the Latin League led by Laevius (or Baebius) Egerius happened under the influence of an alliance with the tyrant of Cuma Aristodemos and is probably connected to the political events at end of the 6th century narrated by Livy and Dionysius, such as the siege of Aricia by Porsenna's son Arruns. It is remarkable that the composition of this league does not reflect that of the Latin people who took part in the Latiar or Feriae Latinae given by Pliny and it has not as its leader the "rex Nemorensis" but a "dictator Latinus". It should thence be considered a political formation and not a traditional society founded on links of blood.

It looks as if the confrontation happened between two groups of Etruscans who fought for supremacy, those from Tarquinia, Vulci and Caere (allied with the Greeks of Capua) and those of Clusium. This is reflected in the legend of the coming of Orestes to Nemi and of the inhumation of his bones in the Roman Forum near the temple of Saturn. The cult introduced by Orestes at Nemi is apparently that of the Artemis Tauropolos. The literary amplification reveals a confused religious background: different Artemis were conflated under the epithet. As far as Nemi's Diana is concerned there are two different versions, by Strabo and Servius Honoratus. Strabo's version looks to be the most authoritative as he had access to first-hand primary sources on the sanctuaries of Artemis, i.e. the priest of Artemis Artemidoros of Ephesus. The meaning of "Tauropolos" denotes an Asiatic goddess with lunar attributes, lady of the herds. The only possible "interpretatio graeca" of high antiquity concerning "Diana Nemorensis" could have been the one based on this ancient aspect of a deity of light, master of wildlife. "Tauropolos" is an ancient epithet attached to Hecate, Artemis and even Athena. According to the legend Orestes founded Nemi together with Iphigenia. At Cuma the Sybil is the priestess of both Phoibos and Trivia. Hesiod and Stesichorus tell the story according to which after her death Iphigenia was divinised under the name of Hecate, fact which would support the assumption that Artemis Tauropolos had a real ancient alliance with the heroine, who was her priestess in Taurid and her human paragon. This religious complex is in turn supported by the triple statue of Artemis-Hecate. A coin minted by P. Accoleius Lariscolus in 43 BC has been acknowledged as representing the archaic statue of Diana Nemorensis. It represents Artemis with the bow at one extremity, Luna-Selene with flowers at the other and a central deity not immediately identifiable, all united by a horizontal bar.

The iconographical analysis allows the dating of this image to the 6th century at which time there are Etruscan models. Two heads found in the sanctuary and the Roman theatre at Nemi, which have a hollow on their back, lend support to this interpretation of an archaic Diana Trivia, in whom three different elements are associated. The presence of a Hellenised Diana at Nemi should be related to the presence of the cult in Campania, as Diana "Tifatina" was called "Trivia" in an imperial age inscription which mentions a "flamen Virbialis" dedicated by "eques" C. Octavius Verus. Cuma too had a cult of a chthonic Hecate and certainly had strict contacts with Latium. The theological complex present in Diana looks very elaborated and certainly Hellenic, while an analogous Latin concept of Diana Trivia seems uncertain, as Latin sources reflect a Hellenised character of the goddess.

Diana was one of the triple goddess, the same goddess being called Luna in heaven, Diana on earth, and Proserpina in hell. Michael Drayton praises the Triple Diana in poem "The Man in the Moone" (1606): "So these great three most powerful of the rest, Phoebe, Diana, Hecate, do tell. Her sovereignty in Heaven, in Earth and Hell".

Though some Roman patrons ordered marble replicas of the specifically Anatolian "Diana" of Ephesus, where the Temple of Artemis stood, Diana was usually depicted for educated Romans in her Greek guise. If she is accompanied by a deer, as in the "Diana of Versailles" ("illustration, above right") this is because Diana was the patroness of hunting. The deer may also offer a covert reference to the myth of Acteon (or Actaeon), who saw her bathing naked. Diana transformed Acteon into a stag and set his own hunting dogs to kill him.

Diana was an ancient goddess common to all Latin tribes. Therefore, many sanctuaries were dedicated to her in the lands inhabited by Latins. The first one is supposed to have been near Alba Longa before the town was destroyed by the Romans.

The Arician wood sanctuary near the lake of Nemi was Latin confederal as testified by the dedicatory epigraph quoted by Cato.

She had a shrine in Rome on the Aventine hill, according to tradition dedicated by king Servius Tullius. Its location is remarkable as the Aventine is situated outside the pomerium, i.e. original territory of the city, in order to comply with the tradition that Diana was a goddess common to all Latins and not exclusively of the Romans.

Other sanctuaries we know about are listed below:

Diana's cult has been related in Early Modern Europe to the cult of Nicevenn (a.k.a. Dame Habond, Perchta, Herodiana, etc.). She was related to myths of a female Wild Hunt.

Today there is a branch of Wicca named for her, which is characterized by an exclusive focus on the feminine aspect of the Divine. Diana's name is also used as the third divine name in a Wiccan energy chant- "Isis Astarte Diana Hecate Demeter Kali Inanna".

In Italy the old religion of Stregheria embraced the goddess Diana as Queen of the Witches; witches being the wise women healers of the time. Diana was said to have created the world of her own being having in herself the seeds of all creation yet to come. It was said that out of herself she divided the darkness and the light, keeping for herself the darkness of creation and creating her brother Apollo, the light. Diana was believed to have loved and ruled with her brother Apollo, the god of the Sun.

Both the Romanian words for "fairy" "Zână" and Sânziană, the Leonese and Portuguese word for "water nymph" "xana", and the Spanish word for "shooting target" and "morning call" ("diana") seem to come from the name of Diana.

Since the Renaissance the myth of Diana has often been represented in the visual and dramatic arts, including the opera "L'arbore di Diana". In the 16th century, Diana's image figured prominently at the châteaus of Fontainebleau, Chenonceau, & at Anet, in deference to Diane de Poitiers, mistress of Henri of France. At Versailles she was incorporated into the Olympian iconography with which Louis XIV, the Apollo-like "Sun King" liked to surround himself. Diana is also a character in the 1876 Léo Delibes ballet "Sylvia". The plot deals with Sylvia, one of Diana's nymphs and sworn to chastity, and Diana's assault on Sylvia's affections for the shepherd Amyntas.

Diana has been one of the most popular themes in art. Painters like Titian, Peter Paul Rubens, François Boucher, Nicholas Poussin made use of her myth as a major theme. Most depictions of Diana in art featured the stories of Diana and Actaeon, or Callisto, or depicted her resting after hunting. Some famous work of arts with a Diana theme are :

Beaux Arts architecture and garden design (late 19th and early 20th centuries) used classic references in a modernized form. Two of the most popular of the period were of Pomona (goddess of orchards) as a metaphor for Agriculture, and Diana, representing Commerce, which is a perpetual hunt for advantage and profits.









</doc>
<doc id="8396" url="https://en.wikipedia.org/wiki?curid=8396" title="December 11">
December 11





</doc>
<doc id="8397" url="https://en.wikipedia.org/wiki?curid=8397" title="Danny Elfman">
Danny Elfman

Daniel Robert Elfman (born May 29, 1953) is an American composer, singer, songwriter, and record producer. Elfman first became known for being the lead singer and songwriter for the band Oingo Boingo from 1974 to 1995. He is well known for scoring films and television shows, particularly his frequent collaborations with director Tim Burton.

In 1976, Elfman entered the film industry as an actor. In 1980, he scored his first film, "Forbidden Zone", directed by his older brother Richard Elfman. Among his honors are four Oscar nominations, a Grammy for "Batman", an Emmy for "Desperate Housewives", six Saturn Awards for Best Music, the 2002 Richard Kirk Award, and the Disney Legend Award.

Danny Elfman was born on May 29, 1953 in Los Angeles, California to a Jewish family of Polish and Russian ancestry. He is the son of Blossom Elfman (née Bernstein), a writer and teacher, and Milton Elfman, a teacher who was in the Air Force. He was raised in a racially mixed affluent community in Baldwin Hills, California. He spent much of his time in the neighborhood's local movie theater, adoring the music of such film composers as Bernard Herrmann and Franz Waxman. Stating that he hung out with the "band geeks" in high school, he started a ska band. After dropping out of high school, he followed his brother Richard to France, where he performed with Le Grand Magic Circus, an avant-garde musical theater group.

He was never officially a student at the CalArts, but an instructor there encouraged him to continue learning. Elfman stated, "He just laughed, and said, 'Sit. Play.' I continued to sit and play for a couple years." At this time, his brother Richard was forming a new musical theater group.

In 1972 Richard Elfman founded the American new wave band/performance art group, originally called The Mystic Knights of the Oingo Boingo. They played several shows throughout the 1970s until Richard Elfman left the band to become a filmmaker. As a send-off to the band's original concept, Richard Elfman created the film "Forbidden Zone" based on their stage performances. Danny Elfman composed his first score for the film and played the role of Satan (the other band members played his minions). By the time the movie was completed, they had taken the name Oingo Boingo and begun recording and touring as a rock group. From 1976 and on, it was led by Danny Elfman, until 1995 when they suddenly retired. The semi-theatrical music and comedy troupe had transformed into a ska-influenced new wave band in 1979, and then changed again towards a more guitar-oriented rock sound, in the late 1980s.. Oingo Boingo, still led by Danny Elfman, performed as themselves in the 1986 movie "Back to School". Additionally, Danny Elfman and Oingo Boingo guitarist Steve Bartek reunited on October 31, 2015 to perform the song "Dead Man's Party" – "for the first time in 20 years to the day", as Elfman said to the audience – during an encore at a Halloween celebration at the Hollywood Bowl.

In 1985, Tim Burton and Paul Reubens invited Elfman to write the score for their first feature film, "Pee-wee's Big Adventure". Elfman was apprehensive at first, because of his lack of formal training, but with orchestration assistance from Oingo Boingo guitarist and arranger Steve Bartek, he achieved his goal of emulating the mood of such composers as Nino Rota and Bernard Herrmann. In the booklet for the first volume of "Music for a Darkened Theatre", Elfman described the first time he heard his music played by a full orchestra as one of the most thrilling experiences of his life. Elfman immediately developed a rapport with Burton and has gone on to score all but three of Burton's major studio releases: "Ed Wood", which was under production while Elfman and Burton were having a serious disagreement, "", and most recently "Miss Peregrine's Home for Peculiar Children". Elfman also provided the singing voice for Jack Skellington in Tim Burton's "The Nightmare Before Christmas" and the voices of both Barrel and the "Clown with the Tear-Away Face". In 1990, Elfman composed the iconic orchestra piece, 'Ice Dance', for the Tim Burton film "Edward Scissorhands". Years later he provided the voice for Bonejangles the skeleton in "Corpse Bride" and the voices of the Oompa-Loompas in "Charlie and the Chocolate Factory".

One of Elfman's notable compositions is "The Simpsons" Theme, which he wrote in 1989.

In 2002 Elfman composed the opening theme for the Sam Raimi "Spider-Man" series. This as well as altered versions made its way to all three Spider-Man movies.

In October 2013, Elfman returned to the stage to sing his vocal parts to a handful of "Nightmare Before Christmas" songs as part of a concert titled "Danny Elfman's Music from the Films of Tim Burton". He composed the film score for "Oz the Great and Powerful" (2013), and composed additional music for "" (2015) together with Brian Tyler.

Elfman composed the score for all three of the "Fifty Shades" films (2015-2018).

Elfman's film scores were featured in the 2017 production "SCORE: A Film Music Documentary". Also that year, he took over the place of composer in the DCEU's "Justice League".

In 2004 Elfman composed "Serenada Schizophrana" for the American Composers Orchestra. It was conducted by John Mauceri on its recording and by Steven Sloane at its premiere at Carnegie Hall in New York City on February 23, 2005. After its premiere, it was recorded in studio and released onto SACD on October 3, 2006. The meeting with Mauceri proved fruitful as the composer was encouraged then to write a new concert piece for Mauceri and the Hollywood Bowl Orchestra. Elfman composed an "overture to a non-existent musical" and called the piece "The Overeager Overture". 

2017 saw the premiere of his 40-minute Concerto for Violin & Orchestra ('Eleven Eleven') in Prague, with soloist Sandy Cameron (for whom it was written) and conducted by John Mauceri with the Czech National Symphony Orchestra. It was jointly commissioned by Prague Proms, Stanford Symphony and the Royal Scottish National Orchestra and has subsequently been performed in Germany and the USA, with further dates planned. A 4-movement, 21-minute Piano Quartet was premiered by the Berlin Philharmonic Piano Quartet as part of their US tour in 2018. Elfman continues to compose his film scores in addition to these other projects.

In November 2010, it was reported that Danny Elfman was writing the music for a planned musical based on the life of Harry Houdini, but, as of January 2012, he was no longer attached to the project.

In 2011 Elfman composed the music for the Cirque du Soleil show "Iris", which was performed at the Dolby Theatre in Hollywood from July 21, 2011 to January 19, 2013.

In October 2016, Elfman composed a horror score for when Donald Trump "loom[ed]" behind Hillary Clinton at the second United States presidential election debates, 2016.

The style of Elfman's music has been influenced by modern composers including Béla Bartók, Philip Glass, Lou Harrison, Carl Orff, Harry Partch, Sergei Prokofiev, Maurice Ravel, Erik Satie, Igor Stravinsky, as well as Romantic composer Pyotr Ilyich Tchaikovsky. Elfman has said that the first time he noticed film music was when he heard Bernard Herrmann's score to "The Day the Earth Stood Still" as an eleven-year-old; afterwards he became a fan of film music. Elfman's influences in film music include the work of Erich Wolfgang Korngold, Max Steiner, David Tamkin, Franz Waxman, and Nino Rota, who served as a significant influence and the main inspiration for Elfman's score for "Pee-wee's Big Adventure". Elfman's work in pop music was influenced by The Specials, Madness, the Selecter, and XTC.

As a teenager, Elfman dated his classmate Kim Gordon, who would later become one of the members of the rock band Sonic Youth.

On November 29, 2003, Elfman married actress Bridget Fonda. They have a son, Oliver. In 1998, Elfman scored "A Simple Plan", starring Fonda.

He is the uncle of actor Bodhi Elfman, who is married to actress Jenna Elfman.

Elfman has been an atheist since the age of 11 or 12. According to him, he is a cynicologist.

Describing his politics during the 1980s, Elfman said, "I'm not a doomist. My attitude is always to be critical of what's around you, but not ever to forget how lucky we are. I've traveled around the world. I left thinking I was a revolutionary. I came back real right-wing patriotic. Since then, I've kind of mellowed in between." In 2008, he expressed support for Barack Obama and said that Sarah Palin was his "worst nightmare".

During the 18 years with Oingo Boingo, Elfman developed significant hearing damage as a result of the continuous exposure to the high noise levels involved in performing in a rock band. Afraid of worsening his condition, he decided to leave the band, saying that he would never return to that kind of performance. His impairment was so bad that he could not "even sit in a loud restaurant or bar anymore." However, he found performing in front of orchestras more tolerable, and returned several times to reprise his live performance of Jack Skellington.

Elfman's scores for "Batman" and "Edward Scissorhands" were nominated for AFI's 100 Years of Film Scores.




</doc>
<doc id="8398" url="https://en.wikipedia.org/wiki?curid=8398" title="Dimension">
Dimension

In physics and mathematics, the dimension of a mathematical space (or object) is informally defined as the minimum number of coordinates needed to specify any point within it. Thus a line has a dimension of one because only one coordinate is needed to specify a point on itfor example, the point at 5 on a number line. A surface such as a plane or the surface of a cylinder or sphere has a dimension of two because two coordinates are needed to specify a point on itfor example, both a latitude and longitude are required to locate a point on the surface of a sphere. The inside of a cube, a cylinder or a sphere is three-dimensional because three coordinates are needed to locate a point within these spaces.

In classical mechanics, space and time are different categories and refer to absolute space and time. That conception of the world is a four-dimensional space but not the one that was found necessary to describe electromagnetism. The four dimensions of spacetime consist of events that are not absolutely defined spatially and temporally, but rather are known relative to the motion of an observer. Minkowski space first approximates the universe without gravity; the pseudo-Riemannian manifolds of general relativity describe spacetime with matter and gravity. Ten dimensions are used to describe string theory, eleven dimensions can describe supergravity and M-theory, and the state-space of quantum mechanics is an infinite-dimensional function space.

The concept of dimension is not restricted to physical objects. s frequently occur in mathematics and the sciences. They may be parameter spaces or configuration spaces such as in Lagrangian or Hamiltonian mechanics; these are abstract spaces, independent of the physical space we live in.

In mathematics, the dimension of an object is an intrinsic property independent of the space in which the object is embedded. For example, a point on the unit circle in the plane can be specified by two Cartesian coordinates, but a single polar coordinate (the angle) would be sufficient, so the circle is 1-dimensional even though it exists in the 2-dimensional plane. This "intrinsic" notion of dimension is one of the chief ways the mathematical notion of dimension differs from its common usages.

The dimension of Euclidean -space is . When trying to generalize to other types of spaces, one is faced with the question "what makes -dimensional?" One answer is that to cover a fixed ball in by small balls of radius , one needs on the order of such small balls. This observation leads to the definition of the Minkowski dimension and its more sophisticated variant, the Hausdorff dimension, but there are also other answers to that question. For example, the boundary of a ball in looks locally like and this leads to the notion of the inductive dimension. While these notions agree on , they turn out to be different when one looks at more general spaces.

A tesseract is an example of a four-dimensional object. Whereas outside mathematics the use of the term "dimension" is as in: "A tesseract "has four dimensions"", mathematicians usually express this as: "The tesseract "has dimension 4"", or: "The dimension of the tesseract "is" 4".

Although the notion of higher dimensions goes back to René Descartes, substantial development of a higher-dimensional geometry only began in the 19th century, via the work of Arthur Cayley, William Rowan Hamilton, Ludwig Schläfli and Bernhard Riemann. Riemann's 1854 Habilitationsschrift, Schläfli's 1852 "Theorie der vielfachen Kontinuität", Hamilton's 1843 discovery of the quaternions and the construction of the Cayley algebra marked the beginning of higher-dimensional geometry.

The rest of this section examines some of the more important mathematical definitions of dimension.

The dimension of a vector space is the number of vectors in any basis for the space, i.e. the number of coordinates necessary to specify any vector. This notion of dimension (the cardinality of a basis) is often referred to as the "Hamel dimension" or "algebraic dimension" to distinguish it from other notions of dimension. 

For the non-free case, this generalizes to the notion of the length of a module.

The uniquely defined dimension of every connected topological manifold can be calculated. A connected topological manifold is locally homeomorphic to Euclidean -space, in which the number is the manifold's dimension.

For connected differentiable manifolds, the dimension is also the dimension of the tangent vector space at any point.

In geometric topology, the theory of manifolds is characterized by the way dimensions 1 and 2 are relatively elementary, the high-dimensional cases are simplified by having extra space in which to "work"; and the cases and are in some senses the most difficult. This state of affairs was highly marked in the various cases of the Poincaré conjecture, where four different proof methods are applied.

The dimension of a manifold depends on the base field with respect to which Euclidean space is defined. While analysis usually assumes a manifold to be over the real numbers, it is sometimes useful in the study of complex manifolds and algebraic varieties to work over the complex numbers instead. A complex number ("x" + "iy") has a real part "x" and an imaginary part "y", where x and y are both real numbers; hence, the complex dimension is half the real dimension. 

Conversely, in algebraically unconstrained contexts, a single complex coordinate system may be applied to an object having two real dimensions. For example, an ordinary two-dimensional spherical surface, when given a complex metric, becomes a Riemann sphere of one complex dimension.

The dimension of an algebraic variety may be defined in various equivalent ways. The most intuitive way is probably the dimension of the tangent space at any Regular point of an algebraic variety. Another intuitive way is to define the dimension as the number of hyperplanes that are needed in order to have an intersection with the variety that is reduced to a finite number of points (dimension zero). This definition is based on the fact that the intersection of a variety with a hyperplane reduces the dimension by one unless if the hyperplane contains the algebraic variety.

An algebraic set being a finite union of algebraic varieties, its dimension is the maximum of the dimensions of its components. It is equal to the maximal length of the chains formula_1 of sub-varieties of the given algebraic set (the length of such a chain is the number of "formula_2").

Each variety can be considered as an algebraic stack, and its dimension as variety agrees with its dimension as stack. There are however many stacks which do not correspond to varieties, and some of these have negative dimension. Specifically, if "V" is a variety of dimension "m" and "G" is an algebraic group of dimension "n" acting on "V", then the quotient stack ["V"/"G"] has dimension "m"−"n".

The Krull dimension of a commutative ring is the maximal length of chains of prime ideals in it, a chain of length "n" being a sequence formula_3 of prime ideals related by inclusion. It is strongly related to the dimension of an algebraic variety, because of the natural correspondence between sub-varieties and prime ideals of the ring of the polynomials on the variety.

For an algebra over a field, the dimension as vector space is finite if and only if its Krull dimension is 0.

For any normal topological space , the Lebesgue covering dimension of is defined to be "n" if "n" is the smallest integer for which the following holds: any open cover has an open refinement (a second open cover where each element is a subset of an element in the first cover) such that no point is included in more than elements. In this case dim . For a manifold, this coincides with the dimension mentioned above. If no such integer exists, then the dimension of is said to be infinite, and one writes dim . Moreover, has dimension −1, i.e. dim if and only if is empty. This definition of covering dimension can be extended from the class of normal spaces to all Tychonoff spaces merely by replacing the term "open" in the definition by the term "functionally open".

An inductive dimension may be defined inductively as follows. Consider a discrete set of points (such as a finite collection of points) to be 0-dimensional. By dragging a 0-dimensional object in some direction, one obtains a 1-dimensional object. By dragging a 1-dimensional object in a "new direction", one obtains a 2-dimensional object. In general one obtains an ()-dimensional object by dragging an -dimensional object in a "new" direction. The inductive dimension of a topological space may refer to the "small inductive dimension" or the "large inductive dimension", and is based on the analogy that balls have -dimensional boundaries, permitting an inductive definition based on the dimension of the boundaries of open sets.

Similarly, for the class of CW complexes, the dimension of an object is the largest for which the -skeleton is nontrivial. Intuitively, this can be described as follows: if the original space can be continuously deformed into a collection of higher-dimensional triangles joined at their faces with a complicated surface, then the dimension of the object is the dimension of those triangles.

The Hausdorff dimension is useful for studying structurally complicated sets, especially fractals. The Hausdorff dimension is defined for all metric spaces and, unlike the dimensions considered above, can also have non-integer real values. The box dimension or Minkowski dimension is a variant of the same idea. In general, there exist more definitions of fractal dimensions that work for highly irregular sets and attain non-integer positive real values. Fractals have been found useful to describe many natural objects and phenomena.

Every Hilbert space admits an orthonormal basis, and any two such bases for a particular space have the same cardinality. This cardinality is called the dimension of the Hilbert space. This dimension is finite if and only if the space's Hamel dimension is finite, and in this case the two dimensions coincide.

Classical physics theories describe three physical dimensions: from a particular point in space, the basic directions in which we can move are up/down, left/right, and forward/backward. Movement in any other direction can be expressed in terms of just these three. Moving down is the same as moving up a negative distance. Moving diagonally upward and forward is just as the name of the direction implies; "i.e.", moving in a linear combination of up and forward. In its simplest form: a line describes one dimension, a plane describes two dimensions, and a cube describes three dimensions. (See Space and Cartesian coordinate system.)

A temporal dimension is a dimension of time. Time is often referred to as the "fourth dimension" for this reason, but that is not to imply that it is a spatial dimension. A temporal dimension is one way to measure physical change. It is perceived differently from the three spatial dimensions in that there is only one of it, and that we cannot move freely in time but subjectively move in one direction.

The equations used in physics to model reality do not treat time in the same way that humans commonly perceive it. The equations of classical mechanics are symmetric with respect to time, and equations of quantum mechanics are typically symmetric if both time and other quantities (such as charge and parity) are reversed. In these models, the perception of time flowing in one direction is an artifact of the laws of thermodynamics (we perceive time as flowing in the direction of increasing entropy).

The best-known treatment of time as a dimension is Poincaré and Einstein's special relativity (and extended to general relativity), which treats perceived space and time as components of a four-dimensional manifold, known as spacetime, and in the special, flat case as Minkowski space.

In physics, three dimensions of space and one of time is the accepted norm. However, there are theories that attempt to unify the four fundamental forces by introducing extra dimensions. Most notably, superstring theory requires 10 spacetime dimensions, and originates from a more fundamental 11-dimensional theory tentatively called M-theory which subsumes five previously distinct superstring theories. To date, no experimental or observational evidence is available to confirm the existence of these extra dimensions. If extra dimensions exist, they must be hidden from us by some physical mechanism. One well-studied possibility is that the extra dimensions may be "curled up" at such tiny scales as to be effectively invisible to current experiments. Limits on the size and other properties of extra dimensions are set by particle experiments such as those at the Large Hadron Collider.

At the level of quantum field theory, Kaluza–Klein theory unifies gravity with gauge interactions, based on the realization that gravity propagating in small, compact extra dimensions is equivalent to gauge interactions at long distances. In particular when the geometry of the extra dimensions is trivial, it reproduces electromagnetism. However at sufficiently high energies or short distances, this setup still suffers from the same pathologies that famously obstruct direct attempts to describe quantum gravity. Therefore, these models still require a UV completion, of the kind that string theory is intended to provide. In particular, superstring theory requires six compact dimensions forming a Calabi–Yau manifold. Thus Kaluza-Klein theory may be considered either as an incomplete description on its own, or as a subset of string theory model building.
In addition to small and curled up extra dimensions, there may be extra dimensions that instead aren't apparent because the matter associated with our visible universe is localized on a subspace. Thus the extra dimensions need not be small and compact but may be large extra dimensions. D-branes are dynamical extended objects of various dimensionalities predicted by string theory that could play this role. They have the property that open string excitations, which are associated with gauge interactions, are confined to the brane by their endpoints, whereas the closed strings that mediate the gravitational interaction are free to propagate into the whole spacetime, or "the bulk". This could be related to why gravity is exponentially weaker than the other forces, as it effectively dilutes itself as it propagates into a higher-dimensional volume.

Some aspects of brane physics have been applied to cosmology. For example, brane gas cosmology attempts to explain why there are three dimensions of space using topological and thermodynamic considerations. According to this idea it would be because three is the largest number of spatial dimensions where strings can generically intersect. If initially there are lots of windings of strings around compact dimensions, space could only expand to macroscopic sizes once these windings are eliminated, which requires oppositely wound strings to find each other and annihilate. But strings can only find each other to annihilate at a meaningful rate in three dimensions, so it follows that only three dimensions of space are allowed to grow large given this kind of initial configuration.

Extra dimensions are said to be universal if all fields are equally free to propagate within them.

Some complex networks are characterized by fractal dimensions. The concept of dimension can be generalized to include networks embedded in space. The dimension characterize their spatial constraints.

Science fiction texts often mention the concept of "dimension" when referring to parallel or alternate universes or other imagined planes of existence. This usage is derived from the idea that to travel to parallel/alternate universes/planes of existence one must travel in a direction/dimension besides the standard ones. In effect, the other universes/planes are just a small distance away from our own, but the distance is in a fourth (or higher) spatial (or non-spatial) dimension, not the standard ones.

One of the most heralded science fiction stories regarding true geometric dimensionality, and often recommended as a starting point for those just starting to investigate such matters, is the 1884 novella "Flatland" by Edwin A. Abbott. Isaac Asimov, in his foreword to the Signet Classics 1984 edition, described "Flatland" as "The best introduction one can find into the manner of perceiving dimensions."

The idea of other dimensions was incorporated into many early science fiction stories, appearing prominently, for example, in Miles J. Breuer's "The Appendix and the Spectacles" (1928) and Murray Leinster's "The Fifth-Dimension Catapult" (1931); and appeared irregularly in science fiction by the 1940s. Classic stories involving other dimensions include Robert A. Heinlein's "—And He Built a Crooked House" (1941), in which a California architect designs a house based on a three-dimensional projection of a tesseract; and Alan E. Nourse's "Tiger by the Tail" and "The Universe Between" (both 1951). Another reference is Madeleine L'Engle's novel "A Wrinkle In Time" (1962), which uses the fifth dimension as a way for "tesseracting the universe" or "folding" space in order to move across it quickly. The fourth and fifth dimensions were also a key component of the book "The Boy Who Reversed Himself" by William Sleator.

Immanuel Kant, in 1783, wrote: "That everywhere space (which is not itself the boundary of another space) has three dimensions and that space in general cannot have more dimensions is based on the proposition that not more than three lines can intersect at right angles in one point. This proposition cannot at all be shown from concepts, but rests immediately on intuition and indeed on pure intuition "a priori" because it is apodictically (demonstrably) certain."

"Space has Four Dimensions" is a short story published in 1846 by German philosopher and experimental psychologist Gustav Fechner under the pseudonym "Dr. Mises". The protagonist in the tale is a shadow who is aware of and able to communicate with other shadows, but who is trapped on a two-dimensional surface. According to Fechner, this "shadow-man" would conceive of the third dimension as being one of time. The story bears a strong similarity to the "Allegory of the Cave" presented in Plato's "The Republic" (c. 380 BC).

Simon Newcomb wrote an article for the "Bulletin of the American Mathematical Society" in 1898 entitled "The Philosophy of Hyperspace". Linda Dalrymple Henderson coined the term "hyperspace philosophy", used to describe writing that uses higher dimensions to explore metaphysical themes, in her 1983 thesis about the fourth dimension in early-twentieth-century art. Examples of "hyperspace philosophers" include Charles Howard Hinton, the first writer, in 1888, to use the word "tesseract"; and the Russian esotericist P. D. Ouspensky.

Zero
One
Two
Three
Four
Higher dimensionsin mathematics
Infinite



</doc>
