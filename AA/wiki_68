<doc id="8972" url="https://en.wikipedia.org/wiki?curid=8972" title="Dagger">
Dagger

A dagger is a knife with a very sharp point and one or two sharp edges, typically designed or capable of being used as a thrusting or stabbing weapon. Daggers have been used throughout human experience for close combat confrontations, and many cultures have used adorned daggers in ritual and ceremonial contexts. The distinctive shape and historic usage of the dagger have made it iconic and symbolic. A dagger in the modern sense is a weapon designed for close-proximity combat or self-defence; due to its use in historic weapon assemblages, it has associations with maleness and martiality. Double-edged knives, however, play different sorts of roles in different social contexts. In some cultures, they are neither a weapon nor a tool, but a potent symbol of manhood; in others they are ritual objects used in body modifications such as circumcision.

A wide variety of thrusting knives have been described as daggers, including knives that feature only a single cutting edge, such as the European rondel dagger or the Persian pesh-kabz, or, in some instances, no cutting edge at all, such as the stiletto of the Renaissance. However, in the last hundred years or so, in most contexts, a dagger has certain definable characteristics, including a short blade with a sharply tapered point, a central spine or fuller, and usually two cutting edges sharpened the full length of the blade, or nearly so. Most daggers also feature a full crossguard to keep the hand from riding forwards onto the sharpened blade edges.

Daggers are primarily weapons, so knife legislation in many places restricts their manufacture, sale, possession, transport, or use.

The earliest daggers were made of materials such as flint, ivory or bone in Neolithic times.

Copper daggers appeared first in the early Bronze Age, in the 3rd millennium BC, and copper daggers of Early Minoan III (2400–2000 BC) were recovered at Knossos.

In ancient Egypt, daggers were usually made of copper or bronze, while royalty had gold weapons. At least since pre-dynastic Egypt, (c. 3100 BC) daggers were adorned as ceremonial objects with golden hilts and later even more ornate and varied construction. One early silver dagger was recovered with midrib design. The 1924 opening of the tomb of Tutankhamun revealed two daggers, one with a gold blade, and one of smelted iron. It is held that mummies of the Eleventh Dynasty were buried with bronze sabres; and there is a bronze dagger of Thut-mes III. (Eighteenth Dynasty), circa B.C. 1600. As late as Mene-ptah II. of the Nineteenth Dynasty (B.C 1300), we read it in the list of his loot, after the Prosopis battle, of bronze armour, swords and daggers.

Iron production did not begin until 1200 BC, and iron ore was not found in Egypt, making the iron dagger rare, and the context suggests that the iron dagger was valued on a level equal to that of its ceremonial gold counterpart. These facts, and the composition of the dagger had long suggested a meteoritic origin, however, evidence for its meteoritic origin was not entirely conclusive until June 2016 when researchers using x-ray fluorescence spectrometry confirmed similar proportions of metals (Iron, 10% nickel, and 0.6% cobalt) in a meteorite discovered in the area, deposited by an ancient meteor shower.

One of the earliest objects made of smelted iron is a dagger dating to before 2000 BC, found in a context that suggests it was treated as an ornamental object of great value. Found in a Hattic royal tomb dated about 2500 BC, at Alaca Höyük in northern Anatolia, the dagger has a smelted iron blade and a gold handle.

The artisans and blacksmiths of Iberia in what is now southern Spain and southwestern France produced various iron daggers and swords of high quality from the 5th to the 3rd century BC, in ornamentation and patterns influenced by Greek, Punic (Carthaginian), and Phoenician culture. The exceptional purity of Iberian iron and the sophisticated method of forging, which included cold hammering, produced double-edged weapons of excellent quality. One can find technologically advanced designs such as folding knives rusted among the artifacts of many Second Iberian Iron Age cremation burials or in Roman Empire excavations all around Spain and the Mediterranean. Iberian infantrymen carried several types of iron daggers, most of them based on shortened versions of double-edged swords, but the true Iberian dagger had a triangular-shaped blade. Iberian daggers and swords were later adopted by Hannibal and his Carthaginian armies. The Lusitanii, a pre-Celtic people dominating the lands west of Iberia (most of modern Portugal and Extremadura) successfully held off the Roman Empire for many years with a variety of innovative tactics and light weapons, including iron-bladed short spears and daggers modeled after Iberian patterns.

During the Roman Empire, legionaries were issued a "pugio" (from the Latin "pugnō", or “fight”), a double-edged iron thrusting dagger with a blade of 7–12 inches. The design and fabrication of the "pugio" was taken directly from Iberian daggers and short swords; the Romans even adopted the triangular-bladed Iberian dagger, which they called the "parazonium". Like the "gladius", the "pugio" was most often used as a thrusting (stabbing weapon). As an extreme close-quarter combat weapon, the "pugio" was the Roman soldier's last line of defense. When not in battle, the "pugio" served as a convenient utility knife.

The term "dagger" appears only in the Late Middle Ages, reflecting the fact that while the dagger had been known in antiquity, it had disappeared during the Early Middle Ages, replaced by the hewing knife or seax.
The dagger reappeared in the 12th century as the "knightly dagger", or more properly cross-hilt or quillon dagger, and was developed into a common arm and tool for civilian use by the late medieval period.
The earliest known depiction of a cross-hilt dagger is the so-called "Guido relief" inside the Grossmünster of Zürich (c. 1120). A number of depictions of the fully developed cross-hilt dagger are found in the Morgan Bible (c. 1240).
Many of these cross-hilt daggers resemble miniature swords, with cross guards and pommels very similar in form to swords of the period. Others, however, are not an exact match to known sword designs, having for example pommel caps, large hollow star shaped pommels on so-called “Burgundian Heraldic daggers” or antenna style cross and pommel, reminiscent of Hallstatt era daggers.
The cross-hilt type persisted well into the Renaissance 

The Old French term "dague" appears to have referred to these weapons in the 13th century, alongside other terms such as "poignal" and "basilard". The Middle English "dagger" is used from the 1380s.

During this time, the dagger was often employed in the role of a secondary defense weapon in close combat. The knightly dagger evolved into the larger baselard knife in the 14th century. During the 14th century, it became fairly common for knights to fight on foot to strengthen the infantry defensive line. This necessitated greater dagger usage. At Agincourt (1415) archers used them to dispatch dismounted knights by thrusting the narrow blades through helmet vents and other apertures. The baselard was considered an intermediate between a short sword and a long dagger, and became popular also as a civilian weapon. Sloane MS. 2593 (c. 1400) records a song satirizing the use of oversized baselard knives as fashion accessories.

In the Late Middle Ages, knives with blade designs that emphasized thrusting attacks, such as the stiletto, became increasingly popular, and some thrusting knives commonly referred to as 'daggers' ceased to have a cutting edge. This was a response to the deployment of heavy armor, such as maille and plate armour, where cutting attacks were ineffective and focus was on thrusts with narrow blades to punch through mail or aim at armour plate intersections (or the eye slits of the helmet visor). These late medieval thrusting weapons are sometimes classed by the shape of their hilt as either roundel, bollock or ear daggers.
The term "dagger" is coined in this time, as are the Early Modern German equivalents "dolch" ("tolch") and "degen" ("tegen").
In the German school of fencing, Johannes Liechtenauer (Ms. 3227a) and his successors (specifically Andres Lignizer in Cod. 44 A 8) taught fighting with the dagger.

These techniques in some respects resemble modern knife fighting, but emphasized thrusting strokes almost exclusively, instead of slashes and cuts. When used offensively, a standard attack frequently employed the reverse or icepick grip, stabbing downward with the blade to increase thrust and penetrative force. This was done primarily because the blade point frequently had to penetrate or push apart an opponent's steel chain mail or plate armor in order to inflict an injury. The disadvantage of employing the medieval dagger in this manner was that it could easily be blocked by a variety of techniques, most notably by a block with the weaponless arm while simultaneously attacking with a weapon held in the right hand. Another disadvantage was the reduction in effective blade reach to the opponent when using a reverse grip. As the wearing of armor fell out of favor, dagger fighting techniques began to evolve which emphasized the use of the dagger with a conventional or forward grip, while the reverse or icepick grip was retained when attacking an unsuspecting opponent from behind, such as in an assassination.

]

The dagger was very popular as a fencing and personal defense weapon in 17th- and 18th-century Spain, where it was referred to as the "daga" or "puñal". During the Renaissance Age the dagger was used as part of everyday dress, and daggers were the only weapon commoners were allowed to carry on their person.
In English, the terms "poniard" and "dirk" are loaned during the late 16th to early 17th century, the latter in the spelling "dork, durk" (presumably via Low German, Dutch or Scandinavian "dolk, dolch", ultimately from a West Slavic "tulich"), the modern spelling "dirk" dating to 18th-century Scots.

Beginning with the 17th century, another form of dagger—the plug bayonet and later the socket bayonet—was used to convert muskets and other longarms into spears by mounting them on the barrel. They were periodically used for eating; the arm was also used for a variety of other tasks such as mending boots, house repairs and farm jobs. The final function of the dagger was as an obvious and ostentatious means of enhancing a man's personal apparel, conforming to fashion which dictated that all men carried them.

WW1 trench warfare caused daggers and fighting knives to come back in play. They also replaced the sabres worn by officers, which were too long and clumsy for trench warfare. They were worn with pride as a sign of having served front line duty.

Daggers achieved public notoriety in the 20th century as ornamental uniform regalia during the Fascist dictatorships of Mussolini's Italy and Hitler's Germany. The resurgence of these dress daggers and accoutrements in post-World War I Germany gave a much needed boost to the flagging fortunes of the metalworking center Solingen. Dress daggers were used by several other countries as well, including Japan, but never to the same extent as those worn by the military and political bodies of the Third Reich or Fascist Italy. As combat equipment they were carried by many infantry and commando forces during the Second World War. British Commando and other elite units were issued an especially slender dagger, the Fairbairn-Sykes fighting knife, developed by William E. Fairbairn and Eric A. Sykes from real-life close-combat experiences gained while serving on the Shanghai Municipal Police Force. The F-S dagger proved very popular with the commandos, who used it primarily for sentry elimination. Some units of the U.S. Marine Corps Raiders in the Pacific were issued a similar fighting dagger, the Marine Raider Stiletto., though this modified design proved less than successful when used in the type of knife combat encountered in the Pacific theater due to this version using inferior materials and manufacturing techniques.

During the Vietnam War, the Gerber Mark II, designed by US Army Captain Bud Holzman and Al Mar, was a popular fighting knife pattern that was privately purchased by many U.S. soldiers and marines serving in that conflict.

Aside from military forces, most daggers are no longer carried openly, but concealed in clothing. One of the more popular forms of the concealable dagger is the "boot knife". The boot knife is nothing more than a shortened dagger that is compact enough to be worn on the lower leg, usually by means of a sheath clipped or strapped to a boot or other footwear.
The dagger is symbolically ambiguous. Daggers are commonly used as part of the insignias of elite military units or special forces, such as the US Army Airborne Special Operations unit or the Commando Dagger patch for those who have completed the British All Arms Commando Course. Daggers may be associated with deception, stealth, and/or treachery due to the ease of concealment and surprise that someone could inflict with one on an unsuspecting victim, and indeed many assassinations have been carried out with the use of a dagger, including that of Julius Caesar. A cloak and dagger attack is one in which a deceitful, traitorous, or concealed enemy attacks a person. On the other hand, for some cultures and military organizations the dagger symbolizes courage and daring in combat.

Dagger reputation was tainted by its periodic use, or alleged employment, in the commission of disreputable, secretive and unsavoury deeds. Perhaps these are exaggerated; however, it became associated with assassinations performed when the concealed weapon was suddenly flourished and used to kill. Consequently, it developed connotations with murky, cowardly assaults in dark alleys, upon shadowy staircases, and of hired murderers emerging from concealment to stab innocent, sleeping victims.

To a degree, some antipathy towards the dagger has remained unchanged up to modern times. This is perhaps, partly due to the periodic, contemporary broadcasting of bloodthirsty films and television series depicting gangsters employing the stiletto daggers. History is punctuated with accounts of daggers being used in assassinations and coup d'état attempts. On March 15, 44BC, Gaius Julius Caesar was assassinated by a large group of conspirators who stabbed him repeatedly with their daggers. This took place in Rome in a room behind the Theatre of Pompey which was being used for government business whilst the Senate was being rebuilt. 

To some degree, the dagger regained a little social prestige during the rapier age when personal combat became less brutal. Its involvement in this formalized, regulated martial endeavor did something to restore its reputation. When its advantages and purpose eventually declined and weapon carrying ceased, the dagger was saved from obsolescence by its retention as a field sports gadget. In the nineteenth century the custom of wearing a general purpose knife practically ceased and the hunting knife became a specialized instrument. However, its combat and military traditions were incorporated in the bayonet weapon which has continued in use until the present day.

Daggers are a popular form of what is known as the "art knife", due in part to the symmetry of the blade. One of the most famous examples is knifemaker Buster Warenski's replication of the gold dagger found in Tutankhamun's tomb. Warenski's dagger was made with a cast gold blade and the knife contained 32 ounces of pure gold in its construction. One of the knives required of an American Bladesmith Society Mastersmith is the construction of an "art knife" or a "European style" dagger.

Dagger knife examples


</doc>
<doc id="8973" url="https://en.wikipedia.org/wiki?curid=8973" title="Dominican Order">
Dominican Order

The Order of Preachers (, postnominal abbreviation OP), also known as the Dominican Order, is a mendicant Catholic religious order founded by the Spanish priest Dominic of Caleruega in France, approved by Pope Honorius III via the Papal bull "Religiosam vitam" on 22 December 1216. Members of the order, who are referred to as "Dominicans", generally carry the letters "OP" after their names, standing for "Ordinis Praedicatorum", meaning "of the Order of Preachers". Membership in the order includes friars, nuns, active sisters, and affiliated lay or secular Dominicans (formerly known as tertiaries, though recently there has been a growing number of associates who are unrelated to the tertiaries).

Founded to preach the Gospel and to oppose heresy, the teaching activity of the order and its scholastic organisation placed the Preachers in the forefront of the intellectual life of the Middle Ages. The order is famed for its intellectual tradition, having produced many leading theologians and philosophers. In the year 2013 there were 6,058 Dominican friars, including 4,470 priests. The Dominican Order is headed by the Master of the Order, currently Bruno Cadoré.

A number of other names have been used to refer to both the order and its members.


The Dominican Order came into being in the Middle Ages at a time when men of God were no longer expected to stay behind the walls of a cloister. Instead, they travelled among the people, taking as their examples the apostles of the primitive Church. Out of this ideal emerged two orders of mendicant friars: one, the Friars Minor, was led by Francis of Assisi; the other, the Friars Preachers, by Dominic of Guzman. Like his contemporary, Francis, Dominic saw the need for a new type of organization, and the quick growth of the Dominicans and Franciscans during their first century of existence confirms that the orders of mendicant friars met a need.

Dominic sought to establish a new kind of order, one that would bring the dedication and systematic education of the older monastic orders like the Benedictines to bear on the religious problems of the burgeoning population of cities, but with more organizational flexibility than either monastic orders or the secular clergy. The Order of Preachers was founded in response to a then perceived need for informed preaching. Dominic's new order was to be trained to preach in the vernacular languages.

Dominic inspired his followers with loyalty to learning and virtue, a deep recognition of the spiritual power of worldly deprivation and the religious state, and a highly developed governmental structure. At the same time, Dominic inspired the members of his order to develop a "mixed" spirituality. They were both active in preaching, and contemplative in study, prayer and meditation. The brethren of the Dominican Order were urban and learned, as well as contemplative and mystical in their spirituality. While these traits affected the women of the order, the nuns especially absorbed the latter characteristics and made those characteristics their own. In England, the Dominican nuns blended these elements with the defining characteristics of English Dominican spirituality and created a spirituality and collective personality that set them apart.

As an adolescent, he had a particular love of theology and the Scriptures became the foundation of his spirituality. During his studies in Palencia, Spain, he experienced a dreadful famine, prompting Dominic to sell all of his beloved books and other equipment to help his neighbors. After he completed his studies, Bishop Martin Bazan and Prior Diego d'Achebes appointed Dominic to the cathedral chapter and he became a Canon Regular under the Rule of Saint Augustine and the Constitutions for the cathedral church of Osma. At the age of twenty-four or twenty-five, he was ordained to the priesthood.

In 1203, Dominic joined Prior Diego de Acebo on an embassy to Denmark for the monarchy of Spain, to arrange the marriage between the son of King Alfonso VIII of Castile and a niece of King Valdemar II of Denmark. At that time the south of France was the stronghold of the Cathar or Albigensian movement, named after the Duke of Albi, a Cathar sympathiser and opponent to the subsequent Albigensian Crusade (1209–1229). Dominic was fired by a reforming zeal after they encountered Albigensian Christians at Toulouse.

The Albigensians, were a heretical gnostic sect, holding that matter was evil and only spirit was good; this was a fundamental challenge to the notion of incarnation, central to Roman Catholic theology. Dominic saw the need for a response that would attempt to sway members of the Albigensian movement back to mainstream Christian thought.

Prior Diego saw immediately one of the paramount reasons for the spread of the unorthodox movement: the representatives of the Holy Church acted and moved with an offensive amount of pomp and ceremony. On the other hand, the Cathars lived in a state of self-sacrifice that was widely appealing. For these reasons, Prior Diego suggested that the papal legates begin to live a reformed apostolic life. The legates agreed to change if they could find a strong leader. The prior took up the challenge, and he and Dominic dedicated themselves to the conversion of the Albigensians. Despite this particular mission, in winning the Albigensians over by persuasion Dominic met limited success, "for though in his ten years of preaching a large number of converts were made, it has to be said that the results were not such as had been hoped for."

Dominic became the spiritual father to several Albigensian women he had reconciled to the faith, and in 1206 he established them in a convent in Prouille, near Toulouse. This convent would become the foundation of the Dominican nuns, thus making the Dominican nuns older than the Dominican friars. Prior Diego sanctioned the building of a monastery for girls whose parents had sent them to the care of the Albigensians because their families were too poor to fulfill their basic needs. The monastery in Prouille would later become Dominic's headquarters for his missionary effort. After two years on the mission field, Prior Diego died while traveling back to Spain.

Saint Dominic established a religious community in Toulouse in 1214, to be governed by the rule of Saint Augustine and statutes to govern the life of the friars, including the Primitive Constitution. (The statutes borrowed somewhat from the Constitutions of Prémontré). The founding documents establish that the order was founded for two purposes: preaching and the salvation of souls.

In July 1215, with the approbation of Bishop Foulques of Toulouse, Dominic ordered his followers into an institutional life. Its purpose was revolutionary in the pastoral ministry of the Catholic Church. These priests were organized and well trained in religious studies. Dominic needed a framework—a rule—to organize these components. The Rule of Saint Augustine was an obvious choice for the Dominican Order, according to Dominic's successor, Jordan of Saxony, because it lent itself to the "salvation of souls through preaching". By this choice, however, the Dominican brothers designated themselves not monks, but canons-regular. They could practice ministry and common life while existing in individual poverty.

Dominic's education at Palencia gave him the knowledge he needed to overcome the Manicheans. With charity, the other concept that most defines the work and spirituality of the order, study became the method most used by the Dominicans in working to defend the Church against the perils that hounded it, and also of enlarging its authority over larger areas of the known world. In Dominic's thinking, it was impossible for men to preach what they did not or could not understand. When the brethren left Prouille, then, to begin their apostolic work, Dominic sent Matthew of Paris to establish a school near the University of Paris. This was the first of many Dominican schools established by the brethren, some near large universities throughout Europe.

The Order of Preachers was approved in December 1216 and January 1217 by Pope Honorius III in the papal bulls "Religiosam vitam" and "Nos attendentes". On January 21, 1217 Honorius issued the bull "Gratiarum omnium" recognizing Saint Dominic's followers as an order dedicated to study and universally authorized to preach, a power formerly reserved to local episcopal authorization.

On August 15, 1217 Dominic dispatched seven of his followers to the great university center of Paris to establish a priory focused on study and preaching. The Convent of St. Jacques, would eventually become the order's first "studium generale". Saint Dominic was to establish similar foundations at other university towns of the day, Bologna in 1218, Palencia and Montpellier in 1220, and Oxford just before his death in 1221.

In 1219 Pope Honorius III invited Saint Dominic and his companions to take up residence at the ancient Roman basilica of Santa Sabina, which they did by early 1220. Before that time the friars had only a temporary residence in Rome at the convent of San Sisto Vecchio which Honorius III had given to Dominic circa 1218 intending it to become a convent for a reformation of nuns at Rome under Dominic's guidance. In May 1220 at Bologna the order's first General Chapter mandated that each new priory of the order maintain its own "studium conventuale" thus laying the foundation of the Dominican tradition of sponsoring widespread institutions of learning. The official foundation of the Dominican convent at Santa Sabina with its "studium conventuale" occurred with the legal transfer of property from Honorius III to the Order of Preachers on June 5, 1222. This "studium" was transformed into the order's first "studium provinciale" by Saint Thomas Aquinas in 1265. Part of the curriculum of this "studium" was relocated in 1288 at the "studium" of Santa Maria sopra Minerva which in the 16th century world be transformed into the College of Saint Thomas (). In the 20th century the college would be relocated to the convent of Saints Dominic and Sixtus and would be transformed into the Pontifical University of Saint Thomas Aquinas, "Angelicum".

The Dominican friars quickly spread, including to England, where they appeared in Oxford in 1221.
In the 13th century the order reached all classes of Christian society, fought heresy, schism, and paganism by word and book, and by its missions to the north of Europe, to Africa, and Asia passed beyond the frontiers of Christendom. Its schools spread throughout the entire Church; its doctors wrote monumental works in all branches of knowledge, including the extremely important Albertus Magnus and Thomas Aquinas. Its members included popes, cardinals, bishops, legates, inquisitors, confessors of princes, ambassadors, and "paciarii" (enforcers of the peace decreed by popes or councils).

The order's origins in battling heterodoxy influenced its later development and reputation. Many later Dominicans battled heresy as part of their apostolate. Indeed, many years after St. Dominic reacted to the Cathars, the first Grand Inquistor of Spain, Tomás de Torquemada, would be drawn from the Dominican Order.The order was appointed by Pope Gregory IX the duty to carry out the Inquisition. Torture was not regarded as a mode of punishment, but purely as a means of eliciting the truth. In his Papal Bull "Ad extirpanda" of 1252, Pope Innocent IV authorised the Dominicans' use of torture under prescribed circumstances.

The expansion of the order produced changes. A smaller emphasis on doctrinal activity favoured the development here and there of the ascetic and contemplative life and there sprang up, especially in Germany and Italy, the mystical movement with which the names of Meister Eckhart, Heinrich Suso, Johannes Tauler, and Saint Catherine of Siena are associated. (See German mysticism, which has also been called "Dominican mysticism.") This movement was the prelude to the reforms undertaken, at the end of the century, by Raymond of Capua, and continued in the following century. At the same time the order found itself face to face with the Renaissance. It struggled against pagan tendencies in Renaissance humanism, in Italy through Dominici and Savonarola, in Germany through the theologians of Cologne but it also furnished humanism with such advanced writers as Francesco Colonna (probably the writer of the "Hypnerotomachia Poliphili") and Matteo Bandello. Many Dominicans took part in the artistic activity of the age, the most prominent being Fra Angelico and Fra Bartolomeo.

Although Dominic and the early brethren had instituted female Dominican houses at Prouille and other places by 1227, houses of women attached to the Order became so popular that some of the friars had misgivings about the increasing demands of female religious establishments on their time and resources. Nonetheless, women's houses dotted the countryside throughout Europe. There were seventy-four Dominican female houses in Germany, forty-two in Italy, nine in France, eight in Spain, six in Bohemia, three in Hungary, and three in Poland. Many of the German religious houses that lodged women had been home to communities of women, such as Beguines, that became Dominican once they were taught by the traveling preachers and put under the jurisdiction of the Dominican authoritative structure. A number of these houses became centers of study and mystical spirituality in the 14th century. There were one hundred and fifty-seven nunneries in the order by 1358. After that year, the number lessened considerably due to the Black Death.

In places besides Germany, convents were founded as retreats from the world for women of the upper classes. These were original projects funded by wealthy patrons, including other women. Among these was Countess Margaret of Flanders who established the monastery of Lille, while Val-Duchesse at Oudergem near Brussels was built with the wealth of Adelaide of Burgundy, Duchess of Brabant (1262).

Female houses differed from male Dominican houses in that they were enclosed. The sisters chanted the Divine Office and kept all the monastic observances. The nuns lived under the authority of the general and provincial chapters of the order. They shared in all the applicable privileges of the order. The friars served as their confessors, priests, teachers and spiritual mentors.

Women could be professed to the Dominican religious life at the age of thirteen. The formula for profession contained in the Constitutions of Montargis Priory (1250) requires that nuns pledge obedience to God, the Blessed Virgin, their prioress and her successors according to the Rule of Saint Augustine and the institute of the order, until death. The clothing of the sisters consisted of a white tunic and scapular, a leather belt, a black mantle, and a black veil. Candidates to profession were questioned to reveal whether they were actually married women who had merely separated from their husbands. Their intellectual abilities were also tested. Nuns were to be silent in places of prayer, the cloister, the dormitory, and refectory. Silence was maintained unless the prioress granted an exception for a specific cause. Speaking was allowed in the common parlor, but it was subordinate to strict rules, and the prioress, subprioress or other senior nun had to be present.

As well as sewing, embroidery and other genteel pursuits, the nuns participated in a number of intellectual activities, including reading and discussing pious literature. In the Strassburg monastery of Saint Margaret, some of the nuns could converse fluently in Latin. Learning still had an elevated place in the lives of these religious. In fact, Margarette Reglerin, a daughter of a wealthy Nuremberg family, was dismissed from a convent because she did not have the ability or will to learn.

In England, the Dominican Province began at the second general chapter of the Dominican Order in Bologna during the spring of 1221. Dominic dispatched twelve friars to England under the guidance of their English prior, Gilbert of Fresney. They landed in Dover on August 5, 1221. The province officially came into being at its first provincial chapter in 1230.

The English Province was a component of the international order from which it obtained its laws, direction, and instructions. It was also, however, a group of Englishmen. Its direct supervisors were from England, and the members of the English Province dwelt and labored in English cities, towns, villages, and roadways. English and European ingredients constantly came in contact. The international side of the province's existence influenced the national, and the national responded to, adapted, and sometimes constrained the international.

The first Dominican site in England was at Oxford, in the parishes of St. Edward and St. Adelaide. The friars built an oratory to the Blessed Virgin Mary and by 1265, the brethren, in keeping with their devotion to study, began erecting a school. Actually, the Dominican brothers likely began a school immediately after their arrival, as priories were legally schools. Information about the schools of the English Province is limited, but a few facts are known. Much of the information available is taken from visitation records. The "visitation" was a section of the province through which visitors to each priory could describe the state of its religious life and its studies to the next chapter. There were four such visits in England and Wales—Oxford, London, Cambridge and York. All Dominican students were required to learn grammar, old and new logic, natural philosophy and theology. Of all of the curricular areas, however, theology was the most important. This is not surprising when one remembers Dominic's zeal for it.

Dartford Priory was established long after the primary period of monastic foundation in England had ended. It emulated, then, the monasteries found in Europe—mainly France and German—as well as the monastic traditions of their English Dominican brothers. The first nuns to inhabit Dartford were sent from Poissy Priory in France. Even on the eve of the Dissolution, Prioress Jane Vane wrote to Cromwell on behalf of a postulant, saying that though she had not actually been professed, she was professed in her heart and in the eyes of God. This is only one such example of dedication. Profession in Dartford Priory seems, then, to have been made based on personal commitment, and one's personal association with God. 

As heirs of the Dominican priory of Poissy in France, the nuns of Dartford Priory in England were also heirs to a tradition of profound learning and piety. Strict discipline and plain living were characteristic of the monastery throughout its existence.

Bartolomé de Las Casas, as a settler in the New World, was galvanized by witnessing the brutal torture and genocide of the Native Americans by the Spanish colonists. He became famous for his advocacy of the rights of Native Americans, whose cultures, especially in the Caribbean, he describes with care.

Gaspar da Cruz (c.1520–1570), who worked all over the Portuguese colonial empire in Asia, was probably the first Christian missionary to preach (unsuccessfully) in Cambodia. After a (similarly unsuccessful) stint, in 1556, in Guangzhou, China, he eventually returned to Portugal and became the first European to publish a book devoted exclusively to China in 1569/1570.

The beginning of the 16th century confronted the order with the upheavals of Revolution. The spread of Protestantism cost it six or seven provinces and several hundreds of convents, but the discovery of the New World opened up a fresh field of activity. In the 18th century, there were numerous attempts at reform, accompanied by a reduction in the number of devotees. The French Revolution ruined the order in France, and crises that more or less rapidly followed considerably lessened or wholly destroyed numerous provinces.

During the early 19th century, the number of Preachers seems never to have sunk below 3,500. Statistics for 1876 show 3,748, but 500 of these had been expelled from their convents and were engaged in work. Statistics for 1910 show a total of 4,472 nominally or actually engaged in proper activities of the order. By the year 2013 there were 6058 Dominican friars, including 4,470 priests.
In the revival movement France held a foremost place, owing to the reputation and convincing power of the orator, Jean-Baptiste Henri Lacordaire (1802–1861). He took the habit of a Friar Preacher at Rome (1839), and the province of France was canonically erected in 1850. From this province were detached the province of Lyon, called Occitania (1862), that of Toulouse (1869), and that of Canada (1909). The French restoration likewise furnished many laborers to other provinces, to assist in their organization and progress. From it came the master general who remained longest at the head of the administration during the 19th century, Père Vincent Jandel (1850–1872). Here should be mentioned the province of Saint Joseph in the United States. Founded in 1805 by Edward Fenwick, afterwards first Bishop of Cincinnati, Ohio (1821–1832). In 1905, it established a large house of studies at Washington, D.C., called the Dominican House of Studies.

The province of France has produced a large number of preachers. The conferences of Notre-Dame-de-Paris were inaugurated by Père Lacordaire. The Dominicans of the province of France furnished Lacordaire (1835–1836, 1843–1851), Jacques Monsabré, and Joseph Ollivier. The pulpit of Notre Dame has been occupied by a succession of Dominicans. Père Henri Didon (d. 1900) was a Dominican. The house of studies of the province of France publishes "L'Année Dominicaine" (founded 1859), "La Revue des Sciences Philosophiques et Theologiques" (1907), and "La Revue de la Jeunesse" (1909). French Dominicans founded and administer the École Biblique et Archéologique française de Jérusalem founded in 1890 by Marie-Joseph Lagrange (1855–1938), one of the leading international centres for biblical research. It is at the "École Biblique" that the famed Jerusalem Bible (both editions) was prepared. Likewise Cardinal Yves Congar was a product of the French province of the Order of Preachers.

Doctrinal development has had an important place in the restoration of the Preachers. Several institutions, besides those already mentioned, played important parts. Such is the Biblical school at Jerusalem, open to the religious of the order and to secular clerics, which publishes the "Revue Biblique." The "Pontificium Collegium Internationale Angelicum", the future Pontifical University of Saint Thomas Aquinas, "Angelicum" established at Rome in 1908 by Master Hyacinth Cormier, opened its doors to regulars and seculars for the study of the sacred sciences. In addition to the reviews above are the "Revue Thomiste," founded by Père Thomas Coconnier (d. 1908), and the "Analecta Ordinis Prædicatorum" (1893). Among numerous writers of the order in this period are: Cardinals Thomas Zigliara (d. 1893) and Zephirin González (d. 1894), two esteemed philosophers; Alberto Guillelmotti (d. 1893), historian of the Pontifical Navy, and historian Heinrich Denifle (d. 1905).

During the Reformation, many of the monasteries of Dominican nuns were forced to close. One which managed to survive, and afterwards founded many new houses, was St Ursula's in Augsburg. In the seventeenth century, monasteries of Dominican women were often asked by their bishops to undertake apostolic work, particularly educating girls and visiting the sick. St Ursula's returned to an enclosed life in the eighteenth century, but in the nineteenth century, after Napoleon had closed many European women's monasteries, King Louis I of Bavaria in 1828 restored the Religious Orders of women in his realm, provided that the nuns undertook some active work useful to the State (usually teaching or nursing). In 1877, Bishop Ricards in South Africa requested that Augsburg send a group of nuns to start a teaching mission in King Williamstown. From this mission were founded many Third Order Regular congregations of Dominican sisters, with their own constitutions, though still following the Rule of Saint Augustine and affiliated to the Dominican Order. These include the Dominican Sisters of Oakford, KwazuluNatal (1881), the Dominican Missionary Sisters, Zimbabwe, (1890) and the Dominican Sisters of Newcastle, KwazuluNatal (1891).

By the 1850s, the Dominicans had half a million followers in the Philippines and well-established missions in the Chinese province of Fujian and Tonkin, Vietnam, performing thousands of baptisms each year.

The Friars, Nuns, Sisters, Members of Priestly Fraternities of Saint Dominic ,Dominican Laity and Dominican Youths together form the Order of Preachers.

The Dominican nuns were founded by Saint Dominic even before he had established the friars. They are contemplatives in the cloistered life. Properly speaking, the friars and nuns together form the Order of Preachers. The nuns celebrated their 800th anniversary in 2006.

Women have been part of the Dominican Order since the beginning, but distinct active congregations of Dominican sisters in their current form are largely a product of the nineteenth century and afterwards. They draw their origins both from the Dominican nuns and the communities of women tertiaries (lay women) who lived in their own homes and gathered regularly to pray and study: the most famous of these was the Mantellate attached to Saint Dominic's church in Siena, to which Saint Catherine of Siena belonged. In the seventeenth century, some European Dominican monasteries (e.g. St Ursula's, Augsburg) temporarily became no longer enclosed, so they could engage in teaching or nursing or other work in response to pressing local need. Any daughter houses they founded, however, became independent. But in the nineteenth century, in response to increasing missionary fervor, monasteries were asked to send groups of women to found schools and medical clinics around the world. Large numbers of Catholic women traveled to Africa, the America, and the East to teach and support new communities of Catholics there, both settlers and converts. Owing to the large distances involved, these groups needed to be self-governing, and they frequently planted new self-governing congregations in neighboring mission areas in order to respond more effectively to the perceived pastoral needs. Following on from this period of growth in the nineteenth century, and another great period of growth in those joining these congregations in the 1950s, there are currently 24,600 Sisters belonging to 150 Dominican Religious Congregations present in 109 countries affiliated to Dominican Sisters International.

As well as the friars, Dominican sisters live their lives supported by four common values, often referred to as the Four Pillars of Dominican Life, they are: community life, common prayer, study and service. Saint Dominic called this fourfold pattern of life the "holy preaching". Henri Matisse was so moved by the care that he received from the Dominican Sisters that he collaborated in the design and interior decoration of their Chapelle du Saint-Marie du Rosaire in Vence, France.

The Priestly Fraternities of St. Dominic are diocesan priests who are formally affiliated to the Order of Preachers (Dominicans) through a Rule of life that they profess, and so strive for evangelical perfection under the overall direction of the Dominican friars. The origins of the Dominican fraternities can be traced from the Dominican third Order secular, which then included both priests and lay persons as members. Now existing as a separate association from that of the laity, and with its own distinct rule to follow, the Priestly Fraternities of St. Dominic continues to be guided by the Order in embracing the gift of the spirituality of Saint Dominic in the unique context of the diocesan priests. Along with the special grace of the Sacrament of Holy Orders, which helps them to perform the acts of the sacred ministry worthily, they receive new spiritual help from the profession, which makes them members of the Dominican Family and sharers in the grace and mission of the Order. While the Order provides them with these spiritual aids and directs them to their own sanctification, it leaves them free for the complete service of the local Church, under the jurisdiction of their own Bishop.

Lay Dominicans are governed by their own rule, the Rule of the Lay Fraternities of St. Dominic, promulgated by the Master in 1987. It is the fifth Rule of the Dominican Laity; the first was issued in 1285. Lay Dominicans are also governed by the Fundamental Constitution of the Dominican Laity, and their provinces provide a General Directory and Statutes. According to their Fundamental Constitution of the Dominican Laity, sec. 4, "They have a distinctive character in both their spirituality and their service to God and neighbor. As members of the Order, they share in its apostolic mission through prayer, study and preaching according to the state of the laity."

Pope Pius XII, in Chosen Laymen, an Address to the Third Order of St. Dominic (1958), said, "The true condition of salvation is to meet the divine invitation by accepting the Catholic 'credo' and by observing the commandments. But the Lord expects more from you [Lay Dominicans], and the Church urges you to continue seeking the intimate knowledge of God and His works, to search for a more complete and valuable expression of this knowledge, a refinement of the Christian attitudes which derive from this knowledge."

The two greatest saints among them are Saint Catherine of Siena and Saint Rose of Lima, who lived ascetic lives in their family homes, yet both had widespread influence in their societies.

Today, there is a growing number of Associates who share the Dominican charism. Dominican Associates are Christian women and men; married, single, divorced, and widowed; clergy members and lay persons who were first drawn to and then called to live out the charism and continue the mission of the Dominican Order – to praise, to bless, to preach. Associates do not take vows, but rather make a commitment to be partners with vowed members, and to share the mission and charism of the Dominican Family in their own lives, families, churches, neighborhoods, workplaces, and cities. They are most often associated with a particular apostolic work of a congregation of active Dominican sisters.

The Dominican emphasis on learning and on charity distinguishes it from other monastic and mendicant orders. As the order first developed on the European continent, learning continued to be emphasized by these friars and their sisters in Christ. These religious also struggled for a deeply personal, intimate relationship with God. When the order reached England, many of these attributes were kept, but the English gave the order additional, specialized characteristics.

Humbert of Romans, the master general of the order from 1254 to 1263, was a great administrator, as well as preacher and writer. It was under his tenure as master general that the sisters in the order were given official membership. He also wanted his friars to reach excellence in their preaching, and this was his most lasting contribution to the order. Humbert is at the center of ascetic writers in the Dominican Order. He advised his readers,
"[Young Dominicans] are also to be instructed not to be eager to see visions or work miracles, since these avail little to salvation, and sometimes we are fooled by them; but rather they should be eager to do good in which salvation consists. Also, they should be taught not to be sad if they do not enjoy the divine consolations they hear others have; but they should know the loving Father for some reason sometimes withholds these. Again, they should learn that if they lack the grace of compunction or devotion they should not think they are not in the state of grace as long as they have good will, which is all that God regards".

The English Dominicans took this to heart, and made it the focal point of their mysticism.

By 1300, the enthusiasm for preaching and conversion within the order lessened. Mysticism, full of the ideas Albertus Magnus expostulated, became the devotion of the greatest minds and hands within the organization. It became a "powerful instrument of personal and theological transformation both within the Order of Preachers and throughout the wider reaches of Christendom.

Although Albertus Magnus did much to instill mysticism in the Order of Preachers, it is a concept that reaches back to the Hebrew Bible. In the tradition of Holy Writ, the impossibility of coming face to face with God is a recurring motif, thus the commandment against graven images (Exodus 20.4–5). As time passed, Jewish and early Christian writings presented the idea of 'unknowing,' where God's presence was enveloped in a dark cloud. All of these ideas associated with mysticism were at play in the spirituality of the Dominican community, and not only among the men. In Europe, in fact, it was often the female members of the order, such as Catherine of Siena, Mechthild of Magdeburg, Christine of Stommeln, Margaret Ebner, and Elsbet Stagl, that gained reputations for having mystical experiences. Notable male members of the order associated with mysticism include Meister Eckhart and Henry Suso.

Another who contributed significantly to the spirituality of the order is Albertus Magnus, influence on the brotherhood permeated nearly every aspect of Dominican life. One of Albert's greatest contributions was his study of Dionysus the Areopagite, a mystical theologian whose words left an indelible imprint in the medieval period. Magnus' writings made a significant contribution to German mysticism, which became vibrant in the minds of the Beguines and women such as Hildegard of Bingen and Mechthild of Magdeburg. Mysticism refers to the conviction that all believers have the capability to experience God's love. This love may manifest itself through brief ecstatic experiences, such that one may be engulfed by God and gain an immediate knowledge of Him, which is unknowable through the intellect alone.

Albertus Magnus championed the idea, drawn from Dionysus, that positive knowledge of God is possible, but obscure. Thus, it is easier to state what God is not, than to state what God is:
"... we affirm things of God only relatively, that is, casually, whereas we deny things of God absolutely, that is, with reference to what He is in Himself. And there is no contradiction between a relative affirmation and an absolute negation. It is not contradictory to say that someone is white-toothed and not white".

Albert the Great wrote that wisdom and understanding enhance one's faith in God. According to him, these are the tools that God uses to commune with a contemplative. Love in the soul is both the cause and result of true understanding and judgement. It causes not only an intellectual knowledge of God, but a spiritual and emotional knowledge as well. Contemplation is the means whereby one can obtain this goal of understanding. Things that once seemed static and unchanging become full of possibility and perfection. The contemplative then knows that God is, but she does not know what God is. Thus, contemplation forever produces a mystified, imperfect knowledge of God. The soul is exalted beyond the rest of God's creation but it cannot see God Himself.

Concerning humanity as the image of Christ, English Dominican spirituality concentrated on the moral implications of image-bearing rather than the philosophical foundations of the imago Dei. The process of Christ's life, and the process of image-bearing, amends humanity to God's image. The idea of the "image of God" demonstrates both the ability of man to move toward God (as partakers in Christ's redeeming sacrifice), and that, on some level, man is always an image of God. As their love and knowledge of God grows and is sanctified by faith and experience, the image of God within man becomes ever more bright and clear.

English Dominican mysticism in the late medieval period differed from European strands of it in that, whereas European Dominican mysticism tended to concentrate on ecstatic experiences of union with the divine, English Dominican mysticism's ultimate focus was on a crucial dynamic in one's personal relationship with God. This was an essential moral imitation of the Savior as an ideal for religious change, and as the means for reformation of humanity's nature as an image of divinity. This type of mysticism carried with it four elements. First, spiritually it emulated the moral essence of Christ's life. Second, there was a connection linking moral emulation of Christ's life and humanity's disposition as images of the divine. Third, English Dominican mysticism focused on an embodied spirituality with a structured love of fellow men at its center. Finally, the supreme aspiration of this mysticism was either an ethical or an actual union with God.

For English Dominican mystics, the mystical experience was not expressed just in one moment of the full knowledge of God, but in the journey of, or process of, faith. This then led to an understanding that was directed toward an experiential knowledge of divinity. It is important to understand, however, that for these mystics it was possible to pursue mystical life without the visions and voices that are usually associated with such a relationship with God. They experienced a mystical process that allowed them, in the end, to experience what they had already gained knowledge of through their faith only.

The center of all mystical experience is, of course, Christ. English Dominicans sought to gain a full knowledge of Christ through an imitation of His life. English mystics of all types tended to focus on the moral values that the events in Christ's life exemplified. This led to a "progressive understanding of the meanings of Scripture—literal, moral, allegorical, and anagogical"—that was contained within the mystical journey itself. From these considerations of Scripture comes the simplest way to imitate Christ: an emulation of the moral actions and attitudes that Jesus demonstrated in His earthly ministry becomes the most significant way to feel and have knowledge of God.

The English concentrated on the spirit of the events of Christ's life, not the literality of events. They neither expected nor sought the appearance of the stigmata or any other physical manifestation. They wanted to create in themselves that environment that allowed Jesus to fulfill His divine mission, insofar as they were able. At the center of this environment was love: the love that Christ showed for humanity in becoming human. Christ's love reveals the mercy of God and His care for His creation. English Dominican mystics sought through this love to become images of God. Love led to spiritual growth that, in turn, reflected an increase in love for God and humanity. This increase in universal love allowed men's wills to conform to God's will, just as Christ's will submitted to the Father's will.

As the image of God grows within man, he learns to rely less on an intellectual pursuit of virtue and more on an affective pursuit of charity and meekness. Thus, man then directs his path to that One, and the love for, and of, Christ guides man's very nature to become centered on the One, and on his neighbor as well. Charity is the manifestation of the pure love of Christ, both for and by His follower.

Although the ultimate attainment for this type of mysticism is union with God, it is not necessarily visionary, nor does it hope only for ecstatic experiences; instead, mystical life is successful if it is imbued with charity. The goal is just as much to become like Christ as it is to become one with Him. Those who believe in Christ should first have faith in Him without becoming engaged in such overwhelming phenomena.

The Dominican Order was affected by a number of elemental influences. Its early members imbued the order with a mysticism and learning. The Europeans of the order embraced ecstatic mysticism on a grand scale and looked to a union with the Creator. The English Dominicans looked for this complete unity as well, but were not so focused on ecstatic experiences. Instead, their goal was to emulate the moral life of Christ more completely. The Dartford nuns were surrounded by all of these legacies, and used them to create something unique. Though they are not called mystics, they are known for their piety toward God and their determination to live lives devoted to, and in emulation of, Him.

Devotion to the Virgin Mary was another very important aspect of Dominican spirituality. As an order, the Dominicans believed that they were established through the good graces of Christ's mother, and through prayers she sent missionaries to save the souls of nonbelievers. Dominican brothers and sisters who were unable to participate in the Divine Office sang the Little Office of the Blessed Virgin each day and saluted her as their advocate.

Throughout the centuries, the Holy Rosary has been an important element among the Dominicans. Pope Pius XI stated that: "The Rosary of Mary is the principle and foundation on which the very Order of Saint Dominic rests for making perfect the life of its members and obtaining the salvation of others."

Histories of the Holy Rosary often attribute its origin to Saint Dominic himself through the Blessed Virgin Mary. Our Lady of the Rosary is the title related to the purported Marian apparition to Saint Dominic in 1208 in the church of Prouille in which the Virgin Mary gave the Rosary to him. For centuries, Dominicans have been instrumental in spreading the rosary and emphasizing the Catholic belief in the power of the rosary.

On January 1, 2008, the master of the order declared a year of dedication to the Rosary.


The following people belonging to the order have been proclaimed saints throughout history:

Numerous Dominicans were included in the canonization of the 117 martyrs of Vietnam and a group of martyrs in Nagasaki, including Saint Lorenzo Ruiz.

Numerous Dominicans have been beatified, including:

Five Dominican friars have served as Bishop of Rome:

There are two Dominicans in the College of Cardinals:

Other notable Dominicans include:






</doc>
<doc id="8987" url="https://en.wikipedia.org/wiki?curid=8987" title="Don McLean">
Don McLean

Donald McLean III (born October 2, 1945) is an American singer-songwriter. He is best known for his 1971 song "American Pie", which was a number-one US hit for four weeks in 1972 and stayed put at 2 for 3 weeks in the UK, as well as a hit for Madonna in 2000. McLean's other well-known songs include: "And I Love You So", sung by Elvis Presley and Glen Campbell, among others; "Vincent", a tribute to the 19th-century Dutch painter Vincent van Gogh; "Crying" was a surprise number 1 hit in the United Kingdom in 1980 and was a cover of the Roy Orbison song and "Castles in the Air", which McLean recorded twice. In 2004, he was inducted into the Songwriters Hall of Fame.

McLean's grandfather and father, both also named Donald McLean, had roots originating in Scotland. The Buccis, the family of McLean's mother, Elizabeth, came from Abruzzo in central Italy. They left Italy and settled in Port Chester, New York, at the end of the 19th century. He has other extended family in Los Angeles and Boston.

Though some of his early musical influences included Frank Sinatra and Buddy Holly, as a teenager, McLean became interested in folk music, particularly the Weavers' 1955 recording "At Carnegie Hall". He often missed long periods of school because of childhood asthma, particularly music lessons, and although McLean slipped back in his studies, his love of music was allowed to flourish. By age 16, he had bought his first guitar and began making contacts in the music business, becoming friends with the folk singers Erik Darling and Fred Hellerman of the Weavers. Hellerman said, "He called me one day and said, 'I'd like to come and visit you', and that's what he did! We became good friends — he has the most remarkable music memory of anyone I've ever known."

When McLean was 15, his father died. Fulfilling his father's request, the singer graduated from Iona Preparatory School in 1963, and briefly attended Villanova University, dropping out after four months. After leaving Villanova, McLean became associated with the famed folk music agent Harold Leventhal for several months before teaming up with his personal manager, Herb Gart, for 18 years. For the next six years he performed at venues and events including The Bitter End and the Gaslight Cafe in New York, the Newport Folk Festival, the Cellar Door in Washington, D.C., and the Troubadour in Los Angeles. He attended night school at Iona College and received a bachelor's degree in business administration in 1968.

He turned down a scholarship to Columbia University Graduate School in favor of pursuing a career as a singer-songwriter, performing at such venues as Caffè Lena in, Saratoga Springs, New York, and the Main Point, in Bryn Mawr, Pennsylvania.

Later that year, with the help of a grant from the New York State Council on the Arts, McLean began reaching a wider audience, with visits to towns up and down the Hudson River. He learned the art of performing from his friend and mentor Pete Seeger. McLean accompanied Seeger on his Clearwater boat trip up the Hudson River in 1969 to raise awareness about environmental pollution in the river. During this time McLean wrote songs that would appear on his first album, "Tapestry". McLean co-edited the book "Songs and Sketches of the First Clearwater Crew", with sketches by Thomas B. Allen, for which Pete Seeger wrote the foreword. Seeger and McLean sang "Shenandoah" on the 1974 Clearwater album.

McLean recorded "Tapestry" in 1969 in Berkeley, California, during the student riots. After being rejected 72 times by labels, the album was released by Mediarts, a label that had not existed when he first started to look for a label. He worked on the album for a couple of years before putting it out. It attracted good reviews but little notice outside the folk community, though on the Easy Listening chart "Castles in the Air" was a success, and in 1973 "And I Love You So" became a number 1 Adult Contemporary hit for Perry Como.

McLean's major break came when Mediarts was taken over by United Artists Records, thus securing the promotion of a major label for his second album, "American Pie." The album launched two number one hits in the title song and "Vincent". "American Pie"'s success made McLean an international star and piqued interest in his first album, which charted more than two years after its initial release.

McLean's magnum opus, "American Pie", is a sprawling, impressionistic ballad inspired partly by the deaths of Buddy Holly, Ritchie Valens, and J.P. Richardson (The Big Bopper) in a plane crash in 1959, and developments in American youth culture in the subsequent decade. The song popularized the expression "The Day the Music Died" in reference to the crash. 

The song was recorded on May 26, 1971, and a month later received its first radio airplay on New York's WNEW-FM and WPLJ-FM to mark the closing of Fillmore East, the famous New York concert hall. "American Pie" reached number one on the "Billboard" Hot 100 from January 15 to February 5, 1972, and remains McLean's most successful single release. The single also topped the "Billboard" Easy Listening chart. With a total running time of 8:36 encompassing both sides of the single, it is also the longest song to reach number 1. Some stations played only part one of the original split-sided single release.

WCFL DJ Bob Dearborn unraveled the lyrics and first published his interpretation on January 7, 1972, eight days before the song reached number 1 nationally (see "Further reading" under American Pie). Numerous other interpretations, which together largely converged on Dearborn's interpretation, quickly followed. McLean declined to say anything definitive about the lyrics until 1978. Since then McLean has stated that the lyrics are also somewhat autobiographical and present an abstract story of his life from the mid-1950s until the time he wrote the song in the late 1960s.

In 2001 "American Pie" was voted number 5 in a poll of the 365 Songs of the Century compiled by the Recording Industry Association of America and the National Endowment for the Arts. 

On April 7, 2015, McLean’s original working manuscript for "American Pie" sold for $1,205,000 (£809,524/€1,109,182) at Christie’s auction rooms, New York, making it the third highest auction price achieved for an American literary manuscript.

Personnel from the "American Pie" album sessions were retained for his third album, "Don McLean", including the producer, Ed Freeman, Rob Rothstein on bass and Warren Bernhardt on piano. The song "The Pride Parade" provides an insight into McLean's immediate reaction to stardom. McLean told "Melody Maker" magazine in 1973 that "Tapestry" was an album by someone previously concerned with external situations. "American Pie" combines externals with internals and the resultant success of that album makes the third one ("Don McLean") entirely introspective."

Other songs written by McLean for the album include "Dreidel" (number 21 on the Billboard chart) and "If We Try" (number 58), which was subsequently recorded by Olivia Newton-John. "On the Amazon" from the 1920s musical Mr Cinders was an unusual choice but became an audience favorite in concerts and featured in "Till Tomorrow", a documentary film about McLean produced by Bob Elfstrom (Elfstrom held the role of Jesus Christ in Johnny and June Cash's "Gospel Road)". The film shows McLean in concert at Columbia University as he was interrupted by a bomb scare. He left the stage while the audience stood up and checked under their seats for anything that resembled a bomb. After the all-clear, McLean re-appeared and sang "On the Amazon" from exactly where he had left off. Don Heckman reported the bomb scare in his review for "The New York Times" entitled "Don McLean Survives Two Obstacles."

The fourth album, "Playin' Favorites" was a top-40 hit in the UK in 1973 and included the Irish folk classic, "Mountains of Mourne" and Buddy Holly's "Everyday", a live rendition of which returned McLean to the UK Singles Chart. McLean said, "The last album ("Don McLean") was a study in depression whereas the new one ("Playin' Favorites") is almost the quintessence of optimism.

The 1974 album "Homeless Brother", produced by Joel Dorn, was McLean's final studio recording for United Artists. The album featured fine New York session musicians, including Ralph McDonald on percussion, Hugh McCracken on guitar and a guest appearance by Yusef Lateef on flute. The Persuasions sang the background vocals on "Crying in the Chapel", and Cissy Houston provided a backing vocal on "La La Love You". The album's title song was inspired by Jack Kerouac's book "Lonesome Traveler", in which Kerouac tells the story of America's "homeless brothers", or hobos. The song features background vocals by Pete Seeger.

The song "The Legend of Andrew McCrew" was based on an article published in "The New York Times" concerning a black Dallas hobo named Anderson McCrew who was killed when he leapt from a moving train. No one claimed him, so a carnival took his body, mummified it, and toured all over the South with him, calling him "The Famous Mummy Man." McLean's song inspired radio station WGN in Chicago to tell the story and give the song airplay in order to raise money for a headstone for McCrew's grave. Their campaign was successful, and McCrew's body was exhumed and buried in the Lincoln Cemetery in Dallas. The tombstone had an inscription with words from the fourth verse of McLean's song:

What a way to live a life, and what a way to die
Left to live a living death with no one left to cry
A petrified amazement, a wonder beyond worth
A man who found more life in death than life gave him at birth

Joel Dorn later collaborated on the McLean career retrospective "", released in 2005 on Dorn's label, Hyena Records. In 2006, Dorn reflected on working with McLean:Of the more than 200 studio albums I've produced in the past forty plus years, there is a handful; maybe fifteen or so that I can actually listen to from top to bottom. "Homeless Brother" is one of them. It accomplished everything I set out to do. And it did so because it was a true collaboration. Don brought so much to the project that all I really had to do was capture what he did, and complement it properly when necessary.

In 1977 a brief liaison with Arista Records that yielded the album "Prime Time" and, in October 1978, the single "It Doesn't Matter Anymore". This was a track from the album "Chain Lightning" that should have been the second of four with Arista. McLean had started recording in Nashville, with Elvis Presley's backing singers, the Jordanaires, and many of Presey's musicians. However the Arista deal broke down following artistic disagreements between McLean and the Arista chief, Clive Davis. Consequently, McLean was left without a record contract in the United States, but through continuing deals, "Chain Lightning" was released by EMI in Europe and by Festival Records in Australia.

In April 1980, the Roy Orbison song "Crying" from the album began picking up airplay on Dutch radio stations and McLean was called to Europe to appear on several important musical variety shows to plug the song and support its release as a single by EMI. The song achieved number 1 status in the Netherlands first, followed by the UK and then Australia.

McLean's number 1 successes in Europe and Australia led to a new deal in the United States with Millennium Records, which issued "Chain Lightning" two and a half years after it had been recorded in Nashville and two years after its release in Europe. It charted on February 14, 1981, and reached number 28, while "Crying" climbed to number 5 on the pop singles chart. Orbison himself thought that McLean’s version was the best interpretation he’d ever heard of one of his songs. Orbison thought McLean did a better job than he did and even went so far as to say that the voice of Don McLean is one of the great instruments of 20th-century America. According to Brian Wilson of the Beach Boys, "McLean's voice could cut through steel - he is a very pure singer and he's up there with the best of them. He's a very talented singer and songwriter and he deserves his success."

McLean had further chart successes in the United States in the early 1980s with "Since I Don't Have You", a new recording of "Castles in the Air" and "It's Just the Sun". In 1987, the release of the country-based album"Love Tracks" gave rise to the hit singles "Love in My Heart" (a top-10 in Australia), "You Can't Blame the Train" (U.S. country number 49), and "Eventually". The latter two songs were written by Houston native Terri Sharp. In 1991, EMI reissued "American Pie" as a single in the United Kingdom, and McLean performed on "Top of the Pops". In 1992, previously unreleased songs became available on "Favorites and Rarities", while "Don McLean Classics" featured new studio recordings of "Vincent" and "American Pie".

McLean has continued to record new material, including "River of Love" in 1995 on Curb Records and, more recently, the albums "You've Got to Share", "Don McLean Sings Marty Robbins" and "The Western Album" for his own Don McLean Music label. "Addicted to Black" was released in May 2009.

McLean's other well-known songs include the following.

The "American Pie" album features a version of Psalm 137, entitled "Babylon". The song is based on a canon by Philip Hayes and was arranged by McLean and Lee Hays (of The Weavers). "Babylon" was performed in the "Mad Men" episode of the same name despite the fact that the song would not be released until 10 years after the time in which the episode is set.

In 1981, McLean had an international number one hit with a version of the Roy Orbison classic "Crying". It was only after the record became a success overseas that it was released in the United States. The single hit reached number 5 on the "Billboard" Hot 100 in 1981. Orbison himself once described McLean as "the voice of the century", and in a subsequent re-recording of the song, Orbison incorporated elements of McLean's version.

For the 1982 animated cult movie "The Flight of Dragons", produced by Jules Bass and Arthur Rankin, Jr., McLean sang the opening theme. However, no soundtrack has ever been released. Another hit song associated with McLean (though never recorded by him) is "Killing Me Softly with His Song", which was claimed by Lori Lieberman to have been written McLean after she, also a singer-songwriter, saw him singing his composition "Empty Chairs" in concert. Afterwards (according to Lieberman) she wrote a poem about the experience and shared it with Norman Gimbel, who had long been searching for a way to use a phrase he had copied from a novel badly translated from Spanish to English, "killing me softly with his blues". Allegedly, Gimbel and Charles Fox reworked the poem and the phrase into the song "Killing Me Softly with His Song", originally recorded by Lieberman and later by Roberta Flack (and also later recorded by the Fugees). This claim was disputed, notably by Fox. Subsequently, however, the matter reached an unequivocal conclusion when contemporaneous articles from the early 1970s were exhumed, all of them vindicating Lieberman.

In an April 5, 1973, article in the "New York Daily News", Norman Gimbel was quoted as follows: "She [Lori Lieberman] told us about this strong experience she had listening to McLean ("I felt all flushed with fever / Embarrassed by the crowd / I felt he had found my letters / And read each one out loud / I prayed that he would finish / But he just kept right on…"). I had a notion this might make a good song so the three of us discussed it. We talked it over several times, just as we did for the rest of the numbers we wrote for this album and we all felt it had possibilities."

McLean's albums did not match the commercial success of "American Pie", but he became a major concert attraction in the United States and overseas. His repertoire included old concert hall numbers and the catalogues of singers such as Buddy Holly, and another McLean influence, Frank Sinatra. The years spent playing gigs in small clubs and coffee houses in the 1960s transformed into well-paced performances. McLean's first concerts at Carnegie Hall, in New York, and the Albert Hall in London, in 1972 were critically acclaimed.

In recent years McLean has continued to tour the United States, Canada and Europe (2011, 2012) and Australia (2013). In June 2011 McLean appeared at the Glastonbury Festival in Pilton, UK, and in 2014 at California's Stagecoach Country Music Festival.

In May 2015, McLean undertook his 20th nationwide tour of the UK and Ireland. 
In 1991, McLean returned to the UK top 20 with a re-issue of "American Pie".

Iona College conferred an honorary doctorate on him in 2001.

In February 2002, "American Pie" was inducted into the Grammy Hall of Fame. In 2004, McLean was inducted into the Songwriters Hall of Fame. Garth Brooks presented the award and said, "Don McLean: his work, like the man himself, is very deep and very compassionate. His pop anthem 'American Pie' is a cultural phenomenon".

Two years later, Brooks repaid the favor by appearing as a guest (with Nanci Griffith) on McLean's first American TV special, broadcast as the PBS program "Starry Starry Night". A month later, McLean wound up the 20th century by performing "American Pie" at the Lincoln Memorial Gala in Washington D.C.

The biography "The Don McLean Story: Killing Us Softly With His Songs" was published in 2007. Biographer Alan Howard conducted extensive interviews for this, the only book-length biography of the often reclusive McLean to date.

In February 2012 McLean won the BBC Radio 2 Folk Awards Life Time Achievement award.

In March 2012, the PBS network broadcast a feature-length documentary about the life and music of McLean called "Don McLean: American Troubadour" produced by four-time Emmy Award-winning filmmaker Jim Brown.

McLean is one of the primary influences on the UK singer-songwriter Jake Bugg, who said McLean's song "Vincent" was "the first song I liked" after hearing it on an episode of The Simpsons. He devoured McLean's back catalogue and then delved into the artists that inspired McLean, including Buddy Holly and the Weavers. Tupac Shakur also cited McLean's "Vincent" as a personal inspiration.

McLean is credited as the writer of Drake's song "Doing It Wrong", featuring Stevie Wonder. The song includes lyrics from two McLean compositions – "The Wrong Thing to Do" and "When a Good Thing Goes Bad" – both of which were featured on his 1977 album "Prime Time".

In March 2017, McLean's single "American Pie" was designated an "aural treasure" by the Library of Congress, "worthy of preservation" in the National Recording Registry "as part of America’s patrimony".

McLean was raised in the Roman Catholic faith of his mother, Elizabeth McLean; his father, Donald McLean, was a Protestant. When McLean was 15, his father died months after their only vacation, to Washington D.C. 

McLean's first marriage was to Carol Sauvion, which lasted from 1969 to 1972. 

He was married to Patrisha McLean (née Shnier) from 1987 until their divorce in June 2016. They lived in Camden, Maine, with their two children, Jackie and Wyatt. On January 18, 2016, McLean was arrested in Camden for a misdemeanor domestic violence charge. On July 21, 2016, he pleaded guilty to charges of misdemeanour domestic violence assault, domestic violence criminal threatening, criminal mischief and criminal restraint against Patrisha McLean. The charges against McLean were dismissed on July 21, 2017, after he met the terms of a plea agreement.




</doc>
<doc id="8989" url="https://en.wikipedia.org/wiki?curid=8989" title="Defense">
Defense

Defense or defence may refer to:








</doc>
<doc id="8991" url="https://en.wikipedia.org/wiki?curid=8991" title="Dutch Limburg">
Dutch Limburg

Dutch Limburg may refer to the following:



</doc>
<doc id="8992" url="https://en.wikipedia.org/wiki?curid=8992" title="DirkJan">
DirkJan

DirkJan is a Dutch comic strip series, created in 1989 by Dutch author and artist Mark Retera. It is also the name of its main character. The series is a gag-a-day comic.

DirkJan is a loser who stumbles through life in mostly three-panel gag-a-day comic strips. He started out in 1989 as a student at the current Radboud University Nijmegen in the Netherlands, where he lived in a typical student house with all the stereotypical side-kicks, such as the fat boy, the beer drinker, the bossy girl who checks if everybody keeps to the house rules, and the tramps who use the heated shared hallway to stay the night. Early DirkJans contained many references to the student life of Nijmegen.

DirkJan was first published in "Critic", the magazine for the local union of psychology students. It then moved on to monthly publication in the student magazine of Nijmegen (Algemeen Nijmeegs Studentenblad, ANS). DirkJan became known nationally when the then only commercial comics magazine of the Netherlands SjoSji (now defunct) started publishing the strip. 

With the last move, the nature of the strip changed. Most of the student side-kicks got cancelled and DirkJan left university, first for jail (DJ is a notorious Kabouter abuser) and then to wander the globe and indeed space.

As of November 2016, there are 22 DirkJan albums, tentatively named 'DirkJan 1' through 'DirkJan 22'. Several newspapers in the Netherlands publish the comic in their daily edition.

The amateur comics magazine Iris (1990 - 1995) (re)published a number of DirkJan comics, some of which were refused for publication in SjoSji.



</doc>
<doc id="8993" url="https://en.wikipedia.org/wiki?curid=8993" title="Duck Hunt">
Duck Hunt

In "Duck Hunt", players use the NES Zapper to shoot ducks that appear on the television screen. The ducks appear one or two at a time, and the player is given three shots to shoot them down. The player receives points upon shooting each duck. If the player shoots the required number of ducks in a single round, the player will advance to the next round; otherwise, the player will receive a game over.

The game initially received few reviews, but was given mild critical praise and elicited a positive gamer reaction. Prior to the NES version, Nintendo also made a "Duck Hunt" game based on Laser Clay Shooting System released in 1976. It was later a pack-in game, paired with "Super Mario Bros."; the pack later also included "World Class Track Meet".

"Duck Hunt" is a shooter game in which the objective is to shoot moving targets on the television screen in mid-flight. The game is played from a first-person perspective and requires the NES Zapper light gun, which the player aims and fires at the screen. Each round consists of a total of ten targets to shoot. Depending on the game mode the player selects prior to beginning play, one or two targets will appear on the screen at any given time and the player has three shots, or attempts, to hit them before they disappear.

The player is required to successfully shoot a minimum number of targets in order to advance to the next round; failure will result in a game over. The difficulty increases as the player advances to higher rounds; targets will move faster and the minimum number of targets to shoot will increase. The player receives points upon shooting a target and will also receive bonus points for shooting all ten targets in a single round. "Duck Hunt" keeps track of the players' highest score for all games played in a single session; it is lost, however, upon shutting the game off.

"Duck Hunt" has three different game modes to choose from. In "Game A" and "Game B", the targets are flying ducks in a woodland area, and in "Game C" the targets are clay pigeons that are fired away from the player's perspective into the distance. In "Game A", one duck will appear on the screen at a time while in "Game B" two ducks will appear at a time. "Game A" allows a second player to control the movement of the flying ducks by using a normal NES controller. The gameplay starts at Round 1 and may continue up to Round 99. If the player completes Round 99, he or she will advance to Round 0, which is a kill screen (in "Game A") where the game behaves erratically, such as targets that move haphazardly or don't appear at all, and eventually ends.

"Duck Hunt" was released as an arcade game in the "Nintendo Vs." series in 1984 as "Vs. Duck Hunt", and is included in the PlayChoice-10 arcade console. The console supports two light guns, allowing two players at once.

Gameplay consists of alternating rounds of Games B and C, with 12 ducks/targets per round instead of 10 and sometimes requires the player to shoot three ducks/targets at a time instead of two. In addition, the player is given a limited number of lives; every duck/target that is not hit costs one life. When all lives are gone, the game ends.

After every second round, a bonus stage is played in which ducks can be shot for points as they fly out of the grass. However, the hunting dog occasionally jumps out, putting himself in the line of fire and creating a distraction. If the player shoots the dog, the bonus stage immediately ends.

"Duck Hunt" is based on a 1976 electronic toy version titled "Beam Gun: Duck Hunt", part of the "Beam Gun" series. The toy version was designed by Gunpei Yokoi and Masayuki Uemura for Nintendo. Nintendo Research & Development 1 developed both "Duck Hunt" for the NES and the NES Zapper. The game was supervised by Takehiro Izushi, and was produced by Gunpei Yokoi. The music was composed by Hirokazu Tanaka, who did music for several other Nintendo games at the time. The game's music was represented in the classic games medley on the Video Games Live concert tour. Designer Hiroji Kiyotake created the graphics and characters.

"Duck Hunt" has been placed in several combination ROM cartridges. In the Action Set configuration of the NES in the late 1980s, "Duck Hunt" was included with "Super Mario Bros." This particular cartridge is found very often in the United States, due to it being included with the purchase of a NES. A Power Set was also available, which included the Action Set, the Power Pad and a 3-in-1 cartridge that included "Duck Hunt", "World Class Track Meet", and "Super Mario Bros."

"Duck Hunt" was re-released as a downloaded Virtual Console title for the Wii U console in Japan on December 24, 2014, and internationally on December 25. This version is modified to require a Wii Remote controller in place of the NES Zapper to aim and shoot targets on the screen.

Allgame called the game an "attractive but repetitive target shooter" and "utterly mindless… the game is fun for a short time, but gets old after a few rounds of play". Several user groups have rated the game positively. 1UP.com users gave it an 8.7 out of 10, and the GameSpot community gave the Mario-Duck Hunt package a 9.1 out of 10. It was rated the 150th best game made on a Nintendo System in Nintendo Power's Top 200 Games list. IGN also placed the game at number 77 on its "Top 100 NES Games of All Time" feature. Jeremy Parish of "USgamer" stated that "Duck Hunt" paired with the NES Zapper "made the NES memorable" and was one of the key factors behind the success of the NES. Parish related "Duck Hunt" to the Wii Remote of the Wii in that they made their respective consoles more approachable and reach a wider demographic.

"Duck Hunt" features a nameless non-playable hunting dog, known simply as "Dog" according to his collectable trophy in "Super Smash Bros. for Wii U", and often referred to by the media as the "Duck Hunt Dog" or the "Laughing Dog". The dog accompanies the player in the "Game A" and "Game B" modes, in which he serves to both provoke the ducks and retrieve any fallen ones. The dog is infamous and iconic for laughing at the player whenever the player fails to shoot any of the ducks on screen. The dog has been labelled as "one of the most annoying video game characters ever" by numerous gaming critics and journalists, including IGN, GamesRadar, and ScrewAttack, and many have expressed the desire to be able to shoot the dog. Both IGN and "Nintendo Power" have referred to the dog as something players "love to hate".

The dog's perceived "smugness" has helped him appear on several "best of" lists. In their lists for "Top 10 Video Game Dogs", 1UP.com placed the dog seventh, praising his confidence for "laughing at a frustrated human with a loaded rifle", while GameSpy placed the dog in tenth. GameDaily and Official Nintendo Magazine have included the dog in their "Greatest Video Game Moments" lists. Brian Crecente of Kotaku listed him as one of his favorite video game dogs, stating that the dog's character design reminded him of Tex Avery cartoons. Video game developer Mastiff referenced the dog in promoting their video game "Remington Great American Bird Hunt", stating that Rockford, a dog in the game, will never laugh at players for missing the ducks.

UGO.com listed the ability to kill the dog as one of the best video game urban legends, stating that it is one of the few video game urban legends based in actual truth, since players could shoot the dog in the arcade "Vs. Duck Hunt". The dog makes a cameo appearance in the NES game "Barker Bill's Trick Shooting" (another Zapper game) and he can be shot. In "Wii Play" (2006) and its sequel "" (2011) some elements from "Duck Hunt" and "Hogan's Alley" are included in the mini-games "Shooting Range" and "Trigger Twist" in which some of the various targets are ducks and cans.

In the 2014 fighting games "Super Smash Bros. for Nintendo 3DS" and "Wii U", the dog, one of the ducks, and an unseen person who wields the NES Zapper appear collectively as playable characters under the name "Duck Hunt", or "Duck Hunt Duo" in PAL releases. Masahiro Sakurai, the games' director, stated that "Duck Hunt"s commercial success as "the most-sold shooting game in the world" was one of the primary reasons for the team's inclusion. In the games, the Duck Hunt team utilizes multiple attacks related to the NES Zapper, including throwing clay pigeons, kicking an explosive version of the can from "Hogan's Alley", being able to summon the cast of "Wild Gunman" to fire at opponents with their guns, or alerting the unseen person to fire at opponents with the Zapper. The games also feature an unlockable "Duck Hunt"-themed stage. Both the Duck Hunt character and stage will reappear in "Super Smash Bros. Ultimate".

In the 2015 Sony film "Pixels", the dog has a cameo appearance, where he is given as a "trophy" by the aliens when Sam Brenner (Adam Sandler) and Ludlow Lamonsoff (Josh Gad) defeat the creatures of the video game "Centipede". He stays in the house of an old woman in London.

On September 14, 2017, independent video game company Stress Level Zero released "Duck Season", a virtual reality horror game based on Duck Hunt. In the game, the dog serves as an antagonistic serial killer, taking revenge on the player if the player shoots the dog in-game.



</doc>
<doc id="8994" url="https://en.wikipedia.org/wiki?curid=8994" title="Das Boot">
Das Boot

Das Boot (, German: "The Boat") is a 1981 German war film written and directed by Wolfgang Petersen, produced by Günter Rohrbach, and starring Jürgen Prochnow, Herbert Grönemeyer, and Klaus Wennemann. It has been exhibited both as a theatrical release and as a TV miniseries (1985), in several different home video versions of various running times, and in a longer director's cut version supervised by Petersen in 1997.

An adaptation of Lothar-Günther Buchheim's 1973 German novel of the same name, the film is set during World War II and tells the fictional story of U-boat U-96 and its crew, as they set out on yet another hazardous patrol in the Battle of the Atlantic. It depicts both the excitement of battle and the tedium of the fruitless hunt, and shows the men serving aboard U-boats as ordinary individuals with a desire to do their best for their comrades and their country. 

Development began in 1979. Several American directors were considered three years earlier before the film was shelved. During production, Heinrich Lehmann-Willenbrock, the captain of the real" U-96" and one of Germany's top U-boat "tonnage aces" during the war, and Hans-Joachim Krug, former first officer on , served as consultants. One of Petersen's goals was to guide the audience through "a journey to the edge of the mind" (the film's German tagline "Eine Reise ans Ende des Verstandes"), showing "what war is all about".

Produced with a budget of 32 million DM (about $18.5 million), the film's high production cost ranks it among the most expensive films in the history of German cinema. It was the second most expensive up until that time, after "Metropolis". The film was released on September 17, 1981. It grossed over $80 million worldwide. Though not an immediate financial success, the film received highly positive reviews and was nominated for six Academy Awards, two of which (for Best Director and Best Adapted Screenplay) went to Petersen himself; he was also nominated for a BAFTA Award and DGA Award. Today, the film is seen as one of the greatest of all German films.

Lt. Werner (Herbert Grönemeyer), has been assigned as a war correspondent on the in October 1941. He is driven by its captain (Jürgen Prochnow), and chief engineer (Klaus Wennemann), to a raucous French bordello where he meets some of the crew. Thomsen (Otto Sander), another captain, gives a crude drunken speech to celebrate his "Ritterkreuz" award, in which he openly mocks not only Winston Churchill but implicitly Adolf Hitler as well.

The next morning, they sail out of the harbour of La Rochelle to a cheering crowd and playing band. Werner is given a tour of the boat. As time passes, he observes ideological differences between the new crew members and the hardened veterans, particularly the captain, who is embittered and cynical about the war. The new men, including Werner, are often mocked by the rest of the crew, who share a tight bond. After days of boredom, the crew is excited by another U-boat's spotting of an enemy convoy, but they soon locate a British destroyer. While the Old Man attempts to sink the destroyer, it spots the sub's periscope, and attacks. They are bombarded with depth charges, but escape with only light damage.

The next three weeks are spent enduring a relentless North Atlantic gale. Morale drops after a series of misfortunes, but the crew is cheered temporarily by a chance encounter with Thomsen's boat. Shortly after the storm ends, the boat encounters a British convoy and quickly launches four torpedoes, sinking two ships. They are spotted by a destroyer and have to dive below test depth, the submarine's rated limit. During the ensuing depth-charge attack, the chief machinist, Johann, panics and has to be restrained. The boat sustains heavy damage, but is eventually able to safely surface when night falls. The enemy tanker they torpedoed is still afloat and on fire, so they torpedo it again, only to learn there are still sailors aboard. The U-boat men watch in horror as the sailors leap overboard and swim towards them. Unable to accommodate prisoners, the captain orders the boat away.

The worn-out U-boat crew looks forward to returning home to La Rochelle in time for Christmas, but the ship is ordered to La Spezia, Italy, which means passing through the Strait of Gibraltar—an area heavily defended by the Royal Navy. The U-boat makes a secret night rendezvous at the harbour of Vigo, in neutral although Axis-friendly Spain, with the SS "Weser", an interned German merchant ship that clandestinely provides U-boats with fuel, torpedoes, and other supplies. The filthy officers seem out of place at the opulent dinner prepared for them, but are warmly greeted by enthusiastic officers eager to hear their exploits. The captain learns from an envoy of the German consulate that his request for Werner and Chief Engineer to be sent back to Germany has been denied.

The crew finishes resupplying and departs for Italy. As they carefully approach the Straits of Gibraltar and are just about to dive, they are suddenly bombed and strafed by a British fighter plane, wounding the navigator. The captain orders his damaged boat directly south towards the North African coast at full speed determined to save his crew even if he loses the boat. British warships begin shelling and they are forced to dive. When attempting to level off, the boat does not respond and continues to sink until, just before being crushed by the pressure, it lands on a sea shelf, at the depth of 280 metres. The crew work desperately to make numerous repairs before running out of oxygen. After over 16 hours, they are able to surface by blowing their ballast tanks, and with a badly damaged boat limp back towards La Rochelle under cover of darkness with only one engine still operational.

The crew is exhausted when they finally reach La Rochelle on Christmas Eve. Shortly after the wounded navigator is taken ashore to a waiting ambulance, Allied planes bomb and strafe the facilities, wounding or killing many of the crew. Ullmann, Johann, the 2nd Watch Officer, and the Bibelforscher are killed. Frenssen, "Bootsmann" Lamprecht and Hinrich are wounded. After the raid, Werner leaves the U-boat bunker in which he had taken shelter and finds the captain, badly injured by shrapnel, watching his U-boat sink at the dock. Just after the boat disappears under the water, the captain collapses and dies. Werner runs to the captain's lifeless body, recoils, and quickly glances around at the destruction, and back to the captain's body, his face frozen with distress, and with tears in his eyes.


The film features both Standard German-speakers and dialect speakers. Petersen states in the DVD audio commentary that young men from throughout Germany and Austria were recruited for the film, as he wanted faces and dialects that would accurately reflect the diversity of the Third Reich, around 1941. All of the main actors are bilingual in German and English, and when the film was dubbed into English, each actor recorded his own part (with the exception of Martin Semmelrogge, who only dubbed his own role in the Director's Cut). The German version is dubbed as well, as the film was shot "silent", because the dialogue spoken on-set would have been drowned out by the gyroscopes in the special camera developed for filming.

During 1941, war correspondent Lothar-Günther Buchheim joined for a single patrol, in the Battle of the Atlantic. His orders were to photograph and describe the U-boat in action. In 1973, Buchheim published a novel based on his wartime experiences, "Das Boot" (The Boat), a fictionalised autobiographical account narrated by a "Leutnant Werner". It became the best-selling German fiction work on the war.. The follow up sequel "Die Festung" by Buchheim hit the bookshelfs in 1995. 

Production of "Das Boot" took two years (1979–1981). Most of the filming was done in one year; to make the appearance of the actors as realistic as possible, scenes were filmed in sequence over the course of the year. This ensured natural growth of beards and hair, increasing skin pallor, and signs of strain on the actors, who had, just like real U-boat men, spent many months in a cramped, unhealthy atmosphere. 

Production for this film originally began in 1976. Several American directors were considered, and the "Kaleu" ("Kapitänleutnant") was to be played by Robert Redford. Disagreements sprang up among various parties and the project was shelved. Another Hollywood production was attempted with other American directors in mind, this time with the "Kaleu" to be portrayed by Paul Newman. This effort primarily failed due to technical concerns, for example, how to film the close encounter of the two German submarines at sea during a storm.

The production included the construction of several models of different sizes, as well as a complete, detailed reconstruction of the interior of a , a Type VIIC-class U-boat.

Hans-Joachim Krug, former first officer on U-219, served as a consultant, as did Heinrich Lehmann-Willenbrock, the captain of the real U-96.

The director's meticulous attention to detail resulted in a historically accurate depiction. In the film, there is only one ardent Nazi in the crew of 40, namely the First Watch Officer (referred to comically in one scene as "Unser Hitlerjugendführer" or "Our Hitler Youth Leader"). The rest of the officers are either indifferent or openly anti-Nazi (the Captain). The enlisted sailors and NCO are portrayed as apolitical. In his book "Iron Coffins", former U-boat commander Herbert A. Werner states that the selection of naval personnel based on their loyalty to the party only occurred later in the war (from 1943 onward) when the U-boats were suffering high casualties and when morale was declining. Such a degree of skepticism may or may not have occurred. In support of "Das Boot" on this subject, U-Boat historian Michael Gannon maintains that the U-boat navy was one of the least pro-Nazi branches of the German armed forces.

Even though the beginning and the end of the film occur in the port of La Rochelle, it does not correspond historically. The submarine base in La Rochelle was not functional before November 1941, and at the time of the film the port was dried up. While Saint-Nazaire was the base used in the novel, the film was changed to La Rochelle because its appearance had not changed to such a large degree in the years since World War II.

Several different sets were used. Two full-size mock-ups of a Type VIIC boat were built, one representing the portion above water for use in outdoor scenes, and the other a cylindrical tube on a motion mount for the interior scenes. The mock-ups were built according to U-boat plans from Chicago's Museum of Science and Industry. 

The outdoor mock-up was basically a shell propelled with a small engine, and stationed in La Rochelle, France and has a history of its own. One morning the production crew walked out to where they kept it afloat and found it missing. Someone had forgotten to inform the crew that an American filmmaker had rented the mock-up for his own film shooting in the area. This filmmaker was Steven Spielberg and the film he was shooting was "Raiders of the Lost Ark". A few weeks later, during production, the mock-up cracked in a storm and sank, was recovered and patched to stand in for the final scenes. The full-sized mock-up was used during the Gibraltar surface scenes; the attacking aircraft (played by a North American T-6 Texan / Harvard) and rockets were real while the British ships were models.

A mock-up of a conning tower was placed in a water tank at the Bavaria Studios in Munich for outdoor scenes not requiring a full view of the boat's exterior. When filming on the outdoor mockup or the conning tower, jets of cold water were hosed over the actors to simulate the breaking ocean waves. During the filming there was a scene where actor Jan Fedder (Pilgrim) fell off the bridge while the U-boat was surfaced. Fedder broke several ribs. This scene was not scripted and during the take one of the actors exclaims ""Mann über Bord!"" in order to draw attention to Fedder. Petersen, who at first did not realise this was an accident said "Good idea, Jan. We'll do that one more time!" However, since Fedder was genuinely injured and had to be hospitalised, this was the only take available and eventually Petersen kept this scene in the film. In this scene, the pained expression on Fedder's face is authentic and not acted. Petersen also had to rewrite Fedder's character for a portion of the film so that the character was portrayed as bedridden. For his scenes later in the film Fedder had to be brought to and from set from the hospital since he suffered a concussion while filming his accident scene. Fedder eventually recovered enough and Pilgrim is seen on his feet from the scene when the "U-96" abandons the British sailors. A half-sized full hull operating model was used for underwater shots and some surface running shots, in particular the meeting in stormy seas with another U-boat. The tank was also used for the shots of British sailors jumping from their ship; a small portion of the tanker hull was constructed for these shots.

The interior U-boat mock-up was mounted five metres off the floor and was shaken, rocked, and tilted up to 45 degrees by means of a hydraulic apparatus, and was vigorously shaken to simulate depth charge attacks. Petersen was admittedly obsessive about the structural detail of the U-boat set, remarking that "every screw" in the set was an authentic facsimile of the kind used in a World War II U-boat. In this he was considerably assisted by the numerous photographs Lothar-Günther Buchheim had taken during his own voyage on the historical "U-96", some of which had been published in his 1976 book, U-Boot-Krieg ("U-Boat War").

Throughout the filming, the actors were forbidden to go out in sunlight, to create the pallor of men who seldom saw the sun during their missions. The actors went through intensive training to learn how to move quickly through the narrow confines of the vessel.

Most of the interior shots were filmed using a hand-held Arriflex of cinematographer Jost Vacano's design to convey the claustrophobic atmosphere of the boat. It had two gyroscopes to provide stability, a different and smaller scale solution than the Steadicam, so that it could be carried throughout the interior of the mock-up.

Director Wolfgang Petersen has overseen the creation of several different versions of his film. The first to be released was the 149-minute theatrical cut, released to theatres in Germany in 1981 and America in 1982. It was nominated for six Academy Awards for (Cinematography, Directing, Film Editing, Sound (Milan Bor, Trevor Pyke and Mike Le Mare), Sound Effects Editing, and Writing).

The film was partly financed by German television broadcasters WDR and the SDR, and much more footage had been shot than was shown in the theatrical version. A version of three 100-minute episodes was transmitted on BBC Two in the United Kingdom in October 1984, and in Germany and Austria the following year. In 1984 on UK (German broadcast in 1985) television a 6 part series which is partially made of scenes from the movie was shown. In 1988 a version composed of six 50-minute episodes was screened. These episodes had additional flashback scenes summarising past episodes.

Petersen then supervised the editing of six hours of film, from which was distilled a 209-minute version, "Das Boot: The Director's Cut". Released to cinemas worldwide in 1997, this cut combines the action sequences seen in the feature-length version with character development scenes contained in the mini-series. In addition, the audio and video quality was improved from that previously available. Petersen had originally planned to release this version in 1981, but for commercial reasons it was not possible. In 1998 it was released on DVD as a single-disc edition including an audio commentary by Petersen, lead actor Jürgen Prochnow and director's cut producer Ortwin Freyermuth; a 6-minute making-of featurette; and in most territories, the theatrical trailer. In 2003 it was also released as a "Superbit" edition with no extra features, but a superior quality higher bit-rate and the film spread across two discs.

The miniseries version was released on DVD in 2004, as "Das Boot: The Original Uncut Version", also with enhanced audio and video quality. It omits the episode opening flashback scenes of the 1988 television broadcast so is slightly shorter, running 293 minutes.

From 2010 onwards, the 208-minute "Director's Cut", along with various new extras, was released internationally on Blu-ray. The American 2-disc Collector's Set also uniquely included the original 149-minute theatrical cut, which is otherwise unreleased on DVD or Blu-ray.

In 2014 the original miniseries, also known as "The Original Uncut Version", was released on Blu-ray in Germany with optional English audio and subtitles.

For both the "Director's Cut" and "The Original Uncut Version", new English language dubs were recorded featuring most of the original cast, who were bilingual. These dubs are included on all DVD and Blu-ray releases.


Prior to the 55th Academy Awards on 11 April, 1983 the movie 
received 6 nominations.
Cinematography for Jost Vacano, Directing for Wolfgang Petersen, Film Editing for Hannes Nikel, Sound for Milan Bor, Trevor Pyke, Mike Le-Mare, Sound Effects Editing for Mike Le-Mare, Writing (Screenplay based on material from another medium) for Wolfgang Petersen. 

Today, the film is seen as one of the greatest of all German films. The film currently has a "certified fresh" score of 98% based on 46 reviews with an average rating of 9 out of 10 on Rotten Tomatoes. The critical consensus states "Taut, breathtakingly thrilling, and devastatingly intelligent, Das Boot is one of the greatest war films ever made." The film also has a score of 86 out of 100 on Metacritic based on 15 critics indicating "universal acclaim". For its unsurpassed authenticity in tension and realism, it is regarded internationally as pre-eminent among all submarine films. The film was ranked #25 in "Empire" magazine's "The 100 Best Films Of World Cinema" in 2010.

At the 55th Academy Awards, "Das Boot" was nominated for six awards, including Best Director. To this day, it holds the record for the most Academy Award nominations for a German film.

In late 2007, there was an exhibition about the film "Das Boot", as well as about the real U-Boat "U-96", at the Haus der Geschichte (House of German History) in Bonn. Over 100,000 people visited the exhibition during its four-month run.

Even though impressed by the technological accuracy of the film's set-design and port construction buildings, novelist Lothar-Günther Buchheim expressed great disappointment with Petersen's adaptation in a film review published in 1981, describing Petersen's film as converting his clearly anti-war novel into a blend of a "cheap, shallow American action flick" and a "contemporary German propaganda newsreel from World War II". He also criticised the hysterical overacting of the cast, which he called highly unrealistic, despite their talent. Buchheim, after several attempts for an American adaptation had failed, had provided his own script as soon as Petersen was chosen as new director. It would have been a six-hour epic; Petersen turned him down because the producers were aiming for a 90-minute feature for international release. However, today's Director's Cut of "Das Boot" amounts to over 200 minutes, and the complete TV version of the film is 282 minutes long.

The characteristic lead melody of the soundtrack, composed and produced by Klaus Doldinger, took on a life of its own after German rave group U96 created a remixed "techno version" in 1991. The title theme "Das Boot" later became an international hit.

The official soundtrack features only compositions by Doldinger, except for "J'attendrai" sung by Rina Ketty. The soundtrack ("Filmmusik") released following the release of "The Director's Cut" version omits "J'attendrai".

Songs heard in the film, but not included on the album are "La Paloma" sung by Rosita Serrano, the "Erzherzog-Albrecht-Marsch" (a popular military march), "It's a Long Way to Tipperary" performed by the Red Army Chorus, "Heimat, Deine Sterne" and the Westerwald-Marsch.

In July 2016, it was announced that the film will be adapted into a television series
As of September 2017, the series is in production and is planned to premiere at the end of 2018 on multiple European channels of Sky plc
. It is being produced by Bavaria Fiction, Sky Deutschland and Sonar Entertainment..




</doc>
<doc id="8996" url="https://en.wikipedia.org/wiki?curid=8996" title="Dynamic HTML">
Dynamic HTML

Dynamic HTML, or DHTML, is an umbrella term for a collection of technologies used together to create interactive and animated websites by using a combination of a static markup language (such as HTML), a client-side scripting language (such as JavaScript), a presentation definition language (such as CSS), and the Document Object Model (DOM). The application of DHTML was introduced by Microsoft with the release of Internet Explorer 4 in 1997.

DHTML allows scripting languages to change variables in a web page's definition language, which in turn affects the look and function of otherwise "static" HTML page content, after the page has been fully loaded and during the viewing process. Thus the dynamic characteristic of DHTML is the way it functions while a page is viewed, not in its ability to generate a unique page with each page load.

By contrast, a dynamic web page is a broader concept, covering any web page generated differently for each user, load occurrence, or specific variable values. This includes pages created by client-side scripting, and ones created by server-side scripting (such as PHP, Perl, JSP or ASP.NET) where the web server generates content before sending it to the client.

DHTML is differentiated from Ajax by the fact that a DHTML page is still request/reload-based. With DHTML, there may not be any interaction between the client and server after the page is loaded; all processing happens in JavaScript on the client side. By contrast, an Ajax page uses features of DHTML to initiate a request (or 'subrequest') to the server to perform additional actions. For example, if there are multiple tabs on a page, pure DHTML approach would load the contents of all tabs and then dynamically display only the one that is active, while AJAX could load each tab only when it is really needed.

DHTML allows authors to add effects to their pages that are otherwise difficult to achieve, by changing the Document Object Model (DOM) and page style. The combination of HTML, CSS and JavaScript offers ways to:


A less common use is to create browser-based action games. Although a number of games were created using DHTML during the late 1990s and early 2000s, differences between browsers made this difficult: many techniques had to be implemented in code to enable the games to work on multiple platforms. Recently browsers have been converging towards web standards, which has made the design of DHTML games more viable. Those games can be played on all major browsers and they can also be ported to Plasma for KDE, Widgets for macOS and Gadgets for Windows Vista, which are based on DHTML code.

The term "DHTML" has fallen out of use in recent years as it was associated with practices and conventions that tended to not work well between various web browsers. DHTML may now be referred to as unobtrusive JavaScript coding (DOM Scripting), in an effort to place an emphasis on agreed-upon best practices while allowing similar effects in an accessible, standards-compliant way.

DHTML support with extensive DOM access was introduced with Internet Explorer 4.0. Although there was a basic dynamic system with Netscape Navigator 4.0, not all HTML elements were represented in the DOM. When DHTML-style techniques became widespread, varying degrees of support among web browsers for the technologies involved made them difficult to develop and debug. Development became easier when Internet Explorer 5.0+, Mozilla Firefox 2.0+, and Opera 7.0+ adopted a shared DOM inherited from ECMAScript.

More recently, JavaScript libraries such as jQuery have abstracted away many of the day-to-day difficulties in cross-browser DOM manipulation.

Typically a web page using DHTML is set up in the following way:

The following code illustrates an often-used function. An additional part of a web page will only be displayed if the user requests it.

DHTML is not a technology in and of itself; rather, it is the product of three related and complementary technologies: HTML, Cascading Style Sheets (CSS), and JavaScript. To allow scripts and components to access features of HTML and CSS, the contents of the document are represented as objects in a programming model known as the Document Object Model (DOM).

The DOM API is the foundation of DHTML, providing a structured interface that allows access and manipulation of virtually anything in the document. The HTML elements in the document are available as a hierarchical tree of individual objects, meaning you can examine and modify an element and its attributes by reading and setting properties and by calling methods. The text between elements is also available through DOM properties and methods.

The DOM also provides access to user actions such as pressing a key and clicking the mouse. You can intercept and process these and other events by creating event handler functions and routines. The event handler receives control each time a given event occurs and can carry out any appropriate action, including using the DOM to change the document.

Dynamic styles are a key feature of DHTML. By using CSS, you can quickly change the appearance and formatting of elements in a document without adding or removing elements. This helps keep your documents small and the scripts that manipulate the document fast.

The object model provides programmatic access to styles. This means you can change inline styles on individual elements and change style rules using simple JavaScript programming.

Inline styles are CSS style assignments that have been applied to an element using the style attribute. You can examine and set these styles by retrieving the style object for an individual element. For example, to highlight the text in a heading when the user moves the mouse pointer over it, you can use the style object to enlarge the font and change its color, as shown in the following simple example.




</doc>
<doc id="8997" url="https://en.wikipedia.org/wiki?curid=8997" title="Distance education">
Distance education

Distance education or long-distance learning is the education of students who may not always be physically present at a school. Traditionally, this usually involved correspondence courses wherein the student corresponded with the school via post. Today it involves online education. Courses that are conducted (51 percent or more) are either hybrid, blended or 100% distance learning. Massive open online courses (MOOCs), offering large-scale interactive participation and open access through the World Wide Web or other network technologies, are recent developments in distance education. A number of other terms (distributed learning, e-learning, online learning, etc.) are used roughly synonymously with distance education.

One of the earliest attempts was advertised in 1728. This was in the "Boston Gazette" for "Caleb Philipps, Teacher of the new method of Short Hand", who sought students who wanted to learn through weekly mailed lessons.

The first distance education course in the modern sense was provided by Sir Isaac Pitman in the 1840s, who taught a system of shorthand by mailing texts transcribed into shorthand on postcards and receiving transcriptions from his students in return for correction. The element of student feedback was a crucial innovation of Pitman's system. This scheme was made possible by the introduction of uniform postage rates across England in 1840.

This early beginning proved extremely successful, and the Phonographic Correspondence Society was founded three years later to establish these courses on a more formal basis. The Society paved the way for the later formation of Sir Isaac Pitman Colleges across the country.

The first correspondence school in the United States was the Society to Encourage Studies at Home, which was founded in 1873.

The University of London was the first university to offer distance learning degrees, establishing its External Programme in 1858. The background to this innovation lay in the fact that the institution (later known as University College London) was non-denominational and, given the intense religious rivalries at the time, there was an outcry against the "godless" university. The issue soon boiled down to which institutions had degree-granting powers and which institutions did not.
The compromise solution that emerged in 1836 was that the sole authority to conduct the examinations leading to degrees would be given to a new officially recognized entity called the "University of London", which would act as examining body for the University of London colleges, originally University College London and King's College London, and award their students University of London degrees. As Sheldon Rothblatt states: "Thus arose in nearly archetypal form the famous English distinction between teaching and examining, here embodied in separate institutions."

With the state giving examining powers to a separate entity, the groundwork was laid for the creation of a programme within the new university which would both administer examinations and award qualifications to students taking instruction at another institution or pursuing a course of self-directed study.

Referred to as "People's University" by Charles Dickens because it provided access to higher education to students from less affluent backgrounds, the External Programme was chartered by Queen Victoria in 1858, making the University of London the first university to offer distance learning degrees to students. Enrollment increased steadily during the late 19th century, and its example was widely copied elsewhere. This program is now known as the University of London International Programme and includes Postgraduate, Undergraduate and Diploma degrees created by colleges such as the London School of Economics, Royal Holloway and Goldsmiths.
In the United States, William Rainey Harper, first president of the University of Chicago, celebrated the concept of extended education, whereby the research university had satellite colleges in the wider community.

In 1892, Harper encouraged correspondence courses to further promote education, an idea that was put into practice by Chicago, Wisconsin, Columbia, and several dozen other universities by the 1920s Columbia University. Enrollment in the largest private for-profit school based in Scranton, Pennsylvania, the International Correspondence Schools grew explosively in the 1890s. Founded in 1888 to provide training for immigrant coal miners aiming to become state mine inspectors or foremen, it enrolled 2500 new students in 1894 and matriculated 72,000 new students in 1895. By 1906 total enrollments reached 900,000. The growth was due to sending out complete textbooks instead of single lessons, and the use of 1200 aggressive in-person salesmen. There was a stark contrast in pedagogy:

Education was a high priority in the Progressive Era, as American high schools and colleges expanded greatly. For men who were older or were too busy with family responsibilities, night schools were opened, such as the YMCA school in Boston that became Northeastern University. Outside the big cities, private correspondence schools offered a flexible, narrowly focused solution. Large corporations systematized their training programs for new employees. The National Association of Corporation Schools grew from 37 in 1913 to 146 in 1920. Starting in the 1880s, private schools opened across the country which offered specialized technical training to anyone who enrolled, not just the employees of one company. Starting in Milwaukee in 1907, public schools began opening free vocational programs.

Only a third of the American population lived in cities of 100,000 or more population In 1920; to reach the rest, correspondence techniques had to be adopted. Australia, with its vast distances, was especially active; the University of Queensland established its Department of Correspondence Studies in 1911. In South Africa, the University of South Africa, formerly an examining and certification body, started to present distance education tuition in 1946. The International Conference for Correspondence Education held its first meeting in 1938. The goal was to provide individualized education for students, at low cost, by using a pedagogy of testing, recording, classification, and differentiation.

The Open University in the United Kingdom was founded by the-then Labour government led by Prime Minister, Harold Wilson, based on the vision of Michael Young. Planning commenced in 1965 under the Minister of State for Education, Jennie Lee, who established a model for the Open University (OU) as one of widening access to the highest standards of scholarship in higher education, and set up a planning committee consisting of university vice-chancellors, educationalists and television broadcasters, chaired by Sir Peter Venables. The British Broadcasting Corporation (BBC) Assistant Director of Engineering at the time, James Redmond, had obtained most of his qualifications at night school, and his natural enthusiasm for the project did much to overcome the technical difficulties of using television to broadcast teaching programmes.

The Open University revolutionised the scope of the correspondence program and helped to create a respectable learning alternative to the traditional form of education. It has been at the forefront of developing new technologies to improve the distance learning service as well as undertaking research in other disciplines. Walter Perry was appointed the OU's first vice-chancellor in January 1969, and its foundation secretary was Anastasios Christodoulou. The election of the new Conservative government under the leadership of Edward Heath, in 1970; led to budget cuts under Chancellor of the Exchequer Iain Macleod (who had earlier called the idea of an Open University "blithering nonsense"). However, the OU accepted its first 25,000 students in 1971, adopting a radical open admissions policy. At the time, the total student population of conventional universities in the United Kingdom was around 130,000.

Athabasca University, Canada's Open University, was created in 1970 and followed a similar, though independently developed, pattern. The Open University inspired the creation of Spain's National University of Distance Education (1972) and Germany's FernUniversität in Hagen (1974). There are now many similar institutions around the world, often with the name "Open University" (in English or in the local language).

Most open universities use distance education technologies as delivery methods, though some require attendance at local study centres or at regional "summer schools". Some open universities have grown to become "mega-universities", a term coined to denote institutions with more than 100,000 students.

Although the expansion of the Internet blurs the boundaries, distance education technologies are divided into two modes of delivery: synchronous learning and asynchronous learning.

In synchronous learning, all participants are "present" at the same time. In this regard, it resembles traditional classroom teaching methods despite the participants being located remotely. It requires a timetable to be organized. Web conferencing, videoconferencing, educational television, instructional television are examples of synchronous technology, as are direct-broadcast satellite (DBS), internet radio, live streaming, telephone, and web-based VoIP.
Web conferencing software helps to facilitate meetings in distance learning courses and usually contain additional interaction tools such as text chat, polls, hand raising, emoticons etc. These tools also support asynchronous participation by students being able to listen to recordings of synchronous sessions. Immersive environments (notably SecondLife) have also been used to enhance participant presence in distance education courses. Another form of synchronous learning that has been entering the classroom over the last couple of years is the use of robot proxies including those that allow sick students to attend classes.

Some universities have been starting to use robot proxies to enable more engaging synchronous hybrid classes where both remote and in person students can be present and interact using telerobotics devices such as the Kubi Telepresence robot stand that looks around and the Double Robot that roams around. With these telepresence robots, the remote students have a seat at the table or desk instead of being on a screen on the wall.

In asynchronous learning, participants access course materials flexibly on their own schedules. Students are not required to be together at the same time. Mail correspondence, which is the oldest form of distance education, is an asynchronous delivery technology, as are message board forums, e-mail, video and audio recordings, print materials, voicemail, and fax.

The two methods can be combined. Many courses offered by both open universities and an increasing number of campus based institutions use periodic sessions of residential or day teaching to supplement the sessions delivered at a distance. This type of mixed distance and campus based education has recently come to be called "blended learning" or less often "hybrid learning". Many open universities uses a blend of technologies and a blend of learning modalities (face-to-face, distance, and hybrid) all under the rubric of "distance learning".

Distance learning can also use interactive radio instruction (IRI), interactive audio instruction (IAI), online virtual worlds, digital games, webinars, and webcasts, all of which are referred to as e-Learning.

The rapid spread of film in the 1920s and radio in the 1930s led to proposals to use it for distance education. By 1938, at least 200 city school systems, 25 state boards of education, and many colleges and universities broadcast educational programs for the public schools. One line of thought was to use radio as a master teacher.

A typical setup came in Kentucky in 1948 when John Wilkinson Taylor, president of the University of Louisville, teamed up with NBC to use radio as a medium for distance education, The chairman of the Federal Communications Commission endorsed the project and predicted that the "college-by-radio" would put "American education 25 years ahead". The University was owned by the city, and local residents would pay the low tuition rates, receive their study materials in the mail, and listen by radio to live classroom discussions that were held on campus. Physicist Daniel Q. Posin also was a pioneer in the field of distance education when he hosted a televised course through DePaul University.

Charles Wedemeyer of the University of Wisconsin–Madison also promoted new methods. From 1964 to 1968, the Carnegie Foundation funded Wedemeyer's "Articulated Instructional Media Project" (AIM) which brought in a variety of communications technologies aimed at providing learning to an off-campus population. The radio courses faded away in the 1950s. Many efforts to use television along the same lines proved unsuccessful, despite heavy funding by the Ford Foundation.

From 1970 to 1972 the Coordinating Commission for Higher Education in California funded Project Outreach to study the potential of telecourses. The study included the University of California, California State University and the community colleges. This study led to coordinated instructional systems legislation allowing the use of public funds for non-classroom instruction and paved the way for the emergence of telecourses as the precursor to the online courses and programs of today. The Coastline Community Colleges, The Dallas County Community College District, and Miami Dade Community College led the way. The "Adult Learning Service" of the US Public Broadcasting Service came into being and the “wrapped” series, and individually produced telecourse for credit became a significant part of the history of distance education and online learning.

The widespread use of computers and the internet have made distance learning easier and faster, and today virtual schools and virtual universities deliver full curricula online. The capacity of Internet to support voice, video, text and immersion teaching methods made earlier distinct forms of telephone, videoconferencing, radio, television, and text based education somewhat redundant. However, many of the techniques developed and lessons learned with earlier media are used in Internet delivery.

The first completely online courses for graduate credit were offered by Connected Education, starting in the Fall of 1985, leading to the MA in Media Studies from The New School.
The first new and fully online university was founded in 1994 as the Open University of Catalonia, headquartered in Barcelona, Spain. In 1999 Jones International University was launched as the first fully online university accredited by a regional accrediting association in the US.

Between 2000 and 2008, enrollment in distance education courses increased rapidly in almost every country in both developed and developing countries. Many private, public, non-profit and for-profit institutions worldwide now offer distance education courses from the most basic instruction through to the highest levels of degree and doctoral programs. New York University, for example, offers online degrees in engineering and management-related fields through NYU Tandon Online. Levels of accreditation vary: widely respected universities such as Stanford University and Harvard now deliver online courses—but other online schools receive little outside oversight, and some are actually fraudulent, i.e., diploma mills. In the US, the Distance Education Accrediting Commission (DEAC) specializes in the accreditation of distance education institutions.

In the United States in 2011, it was found that a third of all the students enrolled in postsecondary education had taken an accredited online course in a postsecondary institution. Even though growth rates are slowing, enrollment for online courses has been seen to increase with the advance in technology. The majority of public and private colleges now offer full academic programs online. These include, but are not limited to, training programs in the mental health, occupational therapy, family therapy, art therapy, physical therapy, and rehabilitation counseling fields. Even engineering courses that require the manipulation and control of machines and robots that are technically more challenging to learn remotely are subject to distance learning through the internet.

Distance education has a long history, but its popularity and use has grown exponentially as more advanced technology has become available. By 2008, online learning programs were available in the United States in 44 states at the K-12 level.

Internet forums, online discussion group and online learning community can contribute to an efficacious distance education experience. Research shows that socialization plays an important role in some forms of distance education.

E-courses are also a viable option for distance learning. There are many available that cover a broad range of topics.

Distance education can be delivered in a paced format similar to traditional campus based models in which learners commence and complete a course at the same time. Paced delivery is currently the most common mode of distance education delivery. Alternatively, some institutions offer self-paced programs that allow for continuous enrollment and the length of time to complete the course is set by the learner's time, skill and commitment levels. Paced courses may be offered in either synchronus mode, but self-paced courses are almost always offered asynchronously. Each delivery model offers both advantages and disadvantages for students, teachers and institutions.

Kaplan and Haenlein classify distance education into four groups along the dimensions Time dependency and Number of participants: 1) MOOCs (Massive Open Online Courses): Open-access online course (i.e., without specific participation restrictions) that allows for unlimited (massive) participation; 2) SPOCs (Small Private Online Courses): Online course that only offers a limited number of places and therefore requires some form of formal enrollment; 3) SMOCs (Synchronous Massive Online Courses): Open-access online course that allows for unlimited participation but requires students to be "present" at the same time (synchronously); 4) SSOCs (Synchronous Private Online Courses): Online course that only offers a limited number of places and requires students to be "present" at the same time (synchronously).

Paced models are a familiar mode as they are used almost exclusively in campus based schools. Institutes that offer both distance and campus programs usually use paced models as teacher workload, student semester planning, tuition deadlines, exam schedules and other administrative details can be synchronized with campus delivery. Student familiarity and the pressure of deadlines encourages students to readily adapt to and usually succeed in paced models. However, student freedom is sacrificed as a common pace is often too fast for some students and too slow for others. In addition life events, professional or family responsibilities can interfere with a students capability to complete tasks to an external schedule. Finally, paced models allows students to readily form communities of inquiry and to engage in collaborative work.

Self-paced courses maximize student freedom, as not only can students commence studies on any date, but they can complete a course in as little time as a few weeks or up to a year or longer. Students often enroll in self-paced study when they are under pressure to complete programs, have not been able to complete a scheduled course, need additional courses or have pressure which precludes regular study for any length of time. The self-paced nature of the programming, though is an unfamiliar model for many students and can lead to excessive procrastination resulting in course incompletion. Assessment of learning can also be challenging as exams can be written on any day, making it possible for students to share examination questions with resulting loss of academic integrity. Finally, it is extremely challenging to organize collaborative work activities, though some schools are developing cooperative models based upon networked and connectivist pedagogies, for use in self-paced programs.

Distance learning can expand access to education and training for both general populace and businesses since its flexible scheduling structure lessens the effects of the many time-constraints imposed by personal responsibilities and commitments. Devolving some activities off-site alleviates institutional capacity constraints arising from the traditional demand on institutional buildings and infrastructure. Furthermore, there is the potential for increased access to more experts in the field and to other students from diverse geographical, social, cultural, economic, and experiential backgrounds.
As the population at large becomes more involved in lifelong learning beyond the normal schooling age, institutions can benefit financially, and adult learning business courses may be particularly lucrative. Distance education programs can act as a catalyst for institutional innovation and are at least as effective as face-to-face learning programs, especially if the instructor is knowledgeable and skilled.

Distance education can also provide a broader method of communication within the realm of education. With the many tools and programs that technological advancements have to offer, communication appears to increase in distance education amongst students and their professors, as well as students and their classmates. The distance educational increase in communication, particularly communication amongst students and their classmates, is an improvement that has been made to provide distance education students with as many of the opportunities as possible as they would receive in in-person education. The improvement being made in distance education is growing in tandem with the constant technological advancements. Present-day online communication allows students to associate with accredited schools and programs throughout the world that are out of reach for in-person learning. By having the opportunity to be involved in global institutions via distance education, a diverse array of thought is presented to students through communication with their classmates. This is beneficial because students have the opportunity to "combine new opinions with their own, and develop a solid foundation for learning". It has been shown through research that "as learners become aware of the variations in interpretation and construction of meaning among a range of people [they] construct an individual meaning", which can help students become knowledgeable of a wide array of viewpoints in education. To increase the likelihood that students will build effective ties with one another during the course, instructors should use similar assignments for students across different locations to overcome the influence of co-location on relationship building.

The high cost of education affects students in higher education, to which distance education may be an alternative in order to provide some relief. Distance education has been a more cost-effective form of learning, and can sometimes save students a significant amount of money as opposed to traditional education. Distance education may be able to help to save students a considerable amount financially by removing the cost of transportation. In addition, distance education may be able to save students from the economic burden of high-priced course textbooks. Many textbooks are now available as electronic textbooks, known as e-textbooks, which can offer digital textbooks for a reduced price in comparison to traditional textbooks. Also, the increasing improvements in technology have resulted in many school libraries having a partnership with digital publishers that offer course materials for free, which can help students significantly with educational costs.

Within the class, students are able to learn in ways that traditional classrooms would not be able to provide. It is able to promote good learning experiences and therefore, allow students to obtain higher satisfaction with their online learning. For example, students can review their lessons more than once according to their need. Students can then manipulate the coursework to fit their learning by focusing more on their weaker topics while breezing through concepts that they already have or can easily grasp. When course design and the learning environment are at their optimal conditions, distance education can lead students to higher satisfaction with their learning experiences. Studies have shown that high satisfaction correlates to increased learning. For those in a healthcare or mental health distance learning program, online-based interactions have the potential to foster deeper reflections and discussions of client issues as well as a quicker response to client issues, since supervision happens on a regular basis and is not limited to a weekly supervision meeting. This also may contribute to the students feeling a greater sense of support, since they have ongoing and regular access to their instructors and other students.

Distance learning may enable students who are unable to attend a traditional school setting, due to disability or illness such as decreased mobility and immune system suppression, to get a good education. Children who are sick or are unable to attend classes are able to attend them in "person" through the use of robot proxies. This helps the students have experiences of the classroom and social interaction that they are unable to receive at home or the hospital, while still keeping them in a safe learning environment. Over the last few years more students are entering safely back into the classroom thanks to the help of robots. An article from the "New York Times", "A Swiveling Proxy Will Even Wear a Tutu", explains the positive impact of virtual learning in the classroom, and another that explains how even a simple, stationary telepresence robot can help.
Distance education may provide equal access regardless of socioeconomic status or income, area of residence, gender, race, age, or cost per student. Applying universal design strategies to distance learning courses as they are being developed (rather than instituting accommodations for specific students on an as-needed basis) can increase the accessibility of such courses to students with a range of abilities, disabilities, learning styles, and native languages.
Distance education graduates, who would never have been associated with the school under a traditional system, may donate money to the school.

Distance learning may also offer a final opportunity for adolescents that are no longer permitted in the general education population due to behavior disorders. Instead of these students having no other academic opportunities, they may continue their education from their homes and earn their diplomas, offering them another chance to be an integral part of society. 

Distance Learning offers individuals a unique opportunity to benefit from the expertise and resources of the best Universities currently available. Students have the ability to collaborate, share, question, infer and suggest new methods and techniques for continuous improvement of the content. The ability to complete a course at a t pace that is appropriate for each individual is the most effective manner to learn given the personal demands on time and schedule. Self-paced distance learning on a mobile device such is a smartphone provides maximum flexibility and capability. 

Barriers to effective distance education include obstacles such as domestic distractions and unreliable technology, as well as students' program costs, adequate contact with teachers and support services, and a need for more experience.

Some students attempt to participate in distance education without proper training with the tools needed to be successful in the program. Students must be provided with training opportunities (if needed) on each tool that is used throughout the program. The lack of advanced technology skills can lead to an unsuccessful experience. Schools have a responsibility to adopt a proactive policy for managing technology barriers. Time management skills and self discipline in distance education is just as important as complete knowledge of the software and tools being used for learning.

The results of a study of Washington state community college students showed that distance learning students tended to drop out more often than their traditional counterparts due to difficulties in language, time management, and study skills. 

According to Dr. Pankaj Singhm, director of Nims University, "distance learning benefits may outweigh the disadvantages for students in such a technology-driven society; however before indulging into use of educational technology a few more disadvantages should be considered." He describes that over multiple years, "all of the obstacles have been overcome and the world environment for distance education continues to improve." Dr. Pankaj Singhm also claims there is a debate to distance education stating, "due to a lack of direct face-to-face social interaction. However, as more people become used to personal and social interaction online (for example dating, chat rooms, shopping, or blogging), it is becoming easier for learners to both project themselves and socialize with others. This is an obstacle that has dissipated."

Not all courses required to complete a degree may be offered online. Health care profession programs in particular, require some sort of patient interaction through field work before a student may graduate. Studies have also shown that students pursuing a medical professional graduate degree who are participating in distance education courses, favor face to face communication over professor-mediated chat rooms and/or independent studies. However, this is little correlation between student performance when comparing the previous different distance learning strategies.

There is a theoretical problem about the application of traditional teaching methods to online courses because online courses may have no upper size limit. Daniel Barwick noted that there is no evidence that large class size is always worse or that small class size is always better, although a negative link has been established between certain types of instruction in large classes and learning outcomes; he argued that higher education has not made a sufficient effort to experiment with a variety of instructional methods to determine whether large class size is always negatively correlated with a reduction in learning outcomes. Early proponents of Massive Open Online Courses (MOOC)s saw them as just the type of experiment that Barwick had pointed out was lacking in higher education, although Barwick himself has never advocated for MOOCs.

There may also be institutional challenges. Distance learning is new enough that it may be a challenge to gain support for these programs in a traditional brick-and-mortar academic learning environment. Furthermore, it may be more difficult for the instructor to organize and plan a distance learning program, especially since many are new programs and their organizational needs are different from a traditional learning program.
Additionally, though distance education offers industrial countries the opportunity to become globally informed, there are still negative sides to it. Hellman states that "These include its cost and capital intensiveness, time constraints and other pressures on instructors, the isolation of students from instructors and their peers, instructors’ enormous difficulty in adequately evaluating students they never meet face-to-face, and drop-out rates far higher than in classroom-based courses."

A more complex challenge of distance education relates to cultural differences between student and teachers and among students. Distance programmes tend to be more diverse as they could go beyond the geographical borders of regions, countries, and continents, and cross the cultural borders that may exist with respect to race, gender, and religion. That requires a proper understanding and awareness of the norms, differences, preconceptions and potential conflicting issues.

The modern use of electronic educational technology (also called e-learning) facilitates distance learning and independent learning by the extensive use of information and communications technology (ICT), replacing traditional content delivery by postal correspondence. Instruction can be synchronous and asynchronous online communication in an interactive learning environment or virtual communities, in lieu of a physical classroom. "The focus is shifted to the education transaction in the form of virtual community of learners sustainable across time."

One of the most significant issues encountered in the mainstream correspondence model of distance education is transactional distance, which results from the lack of appropriate communication between learner and teacher. This gap has been observed to become wider if there is no communication between the learner and teacher and has direct implications over the learning process and future endeavors in distance education. Distance education providers began to introduce various strategies, techniques, and procedures to increase the amount of interaction between learner and teacher. These measures e.g. more frequent face-to-face tutorials, increased use of information and communication technologies including teleconferencing and the Internet, were designed to close the gap in transactional distance.

Online credentials for learning are digital credentials that are offered in place of traditional paper credentials for a skill or educational achievement. Directly linked to the accelerated development of internet communication technologies, the development of digital badges, electronic passports and massive open online courses (MOOCs) have a very direct bearing on our understanding of learning, recognition and levels as they pose a direct challenge to the status quo. It is useful to distinguish between three forms of online credentials: Test-based credentials, online badges, and online certificates.






</doc>
<doc id="9000" url="https://en.wikipedia.org/wiki?curid=9000" title="Death of a Hero">
Death of a Hero

Death of a Hero is a World War I novel by Richard Aldington. It was his first novel, published by Chatto & Windus in 1929, and thought to be partly autobiographical.

"Death of a Hero" is the story of a young English artist named George Winterbourne who enlists in the army at the beginning of World War I. The book is narrated by an unnamed first-person narrator who claims to have known and served with the main character. It is divided into three parts.

The first part details George's family history. His father, a middle-class man from England's countryside, marries a poor woman who falsely believes she is marrying into a monied family. After George's birth, his mother has a series of lovers.

George is brought up to be a proper and patriotic member of English society. He is encouraged to learn his father's insurance business, but fails to do so. After a disagreement with his parents, he relocates to London to become an artist and live a socialite lifestyle.

The second section of the book deals with George's London life. He ingrains himself in socialite society and engages a number of trendy philosophies.

After he and his lover, Elizabeth, have a pregnancy scare, they decide to marry. Although they do not have a child, the marriage endures. They decide to leave their marriage open. George takes Elizabeth's close friend as a lover, however, and their marriage begins to fall apart. Just as the situation is becoming particularly heated, England declares war on Germany. George decides to enlist.

George trains for the army and is sent to France. (No particular location in France is mentioned. The town behind the front where George spends much of his time is referred to as M---.) He fights on the front for some time. When he returns home, he finds that he has been so affected by the war that he cannot relate to his friends, including his wife and lover.

The casualty rate among officers is particularly high at the front. When a number of officers in George's unit are killed, he is promoted. Upon spending time with the other officers, he finds them to be cynical and utilitarian. He loses faith in the war quickly.

The story ends with George standing up during a machine-gun barrage. He is killed.

At the end of the book there is a poem written from the point of view of a veteran comparing World War I to the Trojan War.

Aldington, a veteran of World War I, claimed that his novel was accurate in terms of speech and style. It contained extensive colloquial speech, including profanity, discussion of sexuality and graphic descriptions of the war and of trench life. There was extensive censorship in England and many war novels had been banned or burned as a result. When Aldington first published his novel, he redacted a number of passages in order to ensure the publication of his book would not be challenged. He insisted that his publishers include a disclaimer in the original printing of the book with the following text:
To my astonishment, my publisher informed me that certain words, phrases, sentences, and even passages, are at present taboo in England. I have recorded nothing which I have not observed in human life, said nothing I do not believe to be true. [...] At my request the publishers are removing what they believe would be considered objectionable, and are placing asterisks to show where omissions have been made. [...] In my opinion it is better for the book to appear mutilated than for me to say what I don't believe. 


</doc>
<doc id="9001" url="https://en.wikipedia.org/wiki?curid=9001" title="Degree Confluence Project">
Degree Confluence Project

The Degree Confluence Project is a World Wide Web-based, all-volunteer project which aims to have people visit each of the integer degree intersections of latitude and longitude on Earth, posting photographs and a narrative of each visit online. The project describes itself as "an organized sampling of the world".

The precise location of each degree confluence uses the WGS 84 horizontal datum, and visitors to degree confluences almost always make use of GPS receivers. For a "successful visit", the visitor must get within 100 metres of the confluence point, and post a narrative and several photographs to the project website. A visit, or attempted visit, which does not conform to these rules may still be recorded on the website as an "incomplete visit". The project encourages visits to degree confluences which have been visited previously, and many confluence points in North America and Europe have been visited several times.

The total number of degree confluences is 64,442, of which 21,543 are on land, 38,409 on water, and 4,490 on the Antarctic and Arctic ice caps. The project categorizes degree confluences as either "primary" or "secondary". A confluence is primary only if it is on land or within sight of land. In addition, at latitudes greater than 48 only some points are designated primary because confluences crowd together near the poles. Both primary and secondary confluences may be visited and recorded.

In addition the visits of special geographical locations can also be reported (special visits) as there are


The project was started by Alex Jarrett in February 1996 because he "liked the idea of visiting a location represented by a round number such as 43°00'00"N 72°00'00"W. What would be there? Would other people have recognized this as a unique spot?"

As of June 2018, 6,495 (39.73%) of 16,348 primary confluences have been visited, covering 189 countries and territories. The project's website is hosted by ibiblio.





</doc>
<doc id="9002" url="https://en.wikipedia.org/wiki?curid=9002" title="Danny Kaye">
Danny Kaye

Danny Kaye (born David Daniel Kaminsky; January 18, 1911 – March 3, 1987) was an American actor, singer, dancer, comedian and musician. His performances featured physical comedy, idiosyncratic pantomimes and rapid-fire novelty songs.

Kaye starred in 17 movies, notably "Wonder Man" (1945), "The Kid from Brooklyn" (1946), "The Secret Life of Walter Mitty" (1947), "The Inspector General" (1949), "Hans Christian Andersen" (1952), "White Christmas" (1954) and "The Court Jester" (1956). 

His films were popular, especially his performances of patter songs and favorites such as "Inchworm" and "The Ugly Duckling". He was the first ambassador-at-large of UNICEF in 1954 and received the French Legion of Honour in 1986 for his years of work with the organization.

David Daniel Kaminsky was born in Brooklyn, New York on January 18, 1911 (though he would later say 1913), to Ukrainian Jewish immigrants Jacob and Clara ("née" Nemerovsky) Kaminsky. He was the youngest of three sons. Jacob and Clara and their older sons Larry and Mac left Dnipropetrovsk two years before Danny's birth; he was their only son born in the United States.

He attended Public School 149 in East New York, Brooklyn — which eventually was renamed to honor him—where he began entertaining his classmates with songs and jokes. He attended Thomas Jefferson High School in Brooklyn but he did not graduate.

His mother died when he was in his early teens. Not long after his mother's death Kaye and his friend Louis ran away to Florida. Kaye sang while Louis played the guitar and the pair eked out a living for a while. When Kaye returned to New York, his father did not pressure him to return to school or work, giving his son the chance to mature and discover his own abilities. Kaye said that as a young boy he had wanted to be a surgeon but the family could not afford a medical school education.

He held a succession of jobs after leaving school: as a soda jerk, insurance investigator and office clerk. Most ended with his being fired. He lost the insurance job when he made an error that cost the insurance company $40,000. The dentist who hired him to look after his office at lunch hour did the same when he found Kaye using his drill on the office woodwork. Years later Kaye married the dentist's daughter, Sylvia. He learned his trade in his teenage years in the Catskills as a tummler in the Borscht Belt.

Kaye's first break came in 1933 when he joined the "Three Terpsichoreans", a vaudeville dance act. They opened in Utica, New York, where he used the name Danny Kaye for the first time. The act toured the United States, then performed in Asia with the show "La Vie Paree". 

The troupe left for a six-month tour of the Far East on February 8, 1934. While they were in Osaka, Japan, a typhoon hit the city. The hotel where Kaye and his colleagues stayed suffered heavy damage. The strong wind hurled a piece of the hotel's cornice into Kaye's room; had he been hit, he might well have been killed. By performance time that evening the city was in the grip of the storm. There was no power and the audience was restless and nervous. To calm them Kaye went on stage holding a flashlight to illuminate his face and sang every song he could recall as loudly as he was able. 

The experience of trying to entertain audiences who did not speak English inspired him to the pantomime, gestures, songs and facial expressions that eventually made his reputation. Sometimes he found pantomime necessary when ordering a meal. Kaye's daughter, Dena, tells a story her father related about being in a restaurant in China and trying to order chicken. Kaye flapped his arms and clucked, giving the waiter an imitation of a chicken. The waiter nodded in understanding, bringing Kaye two eggs. His interest in cooking began on the tour.

Jobs were in short supply when Kaye returned to the United States and he struggled for bookings. One job was working in a burlesque revue with fan dancer Sally Rand. After the dancer dropped a fan while trying to chase away a fly, Kaye was hired to watch the fans so they were always held in front of her.

Danny Kaye made his film debut in a 1935 comedy short "Moon Over Manhattan". In 1937 he signed with New York–based Educational Pictures for a series of two-reel comedies. He usually played a manic, dark-haired, fast-talking Russian in these low-budget shorts, opposite young hopefuls June Allyson and Imogene Coca. The Kaye series ended abruptly when the studio shut down in 1938. He was working in the Catskills in 1937 under the name Danny Kolbin.

His next venture was a short-lived Broadway show with Sylvia Fine as the pianist, lyricist and composer. The "Straw Hat Revue" opened on September 29, 1939 and closed after 10 weeks but critics took notice of Kaye's work. The reviews brought an offer for both Kaye and his bride Sylvia to work at La Martinique, a New York City nightclub. Kaye performed with Sylvia as his accompanist. At La Martinique playwright Moss Hart saw Danny perform which led to Hart casting him in his hit Broadway comedy "Lady in the Dark".

Kaye scored a triumph at age 30 in 1941 playing Russell Paxton in "Lady in the Dark" starring Gertrude Lawrence. His show-stopping number was "Tchaikovsky" by Kurt Weill and Ira Gershwin in which he sang the names of a string of Russian composers at breakneck speed, seemingly without taking a breath. In the next Broadway season he was the star of a show about a young man who is drafted called "Let's Face It!".

His feature film debut was in producer Samuel Goldwyn's Technicolor 1944 comedy "Up in Arms", a remake of Goldwyn's Eddie Cantor comedy "Whoopee!" (1930). Rival producer Robert M. Savini cashed in by compiling three of Kaye's Educational Pictures shorts into a patchwork feature entitled "The Birth of a Star" (1945). Studio mogul Goldwyn wanted Kaye's prominent nose fixed to look less Jewish, Kaye refused but did allow his red hair to be dyed blonde, apparently because it looked better in Technicolor.
Kaye starred in a radio program, "The Danny Kaye Show", on CBS in 1945–46. The program's popularity rose quickly. Before a year he tied with Jimmy Durante for fifth place in the "Radio Daily" popularity poll. Kaye was asked to participate in a USO tour following the end of World War II. It meant that he would be absent from his radio show for nearly two months at the beginning of the season. Kaye's friends filled in, with a different guest host each week. Kaye was the first American actor to visit postwar Tokyo. He had toured there some ten years before with the vaudeville troupe. When Kaye asked to be released from his radio contract in mid-1946 he agreed not to accept a regular radio show for one year and only limited guest appearances on other radio programs. Many of the show's episodes survive today, notable for Kaye's opening "signature" patter ("Git gat gittle, giddle-di-ap, giddle-de-tommy, riddle de biddle de roop, da-reep, fa-san, skeedle de woo-da, fiddle de wada, reep!").

Kaye starred in several movies with actress Virginia Mayo in the 1940s and is known for films such as "The Secret Life of Walter Mitty" (1947), "The Inspector General" (1949), "On the Riviera" (1951) co-starring Gene Tierney, "Knock on Wood" (1954), "White Christmas" (1954), "The Court Jester" (1956) and "Merry Andrew" (1958). Kaye starred in two pictures based on biographies, "Hans Christian Andersen" (1952) the Danish story-teller and "The Five Pennies" (1959) about jazz pioneer Red Nichols. His wife, writer/lyricist Sylvia Fine, wrote many tongue-twisting songs for which Kaye became famous. She was also an associate film producer. Some of Kaye's films included the theme of doubles, two people who look identical (both Danny Kaye) being mistaken for each other to comic effect.

Kaye teamed with the Andrews Sisters on Decca Records in 1947, producing the number-three Billboard smash hit "Civilization (Bongo, Bongo, Bongo)". The success of the pairing prompted both acts to record through 1950, producing several hits including "The Woody Woodpecker Song".

When he appeared at the London Palladium in 1948 he "roused the Royal family to laughter and was the first of many performers who have turned British variety into an American preserve." "Life" magazine described his reception as "worshipful hysteria" and noted that the royal family, for the first time, left the royal box to watch from the front row of the orchestra. He related that he had no idea of the familial connections when the Marquess of Milford Haven introduced himself after a show and said he would like his cousins to see Kaye perform. Kaye stated he never returned to the venue because there was no way to recreate the magic of that time. Kaye had an invitation to return to London for a "Royal Variety Performance" in November of the same year. 

When the invitation arrived, Kaye was busy with "The Inspector General" (which had a working title of "Happy Times"). Warner Bros. stopped the film to allow their star to attend. When his Decca co-workers the Andrews Sisters began their engagement at the London Palladium on the heels of Kaye's successful 1948 appearance there, the trio was well received and David Lewin of the "Daily Express" declared: "The audience gave the Andrews Sisters the Danny Kaye roar!"

He hosted the 24th Academy Awards in 1952. The program was broadcast on radio. Telecasts of the Oscar ceremony came later. During the 1950s Kaye visited Australia, where he played "Buttons" in a production of "Cinderella" in Sydney. In 1953 Kaye started a production company, Dena Pictures, named for his daughter. "Knock on Wood" was the first film produced by his firm. The firm expanded into television in 1960 under the name Belmont Television.

Kaye entered television in 1956, on the CBS show "See It Now" with Edward R. Murrow. "The Secret Life of Danny Kaye" combined his 50,000-mile, ten-country tour as UNICEF ambassador with music and humor. His first solo effort was in 1960 with an hour special produced by Sylvia and sponsored by General Motors; with similar specials in 1961 and 1962.
He hosted a "The Danny Kaye Show" from 1963 to 1967, which won four Emmy awards and a Peabody award. His last cinematic starring role came in 1963's "The Man from the Diners' Club".

Beginning in 1964, he acted as television host to the CBS telecasts of MGM's "The Wizard of Oz". Kaye did a stint as a "What's My Line?" Mystery Guest on the Sunday night CBS-TV quiz program. Kaye was later a guest panelist on that show. He also appeared on the interview program "Here's Hollywood". In the 1970s Kaye tore a ligament in his leg during the run of the Richard Rodgers musical "Two by Two", but went on with the show, appearing with his leg in a cast and cavorting on stage in a wheelchair. He had done much the same on his television show in 1964 when his right leg and foot were burned from a cooking accident. Camera shots were planned so television viewers did not see Kaye in his wheelchair.

In 1976, he played Mister Geppetto in a television musical adaptation of "Pinocchio" with Sandy Duncan in the title role. Kaye portrayed Captain Hook opposite Mia Farrow in a musical version of "Peter Pan" featuring songs by Anthony Newley and Leslie Bricusse. He later guest-starred in episodes of "The Muppet Show", "The Cosby Show" and in the 1980s revival of "New Twilight Zone".

In many films, as well as on stage, Kaye proved to be an able actor, singer, dancer and comedian. He showed his serious side as Ambassador for UNICEF and in his dramatic role in the memorable TV film "Skokie", when he played a Holocaust survivor. Before his death in 1987, Kaye conducted an orchestra during a comical series of concerts organized for UNICEF fundraising. Kaye received two Academy Awards: an Academy Honorary Award in 1955 and the Jean Hersholt Humanitarian Award in 1982. That year he received the Screen Actors Guild Annual Award.

In 1980, Kaye hosted and sang in the 25th Anniversary of Disneyland celebration and hosted the opening celebration for Epcot in 1982 (EPCOT Center at the time). Both were aired on prime time television in the US.

Kaye was enamored of music. While he claimed an inability to read music, he was said to have perfect pitch. A flamboyant performer with his own distinctive style, "easily adapting from outrageous novelty songs to tender ballads" (according to critic Jason Ankeny), in 1945 Kaye began hosting his own CBS radio program, launching a number of hit songs including "Dinah" and "Minnie the Moocher".

In 1947 Kaye teamed with the popular Andrews Sisters (Patty, Maxene, and LaVerne) on Decca Records, producing the number-three Billboard hit "Civilization (Bongo, Bongo, Bongo)". The success of the pairing prompted both acts to record through 1950, producing rhythmically comical fare as "The Woody Woodpecker Song" (based on the bird from the Walter Lantz cartoons and a "Billboard" hit for the quartet), "Put 'em in a Box, Tie 'em with a Ribbon (And Throw 'em in the Deep Blue Sea)", "The Big Brass Band from Brazil", "It's a Quiet Town (In Crossbone County)", "Amelia Cordelia McHugh (Mc Who?)", "Ching-a-ra-sa-sa" and a duet by Danny and Patty Andrews of "Orange Colored Sky". The acts teamed for two yuletide favorites: a frantic, harmonic rendition of "A Merry Christmas at Grandmother's House (Over the River and Through the Woods)" and a duet by Danny & Patty, "All I Want for Christmas Is My Two Front Teeth".

Kaye's debut album "Columbia Presents Danny Kaye" had been released in 1942 by Columbia Records with songs performed to the accompaniment of Maurice Abravanel and Johnny Green. The album was reissued as a Columbia LP in 1949 and is described by the critic Bruce Eder as "a bit tamer than some of the stuff that Kaye hit with later in the '40s and in the '50s and, for reasons best understood by the public, doesn't attract nearly the interest of his kids' records and overt comedy routines."

1950 saw the release of a Decca single "I've Got a Lovely Bunch of Coconuts", his sole big U.S. chart hit. His second Columbia LP album "Danny Kaye Entertains" (1953, Columbia), included six songs recorded in 1941 from his Broadway musical "Lady in the Dark"; most notably "Tchaikovsky". 

Following the success of the film "Hans Christian Andersen" (1952), two of its songs written by Frank Loesser and sung by Kaye, "The Ugly Duckling" and "Wonderful Copenhagen", reached the Top Five on the UK pop charts. In 1953 Decca released "Danny at the Palace", a live recording made at the New York Palace Theater, followed by "Knock On Wood" (Decca, 1954) a set of songs from the movie of the same name sung by Kaye, accompanied by Victor Young and His Singing Strings.

In 1956, Kaye signed a three-year recording contract with Capitol Records, which released his single "Love Me Do" in December of that year. The B-side, "Ciu Ciu Bella", with lyrics written by Sylvia Fine, was inspired by an episode in Rome when Kaye, on a mission for UNICEF, befriended a 7-year-old polio victim in a children's hospital, who sang this song for him in Italian.

In 1958, Saul Chaplin and Johnny Mercer wrote songs for "Merry Andrew", a film starring Kaye as a British teacher attracted to the circus. The score added up to six numbers, all sung by Kaye; conductor Billy May's 1950 composition "Bozo's Circus Band" (renamed "Music of the Big Top Circus Band") was deposited on the second side of the Merry Andrew soundtrack, released in 1958. A year later another soundtrack came out, "The Five Pennies" (Kaye starred there as 1920s cornet player Loring Red Nichols), featuring Louis Armstrong.

In the 1960s and 1970s Kaye regularly conducted world-famous orchestras, although he had to learn the scores by ear. Kaye's style, even if accompanied by unpredictable antics (he once traded the baton for a fly swatter to conduct "The Flight of the Bumblebee") was praised by the likes of Zubin Mehta who once stated that Kaye "has a very efficient conducting style." His ability with an orchestra was mentioned by Dimitri Mitropoulos, then conductor of the New York Philharmonic Orchestra. After Kaye's appearance Mitropoulos remarked, "Here is a man who is not musically trained, who cannot even read music and he gets more out of my orchestra than I have." Kaye was invited to conduct symphonies as charity fundraisers and was the conductor of the all-city marching band at the season opener of the Los Angeles Dodgers in 1984. Over his career he raised over US$5 million in support of musician pension funds.

Kaye was sufficiently popular to inspire imitations:

In his later years, Kaye entertained at home as chef. He specialized in Chinese and Italian cooking. He had a custom made Chinese restaurant installed at the rear of his house by its alley, then had a kitchen and dining area built around it. The stove that Kaye used for his Chinese dishes was fitted with metal rings for the burners to allow the heat to be highly concentrated, and a trough with circulating ice water cooled the area to keep the intense heat tolerable for those who were cooking. He learned "at Johnny Kan's restaurant in San Francisco and with Cecilia Chang at her Mandarin restaurants in San Francisco and Los Angeles." He taught Chinese cooking classes at a San Francisco Chinese restaurant in the 1970s. The theater and demonstration kitchen under the library at the Hyde Park, New York campus of the Culinary Institute of America is named for him.

Kaye referred to his kitchen as "Ying's Thing". While filming "The Madwoman of Chaillot" in France, he phoned home to ask his family if they would like to eat at Ying's Thing that evening; Kaye flew home for dinner. Not all of his efforts in the kitchen went well. After flying to San Francisco for a recipe for sourdough bread, he came home and spent hours preparing loaves. When his daughter asked about the bread, Kaye hit the bread on the kitchen table; his bread was hard enough to chip it. Kaye approached kitchen work with enthusiasm, making sausages and other foods needed for his cuisine. His work as a chef earned him the "Les Meilleurs Ouvriers de France" culinary award. Kaye is the only nonprofessional chef to have received this honor.

Kaye was an aviation enthusiast and pilot. He became interested in getting a pilot's license in 1959. An enthusiastic and accomplished golfer, he gave up golf in favor of flying. The first plane Kaye owned was a Piper Aztec. Kaye received his first license as a private pilot of multi-engine aircraft, not being certified for operating a single-engine plane until six years later. He was an accomplished pilot, rated for airplanes ranging from single-engine light aircraft to multi-engine jets. Kaye held a commercial pilot's license and had flown every type of aircraft except military planes. 

Kaye received a type rating in a Learjet, and he was named vice president of the Learjet company by Bill Lear as an honorary title (he had no line responsibility at the company). He supported many flying projects. In 1968 he was honorary chairman of the Las Vegas International Exposition of Flight, a show that utilized many facets of the city's entertainment industry while presenting an air show. The operational show chairman was well-known aviation figure Lynn Garrison. Kaye flew a Learjet to 65 cities in five days on a mission to help UNICEF.

In 1958 Kaye and partner Lester Smith formed Kaye–Smith Enterprises. The company owned a chain of radio stations, mostly in the Pacific Northwest. Other Kaye–Smith divisions included a concert promotion company, a video production company, and a recording studio. Kaye sold his share of the company to the Smith family in 1985.

A lifelong fan of the Brooklyn/Los Angeles Dodgers, Kaye recorded a song called "The D-O-D-G-E-R-S Song (Oh really? No, O'Malley!)," describing a fictitious encounter with the San Francisco Giants, a hit during the real-life pennant chase of 1962. That song is included on "Baseball's Greatest Hits" compact discs. A good friend of Leo Durocher, he often traveled with the team. He also possessed an encyclopedic knowledge of the game.

Kaye and business partner Lester Smith formed in 1958 Kaye-Smith Enterprises which owned and operated radio stations primarily in the Pacific Northwest. Both led an investment group which was awarded the American League's thirteenth franchise which became the Seattle Mariners for $6.2 million US on February 7, 1976. The ownership percentages of Kaye, Smith and two other remaining original investors were reduced to five percent each when George Argyros purchased 80 percent of the Mariners for $10.4 million on January 30, 1981. Kaye sold all of his business interests to Smith's family in 1985.

Kaye was an honorary member of the American College of Surgeons and the American Academy of Pediatrics.

Working alongside UNICEF's Halloween fundraiser founder, Ward Simon Kimball Jr., the actor educated the public on impoverished children in deplorable living conditions overseas and assisted in the distribution of donated goods and funds. His involvement with UNICEF came about in an unusual way. Kaye was flying home from London in 1949 when one of the plane's four engines lost its propeller and caught fire. The problem was initially thought serious enough that it might make an ocean landing; life jackets and liferafts were made ready.

The plane was able to head back over 500 miles to land at Shannon Airport, Ireland. On the way back to Shannon, the head of the Children's Fund, Maurice Pate, had the seat next to Danny Kaye and spoke at length about the need for recognition for the fund. Their discussion continued on the flight from Shannon to New York; it was the beginning of the actor's long association with UNICEF.

"For all of his success as a performer...his greatest legacy remains his tireless humanitarian work—so close were his ties to the United Nations International Children's Emergency Fund (UNICEF) that when the organization received the Nobel Peace Prize, Kaye was tapped to accept it", according to music critic Jason Ankeny.

Kaye died of heart failure on March 3, 1987, aged 76, brought on by internal bleeding and complications of hepatitis C. Kaye had quadruple bypass heart surgery in February 1983; he contracted hepatitis C from a blood transfusion. 

His ashes are interred in Kensico Cemetery in Valhalla, New York. His grave is adorned with a bench that contains friezes of a baseball and bat, an aircraft, a piano, a flower pot, musical notes, and a chef's toque. His name and birth and death dates are inscribed on the toque. The United Nations held a memorial tribute to him at their New York headquarters on the evening of October 21, 1987.

Kaye and Sylvia Fine grew up in Brooklyn, living a few blocks apart, but they did not meet until they were working on an off-Broadway show in 1939. Sylvia was an audition pianist.

Sylvia discovered that Danny had worked for her father Samuel Fine, a dentist. Kaye, working in Florida, proposed on the telephone; the couple were married in Fort Lauderdale on January 3, 1940.

The couple's only child, daughter Dena, was born on December 17, 1946. When she was very young, Dena did not like seeing her father perform because she did not understand that people were supposed to laugh at what he did. Kaye said in a 1954 interview, "Whatever she wants to be she will be without interference from her mother nor from me." Dena grew up to become a journalist.

On 18 January 2013, during a 24-hour salute to Kaye on Turner Classic Movies in celebration of what TCM thought was his 100th birthday, Kaye's daughter Dena revealed to TCM host Ben Mankiewicz that Kaye's stated birth year of 1913 was incorrect, and that he was actually born in 1911. 














</doc>
<doc id="9003" url="https://en.wikipedia.org/wiki?curid=9003" title="Dan DeCarlo">
Dan DeCarlo

Daniel S. DeCarlo (December 12, 1919 – December 18, 2001) was an American cartoonist best known as the artist who developed the look of Archie Comics in the late 1950s and early 1960s, modernizing the characters to their contemporary appearance and establishing the publisher's house style up until his death. As well, he is the generally recognized co-creator of the characters Sabrina the Teenage Witch, Josie and the Pussycats (with the lead character named for his wife), and Cheryl Blossom.

Dan DeCarlo was born in New Rochelle, New York, the son of a gardener. He attended New Rochelle High School in his hometown, followed by Manhattan's Art Students League from 1938 to 1941, when he was drafted into the U.S. Army. Stationed in Great Britain, he worked in the motor pool and as a draftsman, and painted company mascots on the noses of airplanes. He also drew a weekly military comic strip, "418th Scandal Sheet". He met his wife, French citizen Josie Dumont, on a blind date in Belgium not long after the Battle of the Bulge.

DeCarlo was married, with a pregnant wife, and a laborer working for his father when he began to pursue a professional art career. Circa 1947, answering an ad, he broke into the comic book industry at Timely Comics, the 1940s iteration of Marvel Comics. Under editor-in-chief Stan Lee, his first assignment was the teen-humor series "Jeanie". DeCarlo went uncredited, as was typical for most comic-book writers and artists of the era, and he recalled in 2001, "I went on with her maybe ten books. They used to call me 'The Jeanie Machine' because that was all Stan used to give me, was "Jeanie"... Then he took me off "Jeanie" and he gave me "Millie the Model". That was a big break for me. It wasn't doing too well and somehow when I got on it became quite successful."

He went on to an atypically long, 10-year run on that humor series, from issues #18–93 (June 1949 – Nov. 1959), most of them published by Marvel's 1950s predecessor, Atlas Comics. DeCarlo and Lee also took over the "My Friend Irma" comic strip, spun off from the hit Marie Wilson radio comedy. For a decade, DeCarlo wrote and drew the slapsticky adventures of Millie Collins, her redheaded friendly nemesis Chili Storm and the rest of the cast. He also contributed the short-lived "Sherry the Showgirl" and "Showgirls" for Atlas. In 1960, he and Atlas editor-in-chief Stan Lee co-created the short-lived syndicated comic strip "Willie Lumpkin", about a suburban mail carrier, for the Chicago-based Publishers Syndicate. A version of the character later appeared as a long-running minor supporting character in Lee's later co-creation, the Marvel Comics series "Fantastic Four"

As well during this period, DeCarlo created and drew Standard Comics' futuristic teen-humor comic book "Jetta of the 21st Century". Running three issues, #5-7 (Dec. 1952 - April 1953), it featured red-haired Jetta Raye and her friends at Neutron High School.

In addition to his comic-book work, DeCarlo drew freelance pieces for the magazines "The Saturday Evening Post" and "Argosy", as well as Timely/Atlas publisher Martin Goodman's Humorama line of pin-up girl cartoon digests.

DeCarlo first freelanced for Archie Comics, the company with which he would become most closely associated, in the late 1950s while still freelancing for Atlas. He said in 2001,
DeCarlo is tentatively identified with Archie as early as the Jughead story "The Big Shot" in "Archie Comics" #48 (Feb. 1951), with his earliest confirmed credit the 3 3/4-page story "No Picnic" in "Archie's Girls Betty and Veronica" #4 (undated; published in late 1951 or early to mid-1952). His art soon established the publisher's house style. As well, he is the generally recognized creator of the teen-humor characters Sabrina the Teenage Witch, Josie and the Pussycats, and Cheryl Blossom.

DeCarlo said he created Josie on his own in the late 1950s; his wife, named Josie, said in an interview quoted in a DeCarlo obituary, "We went on a Caribbean cruise, and I had a [cat] costume for the cruise, and that's the way it started." DeCarlo first tried to sell the character as a syndicated comic strip called "Here's Josie", recalling in 2001:
Josie was introduced in "Archie's Pals 'n' Gals" No. 23. The first issue of "She's Josie" followed, cover-dated February 1963. The series featured levelheaded, sweet-natured Josie (whose last name was given as either Jones or James), her blond bombshell friend Melody, and bookwormish brunette Pepper. These early years also featured the characters of Josie and Pepper's boyfriends Albert and Sock (real name Socrates); Albert's rival Alexander Cabot III; and Alex's twin sister Alexandra. Occasionally Josie and her friends would appear in "crossover" issues with the main Archie characters. "She's Josie" was renamed "Josie" with issue No. 17 (Dec. 1965), and again renamed, to "Josie and the Pussycats", with issue No. 45 (Dec. 1969), whereby Pepper was replaced by Valerie and Albert was replaced by Alan M. Under this title, the series finished its run with issue No. 106 (Oct. 1982). Josie and her gang also made irregular appearances in "Pep Comics" and "Laugh Comics" during the 1960s.

When Universal Pictures was preparing the live-action movie adaptation "Josie and the Pussycats" in 2001, DeCarlo and Archie Comics became involved in a lawsuit over the character's creation, leading the publisher to terminate its 43-year relationship with him. A federal district court ruled in 2001 that Archie Comics owned the copyright to the Josie characters; this decision was affirmed by the Second Circuit Court of Appeals. On December 11, 2001, the U.S. Supreme Court rejected an appeal filed by DeCarlo's attorney, Whitney Seymour Jr., who had argued that the issue was a matter of state property law and not federal copyright law.

DeCarlo was listed as a creator in the end credits of the film "Josie and the Pussycats". He received credit as co-creator of the live-action television show "Sabrina the Teenage Witch".

Among DeCarlo's final works were a story for Paul Dini's independent comics series "Jingle Belle", and stories for Bongo Comics' "The Simpsons" TV tie-in comic, "Bart Simpson".

DeCarlo died in New Rochelle, New York, of pneumonia. Comics creator Paul Dini said upon DeCarlo's death, "It was tragic that when he was at an age when many cartoonists are revered as treasures by more beneficent publishers, Dan felt spurned and slighted by the owners of properties that prospered greatly from his contributions."

His twin sons, Dan Jr. and James "Jim" DeCarlo (born January 27, 1948) were also prolific Archie artists, penciling and inking respectively. The two predeceased their father. Dan Jr. died in October 1990 of stomach cancer, and James died in August 1991 from complications from a stroke. Josie DeCarlo, the inspiration for singer-guitarist Josie McCoy of the 1970s Hanna-Barbera series "Josie and the Pussycats" and its successors, died in her sleep on March 14, 2012.

Josette Marie "Josie" DeCarlo (née Dumont; September 8, 1923 – March 14, 2012) was a French-born model who became the inspiration and namesake for Josie McCoy of "Josie and the Pussycats" comics and the 1970 Hanna-Barbera Saturday morning cartoon series.

She met her future husband, Dan DeCarlo, on a blind date in Belgium in 1945, just a short time after the end of the Battle of the Bulge. At the time of their meeting, Dumont did not speak any English, while DeCarlo, a member of the U.S. Army during World War II, spoke very little French.

Unable to have a conversation due to their language barrier, Dumont and DeCarlo communicated through his cartoons. In a later interview, Josie explained their early courtship, "We communicated with drawing...He would draw things for me to make me understand what he had in mind. He was really so amusing. Instead of just using words, he would use cartoons to express himself. Right away, we knew that we were meant for each other."

Decarlo and Dumont married. She became the inspiration for "Josie and the Pussycats" almost accidentally while the couple were taking a cruise. DeCarlo wore a cat suit costume during the cruise, which became the basis for the fictional "Josie and the Pussycats" trademark outfits.

Later, when she got a new hairdo, Dan DeCarlo incorporated it into the Josie character as well, "The hairdo came after...One day, I came in with a new hairdo with a little bow in my hair, and he said, ‘That’s it!’" Dan DeCarlo drew his wife with the cat costume as Josie McCoy, initially giving her the name "star" before deciding on Josie. Josie first appeared in Archie Comics in 1962. While DeCarlo became the model for Josie, the character was voiced by actress Janet Waldo in the television series.

Dan DeCarlo died in 2001. However, Josie DeCarlo remained active in the comic and animation industries following his death. She promoted her late husband's work within the industry, particularly the "Josie and the Pussycats" brand during the 2000s.

Josie DeCarlo died in her sleep on March 14, 2012, aged 88. Her funeral was held in Scarsdale, New York. She was predeceased by her husband and their twin sons, Dan DeCarlo Jr. and Jim DeCarlo. She was survived by two grandchildren.

DeCarlo won the National Cartoonists Society Award for Best Comic Book in 2000 for "Betty & Veronica". He was nominated for the Academy of Comic Book Arts' Shazam Award for Best Penciller (Humor Division) in 1974.

"Love and Rockets" co-creators Jaime Hernandez and Gilberto Hernandez cite DeCarlo, along with fellow Archie artist Harry Lucey and others, as an artistic influence.



</doc>
<doc id="9008" url="https://en.wikipedia.org/wiki?curid=9008" title="Debit card">
Debit card

A debit card (also known as a bank card, plastic card or check card) is a plastic payment card that can be used instead of cash when making purchases. It is similar to a credit card, but unlike a credit card, the money comes directly from the user's bank account when performing a transaction.

Some cards may carry a stored value with which a payment is made, while most relay a message to the cardholder's bank to withdraw funds from a payer's designated bank account. In some cases, the primary account number is assigned exclusively for use on the Internet and there is no physical card.

In many countries, the use of debit cards has become so widespread that their volume has overtaken or entirely replaced cheques and, in some instances, cash transactions. The development of debit cards, unlike credit cards and charge cards, has generally been country specific resulting in a number of different systems around the world, which were often incompatible. Since the mid-2000s, a number of initiatives have allowed debit cards issued in one country to be used in other countries and allowed their use for internet and phone purchases.

Unlike credit and charge cards, payments using a debit card are immediately transferred from the cardholder's designated bank account, instead of them paying the money back at a later date.

Debit cards usually also allow for instant withdrawal of cash, acting as an ATM card for withdrawing cash. Merchants may also offer cashback facilities to customers, where a customer can withdraw cash along with their purchase.

There are currently three ways that debit card transactions are EFTPOS (also known as "online debit" or "PIN debit"), offline debit (also known as "signature debit"), and the Electronic Purse Card System. One physical card can include the functions of all three types, so that it can be used in a number of different circumstances.

Although the four largest bank card issuers (American Express, Discover Card, MasterCard, and Visa) all offer debit cards, there are many other types of debit card, each accepted only within a particular country or region, for example Switch (now: Maestro) and Solo in the United Kingdom, Interac in Canada, Carte Bleue in France, EC electronic cash (formerly Eurocheque) in Germany, UnionPay in China, RuPay in India and EFTPOS cards in Australia and New Zealand. The need for cross-border compatibility and the advent of the euro recently led to many of these card networks (such as Switzerland's "EC direkt," Austria's "Bankomatkasse," and Switch in the United Kingdom) being re-branded with the internationally recognized Maestro logo, which is part of the MasterCard brand. Some debit cards are dual branded with the logo of the (former) national card as well as Maestro (for example, EC cards in Germany, Switch and Solo in the UK, Pinpas cards in the Netherlands, Bancontact cards in Belgium, etc.). The use of a debit card system allows operators to package their product more effectively while monitoring customer spending.

Online debit cards require electronic authorization of every transaction and the debits are reflected in the user’s account immediately. The transaction may be additionally secured with the personal identification number (PIN) authentication system; some online cards require such authentication for every transaction, essentially becoming enhanced automatic teller machine (ATM) cards.

One difficulty with using online debit cards is the necessity of an electronic authorization device at the point of sale (POS) and sometimes also a separate PINpad to enter the PIN, although this is becoming commonplace for all card transactions in many countries.

Overall, the online debit card is generally viewed as superior to the offline debit card because of its more secure authentication system and live status, which alleviates problems with processing lag on transactions that may only issue online debit cards. Some on-line debit systems are using the normal authentication processes of Internet banking to provide real-time online debit transactions.

Offline debit cards have the logos of major credit cards (for example, Visa or MasterCard) or major debit cards (for example, Maestro in the United Kingdom and other countries, but not the United States) and are used at the point of sale like a credit card (with payer's signature). This type of debit card may be subject to a daily limit, and/or a maximum limit equal to the current/checking account balance from which it draws funds. Transactions conducted with offline debit cards require 2–3 days to be reflected on users’ account balances.

In some countries and with some banks and merchant service organizations, a "credit" or offline debit transaction is without cost to the purchaser beyond the face value of the transaction, while a fee may be charged for a "debit" or online debit transaction (although it is often absorbed by the retailer). Other differences are that online debit purchasers may opt to withdraw cash in addition to the amount of the debit purchase (if the merchant supports that functionality); also, from the merchant's standpoint, the merchant pays lower fees on online debit transaction as compared to "credit" (offline).

Smart-card-based electronic purse systems (in which value is stored on the card chip, not in an externally recorded account, so that machines accepting the card need no network connectivity) are in use throughout Europe since the mid-1990s, most notably in Germany (Geldkarte), Austria (Quick Wertkarte), the Netherlands (Chipknip), Belgium (Proton), Switzerland (CASH) and France (Moneo, which is usually carried by a debit card). In Austria and Germany, almost all current bank cards now include electronic purses, whereas the electronic purse has been recently phased out in the Netherlands.

Prepaid debit cards that can be reloaded are also called reloadable debit cards.

The primary market for prepaid debit cards has traditionally been unbanked people; that is, people who do not use banks or credit unions for their financial transactions. But prepaid cards also appeal to other users attracted by their advantages.

Advantages of prepaid debit cards include being safer than carrying cash, worldwide functionality due to Visa and MasterCard merchant acceptance, not having to worry about paying a credit card bill or going into debt, the opportunity for anyone over the age of 18 to apply and be accepted without regard to credit quality, and the option to directly deposit paychecks and government benefits onto the card for free.

If the card provider offers an insecure website for letting you check the card's balance, this could give an attacker access to the card information.
If you lose the card, and have not somehow registered it, you likely lose the money. If a provider has technical issues, the money might not be accessible when you need it. Some companies' payment systems do not appear to accept prepaid debit cards. And there is a risk that prolific use of prepaid debit cards could lead data provider companies to miscategorize you in unfortunate ways.

Some of the first companies to enter this market were: MiCash, RushCard, Netspend, and Green Dot who gained market share as a result of being first to market. However, since 1999, there have been several new providers, such as TransCash, 247card, iKobo. These prepaid card companies offer a number of benefits, such as money remittance services, card-to-card transfers, and the ability to apply without a social security number.

In 2009 a company called PEX Card launched a corporate expense card service aimed at business users.

As of 2017, many other companies also offer the cards.

As of 2013, several city governments (including Oakland, California and Chicago, Illinois) are now offering prepaid debit cards, either as part of a municipal ID card (for people such as illegal immigrants who are unable to obtain a state driver's license or DMV ID card) in the case of Oakland, or in conjunction with a prepaid transit pass (Chicago). These cards have been heavily criticized for their higher-than-average fees, including some (such as a flat fee added onto every purchase made with the card) that similar products offered by Green Dot and American Express do not have.

The U.S. federal government uses prepaid debit cards to make benefits payments to people who do not have bank accounts. In 2008, the U.S. Treasury Department paired with Comerica Bank to offer the Direct Express Debit MasterCard prepaid debit card.

In July 2013, the Association of Government Accountants released a report on government use of prepaid cards, concluding that such programs offer a number of advantages to governments and those who receive payments on a prepaid card rather than by check. The prepaid card programs benefit payments largely for cost savings they offer and provide easier access to cash for recipients, as well as increased security. The report also advises that governments should consider replacing any remaining cheque-based payments with prepaid card programs in order to realize substantial savings for taxpayers, as well as benefits for payees.

In January 2016, the UK government introduced fee-free basic bank accounts for all, having a significant impact on the prepaid industry, including the departure of a number of firms.

Consumer protections vary, depending on the network used. Visa and MasterCard, for instance, prohibit minimum and maximum purchase sizes, surcharges, and arbitrary security procedures on the part of merchants. Merchants are usually charged higher transaction fees for credit transactions, since debit network transactions are less likely to be fraudulent. This may lead them to "steer" customers to debit transactions. Consumers disputing charges may find it easier to do so with a credit card, since the money will not immediately leave their control. Fraudulent charges on a debit card can also cause problems with a checking account because the money is withdrawn immediately and may thus result in an overdraft or bounced checks. In some cases debit card-issuing banks will promptly refund any disputed charges until the matter can be settled, and in some jurisdictions the consumer liability for unauthorized charges is the same for both debit and credit cards.

In some countries, like India and Sweden, the consumer protection is the same regardless of the network used. Some banks set minimum and maximum purchase sizes, mostly for online-only cards. However, this has nothing to do with the card networks, but rather with the bank's judgement of the person's age and credit records. Any fees that the customers have to pay to the bank are the same regardless of whether the transaction is conducted as a credit or as a debit transaction, so there is no advantage for the customers to choose one transaction mode over another. Shops may add surcharges to the price of the goods or services in accordance with laws allowing them to do so. Banks consider the purchases as having been made at the moment when the card was swiped, regardless of when the purchase settlement was made. Regardless of which transaction type was used, the purchase may result in an overdraft because the money is considered to have left the account at the moment of the card swiping.

Debit cards and secured credit cards are popular among college students who have not yet established a credit history. Debit cards may also be used by expatriated workers to send money home to their families holding an affiliated debit card.

To the consumer, a debit transaction is perceived as occurring in real-time; "i.e." the money is withdrawn from their account immediately following the authorization request from the merchant, which in many countries, is the case when making an online debit purchase. However, when a purchase is made using the "credit" (offline debit) option, the transaction merely places an authorization hold on the customer's account; funds are not actually withdrawn until the transaction is reconciled and hard-posted to the customer's account, usually a few days later. However, the previous sentence applies to all kinds of transaction types, at least when using a card issued by a European bank. This is in contrast to a typical credit card transaction; though it can also have a lag time of a few days before the transaction is posted to the account, it can be many days to a month or more before the consumer makes repayment with actual money.

Because of this, in the case of a benign or malicious error by the merchant or bank, a debit transaction may cause more serious problems (for example, money not accessible; overdrawn account) than in the case of a credit card transaction (for example, credit not accessible; over credit limit). This is especially true in the United States, where check fraud is a crime in every state, but exceeding your credit limit is not.

Debit cards may also be used on the Internet either with or without using a PIN. Internet transactions may be conducted in either online or offline mode, although shops accepting online-only cards are rare in some countries (such as Sweden), while they are common in other countries (such as the Netherlands). For a comparison, PayPal offers the customer to use an online-only Maestro card if the customer enters a Dutch address of residence, but not if the same customer enters a Swedish address of residence.

Internet purchases can be authenticated by the consumer entering their PIN if the merchant has enabled a secure online PIN pad, in which case the transaction is conducted in debit mode. Otherwise, transactions may be conducted in either credit or debit mode (which is sometimes, but not always, indicated on the receipt), and this has nothing to do with whether the transaction was conducted in online or offline mode, since both credit and debit transactions may be conducted in both modes.

In some countries, banks tend to levy a small fee for each debit card transaction. In some countries (for example, the UK) the merchants bear all the costs and customers are not charged. There are many people who routinely use debit cards for all transactions, no matter how small. Some (small) retailers refuse to accept debit cards for small transactions, where paying the transaction fee would absorb the profit margin on the sale, making the transaction uneconomic for the retailer.

The banks in Angola issue by official regulation only one brand of debit cards: Multicaixa, which is also the brand name of the one and only network of ATMs and POS terminals.

ArCa (Armenian Card) - a national system of debit (ArCa Debit and ArCa Classic) and credit (ArCa Gold, ArCa Business, ArCA Platinum, ArCa Affinity and ArCa Co-branded) cards popular in the Republic of Armenia. Established in 2000 by 17 largest Armenian banks.

Debit cards in Australia are called different names depending on the issuing bank: Commonwealth Bank of Australia: Keycard; Westpac Banking Corporation: Handycard; National Australia Bank: FlexiCard; ANZ Bank: Access card; Bendigo Bank: Cashcard.

EFTPOS is very popular in Australia and has been operating there since the 1980s. EFTPOS-enabled cards are accepted at almost all swipe terminals able to accept credit cards, regardless of the bank that issued the card, including Maestro cards issued by foreign banks, with most businesses accepting them, with 450,000 point of sale terminals.

EFTPOS cards can also be used to deposit and withdraw cash over the counter at Australia Post outlets participating in GiroPost, just as if the transaction was conducted at a bank branch, even if the bank branch is closed. Electronic transactions in Australia are generally processed via the Telstra Argent and Optus Transact Plus network - which has recently superseded the old Transcend network in the last few years. Most early keycards were only usable for EFTPOS and at ATM or bank branches, whilst the new debit card system works in the same way as a credit card, except it will only use funds in the specified bank account. This means that, among other advantages, the new system is suitable for electronic purchases without a delay of two to four days for bank-to-bank money transfers.

Australia operates both electronic credit card transaction authorization and traditional EFTPOS debit card authorization systems, the difference between the two being that EFTPOS transactions are authorized by a personal identification number (PIN) while credit card transactions can additionally be authorized using a contactless payment mechanism. If the user fails to enter the correct pin three times, the consequences range from the card being locked out for a minimum 24-hour period, a phone call or trip to the branch to reactivate with a new PIN, the card being cut up by the merchant, or in the case of an ATM, being kept inside the machine, both of which require a new card to be ordered.

Generally credit card transaction costs are borne by the merchant with no fee applied to the end user (although a direct consumer surcharge of 0.5 - 3% is not uncommon) while EFTPOS transactions cost the consumer an applicable withdrawal fee charged by their bank.

The introduction of Visa and MasterCard debit cards along with regulation in the settlement fees charged by the operators of both EFTPOS and credit cards by the Reserve Bank has seen a continuation in the increasing ubiquity of credit card use among Australians and a general decline in the profile of EFTPOS. However, the regulation of settlement fees also removed the ability of banks, who typically provide merchant services to retailers on behalf of Visa or MasterCard, from stopping those retailers charging extra fees to take payment by credit card instead of cash or EFTPOS.

In Bahrain debit cards are under Benefit, the interbanking network for Bahrain. Benefit is also accepted in other countries though, mainly GCC, similar to the Saudi Payments Network and the Kuwaiti KNET.

In Brazil debit cards are called "cartão de débito" (singular) and got popular from 2008 and on. In 2013, the 100 millionth Brazilian debit card was issued. Debit cards replaced cheques, common until the first decade of the 2000s.

Today, the majority of the financial transactions (like shopping, etc.) are made using debit cards (and this system is quickly replacing cash payments). Nowadays, the majority of debit payments are processed using a card + pin combination, and almost every card comes with a chip to make transactions.

The major debit card vendors in Brazil are Visa (with Visa Electron cards) and MasterCard (with Maestro cards), as well as local brand Elo.

In Bulgaria, debit cards are accepted in almost all stores and shops, as well as in most of the hotels and restaurants in the bigger cities. Smaller restaurants or small shops often accept cash only. All Bulgarian banks can provide debit cards when you open a bank account, for maintenance costs. Usually, it is free to use debit cards on ATMs owned by the issuing bank are free of charge, and they can also be used on the ATMs of other banks for a small fee (3-10 times cheaper than using a credit card). The most common cards in Bulgaria are Maestro and Visa Electron, accepted everywhere together with Visa and MasterCard.

Canada has a nationwide EFTPOS system, called Interac Direct Payment (IDP). Since being introduced in 1994, IDP has become the most popular payment method in the country. Previously, debit cards have been in use for ABM usage since the late 1970s, with credit unions in Saskatchewan and Alberta introducing the first card-based, networked ATMs beginning in June 1977. Debit cards, which could be used anywhere a credit card was accepted, were first introduced in Canada by Saskatchewan Credit Unions in 1982. In the early 1990s, pilot projects were conducted among Canada's six largest banks to gauge security, accuracy and feasibility of the Interac system. Slowly in the later half of the 1990s, it was estimated that approximately 50% of retailers offered Interac as a source of payment. Retailers, many small transaction retailers like coffee shops, resisted offering IDP to promote faster service. In 2009, 99% of retailers offer IDP as an alternative payment form.

In Canada, the debit card is sometimes referred to as a "bank card". It is a client card issued by a bank that provides access to funds and other bank account transactions, such as transferring funds, checking balances, paying bills, etc., as well as point of purchase transactions connected on the Interac network. Since its national launch in 1994, Interac Direct Payment has become so widespread that, as of 2001, more transactions in Canada were completed using debit cards than cash. This popularity may be partially attributable to two main factors: the convenience of not having to carry cash, and the availability of automated bank machines (ABMs) and direct payment merchants on the network.

Debit cards may be considered similar to stored-value cards in that they represent a finite amount of money owed by the card issuer to the holder. They are different in that stored-value cards are generally anonymous and are only usable at the issuer, while debit cards are generally associated with an individual's bank account and can be used anywhere on the Interac network.

In Canada, the bank cards can be used at POS and ABMs. Interac Online has also been introduced in recent years allowing clients of most major Canadian banks to use their debit cards for online payment with certain merchants as well. Certain financial institutions also allow their clients to use their debit cards in the United States on the NYCE network.

Consumers in Canada are protected under a voluntary code entered into by all providers of debit card services, The Canadian Code of Practice for Consumer Debit Card Services (sometimes called the "Debit Card Code"). Adherence to the Code is overseen by the Financial Consumer Agency of Canada (FCAC), which investigates consumer complaints.

According to the FCAC website, revisions to the code that came into effect in 2005 put the onus on the financial institution to prove that a consumer was responsible for a disputed transaction, and also place a limit on the number of days that an account can be frozen during the financial institution's investigation of a transaction.

Chile has an EFTPOS system called "Redcompra" (Purchase Network) which is currently used in at least 23,000 establishments throughout the country. Goods may be purchased using this system at most supermarkets, retail stores, pubs and restaurants in major urban centers. Chilean banks issue Maestro, Visa Electron and Visa Debit cards.

Colombia has a system called Redeban-Multicolor and Credibanco Visa which are currently used in at least 23,000 establishments throughout the country. Goods may be purchased using this system at most supermarkets, retail stores, pubs and restaurants in major urban centers. Colombian debit cards are Maestro (pin), Visa Electron (pin), Visa Debit (as credit) and MasterCard-Debit (as credit).

The Danish debit card Dankort is ubiquitous in Denmark. It was introduced on 1 September 1983, and despite the initial transactions being paper-based, the Dankort quickly won widespread acceptance. By 1985 the first EFTPOS terminals were introduced, and 1985 was also the year when the number of Dankort transactions first exceeded 1 million. Today Dankort is primarily issued as a multicard combining the national Dankort with the more internationally recognized Visa (denoted simply as a "Visa/Dankort" card). In September 2008, 4 million cards have been issued, of which three million cards were Visa/Dankort cards. It is also possible to get a Visa Electron debit card and MasterCard.


Most daily customer transactions are carried out with debit cards or online giro/electronic bill payment, although credit cards and cash are accepted. Checks are no longer used. Prior to European standardization, Finland had a national standard ("pankkikortti"). Physically, a "pankkikortti" was the same as an international credit card, and the same card imprinters and slips were used for "pankkikortti" and credit cards, but the cards were not accepted abroad. This has now been replaced by the Visa and MasterCard debit card systems, and Finnish cards can be used elsewhere in the European Union and the world.

An electronic purse system, with a chipped card, was introduced, but did not gain much traction.

Signing a payment offline entails incurring debt, thus offline payment is not available to minors. However, online transactions are permitted, and since almost all stores have electronic terminals, today also minors can use debit cards. Previously, only cash withdrawal from ATMs was available to minors ("automaattikortti" or Visa).

Carte Bancaire (CB), the national payment scheme, in 2008, had 57.5 million cards carrying its logo and 7.76 billion transactions (POS and ATM) were processed through the e-rsb network (135 transactions per card mostly debit or deferred debit). Most CB cards are debit cards, either debit or deferred debit. Less than 10% of CB cards were credit cards.

Banks in France usually charge annual fees for debit cards (despite card payments being very cost efficient for the banks), yet they do not charge personal customers for checkbooks or processing checks (despite checks being very costly for the banks). This imbalance dates from the unilateral introduction in France of Chip and PIN debit cards in the early 1990s, when the cost of this technology was much higher than it is now. Credit cards of the type found in the United Kingdom and United States are unusual in France and the closest equivalent is the deferred debit card, which operates like a normal debit card, except that all purchase transactions are postponed until the end of the month, thereby giving the customer between 1 and 31 days of "interest-free" credit.

The annual fee for a deferred debit card is around €10 more than for one with immediate debit. Most France debit cards are branded with the Carte Bleue logo, which assures acceptance throughout France. Most card holders choose to pay around €5 more in their annual fee to additionally have a Visa or a MasterCard logo on their Carte Bleue, so that the card is accepted internationally. A Carte Bleue without a Visa or a MasterCard logo is often known as a "Carte Bleue Nationale" and a Carte Bleue with a Visa or a MasterCard logo is known as a "Carte Bleue Internationale", or more frequently, simply called a "Visa" or "MasterCard".

Many smaller merchants in France refuse to accept debit cards for transactions under a certain amount because of the minimum fee charged by merchants' banks per transaction (this minimum amount varies from €5 to €15.25, or in some rare cases even more). But more and more merchants accept debit cards for small amounts, due to the massive daily use of debit card nowadays. Merchants in France do not differentiate between debit and credit cards, and so both have equal acceptance. It is legal in France to set a minimum amount to transactions, but the merchants must display it clearly.

In January 2016, 57.2% of all the debits cards in France also had a contactless payment chip . The maximum amount per transaction is set to €20 and the maximum amount of all contactless payments per day is between 50 and €100 depending on the bank.

According to French law, banks are liable for any transaction made with a copy of the original card and for any transaction made without a card (on the phone or on the Internet), so banks have to pay back any fraudulent transaction to the card holder if the previous criteria are met. Fighting card fraud is therefore more interesting for banks. As a consequence, French banks websites usually propose an "e-card" service ("electronic (bank) card"), where a new virtual card is created and linked to a physical card. Such virtual card can be used only once and for the maximum amount given by the card holder. If the virtual card number is intercepted or used to try to get a higher amount than expected, the transaction is blocked.

Debit cards have enjoyed wide acceptance in Germany for years. Facilities already existed before EFTPOS became popular with the Eurocheque card, an authorization system initially developed for paper checks where, in addition to signing the actual check, customers also needed to show the card alongside the check as a security measure. Those cards could also be used at ATMs and for card-based electronic funds transfer (called Girocard) with PIN entry. These are now the only functions of such cards: the Eurocheque system (along with the brand) was abandoned in 2002 during the transition from the Deutsche Mark to the euro. As of 2005, most stores and petrol outlets have EFTPOS facilities. Processing fees are paid by the businesses, which leads to some business owners refusing debit card payments for sales totalling less than a certain amount, usually 5 or 10 euro.

To avoid the processing fees, many businesses resorted to using direct debit, which is then called "electronic" direct debit (, abbr. "ELV"). The point-of-sale terminal reads the bank sort code and account number from the card but instead of handling the transaction through the Girocard network it simply prints a form, which the customer signs to authorise the debit note. However, this method also avoids any verification or payment guarantee provided by the network. Further, customers can return debit notes by notifying their bank without giving a reason. This means that the beneficiary bears the risk of fraud and illiquidity. Some business mitigate the risk by consulting a proprietary blacklist or by switching to Girocard for higher transaction amounts.

Around 2000, an Electronic Purse Card was introduced, dubbed Geldkarte ("money card"). It makes use of the smart card chip on the front of the standard issue debit card. This chip can be charged with up to 200 euro, and is advertised as a means of making medium to very small payments, even down to several euros or cent payments. The key factor here is that no processing fees are deducted by banks. It did not gain the popularity its inventors had hoped for. However, this could change as this chip is now used as means of age verification at cigarette vending machines, which has been mandatory since January 2007. Furthermore, some payment discounts are being offered ("e.g." a 10% reduction for public transport fares) when paying with "Geldkarte". The "Geldkarte" payment lacks all security measures, since it does not require the user to enter a PIN or sign a sales slip: the loss of a "Geldkarte" is similar to the loss of a wallet or purse - anyone who finds it can then use their find to pay for their own purchases.

Guinée Bissau

Please, see below on "UEMOA"

Debit card usage surged in Greece after the introduction of Capital Controls in 2015.

Most bank cards in Hong Kong for saving / current accounts are equipped with EPS and UnionPay, which function as a debit card and can be used at merchants for purchases, where funds are withdrawn from the associated account immediately.

EPS is a Hong Kong only system and is widely accepted in merchants and government departments. However, as UnionPay cards are accepted more widely overseas, consumers can use the UnionPay functionality of the bank card to make purchases directly from the bank account.

Visa debit cards are uncommon in Hong Kong. The British banking firm HSBC's subsidiary Hang Seng Bank's Enjoy card and American firm Citibank's ATM Visa are two of the Visa debit cards available in Hong Kong.

Debit cards usage in Hong Kong is relatively low, as the credit card penetration rate is high in Hong Kong. In Q1 2017, there are near 20 million credit cards in circulation, about 3 times the adult population. There are 145800 thousand transaction made by credit cards but only 34001 thousand transactions made by debit cards.

In Hungary debit cards are far more common and popular than credit cards. Many Hungarians even refer to their debit card ("betéti kártya") mistakenly using the word for credit card ("hitelkártya").

After the demonetization by current government there has been a surge in cashless transactions, so nowadays you could find card acceptance in maximum places. The debit card was mostly used for ATM transactions. RBI has announced that such fees are not justified so the transaction has no processing fee. Most Indian banks issue Visa debit cards, though some banks (like SBI and Citibank India) also issue Maestro cards. The debit card transactions are routed through Visa or MasterCard networks in India and overseas rather than directly via the issuing bank.

The National Payments Corporation of India (NPCI) has launched a new card called RuPay. It is similar to Singapore's NETS and Mainland China's UnionPay
Foreign-owned brands issuing Indonesian debit cards include Visa, Maestro, MasterCard, and MEPS. Domestically-owned debit card networks operating in Indonesia include Debit BCA (and its Prima network's counterpart, Prima Debit) and Mandiri Debit.

Iraq's two biggest state-owned banks, Rafidain Bank and Rasheed Bank, together with the "Iraqi Electronic Payment System (IEPS)" have established a company called International Smart Card, which has developed a national credit card called 'Qi Card', which they have issued since 2008. According to the company's website: 'after less than two years of the initial launch of the Qi card solution, we have hit 1.6 million cardholder with the potential to issue 2 million cards by the end of 2010, issuing about 100,000 card monthly is a testament to the huge success of the Qi card solution. Parallel to this will be the expansion into retail stores through a network of points of sales of about 30,000 units by 2015'

Today, Irish debit cards are exclusively Chip and PIN and almost entirely Visa Debit. These can be used anywhere the Visa logo is seen and in much the same way as a credit card. MasterCard debit is also used by a small minority of institutions and operates in a very similar manner.

Irish debit cards are normally multi-functional and combine ATM card facilities. The cards are also sometimes used for authenticating transactions together with a card reader for 2-factor authentication on online banking.

The majority of Irish Visa Debit cards are also enabled for contactless payment for small, frequent transactions (with a maximum value of €15 or €30). Three consecutive contactless transactions are allowed, after which, the card software will refuse contactless transactions until a standard Chip and PIN transaction has been completed and the counter resets. This measure was put in place to minimise issuers' exposure to fraudulent charges.

The cards are usually processed online, but some cards can also be processed offline depending on the rules applied by the card issuer.

A number of card issuers also provide prepaid debit card accounts primarily for use as gift cards / vouchers or for added security and anonymity online. These may be disposable or reloadable and are usually either Visa or MasterCard branded.

Previous system (defunct since 28 February 2014):

Laser was launched by the Irish banks in 1996 as an extension of the existing ATM and Cheque guarantee card systems that had existed for many years. When the service was added, it became possible to make payments with a multifunctional card that combined ATM, cheque and debit card and international ATM facilities through MasterCard Cirrus or Visa Plus and sometimes the British Link ATM system. Their functionality was similar to the British Switch card.

The system first launched as a swipe & sign card and could be used in Ireland in much the same way as a credit card and were compatible standard card terminals (online or offline, although they were usually processed online). They could also be used in cardholder-not-present transactions over the phone, by mail or on the internet or for processing recurring payments. Laser also offered 'cash back' facilities where customers could ask retailers (where offered) for an amount of cash along with their transaction. This service allowed retailers to reduce volumes of cash in tills and allowed consumers to avoid having to use ATMs. Laser adopted EMV 'Chip and PIN' security in 2002 in common with other credit and debit cards right across Europe. In 2005, some banks issued customers with Lasers cards that were cobranded with Maestro. This allowed them to be used in POS terminals overseas, internet transactions were usually restricted to sites that specifically accepted Laser.

Since 2006, Irish banks have progressively replaced Laser with international schemes, primarily Visa Debit and by 28 February 2014 the Laser Card system had been withdrawn entirely and is no longer accepted by retailers.

The Israel bank card system is somewhat confusing to newcomers, comprising a blend of features taken from different types of cards. What may be referred to as a credit card, is most likely to be a deferred debit card on an associated bank current account, the most common type of card in Israel, somewhat like the situation in France, though the term "debit card" is not in common usage. Cards are nearly universally called "cartis ashrai" (כרטיס אשראי), literally, "credit card", a term which may bely the card's characteristics. Its main feature may be a direct link to a connected bank account (through which they are mostly issued), with the total value of the transactions made on the card being debited from the bank account in full on a regular date once a month, without the option to carry the balance over; indeed certain types of transactions (such as online and/or foreign currency) may be debited directly from the connected bank account at the time of the transaction. Any such limited credit enjoyed is a result of the customer's assets and credibility with the bank, and not granted by the credit card company. The card usually enables immediate ATM cash withdrawals & balance inquiries (as debit cards do), instalment & deferred charge interest free transactions offered by merchants (also applicable in Brazil), interest bearing instalment plans/deferred charge/revolving credit which is transaction specific at the point of sale (though granted by the issuer, hence the interest), and a variety of automated/upon request types of credit schemes including loans, some of which revolve or resemble the extended payment options sometimes offered by charge cards.

Thus the "true" debit card is not so common in Israel, though it has existed since 1994. It is offered by two credit companies in Israel: One is ICC, short for "Israeli Credit Cards" (referred to as "CAL", an acronym formed from its abbreviation in Hebrew), which issues it in the form of a Visa Electron card valid only in Israel. It is offered mainly through the Israel Post (post office) bank (which is not allowed, by regulation, to offer any type of credit) or through Israel Discount Bank, its main owner (where it is branded as "Discount Money Key" card). This branded Israel Discount Bank branded debit card also offered as valid worldwide card, either as Visa Electron or MasterCard Debit cards. The second & more common debit card is offered by the Isracard consortium to its affiliate banks and is branded "Direct". It is valid only in Israel, under its local & unique - though immensely popular - private label brand, as "Isracard Direct" (which was known as "Electro Cheque" until 2002 and while the local brand Isracard is often viewed as a MasterCard for local use only). Since 2006, Isracard has also offered an international version, branded "MasterCard Direct", which is less common. These two debit card brands operate offline in Israel (meaning the transaction operates under the credit cards systems & debited officially from the cardholder account only few days later, after being processed - though reflected on the current account immediately). In 2014 the Isracard Direct card (a.k.a. the valid only in Israel version) was relaunched as Isracash, though the former subbrand still being marketed - & replaced ICC Visa Electron as Israel Post bank debit card.

Overall, banks routinely offer deferred debit cards to their new customers, with "true" debit cards usually offered only to those who cannot obtain credit. These latter cards are not attractive to the average customer since they attract both a monthly fee from the credit company and a bank account fee for each day's debits. Isracard Direct is by far more common than the ICC Visa Electron debit card. Banks who issue mainly Visa cards will rather offer electronic use, mandate authorized transaction only, unembossed version of Visa Electron deferred debit cards (branded as "Visa Basic" or "Visa Classic") to its customers - sometimes even in the form of revolving credit card.

Credit/debit card transactions in Israel are not PIN based (other than at ATMs) and it is only in recent years that EMV chip smart cards have begun to be issued, with the Bank of Israel ordering the banks and credit card companies - in 2013 - to switch customers to credit cards with the EMV security standard within 3.5 years.

Debit cards are quite popular in Italy. There are both classic and prepaid cards. The main classic debit card in Italy is Bancomat/PagoBancomat: this kind of card is issued by Italian banks. Bancomat is the commercial brand for the cash withdrawal circuit, while PagoBancomat is used for POS transactions. Unlike other European countries such as UK, only a few Italian banks are issuing Visa/MasterCard debit cards (such as Intesa Sanpaolo NextCard). The main international debit circuit used by Italian banks is Mastercard's Maestro: for this reason almost every debit card issued in Italy has both PagoBancomat and Maestro logos, with Bancomat/PagoBancomat being used in Italy and the Maestro circuit when abroad. Sometimes, instead of using the Maestro circuit, the Bancomat/PagoBancomat debit card is issued along with V-Pay or Visa Electron logos, or sometimes with credit card functions (so you get a dual-mode card). In this last case, only the credit-card mode is allowed for abroad/Internet transactions, while the debit card mode is used only in Italy. The most popular prepaid debit card is "Postepay". It is issued by Poste italiane S.p.A., and usually runs on the Visa Electron circuit, but there are some versions that run on MasterCard. It can be used on Poste Italiane's ATMs (Postamat) and on Visa's Electron-compatible bank ATMs all over the world. It has no fees when used on the Internet and in POS-based transactions. Other cards are issued by other companies, such as Vodafone CashCard, Banca Popolare di Milano's Carta Jeans and Carta Moneta Online.

In Japan people usually use their , originally intended only for use with cash machines, as debit cards. The debit functionality of these cards is usually referred to as , and only cash cards from certain banks can be used. A cash card has the same size as a Visa/MasterCard. As identification, the user will have to enter his or her four-digit PIN when paying. J-Debit was started in Japan on March 6, 2000. However, J-Debit has not been that popular since then.

Suruga Bank began service of Japan's first Visa Debit in 2006. Rakuten Bank, formally known as Ebank, offers a Visa debit card.

Resona Bank and The Bank of Tokyo-Mitsubishi UFJ bank also offer a Visa branded debit card.

In Kuwait, all banks provide a debit card to their account holders. This card is branded as KNET, which is the central switch in Kuwait. KNET card transactions are free for both customer and the merchant and therefore KNET debit cards are used for low valued transactions as well. KNET cards are mostly co-branded as Maestro or Visa Electron which makes it possible to use the same card outside Kuwait on any terminal supporting these payment schemes.

In Malaysia, the local debit card network is operated by the Malaysian Electronic Clearing Corporation (MyClear), which had taken over the scheme from MEPS in 2008. The new name for the local debit card in Malaysia is MyDebit, which was previously known as either bankcard or e-debit. Debit cards in Malaysia are now issued on a combo basis where the card has both the local debit card payment application as well as having that of an International scheme (Visa or MasterCard). All newly issued MyDebit combo cards with Visa or MasterCard have the contactless payment feature. The same card also acts as the ATM card for cash withdrawals.

Mali

Please, see below on "UEMOA"

In Mexico, many companies use a type of debit card called a payroll card (tarjeta de nómina), in which they deposit their employee's payrolls, instead of paying them in cash or through checks. This method is preferred in many places because it is a much safer and secure alternative compared to the more traditional forms of payment.

In the Netherlands using EFTPOS is known as "pinnen" (pinning), a term derived from the use of a personal identification number (PIN). PINs are also used for ATM transactions, and the term is used interchangeably by many people, although it was introduced as a marketing brand for EFTPOS. The system was launched in 1987, and in 2010 there were 258,585 terminals throughout the country, including mobile terminals used by delivery services and on markets. All banks offer a debit card suitable for EFTPOS with current accounts.

PIN transactions are usually free to the customer, but the retailer is charged per-transaction and monthly fees. Equens, an association with all major banks as its members, runs the system, and until August 2005 also charged for it. Responding to allegations of monopoly abuse, it has handed over contractual responsibilities to its member banks through who now offer competing contracts. The system is organised through a special banking association Currence set up specifically to coordinate access to payment systems in the Netherlands. Interpay, a legal predecessor of Equens, was fined €47 million in 2004, but the fine was later dropped, and a related fine for banks was lowered from €17 million to €14 million. Per-transaction fees are between 5-10 eurocents, depending on volume.

Credit card use in the Netherlands is very low, and most credit cards cannot be used with EFTPOS, or charge very high fees to the customer. Debit cards can often, though not always, be used in the entire EU for EFTPOS. Most debit cards are Mastercard Maestro cards. Visa's V Pay cards are also accepted at most locations.
In 2011 spending money using debit cards rose to 83 billion euro whilst cash spending dropped to 51 billion euro and creditcard spending grew to 5 billion.

Electronic Purse Cards (called Chipknip) were introduced in 1996, but have never become very popular. The system was abolished at the end of 2014.

EFTPOS (electronic fund transfer at point of sale) in New Zealand is highly popular. In 2006, 70 percent of all retail transactions were made by Eftpos, with an average of 306 Eftpos transaction being made per person. At the same time, there were 125,000 Eftpos terminals in operation (one for every 30 people), and 5.1 million Eftpos cards in circulation (1.27 per capita).

The system involves the merchant swiping (or inserting) the customer's card and entering the purchase amount. Point of sale systems with integrated EFTPOS often sent the purchase total to the terminal and the customer swipes their own card. The customer then selects the account they wish to use: Current/Cheque (CHQ), Savings (SAV), or Credit Card (CRD), before entering in their PIN. After a short processing time in which the terminal contacts the EFTPOS network and the bank, the transaction is approved (or declined) and a receipt is printed. The EFTPOS system is used for credit cards as well, with a customer selecting Credit Card and entering their PIN, or for older credit cards without loaded PIN, pressing OK and signing their receipt with identification through matching signatures. Fixed EFTPOS terminals in most businesses utilise the public switched telephone network to contact the EFTPOS network, either utilising dedicated phone lines or sharing the merchant's voice line (especially in smaller businesses). The uptake of broadband internet in the 21st century has seen some terminals move to internet protocol connections.

Virtually all retail outlets have EFTPOS facilities, so much that retailers without EFTPOS have to advertise so. In addition, an increasing number of mobile operator, such as taxis, stall holders and pizza deliverers have mobile EFTPOS systems. The system is made up of two primary networks: EFTPOS NZ, which is owned by VeriFone and Paymark Limited (formerly Electronic Transaction Services Limited), which is owned by ANZ Bank New Zealand, ASB Bank, Westpac and the Bank of New Zealand. The two networks are intertwined and highly sophisticated and secure, able to handle huge volumes of transactions during busy periods such as the lead-up to Christmas: on 24 December 2012, the Paymark network alone recorded an average of 132 transactions per second between 12:00 and 13:00. Network failures are rare, but when they occur they cause massive disruption, resulting in major delays and loss of income for businesses. Most businesses have to resort to manual "zip-zap" swipe machines in such case. Newer POS-based terminals have the ability to "capture" transactions in the event of a communications break-down - instead of entering a PIN, the customer signs their receipt and the transaction is approved on a matching signature, The transaction details are stored and sent for processing once the connection to the network is restored. A notable example of this occurs on the Cook Strait ferries, where in the middle of Cook Strait there is no mobile phone reception to connect to the EFTPOS network.

Depending on the user's bank, a fee may be charged for use of EFTPOS. Most youth accounts (the minimum age to obtain an Eftpos card from most banks in New Zealand is 13 years) and an increasing number of 'electronic transaction accounts' do not attract fees for electronic transactions, meaning the use of Eftpos by younger generations has become ubiquitous and subsequently cash use has become rare. Typically merchants don't pay fees for transactions, most only having to pay for the equipment rental.

One of the disadvantages of New Zealand's well-established EFTPOS system is that it is incompatible with overseas systems and non-face-to-face purchases. In response to this, many banks since 2005 have introduced international debit cards such as Maestro and Visa Debit which work online and overseas as well as on the New Zealand EFTPOS system.

Niger

Please, see below on "UEMOA"

In the Philippines, all three national ATM network consortia offer proprietary PIN debit. This was first offered by Express Payment System in 1987, followed by Megalink with Paylink in 1993 then BancNet with the Point-of-Sale in 1994.

Express Payment System or EPS was the pioneer provider, having launched the service in 1987 on behalf of the Bank of the Philippine Islands. The EPS service has subsequently been extended in late 2005 to include the other Expressnet members: Banco de Oro and Land Bank of the Philippines. They currently operate 10,000 terminals for their cardholders.

Megalink launched Paylink EFTPOS system in 1993. Terminal services are provided by Equitable Card Network on behalf of the consortium. Service is available in 2,000 terminals, mostly in Metro Manila.

BancNet introduced their point of sale system in 1994 as the first consortium-operated EFTPOS service in the country. The service is available in over 1,400 locations throughout the Philippines, including second and third-class municipalities. In 2005, BancNet signed a Memorandum of Agreement to serve as the local gateway for China UnionPay, the sole ATM switch in the People's Republic of China. This will allow the estimated 1.0 billion Chinese ATM cardholders to use the BancNet ATMs and the EFTPOS in all participating merchants.

Visa debit cards are issued by Union Bank of the Philippines (e-Wallet & eon), Chinatrust, Equicom Savings Bank (Key Card & Cash Card), Banco De Oro, HSBC, HSBC Savings Bank, Sterling Bank of Asia (Visa ShopNPay prepaid and debit cards)& EastWest Bank. Union Bank of the Philippines cards, EastWest Visa Debit Card, Equicom Savings Bank & Sterling Bank of Asia EMV cards which can also be used for internet purchases. Sterling Bank of Asia has released its first line of prepaid and debit Visa cards with EMV chip.

MasterCard debit cards are issued by Banco de Oro, Security Bank (Cashlink & Cash Card) & Smart Communications (Smart Money) tied up with Banco De Oro. MasterCard Electronic cards are issued by BPI (Express Cash) and Security Bank (CashLink Plus).

Originally, all Visa and MasterCard based debit cards in the Philippines are non-embossed and are marked either for "Electronic Use Only" (Visa/MasterCard) or "Valid only where MasterCard Electronic is Accepted" (MasterCard Electronic). However, EastWest Bank started to offer embossed Visa Debit Cards without the for "Electronic Use Only" mark. Paypass Debit MasterCard from other banks also have embossed labels without the for "Electronic Use Only" mark. Unlike credit cards issued by some banks, these Visa and MasterCard-branded debit cards do not feature EMV chips, hence they can only be read by the machines through swiping.

By March 21, 2016, BDO has started issuing sets of Debit MasterCards having the EMV chip and is the first Philippine bank to have it. This is a response to the BSP's monitor of the EMV shift progress in the country. By 2017, all Debit Cards in the country should have an EMV chip on it.

In Poland, the first system of electronic payments was operated by Orbis, which later was changed to PolCard in 1991 (which also issued its own cards) and then that system was bought by First Data Poland Holding SA. In the mid-1990s international brands such as Visa, MasterCard, and the unembossed Visa Electron or Maestro were introduced.
Visa Electron and Maestro work as a standard debit cards: the transactions are debited instantly, although it may happen on some occasions that a transaction is processed with some delay (hours, up to one day). These cards do not possess the options that credit cards have.

In the late 2000s contactless cards started to be introduced. The first technology to be used was MasterCard PayPass, later joined by Visa's payWave. This payment method is now universal and accepted almost everywhere. In an everyday use this payment method is always called Paypass.
Almost all business and stores in Poland accept debit and credit cards.

In the mid-2010s Polish banks started to replace unembossed cards with embossed electronic cards such as Debit MasterCard and Visa Debit, allowing the customers to own a card that has all qualities of a credit card (given that credit cards are not popular in Poland).

There are also some banks that do not possess an identification system to allow customers to order debit cards online.

In Portugal, debit cards are accepted almost everywhere: ATMs, stores, and so on. The most commonly accepted are Visa and MasterCard, or the unembossed Visa Electron or Maestro. Regarding Internet payments debit cards cannot be used for transfers, due to its unsafeness, so banks recommend the use of 'MBnet', a pre-registered safe system that creates a virtual card with a pre-selected credit limit. All the card system is regulated by SIBS, the institution created by Portuguese banks to manage all the regulations and communication processes proply. SIBS' shareholders are all the 27 banks operating in Portugal.

In addition to Visa, MasterCard and American Express, there are some local payment systems based in general on smart card technology.


Nearly every transaction, regardless of brand or system, is processed as an immediate debit transaction. Non-debit transactions within these systems have spending limits that are strictly limited when compared with typical Visa or MasterCard accounts.

In Saudi Arabia, all debit card transactions are routed through Saudi Payments Network (SPAN), the only electronic payment system in the Kingdom and all banks are required by the Saudi Arabian Monetary Agency (SAMA) to issue cards fully compatible with the network. It connects all point of sale (POS) terminals throughout the country to a central payment switch which in turn re-routes the financial transactions to the card issuer, local bank, Visa, Amex or MasterCard.

As well as its use for debit cards, the network is also used for ATM and credit card transactions.

Singapore's debit service is managed by the Network for Electronic Transfers (NETS), founded by Singapore’s leading banks and shareholders namely DBS, Keppel Bank, OCBC and its associates, OUB, IBS, POSB, Tat Lee Bank and UOB in 1985 as a result of a need for a centralised e-Payment operator.

However, due to the banking restructuring and mergers, the local banks remaining were UOB, OCBC, DBS-POSB as the shareholders of NETS with Standard Chartered Bank to offer NETS to their customers. However, DBS and POSB customers can use their network atms on their own and not be shared with UOB, OCBC or SCB (StanChart). The mega failure of 5 July 2010 of POSB-DBS ATM Networks (about 97,000 machines) made the government to rethink the shared ATM system again as it affected the NETS system too.

In 2010, in line with the mandatory EMV system, Local Singapore Banks started to reissue their Debit Visa/MasterCard branded debit cards with EMV Chip compliant ones to replace the magnetic stripe system. Banks involved included NETS Members of POSB-DBS, UOB-OCBC-SCB along with the SharedATM alliance (NON-NETS) of HSBC, Citibank, State Bank of India, and Maybank. Standard Chartered Bank (SCB) is also a SharedATM alliance member. Non branded cards of POSB and Maybank local ATM Cards are kept without a chip but have a Plus or Maestro sign which can be used to withdraw cash locally or overseas.

Maybank Debit MasterCards can be used in Malaysia just like a normal ATM or Debit MEPS card.

Singapore also uses the e-purse systems of NETS CASHCARD and the CEPAS wave system by EZ-Link and NETS.

Debit cards are accepted in a relatively larger amount of stores, both large and small in Spain. Banks often offer debit cards for small fees in connection with a chequing account. These cards are used more often than credit cards at ATMs because it is a cheaper alternative.

Most banks issue major-brand debit cards that can be used internationally such as Visa, MasterCard and JCB, often with contactless functionality. Payments at brick-and-mortar stores generally require a signature except for contactless payments.

A separate, local debit system, known as Smart Pay, can be used by the majority of debit and ATM cards, even major-brand cards. This system is available only in Taiwan and a few locations in Japan as of 2016. Non-contactless payments require a PIN instead of a signature. Cards from a few banks support contactless payment with Smart Pay.

Debit cards are widely accepted from different debit card issuers including the Network International local subsidiary of Emirates Bank.

In the UK debit cards (an integrated EFTPOS system) are an established part of the retail market and are widely accepted both by bricks and mortar stores and by internet stores. The term EFTPOS is not widely used by the public; debit card is the generic term used. Debit cards commonly issued are Debit MasterCard and Visa Debit, with Maestro, Visa Electron and UnionPay also in circulation. Banks do not charge customers for EFTPOS transactions in the UK, but some retailers make small charges, particularly where the transaction amount in question is small. The UK has converted all debit cards in circulation to Chip and PIN (except for Chip and Signature cards issued to people with certain disabilities and non-reloadable prepaid cards), based on the EMV standard, to increase transaction security; however, PINs are not required for Internet transactions (though some banks employ additional security measures for online transactions such as Verified by Visa and MasterCard Secure Code), nor for most contactless transactions.

In the United Kingdom, banks started to issue debit cards in the mid-1980s in a bid to reduce the number of cheques being used at the point of sale, which are costly for the banks to process; the first bank to do so was Barclays with the "Barclays Connect" card. As in most countries, fees paid by merchants in the United Kingdom to accept credit cards are a percentage of the transaction amount, which funds card holders' interest-free credit periods as well as incentive schemes such as points or cashback. For consumer credit cards issued within the EEA, the interchange fee is capped at 0.3%, with a cap of 0.2% for debit cards, although the merchant acquirers may charge the merchant a higher fee. Although merchants won the right through The Credit Cards (Price Discrimination) Order 1990 to charge customers different prices according to the payment method, few merchants in the UK charge less for payment by debit card than by credit card, the most notable exceptions being budget airlines and travel agents. Most debit cards in the UK lack the advantages offered to holders of UK-issued credit cards, such as free incentives (points, cashback etc. (the Tesco Bank debit card being one exception)), interest-free credit and protection against defaulting merchants under Section 75 of the Consumer Credit Act 1974. Almost all establishments in the United Kingdom that accept credit cards also accept debit cards, but a minority of merchants, for cost reasons, accept debit cards and not credit cards.

It is the West Africa Economic and Monetary Union federating eight countries: Benin, Burkina Faso, Côte d'Ivoire, Guinée Bissau, Mali, Niger, Senegal and Togo.

GIM-UEMOA is the regional switch féderating more than 120 members (banks, microfinances, electronic money issuers, etc.). All interbank cards transactions between banks in the same country or between banks in two different countries UEMOA zone are routed and cleared by GIM-UEMOA. The settlement is done on Central Bank RTGS.

GIM-UEMOA also provides some processing products and services to more than 50 banks in UEMOA zone and out of UEMOA zone.

In the U.S., EFTPOS is universally referred to simply as "debit". The largest pre-paid debit card company is Green Dot Corporation, by market capitalization. The same interbank networks that operate the ATM network also operate the POS network. Most interbank networks, such as Pulse, NYCE, MAC, Tyme, SHAZAM, STAR, and so on, are regional and do not overlap, however, most ATM/POS networks have agreements to accept each other's cards. This means that cards issued by one network will typically work anywhere they accept ATM/POS cards for payment. For example, a NYCE card will work at a Pulse POS terminal or ATM, and vice versa. Debit cards in the United States are usually issued with a Visa, MasterCard, Discover or American Express logo allowing use of their signature-based networks.

U.S. Federal law caps the liability of a U.S. debit card user in case of loss or theft at $50 USD if the loss or theft is reported to the issuing bank in two business days after the customer notices the loss. Most banks will, however, set this limit to $0 for debit cards issued to their customers which are linked to their checking or savings account.

The fees charged to merchants for offline debit purchases vs. the lack of fees charged to merchants for processing online debit purchases and paper checks have prompted some major merchants in the U.S. to file lawsuits against debit-card transaction processors, such as Visa and MasterCard. In 2003, Visa and MasterCard agreed to settle the largest of these lawsuits for $2 billion and $1 billion respectively.

Some consumers prefer "credit" transactions because of the lack of a fee charged to the consumer/purchaser. A few debit cards in the U.S. offer rewards for using "credit". However, since "credit" transactions cost more for merchants, many terminals at PIN-accepting merchant locations now make the "credit" function more difficult to access. For example, if you swipe a debit card at Wal-Mart or Ross in the U.S., you are immediately presented with the PIN screen for online debit. To use offline debit you must press "cancel" to exit the PIN screen, and then press "credit" on the next screen.

As a result of the Dodd–Frank Wall Street Reform and Consumer Protection Act, U.S. merchants can now set a minimum purchase amount for credit card transactions, as long as it does not to exceed $10.

In the United States, an FSA debit card only allow medical expenses. It is used by some banks for withdrawals from their FSAs, medical savings accounts (MSA), and health savings accounts (HSA) as well. They have Visa or MasterCard logos, but cannot be used as "debit cards", only as "credit cards". Furthermore, they are not accepted by all merchants that accept debit and credit cards, but only by those that specifically accept FSA debit cards. Merchant codes and product codes are used at the point of sale (required by law by certain merchants by certain states in the US) to restrict sales if they do not qualify. Because of the extra checking and documenting that goes on, later, the statement can be used to substantiate these purchases for tax deductions. In the occasional instance that a qualifying purchase is rejected, another form of payment must be used (a check or payment from another account and a claim for reimbursement later). In the more likely case that non-qualifying items are accepted, the consumer is technically still responsible, and the discrepancy could be revealed during an audit. A small but growing segment of the debit card business in the U.S. involves access to tax-favored spending accounts such as FSAs, HRAs, and HSAs. Most of these debit cards are for medical expenses, though a few are also issued for dependent care and transportation expenses.

Traditionally, FSAs (the oldest of these accounts) were accessed only through claims for reimbursement after incurring, and often paying, an out-of-pocket expense; this often happens after the funds have already been deducted from the employee's paycheck. (FSAs are usually funded by payroll deduction.) The only method permitted by the Internal Revenue Service (IRS) to avoid this "double-dipping" for medical FSAs and HRAs is through accurate and auditable reporting on the tax return. Statements on the debit card that say "for medical uses only" are invalid for several reasons: (1) The merchant and issuing banks have no way of quickly determining whether the entire purchase qualifies for the customer's type of tax benefit; (2) the customer also has no quick way of knowing; often has mixed purchases by necessity or convenience; and can easily make mistakes; (3) extra contractual clauses between the customer and issuing bank would cross-over into the payment processing standards, creating additional confusion (for example if a customer was penalized for accidentally purchasing a non-qualifying item, it would undercut the potential savings advantages of the account). Therefore, using the card exclusively for qualifying purchases may be convenient for the customer, but it has nothing to do with how the card can actually be used. If the bank rejects a transaction, for instance, because it is not at a recognized drug store, then it would be causing harm and confusion to the cardholder. In the United States, not all medical service or supply stores are capable of providing the correct information so an FSA debit card issuer can honor every transaction-if rejected or documentation is not deemed enough to satisfy regulations, cardholders may have to send in forms manually.

Debit cards are accepted in a relatively large number of stores, both large and small in Uruguay; but their use has so far remained low as compared to credit cards at ATMs. Since August 2014, with the Financial Inclusion Law coming into force, end consumers obtain a 4% VAT deduction for using debit cards in their purchases.


</doc>
<doc id="9010" url="https://en.wikipedia.org/wiki?curid=9010" title="Dance Dance Revolution">
Dance Dance Revolution

"Dance Dance Revolution" has been met with critical acclaim for its originality and stamina in the video game market. There have been dozens of arcade-based releases across several countries and hundreds of home video game console releases, promoting a music library of original songs produced by Konami's in-house artists and an eclectic set of licensed music from many different genres. The "DDR" series has inspired similar games such as "Pump it Up" by Andamiro and "In the Groove" by Roxor Games. The latest release is "Dance Dance Revolution A", which premiered in 2016.

The core gameplay involves the player stepping their feet to correspond with the arrows that appears on screen and the beat. During normal gameplay, arrows scroll upwards from the bottom of the screen and pass over a set of stationary arrows near the top (referred to as the "guide arrows" or "receptors", officially known as the Step Zone). When the scrolling arrows overlap the stationary ones, the player must step on the corresponding arrows on the dance platform, and the player is given a judgement for their accuracy of every streaked notes (From highest to lowest: Marvelous, Perfect, Great, Good, Almost, Miss).

Additional arrow types are added in later mixes. "Freeze Arrows", introduced in "DDRMAX", are long green arrows that must be held down until they completely travel through the Step Zone. Each of these arrows awards an "O.K.!" if successfully pressed or an "N.G." when the arrow is released too quickly. An "N.G." decreases the life bar and, starting with "DDR X", also breaks any existing combo. "DDR X" also introduced "Shock Arrows", walls of arrows with lightning effects which must be avoided, awarding an "O.K.!" if successfully avoided or an "N.G." if any of the dancer's panels are stepped on. An "N.G." for shock arrows has the same consequences found with freeze arrows, but hitting a shock arrow additionally hides future steps for a short period of time.

Successfully hitting the arrows in time with the music fills the "Dance Gauge", or life bar, while failure to do so drains it. If the Dance Gauge is fully exhausted during gameplay, the player will fail the song, and the game will be over. Otherwise, the player is taken to the Results Screen, which rates the player's performance with a letter grade and a numerical score, among other statistics. The player may then be given a chance to play again, depending on the settings of the particular machine. The default limit is of three songs, though operators can set the limit between one and five.

Aside from play style Single, Dance Dance Revolution provides two other play styles: Versus, where two players can play Single simultaneously, and Double, where one player uses all eight panels. Prior to the 2013 release of "Dance Dance Revolution", some games offer additional modes, such as Course mode (players must play a set of songs back-to-back) and Battle mode (two players compete with a tug-of-war life bar by sending distracting modifiers to each other). Earlier versions also have Couple/Unison Mode, where two players must cooperate to play the song. This mode later become the basis for "TAG Play" in newer games.

Depending on the edition of the game, dance steps are broken into various levels of difficulty, often by colour. Difficulty is loosely separated into 3–5 categories depending on timeline:

DDR 1st Mix established the three main difficulties (Basic, Another, and Maniac) and it began using the foot rating with a scale of 1 to 8. In addition, each difficulty rating would also be labeled with a title. DDR 2nd Mix Club Version 2 increased the scale to 9, which would be implemented in the main series beginning in DDR 3rd Mix. DDR 3rd Mix also renamed the Maniac difficulty to "SSR" and made it playable through a special mode (SSR Mode), which can only be accessed via input code and is played on Flat (all arrows are the same color) by default. The SSR mode was eliminated in 3rdMix Plus, and the Maniac routines were folded back into the regular game. In addition to the standard three difficulties, the first three titles of the series and their derivations also featured a "Easy" mode ("Soft" in 3rd Mix), which provided simplified step charts for songs (and reduced song list in some versions). In this mode, one cannot access other difficulties, akin to the aforementioned SSR mode. While this mode is never featured again, it would become the basis for the fully accessible Beginner difficulty implemented in newer games. DDR 4th Mix removed the names of the song and made it simple by removing those names and organizing the difficulty by order. DDR 4th Mix Plus renamed several song's Maniac charts as Maniac-S (for Single) and Maniac-D (for Double), while adding newer and harder stepcharts for the old ones as the "second" Maniac. These new charts were used as the default Maniac stepchart in DDR 5th Mix while the older ones were removed.

Beginning in DDRMAX, a "Groove Radar" was introduced, showing how difficult a particular sequence is in various categories, such as the maximum density of steps, and so on. The step difficulty was removed in favor of the Groove Radar. DDRMAX2 (and subsequent versions) re-added the foot ratings and restored the pre-4th Mix Plus Maniac stepcharts as the default Heavy stepcharts. DDRMAX2 also increased the difficulty scale to 10 (with the existing boss song, "MAX 300" from DDRMAX revealed to be the first) and added an official Oni/Challenge difficulty which can only be accessed in Oni/Challenging Mode. On DDR Extreme, Beginner difficulty is added for beginners and the Oni/Challenge is freely accessible. The game also adds the infamous "flashing 10" foot for songs that are considered too hard to be rated normally and only exists in several songs.

Although DDR SuperNova still has the foot ratings, it removed the flashing 10-foot that existed on certain songs for unknown reasons. Later on, DDR SuperNOVA2 removed the foot rating and replaced it with bars. However, all songs from the previous games remain identical, with very few changes to certain song difficulties.

On "Dance Dance Revolution X", the foot/bar rating system was given its first major overhaul, now ranking songs on a scale of 1-20, the first 10 represented by yellow bars, and the second 10 represented by additional red blocks shown in place of yellow bars. All songs from previous versions were re-rated on the new scale. The same system was carried over to "Dance Dance Revolution X2", although the difficulty bars were removed, replaced by simple difficulty numbers with the foot mark returning as the difficulty symbol for the first time since DDR SuperNova. There is currently no song that is officially rated maximum (20); the highest rating available is 19, shared between six songs: "POSSESSION" on Double Challenge, and "EGOISM 440", "ENDYMION", "Over The "Period"", "PARANOiA Revolution", and "Valkyrie dimension" on Single Challenge and Double Challenge. However, the game still allows players to rate their custom edit data up to maximum.

The foot-rating system was completely removed for 6th Mix, and replaced by the "Groove Radar". The Groove Radar is a graphical representation of the difficulty of a song based in five different areas: Stream, Voltage, Air, Chaos, and Freeze.


Each game usually has a song that max out a category within the radar. If a song in a following mix or update has a higher category measurement, then the groove radar is renewed so the new song can max out that category, while all previous songs are re-rated in respect to the new radar.

As of the 2014 update to "Dance Dance Revolution", the groove radar also employs a numerical measurement in addition to a graphical representation. Before the update, the radar did not disclose the number by default, though it could be shown by holding the SELECT button while heading to the song select screen.

The Groove Radar was not very popular among seasoned DDR veterans. The foot-rating system would be restored to work with the Groove Radar in the North American home version of the game and in the next arcade version, "DDRMAX2", and almost all future versions (except for versions based on the North American version of Extreme, which only use foot ratings). All of the 6th Mix songs on 7thMix received foot-ratings, excluding songs that are removed from DDRMAX2.

SuperNOVA 2 featured special edits of songs specifically meant to max out specific categories on the radar, culminating with "Dead End (Groove Radar Special)", maxing out all 5 categories. While not related, SuperNOVA 2 also featured a variation known as "My Groove Radar" as part of e-Amusement, which is also divided into five categories, though it is meant to measure the player's stats on songs rather than showing the song's difficulty.

The Extra Stage, originally introduced in 1st Mix and reintroduced in DDRMAX (and appears in subsequent arcade versions), rewards a player for receiving a grade of "AA" or higher on either Expert or Challenge difficulties on the final stage. The player receives the opportunity to play a free extra song, which often defaults to a very difficult song with forced modifiers (such as 1.5x speed and Reverse) and a life bar identical to the battery bar similar to Challenge mode with 1-4 lives depending on their score in the final stage (or a non-regaining life bar before Supernova 2). Beginning on SuperNova 2, players may be able to access the modifier menu and the forced modifiers (save for the battery bar) are no longer used. However, the Replicant-D Action event in DDR X2 did not allow players to select modifiers for its Encore Extra Stage.

The default song for the extra stage is predetermined, although as of Extreme, any song can be played on the extra stage, although there is still a song that is designated as "the" Extra Stage (which usually is marked with red letters* on the song wheel, and must be unlocked for regular play). A player who attains a grade of "AA" (or "A" in SuperNova) on the Extra Stage is invited to play an additional stage, "One More Extra Stage" (OMES, or Encore Extra Stage post-SuperNova), with another special song option played in sudden death mode, in which any combo-breaking step or missed freeze will cause an instant failure. Since DDR X3 vs 2ndMIX, some Encore Extra Stage songs are marked as "ATTACK PERFECT FULL COMBO", where any judgement less than Perfect will cause the player to fail the song. SuperNova 2 and X allowed players to play any song for Encore Extra Stage, but X2 went back to the original predetermined songs, though the players are still able to change the modifiers. Usually if this final boss is beaten, a special credits sequence is played.

With the implementation of e-Amusement in DDR, mixes after SuperNova have contained multiple songs as extra stages, often based on specific conditions, such as playing specific difficulties or songs.

From 7th Mix onward, the BPM of Extra Stage songs was displayed as a random, changing number, instead of the song's true BPM. For every Extra Stage song except for MAX. (period), the random BPM display was replaced with the normal BPM display in the next mix, and as of "Dance Dance Revolution X", after said song has been unlocked for normal play.

A standard "Dance Dance Revolution" arcade machine consists of two parts, the cabinet and the dance platform. The cabinet has a wide bottom section, which houses large floor speakers and glowing neon lamps (led on X cabinets and hide lights on white cabinets). Above this sits a narrower section that contains the monitor, and on top is a lighted marquee graphic, with two small speakers and flashing lights on either side. Below the monitor are two sets of buttons (one for each player), each consisting of two triangular selection buttons (four on X and white cabinets) and a center rectangular button, used mainly to confirm a selection or start the game.

The dance stage is a raised metal platform divided into two sides. Each side houses a set of four acrylic glass pads arranged and pointing in the orthogonal directions (left, up, down and right), separated by metal squares. Each pad sits atop four pressure activated switches, one at each edge of each pad, and a software-controlled cold cathode lamp illuminating the translucent pad, not available on the white cabinet. A metal safety bar in the shape of an upside-down "U" is mounted to the dance stage behind each player. Some players make use of this safety bar to help maintain proper balance, and to relieve weight from the legs so that arrows can be pressed with greater speed and accuracy.

Some DDR cabinets are equipped with Sony PlayStation memory card slots, allowing the player to insert a compatible memory card before starting a game and save their high scores to the card. Additionally, the equivalent home versions of DDR allow players to create and save custom step patterns (edits) to their memory card — the player can then play those steps on the arcade machine if the same song exists on that machine. This feature is supported in 2ndMix through Extreme. SuperNova didn't support memory card slots. However, it introduced Konami's internet based link system "e-Amusement" to the series, which can save stats and unlocks for individual players (but cannot store edits). This functionality however, could only be used in Japan. During the North American release of Dance Dance Revolution SuperNOVA 2, an e-Amuse capable machine was made available at a Brunswick Zone Arcade in Naperville, Illinois. This machine was hosted on a different network than the Japanese version, and the only other machine on the network was located in Konami's American branch in El Segundo, California. e-Amusement functionality would later be made available in North America with the release of Dance Dance Revolution A.

The Solo arcade cabinet is smaller and contains only one dance pad, modified to include six arrow panels instead of four (the additional panels are "upper-left" and "upper-right"). These pads generally don't come with a safety bar, but include the option for one to be installed at a later date. The Solo pad also lacks some of the metal plating that the standard pad has, which can make stepping difficult for players who are used to playing on standard machines. An upgrade was available for Solo machines called the "Deluxe pad", which was closer to the standard cabinet's pad. Additionally Solo machines only incorporate two sensors, located horizontally in the center of the arrow, instead of four sensors (one on each edge).

The first "Dance Dance Revolution" as well as its followup "DDR 2ndMix" uses Bemani System 573 Analog as its hardware. DDR 3rdMix replaces this with a slightly upgraded Bemani System 573 Digital which would be used up to "DDR Extreme". Both of these are based on PlayStation.

Beginning in "Dancing Stage Fusion", the hardware is replaced by Bemani Python, a PlayStation 2-based hardware. In the next version, "DDR SuperNova", this was changed to Bemani Python 2 which was first introduced on the fellow Bemani game "GuitarFreaks V and Drummania V". Bemani Python 2 would also be used in the followup "DDR SuperNova 2".

Along with the cabinet change, "DDR X" also changes its hardware to the PC-based Bemani PC Type 4. This more powerful hardware allows for high definition graphics and enhanced features. With "DDR A", Bemani PC Type 4 is replaced by Type 5, that is still used to this day.

"Dance Dance Revolution" has been released in many different countries on many different platforms. Originally released in Japan as an arcade game and then a Sony PlayStation game, which was a bestseller. DDR was later released in North American, Europe, Korea, the whole of Asia, Australia, New Zealand, South America and Mexico on multiple platforms including the Sony PlayStation 2, Microsoft Xbox, Nintendo Wii, and many others. Due to demand, Japanese versions of the game, which are usually different from the games released in other countries, are often imported or bootlegged. DDR fansites make an attempt to keep track of the locations of arcade machines throughout the major regions.

DDR games have been released on various video game consoles, including the PlayStation, Dreamcast, Nintendo 64, PlayStation 2, PlayStation 3, GameCube, Wii, Xbox and Xbox 360, and even PCs. Home versions often contain new songs, songs from the arcade version, and additional features that take advantage of the capabilities of the console (e.g.; Xbox 360 versions such as the Dance Dance Revolution Universe series include support for online multiplayer and downloadable songs over Xbox Live, and high definition graphics). DDR has even reached Nintendo's Game Boy Color, with five versions of "Dance Dance Revolution GB" released in Japan; these included a series of three mainstream DDR games, a Disney Mix, and an Oha Star. The games come with a small thumb pad that fits over the Game Boy Color's controls to simulate the dance pad.

Home versions are commonly bundled with soft plastic dance pads that are similar in appearance and function to the Nintendo Power Pad. Some third-party manufacturers produce hard metal pads at a higher price.

A version of DDR was also produced for the PC in North America. It uses the interface of "Dance Dance Revolution 4thMix", and contains around 40 songs from the first six mainstream arcade releases. It has not been as well received as the console versions.
Due to the success of the "Dance Dance Revolution" franchise, many other games with similar or identical gameplay have been created.

Commercial competitors of "DDR" include the Korean series "Pump It Up" and the American series "In the Groove" by Roxor Games, as well as TechnoMotion by F2 Systems, "EZ2Dancer" by Amuseworld, and "MC Groovz Dance Craze" by Mad Catz.

"In the Groove" was met with legal action by Konami and resulted in Konami's acquisition of the game's intellectual property.

A Christian version of "DDR", named "Dance Praise", has been made by Digital Praise. Ubisoft produced a dance game based on Disney's "The Jungle Book" titled "The Jungle Book Groove Party".

Fan-made versions of "DDR" have also been created, many freely available to the public under open source licenses. The most popular of these is "StepMania", upon which the game "In the Groove" is based. These simulators allow for players to create and play their own songs to their own programmed steps. As a result, many "DDR" fans have held contests and released "mixes" of custom songs and steps for these simulators. Notably the Japanese "Foonmix" series and the DDR East Invasion Tournamix competitions. Other simulators include "Dance With Intensity" and "pyDance" for Windows, both of which are no longer developed, and "Feet of Fury", a homebrew game for the Sega Dreamcast.

Besides direct clones, many other games have been released that center around rhythm and dance due to "DDR"s popularity. "Dance! Online" released by Acclaim combines dance pad play with an MMO element. ABC's "Dancing With the Stars" and Codemasters' "Dance Factory" are more recent examples of games that pay homage to "DDR" and the genre it created. Konami uses music from its other rhythm game series such as "Beatmania" and "Beatmania IIDX", "Drummania", "GuitarFreaks", and "Pop'n Music", as well as making references to "DDR" in its other games and vice versa.

Tournaments are held worldwide, with participants usually competing for higher scores or number of Perfects (referred to as "Perfect Attack" tournaments). Less common are "freestyle" tournaments, where players develop actual dance routines to perform while following the steps in the game.

Many "DDR" players, in order to better get better scores by focusing on timing and pattern reading, will minimize any extraneous body movement during gameplay. These players are commonly referred to as "technical", "tech" or "perfect attack" (PA) players. These technical players usually play the most difficult songs on the highest difficulty levels in an attempt to perfect their scores, and the most elite players are able to get perfect or near perfect scores on all of the hardest songs in the game. The more "technical" a song gets the more the player must use minimalistic movements in order to hit all the arrows with perfection. These players perfect using their heels as well and often hold on to the bar to take weight off their feet enabling them to move faster and tire more slowly. This style of play is the focus of most competitions.

Other "DDR" players choose to incorporate complex or flashy techniques into their play movements, and some of these "freestyle" players develop intricate dance routines to perform during a song. Freestyle players tend to choose songs on lower difficulty levels, so that the player is not restricted in their movements by large quantities of required steps. Some players can even dance facing away from the screen.

Somewhere in the middle are the players which choose to do a little bit of both of the formers. There are criticisms of the In The Groove style of play which focuses on "perfect attack". More traditional players say it takes the fun away from the game the harder the step-charts get, which makes players use much less movement overall to conserve stamina. By doing this, it is no longer a dance game and many arrows do not fit perfectly with the beat because there are simply too many of them. The middle players enjoy moving to the beat and still trying to improve their scores without having to adopt the In the Groove style of play.

A freestyling act can also involve performing other stunts while playing. On an episode of ABC's short-lived series "Master of Champions", Billy Matsumoto won the episode when he played 5th Mix's "Can't Stop Fallin' In Love (Speed Mix)" on Heavy mode while juggling three lit torches.

Many news outlets have reported how playing "DDR" can be good aerobic exercise; some regular players have reported weight loss of 10–50 pounds (5–20 kg). In one example, a player found that including "DDR" in her day-to-day life resulted in a loss of . Some other examples would be Matthew Keene's account of losing upwards of and Yashar Esfandi's claim of losing in four months through incorporation of "DDR". Although the quantity of calories burned by playing "DDR" have not been scientifically measured, the amount of active movement required to play implies that "DDR" provides at least some degree of healthy exercise.

Many home versions of the game have a function to estimate calories burned, given a player's weight. Additionally, players can use "workout mode" to make a diary of calories burned playing DDR and any self-reported changes in the player's weight.

At the start of 2006, Konami announced that the "DDR" games would be used as part of a fitness program to be phased into West Virginia's 765 state schools, starting with its 103 middle schools, over the next two years. The program was conceived by a researcher at West Virginia University's Motor Development Center.

California Institute of Technology allows its students to use "DDR" to fulfill part of its physical education requirement, as students may design their own fitness program.

University of Kansas has a class for "Dance Dance Revolution" open for students to take as a 1 credit hour course.

Cyber Coach has sold in excess of 600 systems in schools in the UK and features the "DDR" game "Disco Disco 2".

In 2004, "Dance Dance Revolution" became an official sporting event in Norway. The first official club, DDR Oslo, was founded in 2004. The tournaments in Norway were divided into two parts, first there was a group play where the 2 or 3 best players from each group went to the final rounds. Elimination of the player with the lowest game score was used for each round in the finals. The scoring system used was based on people dancing to 2 or 3 songs. Some of the songs were selected randomly and had to be played by everyone. The others were player-chosen, which introduced some strategy into the game, as some songs had higher possible scoring than others. "Dancing Stage EuroMix 2" was used for the Norwegian tournaments.

In recent years, "Dance Dance Revolution" has been promoted by Konami as an e-Sport, mainly through their own competitive tournament, the "Konami Arcade Championship". The tournament allows players in Japan, South Korea, and select Asian regions to sign up and play in specific online events to earn a spot in the grand finals, typically held in Tokyo, Japan. The 6th Annual tournament, which concluded on February 11, 2017, was notable for being the first time that competitors from the United States were eligible to enter. The 7th Annual event, which concluded on February 10, 2018, added Indonesia and Canada as eligible competitor regions.

The success of the Dance Dance Revolution series has resulted in two Guinness World Records: "Longest Dance Dance Revolution Marathon" which is currently held by Alex Skudlarek at 16 hours, 18 minutes, and nine seconds. and "Most Widely Used Video Game in Schools." 




</doc>
<doc id="9011" url="https://en.wikipedia.org/wiki?curid=9011" title="Dual Alliance (1879)">
Dual Alliance (1879)

The Dual Alliance was a defensive alliance between Germany and Austria-Hungary, which was created by treaty on 7 October 1879 as part of Bismarck's system of alliances to prevent or limit war. The two powers promised each other support in case of attack by Russia. Also, each state promised benevolent neutrality to the other if one of them was attacked by another European power (generally taken to be France, even more so after the Franco-Russian Alliance of 1894). Germany’s Otto von Bismarck saw the alliance as a way to prevent the isolation of Germany and to preserve peace, as Russia would not wage war against both empires.

When Austria-Hungary and Germany formed an alliance in 1879, it was one of the more surprising alliances of its time. Though both realms shared the German language and a similar culture, Austria-Hungary and Germany were often driven apart, most notably in the recent Austro-Prussian War. Additionally, the Habsburg rulers of Austria believed that the promotion of nationalism, which was favored by Germany, would destroy their empire. However, their common dislike for Russia brought the two nations together for a common cause.

After the formation of the German Empire in 1871, German chancellor Otto von Bismarck wanted to portray his nation as a peacemaker and preserver of the European status quo, as well as gain more power for the German Empire and unify Germany. In 1878, Russia defeated the Ottoman Empire in the Russo-Turkish War; the resulting Treaty of San Stefano gave Russia considerable influence in the Balkans. This development outraged Austria-Hungary, who was Russia's chief competitor for influence in the Balkan region (despite being an ally of the Russians and the Germans in the League of the Three Emperors). Hence, in 1878, Bismarck called an international conference (the Congress of Berlin) in order to sort out the problem. The Treaty of Berlin that resulted from the conference reversed Russia's gains from the Treaty of San Stefano and provided the Austrians with compensation in the form of Bosnia. Despite Bismarck's attempts to play the role of an "honest broker" at the Congress of Berlin, Russo-German relations deteriorated following the conference. The Three Emperors' League was discontinued, and Germany and Austria-Hungary were free to ally with one another against Russia.

In 1881, Italy lost in the competition with France to establish a colony in "Tunis" (present-day Tunisia). To enlist diplomatic support, Italy joined Germany and Austria-Hungary to form the "Triple Alliance" in 1882, which was the first formal war-camp in Europe, the second being the Triple Entente formed in 1907.

During World War I, however, Italy did not go to war immediately with her allies but stayed neutral. In 1915, she joined the Entente powers and declared war on Austria-Hungary, and later Germany in the subsequent year. The Dual Alliance persisted throughout the war and ended with their defeats in 1918.



</doc>
<doc id="9014" url="https://en.wikipedia.org/wiki?curid=9014" title="Developmental psychology">
Developmental psychology

Developmental psychology is the scientific study of how and why human beings change over the course of their life. Originally concerned with infants and children, the field has expanded to include adolescence, adult development, aging, and the entire lifespan. Developmental psychologists aim to explain how thinking, feeling and behaviour change throughout life. This field examines change across three major dimensions: physical development, cognitive development, and socioemotional development. Within these three dimensions are a broad range of topics including motor skills, executive functions, moral understanding, language acquisition, social change, personality, emotional development, self-concept and identity formation.

Developmental psychology examines the influences of nature "and" nurture on the process of human development, and processes of change in context and across time. Many researchers are interested in the interaction between personal characteristics, the individual's behavior and environmental factors, including social context and the built environment. Ongoing debates include biological essentialism vs. neuroplasticity and stages of development vs. dynamic systems of development.

Developmental psychology involves a range of fields, such as, educational psychology, child psychopathology, forensic developmental psychology, child development, cognitive psychology, ecological psychology, and cultural psychology. Influential developmental psychologists from the 20th century include Urie Bronfenbrenner, Erik Erikson, Sigmund Freud, Jean Piaget, Barbara Rogoff, Esther Thelen, and Lev Vygotsky.

John B. Watson and Jean-Jacques Rousseau are typically cited as providing the foundations for modern developmental psychology. In the mid-18th century Jean Jacques Rousseau described three stages of development: "infants" (infancy), "puer" (childhood) and "adolescence" in "". Rousseau's ideas were taken up strongly by educators at the time.

It generally focuses on how and why certain modifications throughout an individual’s life-cycle (cognitive, social, intellectual, personality) and human growth change over time. There are many theorists that have made a profound contribution to this area of psychology. For example, Erik Erikson developed a model of eight stages of psychological development. He believed that humans developed in stages throughout their lifetimes and this would affect their behaviors (Similar ideas to Sigmund Freud)

In the late 19th century, psychologists familiar with the evolutionary theory of Darwin began seeking an evolutionary description of psychological development; prominent here was the pioneering psychologist G. Stanley Hall, who attempted to correlate ages of childhood with previous ages of humanity. James Mark Baldwin who wrote essays on topics that included "Imitation: A Chapter in the Natural History of Consciousness" and "Mental Development in the Child and the Race: Methods and Processes". Baldwin was heavily involved in the theory of developmental psychology. Sigmund Freud, whose concepts were developmental, significantly affected public perceptions.

Sigmund Freud believed that we all had a conscious, preconscious, and unconscious level. In the conscious, we are aware of our mental process. The preconscious involves information that, though not currently in our thoughts, can be brought into consciousness. Lastly, the unconscious includes mental processes we are unaware of.

He believed there is tension between the conscious and unconscious because the conscious tries to hold back what the unconscious tries to express. To explain this he developed three personality structures: the id, ego, and superego. The id, the most primitive of the three, functions according to the pleasure principle: seek pleasure and avoid pain. The superego plays the critical and moralizing role; and the ego is the organized, realistic part that mediates between the desires of the id and the superego.

Based on this, he proposed five universal stages of development, that each is characterized by the erogenous zone that is the source of the child's psychosexual energy. The first is the "oral stage", which occurs from birth to 12 months of age. During the oral stage, "the libido is centered in a baby's mouth." The baby is able to suck. The second is the "anal stage", from one to three years of age. During the anal stage, the child defecates from the anus and is often fascinated with their defecation. The third is the "phallic stage", which occurs from three to five years of age (most of a person's personality forms by this age). During the phallic stage, the child is aware of their sexual organs. The fourth is the "latency stage", which occurs from age five until puberty. During the latency stage, the child's sexual interests are repressed. Stage five is the "genital stage", which takes place from puberty until adulthood. During the genital stage, puberty starts happening.

Piaget claimed that logic and morality develop through constructive stages. Expanding on Piaget's work, Lawrence Kohlberg determined that the process of moral development was principally concerned with justice, and that it continued throughout the individual's lifetime.

He suggested three levels of moral reasoning; pre-conventional moral reasoning, conventional moral reasoning, and post-conventional moral reasoning. The pre-conventional moral reasoning is typical of children and is characterized by reasoning that is based on rewards and punishments associated with different courses of action. Conventional moral reason occurs during late childhood and early adolescence and is characterized by reasoning based on rules and conventions of society. Lastly, post-conventional moral reasoning is a stage during which the individual sees society's rules and conventions as relative and subjective, rather than as authoritative.

Kohlberg used the Heinz Dilemma to apply to his stages of moral development. The Heinz Dilemma involves Heinz's wife dying from cancer and Heinz having the dilemma to save his wife by stealing a drug. Preconventional morality, conventional morality, and post-conventional morality applies to Heinz's situation.

German-American psychologist Erik Erikson and his collaborator and wife, Joan Erikson, conceptualized eight stages of psychosocial development that they theorized healthy individuals pass through as they develop from infancy to adulthood. The first stage is called "Trust vs. Mistrust" takes place in infancy. The best virtue for the first stage is hope, in the infant learning who to trust and having hope for a supportive group of people to be there for him/her. The second stage is "Autonomy vs. Shame and Doubt" with the best virtue being will. This takes place in early childhood where the child learns to become more independent by discovering what they are capable of where if the child is overly controlled, they believe to feel inadequate on surviving by themselves, which can lead to low self-esteem and doubt. The third stage is "Initiative vs. Guilt". The basic virtue that would be gained is the purpose and takes place in the play age. This is the stage where the child will be curious and have many interactions with other kids. They will ask many questions as their curiosity grows. If too much guilt is present, the child may have a slower and harder time interacting with other children. The fourth stage is "Industry (competence) vs. Inferiority". The basic virtue for this stage is competency which happens at the school age. This stage is when the child will try to win the approval of others and fit in and understand the value of their accomplishments. The fifth stage is "Identity vs. Role Confusion". The basic virtue gained is fidelity which takes place in adolescence. This is where the child will start to find who he/she is as a person in society. What sex role he/she picks. The sixth stage is "Intimacy vs. Isolation", which happens in young adults and the virtue gained is love. This is where the person will start to share his/her life with someone else intimately and emotionally. In not doing so, it could lead to isolation. The seventh stage is "Generativity vs. Stagnation". This happens in adulthood and the virtue gained would be care. We become stable and start to give back by raising a family and becoming involved in the community. The eighth stage is "Ego Integrity vs. Despair". This happens during maturity and wisdom is gained. When one grows old and they contemplate and look back and see the success or failure of their life. This is also the stage where one can also have closure and accept death without fearing anything.

Jean Piaget, a Swiss theorist, posited that children learn by actively constructing knowledge through hands-on experience. He suggested that the adult's role in helping the child learn was to provide appropriate materials that the child can interact with and use to construct. He used Socratic questioning to get children to reflect on what they were doing, and he tried to get them to see contradictions in their explanations.

Piaget believed that intellectual development takes place through a series of stages, which he described in his theory on cognitive development. Each stage consists of steps the child must master before moving to the next step. He believed that these stages are not separate from one another, but rather that each stage builds on the previous one in a continuous learning process. He proposed four stages: "sensorimotor", "pre-operational", "concrete operational", and "formal operational". Though he did not believe these stages occurred at any given age, many studies have determined when these cognitive abilities should take place.

Michael Commons enhanced and simplified of Inhelder and Piaget's developmental and offers a standard method of examining the universal pattern of development. The Model of Hierarchical Complexity (MHC) is not based on the assessment of domain-specific information, It divides the Order of Hierarchical Complexity of tasks to be addressed from the Stage performance on those tasks. A stage is the order hierarchical complexity of the tasks the participant's successfully addresses. He expanded Piaget's original eight stage (counting the half stages) to fifteen stages. The stages are : 0 Calculatory; 1 Sensory & Motor; 2 Circular sensory-motor; 3 Sensory-motor; 4 Nominal; 5 Sentential; 6 Preoperational; 7 Primary; 8 Concrete; 9 Abstract; 10 Formal; 11 Systematic; 12 Metasystematic; 13 Paradigmatic; 14 Cross-paradigmatic; 15 Meta-Cross-paradigmatic. The order of hierarchical complexity of tasks predicts how difficult the performance is with an R ranging from 0.9 to 0.98.

In the MHC, there are three main axioms for an order to meet in order for the higher order task to coordinate the next lower order task. Axioms are rules that are followed to determine how the MHC orders actions to form a hierarchy. These axioms are: a) defined in terms of tasks at the next lower order of hierarchical complexity task action; b) defined as the higher order task action that organizes two or more less complex actions; that is, the more complex action specifies the way in which the less complex actions combine; c) defined as the lower order task actions have to be carried out non-arbitrarily.

Ecological systems theory, originally formulated by Urie Bronfenbrenner, specifies four types of nested environmental systems, with bi-directional influences within and between the systems. The four systems are microsystem, mesosystem, exosystem, and macrosystem. Each system contains roles, norms and rules that can powerfully shape development. The microsystem is the direct environment in our lives such as our home and school. Mesosystem is how relationships connect to the microsystem. Exosystem is a larger social system where the child plays no role. Macrosystem refers to the cultural values, customs and laws of society.

The microsystem is the immediate environment surrounding and influencing the individual (example: school or the home setting). The mesosystem is the combination of two microsystems and how they influence each other (example: sibling relationships at home vs. peer relationships at school). The exosystem is the interaction among two or more settings that are indirectly linked (example: a father's job requiring more overtime ends up influencing his daughter's performance in school because he can no longer help with her homework). The macrosystem is broader taking into account social economic status, culture, beliefs, customs and morals (example: a child from a wealthier family sees a peer from a less wealthy family as inferior for that reason). Lastly, the chronosystem refers to the chronological nature of life events and how they interact and change the individual and their circumstances through transition (example: a mother losing her own mother to illness and no longer having that support in her life).

Since its publication in 1979, Bronfenbrenner's major statement of this theory, "The Ecology of Human Development" has had widespread influence on the way psychologists and others approach the study of human beings and their environments. As a result of this conceptualization of development, these environments—from the family to economic and political structures—have come to be viewed as part of the life course from childhood through to adulthood.

Lev Vygotsky was a Russian theorist from the Soviet era, who posited that children learn through hands-on experience and social interactions with members of his/her culture. Unlike Piaget, he claimed that timely and sensitive intervention by adults when a child is on the edge of learning a new task (called the "zone of proximal development") could help children learn new tasks. This adult role is often referred to as the skilled "master," whereas the child is considered the learning apprentice through an educational process often termed "cognitive apprenticeship" Martin Hill stated that "The world of reality does not apply to the mind of a child." This technique is called "scaffolding," because it builds upon knowledge children already have with new knowledge that adults can help the child learn. Vygotsky was strongly focused on the role of culture in determining the child's pattern of development, arguing that development moves from the social level to the individual level. In other words, Vygotsky claimed that psychology should focus on the progress of human consciousness through the relationship of an individual and their environment. He felt that if scholars continued to disregard this connection, then this disregard would inhibit the full comprehension of the human consciousness.

Constructivism is a paradigm in psychology that characterizes learning as a process of actively constructing knowledge. Individuals create meaning for themselves or make sense of new information by selecting, organizing, and integrating information with other knowledge, often in the context of social interactions. Constructivism can occur in two ways: individual and social. Individual constructivism is when a person constructs knowledge through cognitive processes of their own experiences rather than by memorizing facts provided by others. Social constructivism is when individuals construct knowledge through an interaction between the knowledge they bring to a situation and social or cultural exchanges within that content.

Jean Piaget, a Swiss developmental psychologist, proposed that learning is an active process because children learn through experience and make mistakes and solve problems. Piaget proposed that learning should be whole by helping students understand that meaning is constructed.

Evolutionary developmental psychology is a research paradigm that applies the basic principles of Darwinian evolution, particularly natural selection, to understand the development of human behavior and cognition. It involves the study of both the genetic and environmental mechanisms that underlie the development of social and cognitive competencies, as well as the epigenetic (gene-environment interactions) processes that adapt these competencies to local conditions.

EDP considers both the reliably developing, species-typical features of ontogeny (developmental adaptations), as well as individual differences in behavior, from an evolutionary perspective. While evolutionary views tend to regard most individual differences as the result of either random genetic noise (evolutionary byproducts) and/or idiosyncrasies (for example, peer groups, education, neighborhoods, and chance encounters) rather than products of natural selection, EDP asserts that natural selection can favor the emergence of individual differences via "adaptive developmental plasticity." From this perspective, human development follows alternative life-history strategies in response to environmental variability, rather than following one species-typical pattern of development.

EDP is closely linked to the theoretical framework of evolutionary psychology (EP), but is also distinct from EP in several domains, including research emphasis (EDP focuses on adaptations of ontogeny, as opposed to adaptations of adulthood) and consideration of proximate ontogenetic and environmental factors (i.e., how development happens) in addition to more ultimate factors (i.e., why development happens), which are the focus of mainstream evolutionary psychology.

Attachment theory, originally developed by John Bowlby, focuses on the importance of open, intimate, emotionally meaningful relationships. Attachment is described as a biological system or powerful survival impulse that evolved to ensure the survival of the infant. A child who is threatened or stressed will move toward caregivers who create a sense of physical, emotional and psychological safety for the individual. Attachment feeds on body contact and familiarity. Later Mary Ainsworth developed the Strange Situation protocol and the concept of the secure base.

Theorists have proposed four types of attachment styles: secure, anxious-avoidant, anxious-resistant, and disorganized. Secure attachment is a healthy attachment between the infant and the caregiver. It is characterized by trust. Anxious-avoidant is an insecure attachment between an infant and a caregiver. This is characterized by the infant's indifference toward the caregiver. Anxious-resistant is an insecure attachment between the infant and the caregiver characterized by distress from the infant when separated and anger when reunited. Disorganized is an attachment style without a consistent pattern of responses upon return of the parent.

A child can be hindered in its natural tendency to form attachments. Some babies are raised without the stimulation and attention of a regular caregiver or locked away under conditions of abuse or extreme neglect. The possible short-term effects of this deprivation are anger, despair, detachment, and temporary delay in intellectual development. Long-term effects include increased aggression, clinging behavior, detachment, psychosomatic disorders, and an increased risk of depression as an adult.

Attachment style can affect the relationships between people. Attachment is established in early childhood and attachment continues into adulthood. An example of secure attachment continuing in adulthood would be when the person feels confident and is able to meet their own needs. An example of anxious attachment during adulthood is when the adult chooses a partner with anxious-avoidant attachment.

A significant issue in developmental psychology is the relationship between innateness and environmental influence in regard to any particular aspect of development. This is often referred to as "nature and nurture" or nativism versus empiricism. A nativist account of development would argue that the processes in question are innate, that is, they are specified by the organism's genes.

An empiricist perspective would argue that those processes are acquired in interaction with the environment. Today developmental psychologists rarely take such polarised positions with regard to most aspects of development; rather they investigate, among many other things, the relationship between innate and environmental influences. One of the ways this relationship has been explored in recent years is through the emerging field of evolutionary developmental psychology.

One area where this innateness debate has been prominently portrayed is in research on language acquisition. A major question in this area is whether or not certain properties of human language are specified genetically or can be acquired through learning. The empiricist position on the issue of language acquisition suggests that the language input provides the necessary information required for learning the structure of language and that infants acquire language through a process of statistical learning. From this perspective, language can be acquired via general learning methods that also apply to other aspects of development, such as perceptual learning.

The nativist position argues that the input from language is too impoverished for infants and children to acquire the structure of language. Linguist Noam Chomsky asserts that, evidenced by the lack of sufficient information in the language input, there is a universal grammar that applies to all human languages and is pre-specified. This has led to the idea that there is a special cognitive module suited for learning language, often called the language acquisition device. Chomsky's critique of the behaviorist model of language acquisition is regarded by many as a key turning point in the decline in the prominence of the theory of behaviorism generally. But Skinner's conception of "Verbal Behavior" has not died, perhaps in part because it has generated successful practical applications.

Since theorists believe that development is a smooth, continuous process, individuals gradually add more of the same types of skills throughout their lives. Other theorists, however, think that development takes place in discontinuous stages. People change rapidly and step up to a new level, and then change very little for a while. With each new step, the person shows interest and responds to the world qualitatively.

This issue involves the degree to which we become older renditions of our early experience or whether we develop into something different from who we were at an earlier point in development. It considers the extent to which early experiences ( especially infancy) or later experiences are the key determinants of a person's development.

Most lifespan developmentalists, recognise that extreme positions are unwise. Therefore, the key to a comprehensive understanding of development at any stage requires the ·interaction of different factors and not only one.

Developmental psychology is concerned not only with describing the characteristics of psychological change over time but also seeks to explain the principles and internal workings underlying these changes. Psychologists have attempted to better understand these factors by using models. Developmental models are sometimes computational, but they do not need to be.

A model must simply account for the means by which a process takes place. This is sometimes done in reference to changes in the brain that may correspond to changes in behavior over the course of the development. Computational accounts of development often use either symbolic, connectionist (neural network), or dynamical systems models to explain the mechanisms of development.

Cognitive development is primarily concerned with the ways that infants and children acquire, develop, and use internal mental capabilities such as:problem-solving, memory, and language. Major topics in cognitive development are the study of language acquisition and the development of perceptual and motor skills. Piaget was one of the influential early psychologists to study the development of cognitive abilities. His theory suggests that development proceeds through a set of stages from infancy to adulthood and that there is an end point or goal.

Other accounts, such as that of Lev Vygotsky, have suggested that development does not progress through stages, but rather that the developmental process that begins at birth and continues until death is too complex for such structure and finality. Rather, from this viewpoint, developmental processes proceed more continuously. Thus, development should be analyzed, instead of treated as a product to obtain.

K. Warner Schaie has expanded the study of cognitive development into adulthood. Rather than being stable from adolescence, Schaie sees adults as progressing in the application of their cognitive abilities.

Modern cognitive development has integrated the considerations of cognitive psychology and the psychology of individual differences into the interpretation and modeling of development. Specifically, the neo-Piagetian theories of cognitive development showed that the successive levels or stages of cognitive development are associated with increasing processing efficiency and working memory capacity. These increases explain differences between stages, progression to higher stages, and individual differences of children who are the same-age and of the same grade-level. However, other theories have moved away from Piagetian stage theories, and are influenced by accounts of domain-specific information processing, which posit that development is guided by innate evolutionarily-specified and content-specific information processing mechanisms.

Developmental psychologists who are interested in social development examine how individuals develop social and emotional competencies. For example, they study how children form friendships, how they understand and deal with emotions, and how identity develops. Research in this area may involve study of the relationship between cognition or cognitive development and social behavior.

Emotional regulation or ER refers to an individual's ability to modulate emotional responses across a variety of contexts. In young children, this modulation is in part controlled externally, by parents and other authority figures. As children develop, they take on more and more responsibility for their internal state. Studies have shown that the development of ER is affected by the emotional regulation children observe in parents and caretakers, the emotional climate in the home, and the reaction of parents and caretakers to the child's emotions.

Music also has an influence on stimulating and enhancing the senses of a child through self-expression.

A child's social and emotional development can be disrupted by motor coordination problems as evidenced by the environmental stress hypothesis. The environmental hypothesis explains how children with coordination problems and developmental coordination disorder are exposed to several psychosocial consequences which act as secondary stressors, leading to an increase in internalizing symptoms such as depression and anxiety. Motor coordination problems affect fine and gross motor movement as well as perceptual-motor skills. Secondary stressors commonly identified include the tendency for children with poor motor skills to be less likely to participate in organized play with other children and more likely to feel socially isolated.

Social and emotional development focuses on 5 keys areas: Self-Awareness, Self Management, Social Awareness, Relationship Skills and Responsible Decision Making.

Physical development concerns the physical maturation of an individual's body until it reaches the adult stature. Although physical growth is a highly regular process, all children differ tremendously in the timing of their growth spurts. Studies are being done to analyze how the differences in these timings affect and are related to other variables of developmental psychology such as information processing speed. Traditional measures of physical maturity using x-rays are less in practice nowadays, compared to simple measurements of body parts such as height, weight, head circumference, and arm span.

A few other studies and practices with physical developmental psychology are the phonological abilities of mature 5- to 11-year-olds, and the controversial hypotheses of left-handers being maturationally delayed compared to right-handers. A study by Eaton, Chipperfield, Ritchot, and Kostiuk in 1996 found in three different samples that there was no difference between right- and left-handers.

Researchers interested in memory development look at the way our memory develops from childhood and onward. According to Fuzzy-trace theory, we have two separate memory processes: verbatim and gist. These two traces begin to develop at different times as well as at a different pace. Children as young as 4 years-old have verbatim memory, memory for surface information, which increases up to early adulthood, at which point it begins to decline. On the other hand, our capacity for gist memory, memory for semantic information, increases up to early adulthood, at which point it is consistent through old age. Furthermore, our reliance on gist memory traces increases as we age.

Developmental psychology employs many of the research methods used in other areas of psychology. However, infants and children cannot be tested in the same ways as adults, so different methods are often used to study their development.

Developmental psychologists have a number of methods to study changes in individuals over time. Common research methods include systematic observation, including naturalistic observation or structured observation; self-reports, which could be clinical interviews or structured interviews; clinical or case study method; and ethnography or participant observation. These methods differ in the extent of control researchers impose on study conditions, and how they construct ideas about which variables to study. Every developmental investigation can be characterized in terms of whether its underlying strategy involves the "experimental", "correlational", or "case study" approach. The experimental method involves "actual manipulation of various treatments, circumstances, or events to which the participant or subject is exposed; the "experimental design" points to cause-and-effect relationships. This method allows for strong inferences to be made of causal relationships between the manipulation of one or more independent variables and subsequent behavior, as measured by the dependent variable. The advantage of using this research method is that it permits determination of cause-and-effect relationships among variables. On the other hand, the limitation is that data obtained in an artificial environment may lack generalizability. The correlational method explores the relationship between two or more events by gathering information about these variables without researcher intervention. The advantage of using a correlational design is that it estimates the strength and direction of relationships among variables in the natural environment; however, the limitation is that it does not permit determination of cause-and-effect relationships among variables. The case study approach allows investigations to obtain an in-depth understanding of an individual participant by collecting data based on interviews, structured questionnaires, observations, and test scores. Each of these methods have its strengths and weaknesses but the experimental method when appropriate is the preferred method of developmental scientists because it provides a controlled situation and conclusions to be drawn about cause-and-effect relationships.

Most developmental studies, regardless of whether they employ the experimental, correlational, or case study method, can also be constructed using research designs. Research designs are logical frameworks used to make key comparisons within research studies such as:

In a longitudinal study, a researcher observes many individuals born at or around the same time (a cohort) and carries out new observations as members of the cohort age. This method can be used to draw conclusions about which types of development are universal (or normative) and occur in most members of a cohort. As an example a longitudinal study of early literacy development examined in detail the early literacy experiences of one child in each of 30 families.

Researchers may also observe ways that development varies between individuals, and hypothesize about the causes of variation in their data. Longitudinal studies often require large amounts of time and funding, making them unfeasible in some situations. Also, because members of a cohort all experience historical events unique to their generation, apparently normative developmental trends may, in fact, be universal only to their cohort.

In a cross-sectional study, a researcher observes differences between individuals of different ages at the same time. This generally requires fewer resources than the longitudinal method, and because the individuals come from different cohorts, shared historical events are not so much of a confounding factor. By the same token, however, cross-sectional research may not be the most effective way to study differences between participants, as these differences may result not from their different ages but from their exposure to "different" historical events.

A third study design, the sequential design, combines both methodologies. Here, a researcher observes members of different birth cohorts at the same time, and then tracks all participants over time, charting changes in the groups. While much more resource-intensive, the format aids in a clearer distinction between what changes can be attributed to an individual or historical environment from those that are truly universal.

Because every method has some weaknesses, developmental psychologists rarely rely on one study or even one method to reach conclusions by finding consistent evidence from as many converging sources as possible.

Prenatal development is of interest to psychologists investigating the context of early psychological development. The whole prenatal development involves three main stages: germinal stage, embryonic stage and fetal stage. Germinal stage begins at conception until 2 weeks; embryonic stage means the development from 2 weeks to 8 weeks; fetal stage represents 9 weeks until birth of the baby. The senses develop in the womb itself: a fetus can both see and hear by the second trimester (13 to 24 weeks of age). The sense of touch develops in the embryonic stage (5 to 8 weeks). Most of the brain's billions of neurons also are developed by the second trimester. Babies are hence born with some odor, taste and sound preferences, largely related to the mother's environment.

Some primitive reflexes too arise before birth and are still present in newborns. One hypothesis is that these reflexes are vestigial and have limited use in early human life. Piaget's theory of cognitive development suggested that some early reflexes are building blocks for infant sensorimotor development. For example, the tonic neck reflex may help development by bringing objects into the infant's field of view.

Other reflexes, such as the walking reflex appear to be replaced by more sophisticated voluntary control later in infancy. This may be because the infant gains too much weight after birth to be strong enough to use the reflex, or because the reflex and subsequent development are functionally different. It has also been suggested that some reflexes (for example the moro and walking reflexes) are predominantly adaptations to life in the womb with little connection to early infant development. Primitive reflexes reappear in adults under certain conditions, such as neurological conditions like dementia or traumatic lesions.

Ultrasound has shown that infants are capable of a range of movements in the womb, many of which appear to be more than simple reflexes. By the time they are born, infants can recognize and have a preference for their mother's voice suggesting some prenatal development of auditory perception. Prenatal development and birth complications may also be connected to neurodevelopmental disorders, for example in schizophrenia. With the advent of cognitive neuroscience, embryology and the neuroscience of prenatal development is of increasing interest to developmental psychology research.

Several environmental agents—teratogens—can cause damage during the prenatal period. These include prescription and nonprescription drugs, illegal drugs, tobacco, alcohol, environmental pollutants, infectious disease agents such as the rubella virus and the toxoplasmosis parasite, maternal malnutrition, maternal emotional stress, and Rh factor blood incompatibility between mother and child. There are many statistics which prove the effects of the aforementioned substances. A leading example of this would be that, in America alone, approximately 100,000-375,000 'cocaine babies' are born on an annual basis. This is a result of an expectant mother abusing the drug while pregnant. 'Cocaine babies' are proven to have quite severe and lasting difficulties which persist throughout infancy and right throughout childhood. The drug also encourages behavioural problems in the affected children, as well as defects of various vital organs.

From birth until the first year, the child is referred to as an infant. Developmental psychologists vary widely in their assessment of infant psychology, and the influence the outside world has upon it, but certain aspects are relatively clear.

The majority of a newborn infant's time is spent in sleep. At first, this sleep is evenly spread throughout the day and night, but after a couple of months, infants generally become diurnal.

Infants can be seen to have six states, grouped into pairs:

Infant perception is what a newborn can see, hear, smell, taste, and touch. These five features are better known as one's "five senses". Infants respond to stimuli differently in these different states.


Babies are born with the ability to discriminate virtually all sounds of all human languages. Infants of around six months can differentiate between phonemes in their own language, but not between similar phonemes in another language. At this stage infants also start to babble, producing phonemes.

Piaget suggested that an infant's perception and understanding of the world depended on their motor development, which was required for the infant to link visual, tactile and motor representations of objects. According to this view, it is through touching and handling objects that infants develop object permanence, the understanding that objects are solid, permanent, and continue to exist when out of sight.
Piaget's sensorimotor stage comprised six sub-stages (see sensorimotor stages for more detail). In the early stages, development arises out of movements caused by primitive reflexes. Discovery of new behaviors results from classical and operant conditioning, and the formation of habits. From eight months the infant is able to uncover a hidden object but will persevere when the object is moved.

Piaget came to his conclusion that infants lacked a complete understanding of object permanence before 18 months after observing infants' failure before this age to look for an object where it was last seen. Instead, infants continue to look for an object where it was first seen, committing the "A-not-B error." Some researchers have suggested that before the age of eight to nine months, infants' inability to understand object permanence extends to people, which explains why infants at this age do not cry when their mothers are gone ("Out of sight, out of mind").

In the 1980s and 1990s, researchers have developed many new methods of assessing infants' understanding of the world with far more precision and subtlety than Piaget was able to do in his time. Since then, many studies based on these methods suggest that young infants understand far more about the world than first thought.

Based on recent findings, some researchers (such as Elizabeth Spelke and Renee Baillargeon) have proposed that an understanding of object permanence is not learned at all, but rather comprises part of the innate cognitive capacities of our species.

Other research has suggested that young infants in their first six months of life may possess an understanding of numerous aspects of the world around them, including:


There are critical periods in infancy and childhood during which development of certain perceptual, sensorimotor, social and language systems depends crucially on environmental stimulation. Feral children such as Genie, deprived of adequate stimulation, fail to acquire important skills and are unable to learn in later childhood. The concept of critical periods is also well-established in neurophysiology, from the work of Hubel and Wiesel among others.

Children with developmental delays (DD) are at heightened risk for developing clinically significant behavioral and emotional difficulties as compared to children with typical development (TD). However, nearly all studies comparing psychopathology in youth with DD employ TD control groups of the same chronological age (CA).This comorbidity of DD and a mental disorder is often referred to as dual diagnosis. Epidemiological studies indicate that 30–50% of youth with DD meet the clinical cutoff for behavioral and emotional problems and/or diagnosable mental disorder. Studies that include comparison samples of children with typical development (TD) highlight the considerable difference in risk for psychopathology, with the relative risk for youth with DD (to youth with TD) ranging from 2.8–4.1 to 1.

Infants shift between ages of one and two to a developmental stage known as toddlerhood. In this stage, an infant's transition into toddlerhood is highlighted through self-awareness, developing maturity in language use, and presence of memory and imagination.

During toddlerhood, babies begin learning how to walk, talk, and make decisions for themselves. An important characteristic of this age period is the development of language, where children are learning how to communicate and express their emotions and desires through the use of vocal sounds, babbling, and eventually words. Self-control also begins to develop. At this age, children take initiative to explore, experiment and learn from making mistakes. Caretakers who encourage toddlers to try new things and test their limits, help the child become autonomous, self-reliant, and confident. If the caretaker is overprotective or disapproving of independent actions, the toddler may begin to doubt their abilities and feel ashamed of the desire for independence. The child's autonomic development is inhibited, leaving them less prepared to deal with the world in the future. Toddlers also begin to identify themselves in gender roles, acting according to their perception of what a man or woman should do.

Socially, the period of toddler-hood is commonly called the "terrible twos". Toddlers often use their new-found language abilities to voice their desires, but are often misunderstood by parents due to their language skills just beginning to develop. A person at this stage testing their independence is another reason behind the stage's infamous label. Tantrums in a fit of frustration are also common.

Erik Erikson divides childhood into four stages, each with its distinct social crisis:

Play (or preschool) ages 3–5.
In the earliest years, children are "completely dependent on the care of others." Therefore, they develop a "social relationship" with their care givers and, later, with family members. During their preschool years (3-5), they "enlarge their social horizons" to include people outside the family.

Preoperational and then operational thinking develops, which means actions are reversible, and egocentric thought diminishes.

The motor skills of preschoolers increase so they can do more things for themselves. They become more independent. No longer completely dependent on the care of others, the world of this age group expands. More people have a role in shaping their individual personalities. Preschoolers explore and question their world. For Jean Piaget, the child is ""a little scientist" exploring and reflecting on these explorations to increase competence" and this is done in "a very independent way."

Play is a major activity for ages 3–5. For Piaget, through play "a child reaches higher levels of cognitive development."

In their expanded world, children in the 3-5 age group attempt to find their own way. If this is done in a socially acceptable way, the child develops the initiative. If not, the child develops guilt. Children who develop "guilt" rather than "initiative" have failed Erikson's psychosocial crisis for the 3-5 age group.

Middle childhood ages 6–12.
For Erik Erikson, the psychosocial crisis during middle childhood is Industry vs. Inferiority which, if successfully met, instills a sense of Competency in the child.

In all cultures, middle childhood is a time for developing "skills that will be needed in their society." School offers an arena in which children can gain a view of themselves as "industrious (and worthy)." They are "graded for their school work and often for their industry." They can also develop industry outside of school in sports, games, and doing volunteer work. Children who achieve "success in school or games might develop a feeling of competence."

The "peril during this period is that feelings of inadequacy and inferiority will develop. Parents and teachers can "undermine" a child's development by failing to recognize accomplishments or being overly critical of a child's efforts.
Children who are "encouraged and praised" develop a belief in their competence. Lack of encouragement or ability to excel lead to "feelings of inadequacy and inferiority".

The Centers for Disease Control (the CDC) divides Middle Childhood into two stages, 6–8 years and 9–11 years, and gives "developmental milestones for each stage."

"Middle Childhood (7-10)."
Entering elementary school, children in this age group begin to thinks about the future and their "place in the world." Working with other students and wanting their friendship and acceptance become more important. This leads to "more independence from parents and family." As students, they develop the mental and verbal skills "to describe experiences and talk about thoughts and feelings". They become less self-centered and show "more concern for others".

"Middle Childhood (9-11)."
For children ages 9–11 "friendships and peer relationships" increase in strength, complexity, and importance. This results in greater "peer pressure." They grow even less dependent on their families and they are challenged academically. To meet this challenge, they increase their attention span and learn to see other points of view.

Adolescence is the period of life between the onset of puberty and the full commitment to an adult social role, such as worker, parent, and/or citizen. It is the period known for the formation of personal and social identity (see Erik Erikson) and the discovery of moral purpose (see William Damon). Intelligence is demonstrated through the logical use of symbols related to abstract concepts and formal reasoning. A return to egocentric thought often occurs early in the period. Only 35% develop the capacity to reason formally during adolescence or adulthood. (Huitt, W. and Hummel, J. January 1998)

It is divided into three parts, namely:

The adolescent unconsciously explores questions such as "Who am I? Who do I want to be?" Like toddlers, adolescents must explore, test limits, become autonomous, and commit to an identity, or sense of self. Different roles, behaviors and ideologies must be tried out to select an identity. Role confusion and inability to choose vocation can result from a failure to achieve a sense of identity through, for example, friends.

Early adulthood generally refers to the period between ages 18 to 25, and according to theorists such as Erik Erikson, is a stage where development is mainly focused on maintaining relationships. Examples include creating bond of intimacy, sustaining friendships, and ultimately making a family. Some theorists state that development of intimacy skills rely on the resolution of previous developmental stages. A sense of identity gained in the previous stages is also necessary for intimacy to develop. If this skill is not learned the alternative is alienation, isolation, a fear of commitment, and the inability to depend on others.

A related framework for studying this part of the lifespan is that of emerging adulthood. Scholars of emerging adulthood, such as Jeffrey Arnett, are not necessarily interested in relationship development. Instead, this concept suggests that people transition after their teenage years into a period not characterized as relationship building and an overall sense of constancy with life, but with years of living with parents, phases of self-discovery, and experimentation.

Middle adulthood generally refers to the period between ages 25 to 69. During this period, middle-aged adults experience a conflict between generativity and stagnation. They may either feel a sense of contributing to society, the next generation, or their immediate community; or develop a sense of purposelessness.

Physically, the middle-aged experience a decline in muscular strength, reaction time, sensory keenness, and cardiac output. Also, women experience menopause and a sharp drop in the hormone estrogen. Men experience an equivalent endocrine system event to menopause. Andropause in males is a hormone fluctuation with physical and psychological effects that can be similar to those seen in menopausal females. As men age lowered testosterone levels can contribute to mood swings and a decline in sperm count. Sexual responsiveness can also be affected, including delays in erection and longer periods of penile stimulation required to achieve ejaculation.

The World Health Organization finds "no general agreement on the age at which a person becomes old." Most "developed countries" set the age as 60 or 65. However, in developing countries inability to make "active contribution" to society, not chronological age, marks the beginning of old age. According to Erikson's stages of psychosocial development, old age is the stage in which individuals assess the quality of their lives. In reflecting on their lives, people in this age group develop a feeling of integrity if deciding that their lives were successful or a feeling of despair if evaluation of one's life indicates a failure to achieve goals.

Physically, older people experience a decline in muscular strength, reaction time, stamina, hearing, distance perception, and the sense of smell. They also are more susceptible to diseases such as cancer and pneumonia due to a weakened immune system. Programs aimed at balance, muscle strength, and mobility have been shown to reduce disability among mildly (but not more severely) disabled elderly.

Sexual expression depends in large part upon the emotional and physical health of the individual. Many older adults continue to be sexually active and satisfied with their sexual activity.

Mental disintegration may also occur, leading to dementia or ailments such as Alzheimer's disease. It is generally believed that crystallized intelligence increases up to old age, while fluid intelligence decreases with age. Whether or not normal intelligence increases or decreases with age depends on the measure and study. Longitudinal studies show that perceptual speed, inductive reasoning, and spatial orientation decline. An article on adult cognitive development reports that cross-sectional studies show that "some abilities remained stable into early old age."

Parenting variables alone have typically accounted for 20 to 50 percent of the variance in child outcomes.

All parents have their own parenting styles. Parenting styles, according to Kimberly Kopoko, are "based upon two aspects of parenting behavior; control and warmth. Parental control refers to the degree to which parents manage their children's behavior. Parental warmth refers to the degree to which parents are accepting and responsive to their children's behavior."

The following parenting styles have been described in the child development literature:


Parenting roles in child development have typically focused on the role of the mother. Recent literature, however, has looked toward the father as having an important role in child development. Affirming a role for fathers, studies have shown that children as young as 15 months benefit significantly from substantial engagement with their father. In particular, a study in the U.S. and New Zealand found the presence of the natural father was the most significant factor in reducing rates of early sexual activity and rates of teenage pregnancy in girls. Furthermore, another argument is that neither a mother nor a father is actually essential in successful parenting, and that single parents as well as homosexual couples can support positive child outcomes. According to this set of research, children need at least one consistently responsible adult with whom the child can have a positive emotional connection. Having more than one of these figures contributes to a higher likelihood of positive child outcomes.

Another parental factor often debated in terms of its effects on child development is divorce. Divorce in itself is not a determining factor of negative child outcomes. In fact, the majority of children from divorcing families fall into the normal range on measures of psychological and cognitive functioning. A number of mediating factors play a role in determining the effects divorce has on a child, for example, divorcing families with young children often face harsher consequences in terms of demographic, social, and economic changes than do families with older children. Positive coparenting after divorce is part of a pattern associated with positive child coping, while hostile parenting behaviors lead to a destructive pattern leaving children at risk. Additionally, direct parental relationship with the child also affects the development of a child after a divorce. Overall, protective factors facilitating positive child development after a divorce are maternal warmth, positive father-child relationship, and cooperation between parents.


Prominent journals in developmental psychology include:



</doc>
<doc id="9015" url="https://en.wikipedia.org/wiki?curid=9015" title="DNA replication">
DNA replication

In molecular biology, DNA replication is the biological process of producing two identical replicas of DNA from one original DNA molecule. This process occurs in all living organisms and is the basis for biological inheritance. The cell possesses the distinctive property of division, which makes replication of DNA essential.

DNA is made up of a double helix of two complementary strands. During replication, these strands are separated. Each strand of the original DNA molecule then serves as a template for the production of its counterpart, a process referred to as semiconservative replication. As a result of semi-conservative replication, the new helix will be composed of an original DNA strand as well as a newly synthesized strand. Cellular proofreading and error-checking mechanisms ensure near perfect fidelity for DNA replication.

In a cell, DNA replication begins at specific locations, or origins of replication, in the genome. Unwinding of DNA at the origin and synthesis of new strands, accommodated by an enzyme known as helicase, results in replication forks growing bi-directionally from the origin. A number of proteins are associated with the replication fork to help in the initiation and continuation of DNA synthesis. Most prominently, DNA polymerase synthesizes the new strands by adding nucleotides that complement each (template) strand. DNA replication occurs during the S-stage of interphase.

DNA replication (DNA amplification) can also be performed "in vitro" (artificially, outside a cell). DNA polymerases isolated from cells and artificial DNA primers can be used to initiate DNA synthesis at known sequences in a template DNA molecule. Polymerase chain reaction (PCR), ligase chain reaction (LCR), and transcription-mediated amplification (TMA) are examples. 

DNA exists as a double-stranded structure, with both strands coiled together to form the characteristic double-helix. Each single strand of DNA is a chain of four types of nucleotides. Nucleotides in DNA contain a deoxyribose sugar, a phosphate, and a nucleobase. The four types of nucleotide correspond to the four nucleobases adenine, cytosine, guanine, and thymine, commonly abbreviated as A, C, G and T. Adenine and guanine are purine bases, while cytosine and thymine are pyrimidines. These nucleotides form phosphodiester bonds, creating the phosphate-deoxyribose backbone of the DNA double helix with the nucleobases pointing inward (i.e., toward the opposing strand). Nucleobases are matched between strands through hydrogen bonds to form base pairs. Adenine pairs with thymine (two hydrogen bonds), and guanine pairs with cytosine (three hydrogen bonds).

DNA strands have a directionality, and the different ends of a single strand are called the "3' (three-prime) end" and the "5' (five-prime) end". By convention, if the base sequence of a single strand of DNA is given, the left end of the sequence is the 5' end, while the right end of the sequence is the 3' end. The strands of the double helix are anti-parallel with one being 5' to 3', and the opposite strand 3' to 5'. These terms refer to the carbon atom in deoxyribose to which the next phosphate in the chain attaches. Directionality has consequences in DNA synthesis, because DNA polymerase can synthesize DNA in only one direction by adding nucleotides to the 3' end of a DNA strand.

The pairing of complementary bases in DNA (through hydrogen bonding) means that the information contained within each strand is useless. phosphoodiester (intra-strand) bonds are stronger than hydrogen (inter-strand) bonds. This allows the strands to be separated from one another. The nucleotides on a single strand can therefore be used to reconstruct nucleotides on a newly synthesized partner strand.

DNA polymerases are a family of enzymes that carry out all forms of DNA replication. DNA polymerases in general cannot initiate synthesis of new strands, but can only extend an existing DNA or RNA strand paired with a template strand. To begin synthesis, a short fragment of RNA, called a primer, must be created and paired with the template DNA strand.

DNA polymerase adds a new strand of DNA by extending the 3' end of an existing nucleotide chain, adding new nucleotides matched to the template strand one at a time via the creation of phosphodiester bonds. The energy for this process of DNA polymerization comes from hydrolysis of the high-energy phosphate (phosphoanhydride) bonds between the three phosphates attached to each unincorporated base. Free bases with their attached phosphate groups are called nucleotides; in particular, bases with three attached phosphate groups are called nucleoside triphosphates. When a nucleotide is being added to a growing DNA strand, the formation of a phosphodiester bond between the proximal phosphate of the nucleotide to the growing chain is accompanied by hydrolysis of a high-energy phosphate bond with release of the two distal phosphates as a pyrophosphate. Enzymatic hydrolysis of the resulting pyrophosphate into inorganic phosphate consumes a second high-energy phosphate bond and renders the reaction effectively irreversible.

In general, DNA polymerases are highly accurate, with an intrinsic error rate of less than one mistake for every 10 nucleotides added. In addition, some DNA polymerases also have proofreading ability; they can remove nucleotides from the end of a growing strand in order to correct mismatched bases. Finally, post-replication mismatch repair mechanisms monitor the DNA for errors, being capable of distinguishing mismatches in the newly synthesized DNA strand from the original strand sequence. Together, these three discrimination steps enable replication fidelity of less than one mistake for every 10 nucleotides added.

The rate of DNA replication in a living cell was first measured as the rate of phage T4 DNA elongation in phage-infected "E. coli". During the period of exponential DNA increase at 37 °C, the rate was 749 nucleotides per second. The mutation rate per base pair per replication during phage T4 DNA synthesis is 1.7 per 10.

DNA replication, like all biological polymerization processes, proceeds in three enzymatically catalyzed and coordinated steps: initiation, elongation and termination.

For a cell to divide, it must first replicate its DNA. This process is initiated at particular points in the DNA, known as "origins", which are targeted by initiator proteins. In "E. coli" this protein is DnaA; in yeast, this is the origin recognition complex. Sequences used by initiator proteins tend to be "AT-rich" (rich in adenine and thymine bases), because A-T base pairs have two hydrogen bonds (rather than the three formed in a C-G pair) and thus are easier to strand-separate. Once the origin has been located, these initiators recruit other proteins and form the pre-replication complex, which unwinds the double-stranded DNA.

DNA polymerase has 5′–3′ activity.
All known DNA replication systems require a free 3' hydroxyl group before synthesis can be initiated (note: the DNA template is read in 3′ to 5′ direction whereas a new strand is synthesized in the 5′ to 3′ direction—this is often confused). Four distinct mechanisms for DNA synthesis are recognized:

The first is the best known of these mechanisms and is used by the cellular organisms. In this mechanism, once the two strands are separated, primase adds RNA primers to the template strands. The leading strand receives one RNA primer while the lagging strand receives several. The leading strand is continuously extended from the primer by a DNA polymerase with high processivity, while the lagging strand is extended discontinuously from each primer forming Okazaki fragments. RNase removes the primer RNA fragments, and a low processivity DNA polymerase distinct from the replicative polymerase enters to fill the gaps. When this is complete, a single nick on the leading strand and several nicks on the lagging strand can be found. Ligase works to fill these nicks in, thus completing the newly replicated DNA molecule.

The primase used in this process differs significantly between bacteria and archaea/eukaryotes. Bacteria use a primase belonging to the DnaG protein superfamily which contains a catalytic domain of the TOPRIM fold type. The TOPRIM fold contains an α/β core with four conserved strands in a Rossmann-like topology. This structure is also found in the catalytic domains of topoisomerase Ia, topoisomerase II, the OLD-family nucleases and DNA repair proteins related to the RecR protein.

The primase used by archaea and eukaryotes, in contrast, contains a highly derived version of the RNA recognition motif (RRM). This primase is structurally similar to many viral RNA-dependent RNA polymerases, reverse transcriptases, cyclic nucleotide generating cyclases and DNA polymerases of the A/B/Y families that are involved in DNA replication and repair. In eukaryotic replication, the primase forms a complex with Pol α.

Multiple DNA polymerases take on different roles in the DNA replication process. In "E. coli", DNA Pol III is the polymerase enzyme primarily responsible for DNA replication. It assembles into a replication complex at the replication fork that exhibits extremely high processivity, remaining intact for the entire replication cycle. In contrast, DNA Pol I is the enzyme responsible for replacing RNA primers with DNA. DNA Pol I has a 5′ to 3′ exonuclease activity in addition to its polymerase activity, and uses its exonuclease activity to degrade the RNA primers ahead of it as it extends the DNA strand behind it, in a process called nick translation. Pol I is much less processive than Pol III because its primary function in DNA replication is to create many short DNA regions rather than a few very long regions.

In eukaryotes, the low-processivity enzyme, Pol α, helps to initiate replication because it forms a complex with primase. In eukaryotes, leading strand synthesis is thought to be conducted by Pol ε; however, this view has recently been challenged, suggesting a role for Pol δ. Primer removal is completed Pol δ while repair of DNA during replication is completed by Pol ε.

As DNA synthesis continues, the original DNA strands continue to unwind on each side of the bubble, forming a replication fork with two prongs. In bacteria, which have a single origin of replication on their circular chromosome, this process creates a "theta structure" (resembling the Greek letter theta: θ). In contrast, eukaryotes have longer linear chromosomes and initiate replication at multiple origins within these.

The replication fork is a structure that forms within the nucleus during DNA replication. It is created by helicases, which break the hydrogen bonds holding the two DNA strands together. The resulting structure has two branching "prongs", each one made up of a single strand of DNA. These two strands serve as the template for the leading and lagging strands, which will be created as DNA polymerase matches complementary nucleotides to the templates; the templates may be properly referred to as the leading strand template and the lagging strand template.

DNA is always synthesized in the 5' to 3' direction. Since the leading and lagging strand templates are oriented in opposite directions at the replication fork, a major issue is how to achieve synthesis of nascent (new) lagging strand DNA, whose direction of synthesis is opposite to the direction of the growing replication fork.

The leading strand is the strand of nascent DNA which is being synthesized in the same direction as the growing replication fork. This sort of DNA replication is continuous.

The lagging strand is the strand of nascent DNA whose direction of synthesis is opposite to the direction of the growing replication fork. Because of its orientation, replication of the lagging strand is more complicated as compared to that of the leading strand. As a consequence, the DNA polymerase on this strand is seen to "lag behind" the other strand.

The lagging strand is synthesized in short, separated segments. On the lagging strand "template", a primase "reads" the template DNA and initiates synthesis of a short complementary RNA primer. A DNA polymerase extends the primed segments, forming Okazaki fragments. The RNA primers are then removed and replaced with DNA, and the fragments of DNA are joined together by DNA ligase.

As helicase unwinds DNA at the replication fork, the DNA ahead is forced to rotate. This process results in a build-up of twists in the DNA ahead. This build-up forms a torsional resistance that would eventually halt the progress of the replication fork. Topoisomerases are enzymes that temporarily break the strands of DNA, relieving the tension caused by unwinding the two strands of the DNA helix; topoisomerases (including DNA gyrase) achieve this by adding negative supercoils to the DNA helix.

Bare single-stranded DNA tends to fold back on itself forming secondary structures; these structures can interfere with the movement of DNA polymerase. To prevent this, single-strand binding proteins bind to the DNA until a second strand is synthesized, preventing secondary structure formation.

Clamp proteins form a sliding clamp around DNA, helping the DNA polymerase maintain contact with its template, thereby assisting with processivity. The inner face of the clamp enables DNA to be threaded through it. Once the polymerase reaches the end of the template or detects double-stranded DNA, the sliding clamp undergoes a conformational change that releases the DNA polymerase. Clamp-loading proteins are used to initially load the clamp, recognizing the junction between template and RNA primers.

At the replication fork, many replication enzymes assemble on the DNA into a complex molecular machine called the replisome. The following is a list of major DNA replication enzymes that participate in the replisome:

Replication machineries consist of factors involved in DNA replication and appearing on template ssDNAs. Replication machineries include primosotors are replication enzymes; DNA polymerase, DNA helicases, DNA clamps and DNA topoisomerases, and replication proteins; e.g. single-stranded DNA binding proteins (SSB). In the replication machineries these components coordinate. In most of the bacteria, all of the factors involved in DNA replication are located on replication forks and the complexes stay on the forks during DNA replication. These replication machineries are called replisomes or DNA replicase systems. These terms are generic terms for proteins located on replication forks. In eukaryotic and some bacterial cells the replisomes are not formed.

Since replication machineries do not move relatively to template DNAs such as factories, they are called a replication factory. In an alternative figure, DNA factories are similar to projectors and DNAs are like as cinematic films passing constantly into the projectors. In the replication factory model, after both DNA helicases for leading strands and lagging strands are loaded on the template DNAs, the helicases run along the DNAs into each other. The helicases remain associated for the remainder of replication process. Peter Meister et al. observed directly replication sites in budding yeast by monitoring green fluorescent protein(GFP)-tagged DNA polymerases α. They detected DNA replication of pairs of the tagged loci spaced apart symmetrically from a replication origin and found that the distance between the pairs decreased markedly by time. This finding suggests that the mechanism of DNA replication goes with DNA factories. That is, couples of replication factories are loaded on replication origins and the factories associated with each other. Also, template DNAs move into the factories, which bring extrusion of the template ssDNAs and nascent DNAs. Meister’s finding is the first direct evidence of replication factory model. Subsequent research has shown that DNA helicases form dimers in many eukaryotic cells and bacterial replication machineries stay in single intranuclear location during DNA synthesis.

The replication factories perform disentanglement of sister chromatids. The disentanglement is essential for distributing the chromatids into daughter cells after DNA replication. Because sister chromatids after DNA replication hold each other by Cohesin rings, there is the only chance for the disentanglement in DNA replication. Fixing of replication machineries as replication factories can improve the success rate of DNA replication. If replication forks move freely in chromosomes, catenation of nuclei is aggravated and impedes mitotic segregation.

Eukaryotes initiate DNA replication at multiple points in the chromosome, so replication forks meet and terminate at many points in the chromosome. Because eukaryotes have linear chromosomes, DNA replication is unable to reach the very end of the chromosomes. Due to this problem, DNA is lost each replication cycle from the end of the chromosome. Telomeres are regions of repetitive DNA close to the ends and help prevent loss of genes due to this shortening. Shortening of the telomeres is a normal process in somatic cells. This shortens the telomeres of the daughter DNA chromosome. As a result, cells can only divide a certain number of times before the DNA loss prevents further division. (This is known as the Hayflick limit.) Within the germ cell line, which passes DNA to the next generation, telomerase extends the repetitive sequences of the telomere region to prevent degradation. Telomerase can become mistakenly active in somatic cells, sometimes leading to cancer formation. Increased telomerase activity is one of the hallmarks of cancer.

Termination requires that the progress of the DNA replication fork must stop or be blocked. Termination at a specific locus, when it occurs, involves the interaction between two components: (1) a termination site sequence in the DNA, and (2) a protein which binds to this sequence to physically stop DNA replication. In various bacterial species, this is named the DNA replication terminus site-binding protein, or Ter protein.

Because bacteria have circular chromosomes, termination of replication occurs when the two replication forks meet each other on the opposite end of the parental chromosome. "E. coli" regulates this process through the use of termination sequences that, when bound by the Tus protein, enable only one direction of replication fork to pass through. As a result, the replication forks are constrained to always meet within the termination region of the chromosome.

Within eukaryotes, DNA replication is controlled within the context of the cell cycle. As the cell grows and divides, it progresses through stages in the cell cycle; DNA replication takes place during the S phase (synthesis phase). The progress of the eukaryotic cell through the cycle is controlled by cell cycle checkpoints. Progression through checkpoints is controlled through complex interactions between various proteins, including cyclins and cyclin-dependent kinases. Unlike bacteria, eukaryotic DNA replicates in the confines of the nucleus.

The G1/S checkpoint (or restriction checkpoint) regulates whether eukaryotic cells enter the process of DNA replication and subsequent division. Cells that do not proceed through this checkpoint remain in the G0 stage and do not replicate their DNA.

Replication of chloroplast and mitochondrial genomes occurs independently of the cell cycle, through the process of D-loop replication.

In vertebrate cells, replication sites concentrate into positions called replication foci. Replication sites can be detected by immunostaining daughter strands and replication enzymes and monitoring GFP-tagged replication factors. By these methods it is found that replication foci of varying size and positions appear in S phase of cell division and their number per nucleus is far smaller than the number of genomic replication forks.

P. Heun et al.(2001) tracked GFP-tagged replication foci in budding yeast cells and revealed that replication origins move constantly in G1 and S phase and the dynamics decreased significantly in S phase. Traditionally, replication sites were fixed on spatial structure of chromosomes by nuclear matrix or lamins. The Heun’s results denied the traditional concepts, budding yeasts don't have lamins, and support that replication origins self-assemble and form replication foci.

By firing of replication origins, controlled spatially and temporally, the formation of replication foci is regulated. D. A. Jackson et al.(1998) revealed that neighboring origins fire simultaneously in mammalian cells. Spatial juxtaposition of replication sites brings clustering of replication forks. The clustering do rescue of stalled replication forks and favors normal progress of replication forks. Progress of replication forks is inhibited by many factors; collision with proteins or with complexes binding strongly on DNA, deficiency of dNTPs, nicks on template DNAs and so on. If replication forks stall and the remaining sequences from the stalled forks are not replicated, the daughter strands have nick obtained un-replicated sites. The un-replicated sites on one parent's strand hold the other strand together but not daughter strands. Therefore, the resulting sister chromatids cannot separate from each other and cannot divide into 2 daughter cells. When neighboring origins fire and a fork from one origin is stalled, fork from other origin access on an opposite direction of the stalled fork and duplicate the un-replicated sites. As other mechanism of the rescue there is application of dormant replication origins that excess origins don't fire in normal DNA replication.

Most bacteria do not go through a well-defined cell cycle but instead continuously copy their DNA; during rapid growth, this can result in the concurrent occurrence of multiple rounds of replication. In "E. coli", the best-characterized bacteria, DNA replication is regulated through several mechanisms, including: the hemimethylation and sequestering of the origin sequence, the ratio of adenosine triphosphate (ATP) to adenosine diphosphate (ADP), and the levels of protein DnaA. All these control the binding of initiator proteins to the origin sequences.

Because "E. coli" methylates GATC DNA sequences, DNA synthesis results in hemimethylated sequences. This hemimethylated DNA is recognized by the protein SeqA, which binds and sequesters the origin sequence; in addition, DnaA (required for initiation of replication) binds less well to hemimethylated DNA. As a result, newly replicated origins are prevented from immediately initiating another round of DNA replication.

ATP builds up when the cell is in a rich medium, triggering DNA replication once the cell has reached a specific size. ATP competes with ADP to bind to DnaA, and the DnaA-ATP complex is able to initiate replication. A certain number of DnaA proteins are also required for DNA replication — each time the origin is copied, the number of binding sites for DnaA doubles, requiring the synthesis of more DnaA to enable another initiation of replication.

Researchers commonly replicate DNA "in vitro" using the polymerase chain reaction (PCR). PCR uses a pair of primers to span a target region in template DNA, and then polymerizes partner strands in each direction from these primers using a thermostable DNA polymerase. Repeating this process through multiple cycles amplifies the targeted DNA region. At the start of each cycle, the mixture of template and primers is heated, separating the newly synthesized molecule and template. Then, as the mixture cools, both of these become templates for annealing of new primers, and the polymerase extends from these. As a result, the number of copies of the target region doubles each round, increasing exponentially.



</doc>
<doc id="9016" url="https://en.wikipedia.org/wiki?curid=9016" title="Dravidian">
Dravidian

Dravidian or Dravida may refer to:








</doc>
<doc id="9020" url="https://en.wikipedia.org/wiki?curid=9020" title="Daisy Duck">
Daisy Duck

Daisy Duck is a cartoon character created in 1940 by Walt Disney Productions as the girlfriend of Donald Duck. Like Donald, Daisy is an anthropomorphic white duck, but has large eyelashes and ruffled tail feathers to suggest a skirt. She is often seen wearing a hair bow, blouse, and heeled shoes. Daisy usually shows a strong affinity towards Donald, although she is often characterized as being more sophisticated than him.

Daisy was introduced in the short film "Mr. Duck Steps Out" (1940) and was incorporated into Donald's comic stories several months later. She appeared in 11 short films between 1940 and 1954, and far later in "Mickey's Christmas Carol" (1983) and "Fantasia 2000" (1999). In these roles, Daisy was always a supporting character, with the exception of "Donald's Dilemma" (1947). Daisy has received considerably more screen time in television, making regular appearances in "Quack Pack" (1996), "Mickey Mouse Works" (1999-2000), "Disney's House of Mouse" (2001–2003), and "Mickey Mouse Clubhouse" (2006–2016). Daisy has also appeared in several direct-to-video films such as "Mickey's Once Upon a Christmas" (1999) and "" (2004).

Daisy is the aunt of April, May, and June, three young girl ducks who act as Huey, Dewey, and Louie's female counterparts. Daisy is a close friend of Clarabelle Cow and Clara Cluck in the comics and Minnie Mouse's best friend.

Since her early appearances, Daisy is attracted to Donald and is devoted to him. This is most clearly seen in "Donald's Dilemma" as Daisy is almost to the point of suicide after Donald forgets her. Despite this, she's shown to have her boyfriend wrapped around her finger and is often shown to keep him in line whenever his anger starts to boil.

Besides her love for Donald, Daisy is also shown to be more sophisticated and intelligent than him. In "Cured Duck" Daisy even gives Donald an ultimatum regarding his temper but later reforms in "Donald's Dilemma". Daisy herself sometimes exhibits a temper, but she has much greater self-control than Donald.

In the "Mouse Works"/"House of Mouse" cartoons, she was sometimes portrayed as intrusive and overly talkative. She would invite herself in without asking and would tag along on trips where she was not wanted.

Daisy is a white duck with an orange bill and legs. She usually has sultry indigo eyeshadow, long distinct eyelashes and ruffled feathers around her lowest region to suggest a skirt.

She's usually seen sporting a blouse with puffed short sleeves and a v-neckline. She also wears a matching bow, heeled shoes and a single bangle on her wrist. The colors of her clothes change very often, but her signature colors are usually purple and pink.

The television series Quack Pack gave Daisy Duck a more mature wardrobe and hairstyle and cast her as a career woman with a television reporter job. House of Mouse got her a blue and purple employee uniform, with a blue bow, and a long ponytail. In Mickey Mouse Clubhouse, Daisy regained her purple blouse with a purple bow and shoes. She also wears a gold bangle and has a short ponytail, similar to the longer one seen in House of Mouse.

Daisy Duck has been voiced by several different voice actors over the years, yet by far the most extensive work has been done by Tress MacNeille, who took on the role in 1999.

Clarence Nash, who provided Donald's voice, voiced Daisy in her debut in "Mr. Duck Steps Out" with a "duck voice" similar to Donald's but pitched higher. For Daisy's second appearance Gloria Blondell took over, marking the debut of Daisy's "normal" voices. Blondell would voice Daisy for six of her nine speaking appearances during the classic shorts era. Daisy's third voice was Ruth Clifford who voiced the character only once in "Donald's Dream Voice" (1948). After Blondell returned for one more performance, Vivi Janiss voiced Daisy (and also her mother) in her final classic cartoon, "Donald's Diary" (1954). In 1983 Daisy was voiced by Patricia Parris in "Mickey's Christmas Carol". Daisy was then voiced by Donald's current voice actor, Kath Soucie, in "Down and Out with Donald Duck" (1987) and then throughout her first regular television series "Quack Pack" (1996). In 1998 Daisy was voiced by Diane Michelle in the anthology film "The Spirit of Mickey". Also in "Mickey's Once Upon a Christmas" (1999) along with Tress MacNeille. In 1999 Tress MacNeille took over as Daisy's full-time voice. MacNeille has voiced Daisy in the television series "Mickey Mouse Works", "Disney's House of Mouse", and "Mickey Mouse Clubhouse". MacNeille has also voiced Daisy in television specials and movies. Daisy was also voiced by Russi Taylor in "Fantasia 2000" although she had no lines; just a scream.

According to some sources, Daisy was introduced in 1937 as Donna Duck, yet there is conflicting evidence as to whether Donna was an early version of Daisy or a separate character entirely. However, the fact that The Walt Disney Company released a collector's pin (See #703 on Pinpics.com) in 1999 which states, "Daisy Duck debuts as Donna Duck 1937," solidifies the fact that Daisy Duck and Donna Duck were, in fact, one and the same.

Donna made her sole animated appearance in the short film "Don Donald" (1937), directed by Ben Sharpsteen. It was the first installment of the "Donald Duck" film series and was also the first time Donald was shown with a love interest. In the story, Donald travels to Mexico to court a duck who is largely a female version of himself. She is portrayed with the same feisty temperament and impatience and was even voiced by Donald's voice actor Clarence Nash. At the end of the story she spitefully abandons Donald in the desert after his car breaks down.

Some sources consider "Don Donald" Daisy's debut. These include The Encyclopedia of Animated Disney Shorts, and the Big Cartoon DataBase. In addition to this, "Don Donald" is included on the Disney-produced DVD "Best Pals: Donald and Daisy." Donna's identification as an early Daisy is aided by the fact that other Disney characters, such as Goofy, were also introduced under different names (Dippy Dawg), appearances, and mannerisms. "Donna" in Italian is also the equivalent of "Don," a title Donald takes in the film's title.

However, in 1951 the character of Donna was retconned in a newspaper comic strip where she appeared as a separate character from Daisy and as an unwitting rival for Donald's affections.

Daisy debuted in theatrical animation and has appeared in a total of 15 films. She appeared in 12 "Donald Duck" short films. These are, in order of release, "Mr. Duck Steps Out" (1940), "Donald's Crime" (1945), "Cured Duck" (1945), "Donald's Double Trouble" (1946), "Dumb Bell of the Yukon", "Sleepy Time Donald" (1947), "Donald's Dilemma", "Donald's Dream Voice" (1948), "Crazy Over Daisy" (1950), "Donald's Diary" (1954) & "How to Have an Accident at Work"(1959) as Donald's unnamed wife. She also made a brief cameo in the "Mickey Mouse" short film "The Nifty Nineties" (1941). After the classic shorts era, Daisy appeared in "Mickey's Christmas Carol" (1983) and "Fantasia 2000" (1999) with another cameo in "Who Framed Roger Rabbit" (1988).

Daisy Duck in her familiar name and design first appeared in "Mr. Duck Steps Out" (June 7, 1940). The short was directed by Jack King and scripted by Carl Barks. There Donald visits the house of his new romantic interest for their first known date. At first, Daisy acts shy and has her back turned to her visitor. But Donald soon notices her tail-feathers taking the form of a hand and signaling for him to come closer. But their time alone is soon interrupted by Huey, Dewey, and Louie who have followed their uncle and clearly compete with him for the attention of Daisy. Uncle and nephews take turns dancing the jitterbug with her while trying to get rid of each other. In their final effort, the three younger Ducks feed their uncle maize (corn) in the process of becoming popcorn. The process is completed within Donald himself who continues to move spastically around the house while maintaining the appearance of dancing. The short ends with an impressed Daisy showering her new boyfriend with kisses. Like her precursor, she was initially voiced by Clarence Nash, but would later have a more ladylike voice.

The short stands out among other Donald shorts of the period for its use of modern music and surreal situations throughout.

One year following her introduction in "Mr. Duck Steps Out", Daisy, along with Donald and the nephews, made a brief cameo in the "Mickey Mouse" short "The Nifty Nineties", cementing her position as a recurring character.

Daisy's second speaking role came four years later in "Donald's Crime". While Daisy has a relatively small role in the film, her date with Donald is central to the plot and shows Donald's infatuation for her. Finding himself broke before the date; Donald steals money from his nephews, but afterward feels guilty. Donald imagines what Daisy might think of him knowing he stole money, and this leads him to reform in the end. Daisy was voiced in the film by actress Gloria Blondell, marking the first time Daisy had a "normal" voice as opposed to a duck voice like Donald's. The film also marked the first time Daisy appeared in an Academy Award nominated film (Best Animated Short).

Later that same year Daisy appeared again in "Cured Duck" (October 26, 1945). The short starts simply enough. Donald visits Daisy at her house. She asks him to open a window. He keeps trying to pull it open and eventually goes into a rage. By the time Daisy returns to the room, Donald has wrecked it. She demonstrates that the locking mechanism was on and criticizes his temper. She refuses to date Donald again until he learns to manage his anger. She claims Donald does not see her losing her own temper. Donald agrees to her terms and follows the surreal method of mail ordering an "insult machine", a device constantly hurling verbal and physical insults at him. He endures the whole process until feeling able to stay calm throughout it. He visits Daisy again and this time calmly opens the window. But when Daisy shows her boyfriend her new hat, his reaction is uncontrollable laughter. Daisy goes into a rage of her own and the short ends by pointing out that Donald is not the only Duck in need of anger management training. There is a continuation regarding her temper at one episode in Mickey Mouse Works where she and Donald have a date in a restaurant wherein they both end up with a bad temper.

Their relationship problems were also focused on in "Donald's Double Trouble" (June 28, 1946). This time Daisy criticizes his poor command of the English language and his less-than-refined manners. Unwilling to lose Daisy, Donald has to find an answer to the problem. But his solution involves his own look-alike who happens to have all the desired qualities. His unnamed look-alike happens to be unemployed at the moment and agrees to this plan. Donald provides the money for his dates with Daisy but soon comes to realize the look-alike serves as a rival suitor. The rest of the short focuses on his increasing jealousy and efforts to replace the look-alike during the next date. However, a failed attempt at a tunnel of love results in the two male Ducks exiting the tunnel in each other's hands by mistake. Daisy walks out completely drenched. She jumps up and down and sounds like a record played too fast as Donald and his look-alike run away.

In "Dumb Bell of the Yukon", Daisy is the motivation behind Donald's hunting trip after he reads a letter from her saying she likes fur coats. Daisy briefly appears in a non-speaking role in Donald's daydream, imagining how pleased she will be.

Her next appearance in "Sleepy Time Donald" (May 9, 1947) involved Daisy attempting to rescue a sleepwalking Donald from wandering into danger. Donald is loose in an urban environment and the humor results from the problems Daisy herself suffers while trying to keep him safe.

Daisy was also the actual protagonist of "Donald's Dilemma" (July 11, 1947). The short starts simply enough. Donald and Daisy are out on a date when a flower pot falls on his head. He regains consciousness soon enough but with some marked differences. Both his speaking and singing voices have been improved to the point of being able to enter a new career as a professional singer. He also acts more refined than usual. Most importantly Donald suffers from partial amnesia and has no memory of Daisy. Donald goes on becoming a well-known crooner and his rendition of When You Wish upon a Star becomes a hit. He is surrounded by female fans in his every step. Meanwhile, Daisy cannot even approach her former lover and her loss results in a number of psychological symptoms. Various scenes feature her suffering from anorexia, insomnia, and self-described insanity. An often censored scene features her losing her will to live and contemplating various methods of suicide. She narrates her story to a psychologist who determines that Donald would regain his memory with another flower pot falling on his head but warns that his improved voice may also be lost along with his singing career. He offers Daisy a dilemma. Either the world has its singer, but Daisy loses him, or Daisy regains her Donald, but the world loses him. Posed with the question "her or the world", Daisy answers with a resounding and possessive scream of "Me, Me, Me". Soon Donald has returned to his old self and has forgotten about his career. His fans forget about him. But Daisy has regained her lover. This is considered a darkly humorous look at their relationship.

Donald would also face problems resulting from his own voice in "Donald's Dream Voice" (May 21, 1948). He works as a door-to-door salesman but his customers do not understand a word he is saying. His attempts at politeness are misinterpreted and customers react angrily to imagined insults. But Daisy convinces him otherwise "Don't give up! I have faith in you!" His problems seem to end when Donald buys a box of "voice pills", a medicine temporarily improving his voice. He gets confident enough in his new-found voice to prepare his marriage proposal for Daisy. But due to an accident he loses all but one of his pills. The rest of the short features his frustrated attempt to regain this last pill in order to propose to her. Something which he is eventually unable to do. After a few minutes of trying to get it, the pill ends up getting swallowed by a cow and makes it able to talk. And tells Donald he can't understand what he's saying. Donald then throws a tantrum.

Daisy would not appear again until "Crazy Over Daisy" (March 24, 1950). The short took place in an 1890s setting. At first, Donald seems in good mood and on his way for his date with Daisy. But when Chip 'n Dale start ridiculing his appearance the short results in one of their typical fights. Interrupted in the end by Daisy herself who accuses Donald of being cruel to the two "innocent" chipmunks. The short ends with Donald having to forget about that date. The short introduced Daisy's theme song "Crazy over Daisy," and in later appearances, Donald can be heard whistling the tune, such as in "Dude Duck" and "Out on a Limb."

Daisy's final appearance in the Golden Age of American animation was in "Donald's Diary" (March 5, 1954). There she played the role of a young lady who manages to start a long-term relationship with Donald. But after having a nightmare about the anxieties that would come from married life, Donald runs out on her and joins the French Foreign Legion. Several scenes of the short imply that Daisy has had several previous relationships with men. Donald carves their names on a tree. Not noticing than the opposing side of the tree features her name alongside that of several other boyfriends. The marriage scene in Donald's dream featured a group of sailors waving goodbye to Daisy and mourning the loss of their apparent lover. The story bore little continuity with the "real" Donald and Daisy as Huey, Dewey, and Louie appeared as Daisy's younger brothers. Even still, it was the only time in which Daisy's parents are seen.

Daisy appeared in "Mickey's Christmas Carol" in 1983, playing the character Isabelle, the neglected love interest of a young Ebenezer Scrooge, played by Scrooge McDuck. The film was Daisy's first theatrical appearance in almost 30 years and was also the first time she appeared apart from Donald. Although the nature of the film was that of Disney characters "playing" other characters and was not part of any story continuity. Daisy was voiced by Patricia Parris in the film.

In 1988 Daisy made a cameo appearance in "Who Framed Roger Rabbit" along with many other Disney characters.

Daisy's most recent theatrical appearance was "Fantasia 2000", released in late 1999. Like the original "Fantasia", the film constituted various musical segments. Donald and Daisy appeared in non-speaking roles for the seventh of eight segments, set to the "Pomp and Circumstance" marches. The segment is a retelling of Noah's Ark with the ducks acting as Noah's assistants. Donald and Daisy become separated in the chaos of the flood and each presumes the other to have drowned until they discover each other afterwards.

Daisy appeared in the direct-to-video films "Mickey's Once Upon a Christmas", "Mickey's Twice Upon a Christmas", and "".

According to the unofficial timeline of Don Rosa, Daisy was born in 1920. According to Rosa, Daisy is the sister of Donald's brother-in-law – Daisy's brother had married Donald's twin sister, Della Duck, and together, the two became the parents of Huey, Dewey, and Louie Duck. This is his explanation of why the triplets tend to call her "Aunt Daisy" while no such courtesy is given to Gladstone Gander for example. Don Rosa has said that he considers Donald and Daisy to be nonrelated and that Duck simply is the Duckburg universe equal to Smith, being a common surname.

Donna Duck served as a precursor for Daisy in both animation and comics. She first appeared in a one-page illustration titled "Don Donald" and published in "Good Housekeeping" #3701 (January 1937). The page was illustrated by Thomas "Tom" Wood (1870s – October 4, 1940) who was head of the Walt Disney Studios' publicity department from 1933 until his death. She went on to appear in the "Donald and Donna" comic strip published in "Mickey Mouse Weekly" from May 15 to August 21, 1937. The "Weekly" was a United Kingdom publication and the strip was illustrated at the time by William Arthur Ward. However, her co-starring role was brief.

Daisy made her first comics appearance on November 4, 1940. She was introduced as the new neighbor of Donald and his potential love interest. The Donald Duck comic strip was at the time scripted by Bob Karp and illustrated by Al Taliaferro. She was seemingly soft-spoken but had a fiery temper and Donald often found himself a victim to her rage. For example, one strip had Daisy waiting for Donald to carve their names and their love for each other on a tree, only to discover the male Duck had carved "Daisy loves Donald" with her name hardly visible and his name in prominent bold letters. Resulting in her breaking her "umbrella" on his head and dismissing him as a "conceited little pup".

Her first original comic book appearance was in the story "The Mighty Trapper" by Carl Barks, first published in "Walt Disney's Comics and Stories" #36 (September 1943). However, this was only a cameo when Huey, Dewey, and Louie ask her to lend them an old fur coat. Barks would not use the character again until "Donald Tames His Temper" (January 1946) when Daisy demands that Donald learns to manage his anger as a New Year's resolution. Donald has to agree but points early on that Daisy herself has the temper of a "wild-eyed wildcat".

Her next appearance by Barks in "Biceps Blues" (June 1946) introduced a key concept to their relationship. When Daisy seems impressed by a certain type of male, Donald is forced to emulate that type, no matter how unsuited Donald is for emulating it successfully. In this early case, Daisy envies her "old school chum" Susy Swan for dating a notable weightlifter. Donald at first protests that she seems too impressed by a "gorilla" just because the "muscle-bound buffalo" can lift 300 pounds. But when Daisy simply ignores him and daydreams about dating Hercules, Donald decides to start weightlifting. The rest of the story focuses on his ineptitude at exercising and the eventual efforts of Huey, Dewey, and Louie to cheer him up by various tricks pointing to Donald becoming stronger. But when Donald arranges a demonstration for Daisy, Susy, and her boyfriend, their tricks are not able to save him from ridicule. Daisy then chases Donald in anger (Donald, in turn, chases Huey, Dewey, and Louie in anger) while Susy boasts about her luck in men to her weightlifter boyfriend, who simply grunts and nods and fails to understand her words. Daisy failed to see that Susy's boyfriend is strong but otherwise not too gifted, whereas Donald is one who would go great lengths for her.

Daisy continued to make frequent appearances in stories by Barks but the next important one for her development was "Wintertime Wager" (January 1948). There she first attempts to act as the voice of reason between competing cousins Donald Duck and Gladstone Gander and in fact manages to prevent Donald losing his house to Gladstone because of a wager. This story established that both of them wanted to be in her good graces. Their next joined meeting in "Gladstone Returns" (August 1948) has Donald and Gladstone competing in raising enough money for her charity effort.

Their rivalry would only increase when "Donald's Love Letters" (December 1949) revealed that both cousins were romantically interested in Daisy. From then on many stories by both Barks and others would develop around this love triangle. Daisy in turns dates both of them but this fact does not prevent the two competing suitors from attempting to earn more of her affection or trying to embarrass each other in front of her. Daisy can be counted on to be making regular appearances alongside either of them for several years to come. Often it would appear as if Gladstone had the upper hand in winning Daisy due to his luck, only to find fate thwarts his plans, such as a contest where the man who hunts the most turkeys gets to have dinner with Daisy, who has won a beauty contest. Gladstone wins the turkey hunt but finds himself having dinner with an ugly woman who is the runner-up queen, as Daisy is incapacitated, and Donald is the one nursing her.

Similarly, Daisy's precursor Donna and Daisy herself were featured together as rivals for Donald's affection in a newspaper strip published on August 7, 1951. In her last appearance, on August 11, 1951, Donna had a fiancé, a caricature of Disney cartoonist Manuel Gonzales, establishing a distinction between her character and Daisy.

In the comics, Daisy is also a member of a local gossip group called the "Chit-Chat Society", which plays bridge and sponsors charity fund-raisers. The core membership includes Clarabelle Cow and Clara Cluck, though occasionally some other unnamed characters appear.

In later years, Carl Barks 'modernized' Daisy in two stories: 'The not-so-ancient mariner' and 'Hall of the mermaid queen'. In the first story, Daisy is wearing a lot of different wigs and outfits. Gladstone Gander is also seen wearing a wig and a new wardrobe in the story. In the second story, Daisy has short, curly hair and a bow that is much smaller than usual.

In the 1950s, Disney launched the series "Daisy Duck's Diary", where Daisy was given more of a leading role. This series, originally by such cartoonists as Dick Moores, Jack Bradbury, Tony Strobl and Carl Barks, have continued to the present day in Italy.
Since the early 1970s, Daisy has been featured as a crimefighter in Italian Disney comics. The character of "Super Daisy" (""Paperinika"" in Italian) was designed as a female counterpart to the "Duck Avenger" (""Paperinik"" in Italian). While the character of the Duck Avenger was originally created to place Donald into situations where he was finally a "winner" (versus his usual portrayal as a "loser"), when Super Daisy appeared in the same story as the Duck Avenger, she then became the "winner" and Donald was once more relegated to the role of "loser." This upset some children, who complained to the comics' editors, which resulted in the Italian comics ceasing to use Super Daisy, though the Brazil Disney comics continue to make use of Daisy's superhero alter ego.

As Super Daisy, Daisy has no superpowers but instead uses devices created by high society fashion designer Genialina Edy Son. Genialina personally designed Daisy's costume, as well as supplying her with crime-fighting gear such as sleeping pills and a James Bond-esque sports car. Very frequently, Super Daisy will both fight alongside and against the Duck Avenger. In the Brazilian stories, Super Daisy often teams up with other Disney comic superheroes, such as "Super Goof" (Goofy), "Super Gilly" ("Gilbert"), the "Red Bat" (Fethry Duck), etc.

While the Duck Avenger's main goal is enforcing justice in Duckburg, and proving himself better than his usual, unlucky self, Super Daisy acts mostly on an extreme, somewhat warped form of feminism, donning her alternate identity to prove that women are better than men at whatever they do, openly antagonizing the Duck Avenger to prove her point However, later stories, as the "Hero Club" inspired Italian story "Ultraheroes" show Super Daisy and the Duck Avenger at the center of a weird love triangle: Super Daisy, despite their bickering eventually warms to the Duck Avenger, feeling drawn to his righteous persona, as the Duck Avenger does. However, both of them feel unable to pursue their relationship, as they feel themselves cheating their respective lovers 

Since 1999 Daisy, like Donald Duck has her own magazine in the Netherlands. She had one in Brazil between 1986 and 1997, and a short-lived series in 2004 with republications of old stories.


At the Walt Disney Parks and Resorts and on the Disney Cruise Line ships, Daisy is a semi-common character for meet-and-greets, parades, and shows, though she doesn't make as many appearances as Donald or Minnie. Her semi-elusiveness has made her extra popular to an extent, adding to the fact that Daisy is a member of the Sensational Six, therefore making Daisy merchandise even more appealing to collectors. After Disney World expanded Fantasyland in 2012 Daisy became available for meet and greets at Pete's Silly Sideshow. At Epcot, she appears at the main entrance. She also appeared in a few restaurants such as the Tusker House, and Minnie's Springtime Dine at Hollywood & Vine.

Daisy appears in an MMORPG game called Toontown Online. She appears in the playground called Daisy Gardens. She is an NPC character that walks around leaving comments about passing Toons.
Curiously, Daisy never appeared on "DuckTales", although Donald's own appearances in that series were limited. In the 1996 television series "Quack Pack", Daisy was presented as a much more liberated (and patient) woman than in her previous appearances, where she was employed as a television station reporter, with Donald as her cameraman. In "Quack Pack", Daisy had a pet Iguana named Knuckles. Some consider Quack Pack to be a spin-off of "DuckTales", however, unlike "DuckTales", the world is not populated purely by animal beings, but also humans.

Daisy also has appeared in the later television series "Mickey Mouse Works", "Disney's House of Mouse" and "Mickey Mouse Clubhouse" as a regular character (all of which were on the Disney Channel). She is also a main character in the "Minnie's Bow-Toons" shorts and "Mickey and the Roadster Racers."

In the "Kingdom Hearts" video game series, appeared as a countess in Disney Castle. Her relations to Donald remain intact, especially in "Kingdom Hearts II" when she was seen scolding him. Goofy- speaking to Sora- refers to her as "Donald's very special sweetheart." She also appeared in the prequel "Kingdom Hearts Birth By Sleep" in the world Disney Town, alongside various other Disney characters. However, she does not have a speaking role in the prequel.

Daisy is a playable character in the video game "Disney Think Fast" and a playable race driver in the Nintendo 64 and Game Boy Color racing game "Mickey's Speedway USA".

In the Wii video game "Epic Mickey", a robot version of Daisy appears in the game, built by the Mad Doctor (before his betrayal) as Oswald wanted to replicate Mickey's friends. Her parts, after the Doctor's Beetleworx attacked her, were then scattered around Ventureland, and Mickey can find them or not. In the ending, depending on if she was restored or not, she is either seen restored and cuddling a tiki mask of Donald Duck, or still as a head in her glass case, harassed by a nearby pirate.

In the Wii video game "Dance Dance Revolution Disney Grooves", Daisy Duck appears as one of the random backup dancers if you are playing with 1 or 2 players.

Daisy runs the Daisy Gardens neighborhood in "Disney's Toontown Online".



</doc>
<doc id="9021" url="https://en.wikipedia.org/wiki?curid=9021" title="Dot-com bubble">
Dot-com bubble

The dot-com bubble (also known as the dot-com boom, the dot-com crash, the tech bubble, the Internet bubble, and the information technology bubble) was a historic economic bubble and period of excessive speculation that occurred roughly from 1995 to 2000, a period of extreme growth in the usage and adaptation of the Internet.

The Nasdaq Composite stock market index, which included many Internet-based companies, peaked in value on March 10, 2000 before crashing. The burst of the bubble lasted from March 11, 2000 to October 9, 2002. After the bubble burst, online retail companies, such as Pets.com, Webvan and Boo.com, failed completely and shut down along with communication companies, such as WorldCom, NorthPoint Communications and Global Crossing. Others, such as Cisco, whose stock declined by 86%, and Qualcomm, lost a large portion of their market capitalization but survived, and some companies, such as eBay and Amazon.com, declined in value but recovered quickly.

In 1993, the release of the Mosaic web browser made access to the World Wide Web easier. Internet usage increased as a result of the reduction of the "digital divide" and advances in connectivity, uses of the Internet, and computer education. Between 1990 and 1997, the percentage of households in the United States owning computers increased from 15% to 35% as computer ownership progressed from a luxury to a necessity. This marked the Information Age, the shift to an economy based on information technology, and many new companies were founded.

At the same time, low interest rates increased the availability of capital. The Taxpayer Relief Act of 1997, which lowered the top marginal capital gains tax in the United States, also made people more willing to make more speculative investments. Alan Greenspan, the former Chair of the Federal Reserve, allegedly fueled investments in the stock market by putting a positive spin on stock valuations. The Telecommunications Act of 1996 was expected to result in many new technologies and people wanted to profit.

As a result of these factors, many investors were eager to invest, at any valuation, in any dot-com company, especially if it had one of the Internet-related prefixes or a ".com" suffix in its name. Venture capital was easy to raise. Investment banks, which profited significantly from initial public offerings (IPO), fueled speculation and encouraged investment in technology. A combination of rapidly increasing stock prices in the quaternary sector of the economy and confidence that the companies would turn future profits created an environment in which many investors were willing to overlook traditional metrics, such as the price–earnings ratio, and base confidence on technological advancements, leading to a stock market bubble. Between 1995 and 2000, the Nasdaq Composite stock market index rose 400%. It reached a price–earnings ratio of 200, dwarfing the peak price–earnings ratio of 80 for the Japanese Nikkei 225 during the Japanese asset price bubble of 1991. In 1999, shares of Qualcomm rose in value by 2,619%, 12 other large-cap stocks each rose over 1,000% value, and 7 additional large-cap stocks each rose over 900% in value. Even though the Nasdaq Composite rose 85.6% and the S&P 500 Index rose 19.5% in 1999, more stocks fell in value than rose in value as investors sold stocks in slower growing companies to invest in Internet stocks.

An unprecedented amount of personal investing occurred during the boom and stories of people quitting their jobs to engage in full-time day trading were common. The news media took advantage of the public's desire to invest in the stock market; an article in "The Wall Street Journal" suggested that investors "re-think" the "quaint idea" of profits, and CNBC reported on the stock market with the same level of suspense as many networks provided to the broadcasting of sports events.

At the height of the boom, it was possible for a promising dot-com company to become a public company via an IPO and raise a substantial amount of money even if it had never made a profit—or, in some cases, realized any material revenue. People who received employee stock options became instant paper millionaires when their companies executed IPOs; however, most employees were barred from selling shares immediately due to lock-up periods. The most successful entrepreneurs, such as Mark Cuban, sold their shares or entered into hedges to protect their gains.

Most dot-com companies incurred net operating losses as they spent heavily on advertising and promotions to harness network effects to build market share or mind share as fast as possible, using the mottos "get big fast" and "get large or get lost". These companies offered their services or products for free or at a discount with the expectation that they could build enough brand awareness to charge profitable rates for their services in the future. In January 2000, there were 16 dot-com commercials during Super Bowl XXXIV, each costing $2 million for a 30-second spot.

The "growth over profits" mentality and the aura of "new economy" invincibility led some companies to engage in lavish spending on elaborate business facilities and luxury vacations for employees. Upon the launch of a new product or website, a company would organize an expensive event called a dot com party.

Telecommunications equipment providers, convinced that the future economy would require ubiquitous broadband access, went deeply into debt to improve their networks with high-speed equipment and fiber optic cables. In many areas, such as the Dulles Technology Corridor in Virginia, governments funded technology infrastructure and created favorable business and tax law to encourage companies to expand. In Europe, mobile phone companies overspent on 3G licences, which led them deep into debt. The investments in infrastructure were far out of proportion to cash flow. These were major factors that led to the telecoms crash.

Around the turn of the millennium, spending on technology was volatile as companies prepared for the Year 2000 problem, which, when the clocks changed to the year 2000, actually had minimal impact.

On January 10, 2000, America Online announced a merger with Time Warner, the largest to date and a move that was questioned by many analysts.

In February 2000, with the Year 2000 problem no longer a worry, Alan Greenspan announced plans to aggressively raise interest rates, which led to significant stock market volatility as analysts disagreed as to whether or not technology companies would be affected by higher borrowing costs.

On March 10, 2000, the NASDAQ Composite stock market index peaked at 5,048.62.

On March 13, 2000, news that Japan had once again entered a recession triggered a global sell off that disproportionately affected technology stocks.

On March 15, 2000, Yahoo! and eBay ended merger talks and the Nasdaq fell 2.6% but the S&P 500 Index rose 2.4% as investors shifted from strong performing technology stocks to poor performing established stocks.

On March 20, 2000, "Barron's" featured a cover article titled "Burning Up; Warning: Internet companies are running out of cash -- fast", which predicted the imminent bankruptcy of many internet companies. This led many people to rethink their investments. That same day, Microstrategy announced a revenue restatement due to aggressive accounting practices. Its stock price, which had risen from $7 per share to as high as $333 per share in a year, fell $120 per share, or 62%, in a day. The next day, the Federal Reserve raised interest rates, leading to an inverted yield curve, although stocks rallied temporarily.

On April 3, 2000, judge Thomas Penfield Jackson issued his "conclusions of law" in the case of United States v. Microsoft Corp. (2001) and ruled that Microsoft was guilty of monopolization and tying in violation of the Sherman Antitrust Act. This led to a one-day 15% decline in the value of shares in Microsoft and a 350-point, or 8%, drop in the value of the Nasdaq. Many people saw the legal actions as bad for technology in general. That same day, Bloomberg published a widely-read article that stated: "It's time, at last, to pay attention to the numbers".

On Friday, April 14, 2000, the Nasdaq Composite index fell 9%, ending a week in which it fell 25%. Investors were forced to sell stocks ahead of Tax Day, the due date to pay taxes on gains realized in the previous year.

By June 2000, dot-com companies were forced to rethink their advertising campaigns.

On November 9, 2000, Pets.com, a much hyped company that had backing from Amazon.com, went out of business only 9 months after completing its IPO. At that time, most internet stocks had declined in value by 75% from their highs, wiping out $1.755 trillion in value.

In January 2001, just 3 dot-com companies bought advertising spots during Super Bowl XXXV: E-Trade, Monster.com, and Yahoo! HotJobs. The September 11 attacks accelerated the stock market drop later that year.

Several accounting scandals and the resulting bankruptcies, including the Enron scandal in October 2001, the Worldcom scandal in June 2002, and the Adelphia Communications Corporation scandal in July 2002 further eroded investor confidence.

By the end of the stock market downturn of 2002, stocks had lost $5 trillion in market capitalization since the peak. At its trough on October 9, 2002, the NASDAQ-100 had dropped to 1,114, down 78% from its peak.

After venture capital was no longer available, the operational mentality of executives and investors completely changed. A dot-com company's lifespan was measured by its burn rate, the rate at which it spent its existing capital. Many dot-com companies ran out of capital and went through liquidation. Supporting industries, such as advertising and shipping, scaled back their operations as demand for services fell. However, many companies were able to endure the crash; 48% of dot-com companies survived through 2004, albeit at lower valuations.

Several companies and their executives were accused or convicted of fraud for misusing shareholders' money, and the U.S. Securities and Exchange Commission levied large fines against investment firms including Citigroup and Merrill Lynch for misleading investors.

After suffering losses, retail investors transitioned their investment portfolios to more cautious positions.

Layoffs of programmers resulted in a general glut in the job market. University enrollment for computer-related degrees dropped noticeably. Anecdotes of unemployed programmers going back to school to become accountants or lawyers were common.

Failed startups liquidated all of their computer equipment and office equipment such as Herman Miller Aeron chairs.

As growth in the information technology sector stabilized, companies consolidated, some, such as Amazon.com, eBay, and Google gained market share and came to dominate their respective fields. The information technology industry came to more closely resemble other traditional sectors of the economy, albeit with still a faster growth rate and higher valuations than other sectors. There are now many information technology companies ranked at the top of the Fortune 500.

In a 2015 book, venture capitalist Fred Wilson, who funded dot-com companies and lost 90% of his net worth when the bubble burst, said about the dot-com bubble: 










</doc>
<doc id="9023" url="https://en.wikipedia.org/wiki?curid=9023" title="Discounted cash flow">
Discounted cash flow

In finance, discounted cash flow (DCF) analysis is a method of valuing a project, company, or asset using the concepts of the time value of money. All future cash flows are estimated and discounted by using cost of capital to give their present values (PVs). The sum of all future cash flows, both incoming and outgoing, is the net present value (NPV), which is taken as the value of the cash flows in question.

Using DCF analysis to compute the NPV takes as input cash flows and a discount rate and gives as output a present value; the opposite process—takes cash flows and a price (present value) as inputs, and provides as output the discount rate—this is used in bond markets to obtain the yield.

Discounted cash flow analysis is widely used in investment finance, real estate development, corporate financial management and patent valuation. It was used in industry as early as the 1700s or 1800s, widely discussed in financial economics in the 1960s, and became widely used in U.S. Courts in the 1980s and 1990s.

The most widely used method of discounting is exponential discounting, which values future cash flows as "how much money would have to be invested currently, at a given rate of return, to yield the cash flow in future." Other methods of discounting, such as hyperbolic discounting, are studied in academia and said to reflect intuitive decision-making, but are not generally used in industry.

The discount rate used is generally the appropriate weighted average cost of capital (WACC), that reflects the risk of the cash flows. This WACC can be found using Perry's calculation model which was developed in 1996. The discount rate reflects two things:


Discounted cash flow calculations have been used in some form since money was first lent at interest in ancient times. Studies of ancient Egyptian and Babylonian mathematics suggest that they used techniques similar to discounting of the future cash flows. This method of asset valuation differentiated between the accounting book value, which is based on the amount paid for the asset. Following the stock market crash of 1929, discounted cash flow analysis gained popularity as a valuation method for stocks. Irving Fisher in his 1930 book "The Theory of Interest" and John Burr Williams's 1938 text "The Theory of Investment Value" first formally expressed the DCF method in modern economic terms.

The discounted cash flow formula is derived from the future value formula for calculating the time value of money and compounding returns.

Thus the discounted present value (for one cash flow in one future period) is expressed as:

where

Where multiple cash flows in multiple time periods are discounted, it is necessary to sum them as follows:

for each future cash flow ("FV") at any time period ("t") in years from the present time, summed over all time periods. The sum can then be used as a net present value figure. If the amount to be paid at time 0 (now) for all the future cash flows is known, then that amount can be substituted for "DPV" and the equation can be solved for "r", that is the internal rate of return.

All the above assumes that the interest rate remains constant throughout the whole period.

If the cash flow stream is assumed to continue indefinitely, the finite forecast is usually combined with the assumption of constant cash flow growth beyond the discrete projection period. The total value of such cash flow stream is the sum of the finite discounted cash flow forecast and the Terminal value (finance).

For continuous cash flows, the summation in the above formula is replaced by an integration:

where formula_6 is now the "rate" of cash flow, and formula_7.

To show how discounted cash flow analysis is performed, consider the following example.

Simple subtraction suggests that the value of his profit on such a transaction would be $150,000 − $100,000 = $50,000, or 50%. If that $50,000 is amortized over the three years, his implied annual return (known as the internal rate of return) would be about 14.5%. Looking at those figures, he might be justified in thinking that the purchase looked like a good idea.

1.145 x $100,000 = $150,000, approximately.

However, since three years have passed between the purchase and the sale, any cash flow from the sale must be discounted accordingly. At the time John Doe buys the house, the 3-year US Treasury Note rate is 5% per annum. Treasury Notes are generally considered to be inherently less risky than real estate, since the value of the Note is guaranteed by the US Government and there is a liquid market for the purchase and sale of T-Notes. If he hadn't put his money into buying the house, he could have invested it in the relatively safe T-Notes instead. This 5% per annum can therefore be regarded as the risk-free interest rate for the relevant period (3 years).

Using the DPV formula above (FV=$150,000, i=0.05, n=3), that means that the value of $150,000 received in three years actually has a present value of $129,576 (rounded off). In other words, we would need to invest $129,576 in a T-Bond now to get $150,000 in 3 years almost risk free. This is a quantitative way of showing that money in the future is not as valuable as money in the present ($150,000 in 3 years isn't worth the same as $150,000 now; it is worth $129,576 now).

Subtracting the purchase price of the house ($100,000) from the present value results in the net present value of the whole transaction, which would be $29,576 or a little more than 29% of the purchase price.

Another way of looking at the deal as the excess return achieved (over the risk-free rate) is (114.5 - 105)/(100 + 5) or approximately 9.0% (still very respectable).

But what about risk?

We assume that the $150,000 is John's best estimate of the sale price that he will be able to achieve in 3 years time (after deducting all expenses). There is a lot of uncertainty about house prices, and the outcome may end up higher or lower than this estimate.

Under normal circumstances, people entering into such transactions are risk-averse, that is to say that they are prepared to accept a lower expected return for the sake of avoiding risk. See Capital asset pricing model for a further discussion of this. For the sake of the example (and this is a gross simplification), let's assume that he values this particular risk at 5% per annum (we could perform a more precise probabilistic analysis of the risk, but that is beyond the scope of this article). Therefore, allowing for this risk, his expected return is now 9.0% per annum (the arithmetic is the same as above).

And the excess return over the risk-free rate is now (109 - 105)/(100 + 5) which comes to approximately 3.8% per annum.

That return rate may seem low, but it is still positive after all of our discounting, suggesting that the investment decision is probably a good one: it produces enough profit to compensate for tying up capital and incurring risk with a little extra left over. When investors and managers perform DCF analysis, the important thing is that the net present value of the decision after discounting all future cash flows at least be positive (more than zero). If it is negative, that means that the investment decision would actually "lose" money even if it appears to generate a nominal profit. For instance, if the expected sale price of John Doe's house in the example above was not $150,000 in three years, but "$130,000" in three years or $150,000 in "five" years, then on the above assumptions buying the house would actually cause John to "lose" money in present-value terms (about $3,000 in the first case, and about $8,000 in the second). Similarly, if the house was located in an undesirable neighborhood and the Federal Reserve Bank was about to raise interest rates by five percentage points, then the risk factor would be a lot higher than 5%: it might not be possible for him to predict a profit in discounted terms even if he thinks he could sell the house for "$200,000" in three years.

In this example, only one future cash flow was considered. For a decision which generates multiple cash flows in multiple time periods, all the cash flows must be discounted and then summed into a single net present value.

This is offered as a simple treatment of a complex subject. More detail is beyond the scope of this article.

For these valuation purposes, a number of different DCF methods are distinguished today, some of which are outlined below. The details are likely to vary depending on the capital structure of the company. However the assumptions used in the appraisal (especially the equity discount rate and the projection of the cash flows to be achieved) are likely to be at least as important as the precise model used.

Both the income stream selected and the associated cost of capital model determine the valuation result obtained with each method.
This is one reason these valuation methods are formally referred to as the Discounted Future Economic Income methods.



Commercial banks have widely used discounted cash flow as a method of valuing commercial real estate construction projects. This practice has two substantial shortcomings. 1) The discount rate assumption relies on the market for competing investments at the time of the analysis, which would likely change, perhaps dramatically, over time, and 2) straight line assumptions about income increasing over ten years are generally based upon historic increases in market rent but never factors in the cyclical nature of many real estate markets. Most loans are made during boom real estate markets and these markets usually last fewer than ten years. Using DCF to analyze commercial real estate during any but the early years of a boom market will lead to overvaluation of the asset.

Discounted cash flow models are powerful, but they do have shortcomings. DCF is merely a mechanical valuation tool, which makes it subject to the principle "garbage in, garbage out". Small changes in inputs can result in large changes in the value of a company. Instead of trying to project the cash flows to infinity, terminal value techniques are often used. A simple perpetuity is used to estimate the terminal value past 10 years, for example. This is done because it is harder to come to a realistic estimate of the cash flows as time goes on involves calculating the period of time likely to recoup the initial outlay.

Another shortcoming is the fact that the Discounted Cash Flow Valuation should only be used as a method of intrinsic valuation for companies with predictable, though not necessarily stable, cash flows. The Discounted Cash Flow valuation method is widely used in valuing mature companies in stable industry sectors such as Utilities. At the same time, this method is often applied to valuation of high growth technology companies. In valuing young companies without much cash flow track record, the Discounted Cash Flow method may be applied a number of times to assess a number of possible future outcomes, such as the best, worst and mostly likely case scenarios.




</doc>
<doc id="9025" url="https://en.wikipedia.org/wiki?curid=9025" title="Lists of deities">
Lists of deities

This is an index of lists of deities of the different religions, cultures and mythologies of the world.




</doc>
<doc id="9030" url="https://en.wikipedia.org/wiki?curid=9030" title="Dachau">
Dachau

Dachau () is a town in Upper Bavaria, in the southern part of Germany. It is a major district town—a "Große Kreisstadt"—of the administrative region of Upper Bavaria, about north-west of Munich. It is now a popular residential area for people working in Munich with roughly 45,000 inhabitants. The historic centre of town with its 18th-century castle is situated on an elevation and visible over a great distance.

Dachau was founded in the 9th century. It was home to many artists during the late 19th and early 20th centuries; well-known author and editor Ludwig Thoma lived here for two years. The town is also known for its proximity to the infamous Dachau concentration camp built in 1933 by the Nazis, in which tens of thousands of prisoners died.

The origin of the name is not known, it possibly originated with the Celts who lived there before the Germans came. An alternative idea is that it comes from the old high German word daha meaning clay, and ouwe, water overflown land.

As the Amper River would divert into backwaters in several places, there were many fords making it possible to cross the river. The oldest findings of human presence here date back to the Stone Age. The most noteworthy findings were discovered near Feldgeding in the adjoining municipality Bergkirchen.
Around 1000 B.C. the Celts arrived in this area and settled. The name “Dachau” originated in the Celtic "Dahauua", which roughly translates to “loamy meadow” and also alludes to the loamy soil of the surrounding hills. Some theories assume the name “Amper” river may derive from the Celtic word for “water”. 
Approximately at the turn of the first millennium the Romans conquered the area and incorporated it into the province of Rhaetia. A Roman trade road between Salzburg and today’s Augsburg is said to have run through Dachau. Remains of this old route are found along the Amper marshlands.

The first known documentation of Dachau occurs in a medieval deed issued by the Noble Erchana of Dahauua to the prince-bishop of Freising, both descendants of the lineage of the Aribonids. With this deed, dated to August 15, 805 A.D. ("the Feast of the Assumption of the Blessed Virgin Mary"), she donated her entire property in Dachau, including five so-called "Colonenhöfe" and some serfs and bondsman, to devolve to the Bishop of the Diocese of Freising after her death.

During much of the 12th century, Dachau was the primary residence of a smaller branch from the House of Wittelsbach led by Otto I, Count of Scheyern-Dauchau. When Conrad III died in 1182, Duke Otto I of Bavaria purchased the land and granted it market rights, that were then affirmed between 1270 and 1280 by Duke Ludwig II der Strenge (the Strict).

In 1467 Sigismund, Duke of Bavaria resigned and then kept only Bavaria-Dachau as his domain until his death in 1501.

Between 1546 and 1577, the House of Wittelsbach had the Dachau Palace erected in the Renaissance style. From June 1715 to Autumn 1717, Joseph Effner remodeled the palace to suit the contemporary taste in style.

At the beginning of the 19th century, the castle's north-, east- and south-wing had to be demolished due to their state of disrepair. The west-wing housing the dance hall with a superb view of the enchanting gardens, still remains today. On the first floor the original renaissance wood carved, coffered ceiling can be admired by visitors.

During the second half of the 19th century, the town began to attract landscape artists. The Dachau art colony, which flourished between 1890 and 1914, brought the town recognition as one of the most important artist's colonies in Germany beside Worpswede.

In 1933 the Dachau concentration camp was built east of the city by the Nazis and operated until 1945. It was the first of what became many camps. 14,100 prisoners were murdered in the camp and almost another 10,000 in its subcamps.

Dachau is northwest of Munich. It is 482 meters above sea level by the river Amper, with a boundary demarcated by lateral moraines formed during the last ice age and the Amper glacial valley. It is also close to a large marshy area called Dachauer Moos. Highest elevation of the district is the so-called "Schlossberg", the lowest point is near the neighborhood of Prittlbach, at the border to the next community of Hebertshausen. The bordering communities are Bergkirchen to the west, Schwabhausen to the northwest, Röhrmoos to the north, Hebertshausen to the northeast, and Karlsfeld to the south. To the east the greater district Dachau borders on the greater district of Munich with the community of Oberschleißheim.

The city is divided into 3 zones:


Since 1972 the former communities of Pellheim with Pullhausen, Assenhausen, Lohfeld, and Viehgarten have been incorporated into Dachau.

Running from the west, the river Amper runs south of Dachau’s old town, changes its direction at the former paper milling plant to the northeast and continues through Prittlbach into Hebertshausen.

Coming from Karlsfeld, the Würm crosses Dachau-East and merges into the river Amper just outside the district limit of Hebertshausen.

The Gröbenbach, which has its source south of Puchheim, runs through town coming from the south and merges into the Amper river at several locations near the festival grounds.

The Mühlbach, a man made canal, is diverted from the river Amper at the electrical power plant and runs parallel and flows back into it after passing the paper mill. The name derives from the frequent mills in former times along the canal which took advantage of the decline between Mühlbach and Amper. West of the so-called Festwiese runs another canal, called Lodererbach.

In town there are still parts of the Schleißheimer canal remaining today. This canal was built in the mid-eighteenth century as part of the northern Munich canal system to which the Nymphenburger Canal belongs as well. 
It functioned as a transportation route between Dachau and Schleißheim. The building material recovered from the demolition of three wings of the Dachau castle was transported to Schleißheim this way.

By allowing it to run to seed and through deliberate cultivation by the town of Dachau the canal is only still recognizable as such between Frühlingstrasse and the Pollnbach. Outside the city limit the original canal continues on to Schloss Schleißheim.

Within the city boundaries, in Dachau Süd (South), there is also a small lake called Stadtweiher.

The city is served by Munich S-Bahn (S2) and Deutsche Bahn via Dachau railway station located in the South of the town. The station is also annexed to the central bus terminal. In Dachau the line S2 is split in two directions: Petershausen and Altomünster. Both lines are named S2 but with different direction names. The offshoot to Altomünster is also served by Dachau Stadt Railway Station which is much smaller than the main railway station. There are five bus lines which are operated by Stadtwerke Dachau: 719, 720, 722, 724 and 726. There is no tramway transport.

Dachau has a well-developed road infrastructure for regional transportation. The city is connected to Bundesautobahn 8 (via Fürstenfeldbruck) with Munich-Pasing southbound, and westbound terminating in Karlsruhe. Dachau is connected to Bundesautobahn 92 via Oberschleißheim connector which is located east of Dachau. Bundesautobahn 99 is connected with Dachau via Karlsfeld which is located south of Dachau. Bundesstraße No. 471 (via Rothschwaige) connects eastbound towns such as the neighboring city Fürstenfeldbruck and westbound towns such as Oberschleißheim. Bundesstraße No. 304 starts in the south of the city and connects southbound towns until the German-Austrian border. Additionally, several Staatsstraßen connect Dachau with surrounding towns and villages.


 City of Dachau

Dachau is twinned with:

There exists also some cooperation with:

Notable people who lived, worked or were born in Dachau include



</doc>
<doc id="9032" url="https://en.wikipedia.org/wiki?curid=9032" title="Drosophila">
Drosophila

Drosophila () is a genus of flies, belonging to the family Drosophilidae, whose members are often called "small fruit flies" or (less frequently) pomace flies, vinegar flies, or wine flies, a reference to the characteristic of many species to linger around overripe or rotting fruit. They should not be confused with the Tephritidae, a related family, which are also called fruit flies (sometimes referred to as "true fruit flies"); tephritids feed primarily on unripe or ripe fruit, with many species being regarded as destructive agricultural pests, especially the Mediterranean fruit fly. One species of "Drosophila" in particular, "D. melanogaster", has been heavily used in research in genetics and is a common model organism in developmental biology. The terms "fruit fly" and ""Drosophila"" are often used synonymously with "D. melanogaster" in modern biological literature. The entire genus, however, contains more than 1,500 species and is very diverse in appearance, behavior, and breeding habitat.

The term ""Drosophila"", meaning "dew-loving", is a modern scientific Latin adaptation from Greek words , ', "dew", and , ', "loving" with the Latin feminine suffix "-a".

"Drosophila" species are small flies, typically pale yellow to reddish brown to black, with red eyes. Many species, including the noted Hawaiian picture-wings, have distinct black patterns on the wings. The plumose (feathery) arista, bristling of the head and thorax, and wing venation are characters used to diagnose the family. Most are small, about 2–4 mm long, but some, especially many of the Hawaiian species, are larger than a house fly.

"Drosophila" species are found all around the world, with more species in the tropical regions. "Drosophila" made their way to the Hawaiian Islands and radiated into over 800 species. They can be found in deserts, tropical rainforest, cities, swamps, and alpine zones. Some northern species hibernate. Most species breed in various kinds of decaying plant and fungal material, including fruit, bark, slime fluxes, flowers, and mushrooms. The larvae of at least one species, "D. suzukii", can also feed in fresh fruit and can sometimes be a pest. A few species have switched to being parasites or predators. Many species can be attracted to baits of fermented bananas or mushrooms, but others are not attracted to any kind of baits. Males may congregate at patches of suitable breeding substrate to compete for the females, or form leks, conducting courtship in an area separate from breeding sites.

Several "Drosophila" species, including "D. melanogaster", "D. immigrans", and "D. simulans", are closely associated with humans, and are often referred to as domestic species. These and other species ("D. subobscura", "Zaprionus indianus") have been accidentally introduced around the world by human activities such as fruit transports.
Males of this genus are known to have the longest sperm cells of any studied organism on Earth, including one species, "Drosophila bifurca", that has sperm cells that are long. The cells are mostly tail, and are delivered to the females in tangled coils. The other members of the genus "Drosophila" also make relatively few giant sperm cells, with that of "D. bifurca" being the longest. "D. melanogaster" sperm cells are a more modest 1.8 mm long, although this is still about 35 times longer than a human sperm. Several species in the "D. melanogaster" species group are known to mate by traumatic insemination.

"Drosophila" species vary widely in their reproductive capacity. Those such as "D. melanogaster" that breed in large, relatively rare resources have ovaries that mature 10–20 eggs at a time, so that they can be laid together on one site. Others that breed in more-abundant but less nutritious substrates, such as leaves, may only lay one egg per day. The eggs have one or more respiratory filaments near the anterior end; the tips of these extend above the surface and allow oxygen to reach the embryo. Larvae feed not on the vegetable matter itself, but on the yeasts and microorganisms present on the decaying breeding substrate. Development time varies widely between species (between 7 and more than 60 days) and depends on the environmental factors such as temperature, breeding substrate, and crowding. 
Fruit flies lay eggs in response to environmental cycles. Eggs laid at a time (e.g., night) during which likelihood of survival is greater than in eggs laid at other times (e.g., day) yield more larvae than eggs that were laid at those times. "Ceterus paribus", the habit of laying eggs at this 'advantageous' time would yield more surviving offspring, and more grandchildren, than the habit of laying eggs during other times. This differential reproductive success would cause "D. melanogaster" to adapt to environmental cycles, because this behavior has a major reproductive advantage.

Their median lifespan is 35–45 days.

The following section is based on the following "Drosophila" species: "Drosophila simulans", and "Drosophila melanogaster". 
Courtship behavior of male "Drosophila" is an attractive behaviour. Females respond via their perception of the behavior portrayed by the male. Male and female "Drosophila" use innate complex behaviors use a variety of sensory cues to initiate and assess courtship readiness of a potential mate. The cues include the following behaviours: positioning, pheromone excretion, following females, making tapping sounds with legs, singing, wing spreading, creating wing vibrations, genitalia licking, bending the stomach, attempt to copulate, and the copulatory act itself. The songs of "Drosophila melanogaster" and "Drosophila simulans" have been studied extensively. These luring songs are sinusoidal in nature and varies within and between species. 
The courtship behavior of "Drosophila melanogaster" has also been assessed for sex-related genes, which have been implicated in courtship behavior in both the male and female. Recent experiments explore the role of fruitless ("fru") and doublesex ("dsx"), a group of sex-behaviour linked genes. This research is currently being explored.

The following section is based on the following "Drosophila" species: "Drosophila serrata", "Drosophila pseudoobscura", "Drosophila melanogaster", and "Drosophila neotestacea". Polyandry is a prominent mating system among "Drosophila". Females mating with multiple sex partners has been a beneficial mating strategy for "Drosophila". The benefits include both pre and post copulatory mating. Pre-copulatory strategies are the behaviours associated with mate choice and the genetic contributions, such as production of gametes, that are exhibited by both male and female "Drosophila" regarding mate choice. Post copulatory strategies include sperm competition, mating frequency, and sex-ratio meiotic drive. These lists are not inclusive.
Polyandry among the "Drosophila pseudoobscura" in North America vary in their number of mating partners. There is a connection between the number of time females choose to mate and chromosomal variants of the third chromosome. It is believed that the presence of the inverted polymorphism is why re-mating by females occurs. The stability of these polymorphisms may be related to the sex-ratio meiotic drive.

The following section is based on the following "Drosophila" species: "Drosophila melanogaster", "Drosophila simulans", and "Drosophila mauritiana". Sperm competition is a process that polyandrous "Drosophila" females use to increase the fitness of their offspring. The female "Drosophila" has two sperm storage organs that allows her to choose the sperm that will be used to inseminate her eggs. Females have little control when it comes to cryptic female choice. Female "Drosophila" through cryptic choice, one of several post-copulatory mechanisms, which allows for the detection and expelling of sperm that reduces inbreeding possibilities. Manier et al. 2013 has categorized the post copulatory sexual selection of "Drosophila melanogaster", "Drosophila simulans", and "Drosophila mauritiana" into the following three stages: insemination, sperm storage, and fertilizable sperm. Among the preceding species there are variations at each stage that play a role in the natural selection process.

"D. melanogaster" is a popular experimental animal because it is easily cultured en masse out of the wild, has a short generation time, and mutant animals are readily obtainable. In 1906, Thomas Hunt Morgan began his work on "D. melanogaster" and reported his first finding of a white eyed mutant in 1910 to the academic community. He was in search of a model organism to study genetic heredity and required a species that could randomly acquire genetic mutation that would visibly manifest as morphological changes in the adult animal. His work on "Drosophila" earned him the 1933 Nobel Prize in Medicine for identifying chromosomes as the vector of inheritance for genes. This and other "Drosophila" species are widely used in studies of genetics, embryogenesis, chronobiology, speciation, and other areas.

However, some species of "Drosophila" are difficult to culture in the laboratory, often because they breed on a single specific host in the wild. For some, it can be done with particular recipes for rearing media, or by introducing chemicals such as sterols that are found in the natural host; for others, it is (so far) impossible. In some cases, the larvae can develop on normal "Drosophila" lab medium, but the female will not lay eggs; for these it is often simply a matter of putting in a small piece of the natural host to receive the eggs. The "Drosophila" Species Stock Center, located at Cornell University in Ithaca, New York, maintains cultures of hundreds of species for researchers.

Like other metazoans, "Drosophila" is associated with various bacteria in its gut. The fly gut microbiota or microbiome seems to have a central influence on "Drosophila" fitness and life history characteristics. The microbiota in the gut of "Drosophila" represents an active current research field.

"Drosophila" species are prey for many generalist predators such as robber flies. In Hawaii, the introduction of yellowjackets from the mainland United States has led to the decline of many of the large species. The larvae are preyed on by other fly larvae, staphylinid beetles, and ants.

The genus "Drosophila" as currently defined is paraphyletic (see below) and contains 1,450 described species, while the total number of species is estimated at thousands. The majority of the species are members of two subgenera: "Drosophila" (about 1,100 species) and "Sophophora" (including "D. (S.) melanogaster"; around 330 species). The Hawaiian species of "Drosophila" (estimated to be more than 500, with roughly 380 species described) are sometimes recognized as a separate genus or subgenus, "Idiomyia", but this is not widely accepted. About 250 species are part of the genus "Scaptomyza", which arose from the Hawaiian "Drosophila" and later recolonized continental areas.

Evidence from phylogenetic studies suggests these genera arose from within the genus "Drosophila":

Several of the subgeneric and generic names are based on anagrams of "Drosophila", including "Dorsilopha", "Lordiphosa", "Siphlodora", "Phloridosa", and "Psilodorha".

"Drosophila" species are extensively used as model organisms in genetics (including population genetics), cell biology, biochemistry, and especially developmental biology. Therefore, extensive efforts are made to sequence drosphilid genomes. The genomes of these species have been fully sequenced:

The data have been used for many purposes, including evolutionary genome comparisons. "D. simulans" and "D. sechellia" are sister species, and provide viable offspring when crossed, while "D. melanogaster" and "D. simulans" produce infertile hybrid offspring. The "Drosophila" genome is often compared with the genomes of more distantly related species such as the honeybee "Apis mellifera" or the mosquito "Anopheles gambiae".

The modEncode consortium is currently sequencing eight more "Drosophila" genomes, and even more genomes are being sequenced by the i5K consortium.

Curated data are available at FlyBase.




</doc>
<doc id="9033" url="https://en.wikipedia.org/wiki?curid=9033" title="Dictatorship">
Dictatorship

A dictatorship is an authoritarian form of government, characterized by a single leader or group of leaders with either no party or a weak party, little mass mobilization, and limited political pluralism. According to other definitions, democracies are regimes in which "those who govern are selected through contested elections"; therefore dictatorships are "not democracies". With the advent of the 19th and 20th centuries, dictatorships and constitutional democracies emerged as the world's two major forms of government, gradually eliminating monarchies, one of the traditional widespread form of government of the time. Typically, in a dictatorial regime, the leader of the country is identified with the title of dictator. A common aspect that characterized dictators, is to take advantage of their strong personality, usually by suppressing freedom of thought and speech of the masses, in order to maintain political and social supremacy and stability. Dictatorship and totalitarian societies generally employ political propaganda to decrease the influence of proponents of alternative governing systems.

The word "dictator" comes from the classical Latin language word "dictātor", agent noun from "dictare (dictāt-" , past participial stem of "dictāre" dictate "v." + "-or" -or "suffix".) In Latin use, a "dictator" was a judge in the Roman republic temporarily invested with absolute power.

Right after the end of World War II, with a more relaxed political and social climate, several studies regarding the classification of various forms of government have been conducted. Among these, has been intensely discussed by historians and political scientists the conceptualization and definition of the dictatorship form of government. Eventually, it has been concluded that dictatorship is a form of government in which the absolute power is concentrated in the hands of a leader (commonly identified as a dictator) or a "small clique" or "government organization", and it aims the abolition of political pluralism and civilian mobilization. On the other hand, democracy, which is generally compared to the concept of dictatorship, is defined as a form of government where the supremacy belongs to the population and rulers are elected through contested elections.

A new form of government that in the 20th century has marked the beginning of a new political era and is commonly linked to the concept of dictatorship, is totalitarianism. This form of government is characterized by the presence of a single political party and more specifically, by a powerful leader (a real role model) who imposes his personal and political prominence. The two fundamental aspects that contribute to the maintenance of the power are: a steadfast collaboration between the government and the police force, and a highly developed ideology. Here, the government has "total control of mass communications and social and economic organizations". According to Hannah Arendt, totalitarianism is a new and extreme form of dictatorship composed of "atomized, isolated individuals". In addition, she affirmed that ideology plays a leading role in defining how the entire society should be organized. According to the political scientist Juan Linz, the distinction between an authoritarian regime and a totalitarian one is that while an authoritarian regime seeks to suffocate politics and political mobilization, totalitarianism seeks to control politics and political mobilization.

However, one of the most recent classification of dictatorships, formulated, do not identify Totalitarianism as a form of dictatorship. In her study, she focused in how elite-leader and elite-mass relations influence authoritarian politics. Geddes typology identifies the key institutions that structure elite politics in dictatorships (i.e. parties and militaries). The study is based and directly related to factors like: the simplicity of the categorizations, cross-national applicability, the emphasis on elites and leaders, and the incorporation of institutions (parties and militaries) as central to shaping politics. According to Barbara Geddes, a dictatorial government may be classified in five typologies: Military Dictatorships, Single-party Dictatorships, Personalist Dictatorships, Monarchies, Hybrid Dictatorships.

Military dictatorships are regimes in which a group of officers holds power, determines who will lead the country, and exercises influence over policy. High-level elites and a leader are the members of the military dictatorship. Military dictatorships are characterized by rule by a professionalized military as an institution. In military regimes, elites are referred to as junta members; they are typically senior officers (and often other high-level officers) in the military.

Single-party dictatorships are regimes in which one party dominates politics. In single-party dictatorships, a single party has access to political posts and control over policy. Other parties may legally exist, compete in elections, and even hold legislative seats, yet true political power lies with the dominant party. In single-party dictatorships, party elites are typically members of the ruling body of the party, sometimes called the central committee, or politburo. This group of individuals controls the selection of party officials and “organizes the distribution of benefits to supporters and mobilizes citizens to vote and show support for party leaders".

Personalist dictatorships are regimes in which all power lies in the hands of a single individual. Personalist dictatorships differ from other forms of dictatorships in their access to key political positions, other fruits of office, and depend much more on the discretion of the personalist dictator. Personalist dictators may be members of the military or leaders of a political party. Yet, neither the military nor the party exercises power independent from the dictator. In personalist dictatorships, the elite corps is usually made up of close friends or family members of the dictator. These individuals are all typically handpicked to serve their posts by the dictator.

Monarchic dictatorships are regimes in which "a person of royal descent has inherited the position of head of state in accordance with accepted practice or constitution". Regimes are not considered dictatorships if the monarch’s role is largely ceremonial. Real political power must be exercised by the monarch for regimes to be classified as such. Elites in monarchies are typically members of the royal family.

Hybrid dictatorships are regimes that blend qualities of personalist, singleparty, and military dictatorships. When regimes share characteristics of all three forms of dictatorships, they are referred to as triple threats. The most common forms of hybrid dictatorships are personalist/single-party hybrids and personalist/military hybrids.

One of the tasks in political science is to measure and classify regimes as either dictatorships or democracies. Freedom House, Polity IV and Democracy-Dictatorship Index are three of the most used data series by political scientists.

Generally, two research approaches exist: the minimalist approach, which focuses on whether a country has continued elections that are competitive, and the substantive approach, which expands the concept of democracy to include human rights, freedom of the press, and the rule of law. The Democracy-Dictatorship Index is seen as an example of the minimalist approach, whereas the Polity data series, is more substantive.

Between the two world wars, four types of dictatorships have been described: Constitutional, Communist (nominally championing the "dictatorship of the proletariat"), Counterrevolutionary and Fascist. Since World War II, a broader range of dictatorships has been recognized, including Third World dictatorships, theocratic or religious dictatorships and dynastic or family-based dictatorships.

During the Republican phase of Ancient Rome, a Roman dictator was the special magistrate who held well defined powers, normally for six months at a time, usually in combination with a consulship. Roman dictators were allocated absolute power during times of emergency. In execution, their power was originally neither arbitrary nor unaccountable, being subject to law and requiring retrospective justification. There were no such dictatorships after the beginning of the 2nd century BC and later dictators such as Sulla and the Roman Emperors exercised power much more personally and arbitrarily. As the Roman Emperor was a king in all but name, a concept that remained anathema to traditional Roman society, the institution was not carried forward into the Roman Empire.

After the collapse of Spanish colonial rule, various dictators came to power in many liberated countries. Often leading a private army, these "caudillos" or self-appointed political-military leaders, attacked weak national governments once they controlled a region's political and economic powers, with examples such as Antonio López de Santa Anna in Mexico and Juan Manuel de Rosas in Argentina. Such dictators have been also referred to as "personalismos".

The wave of military dictatorships in South America in the second half of the twentieth century left a particular mark on Latin American culture. In Latin American literature, the dictator novel challenging dictatorship and "caudillismo" is a significant genre. There are also many films depicting Latin American military dictatorships.

In the first half of the 20th century, Communist and Fascist dictatorships appeared in a variety of scientifically and technologically advanced countries, which are distinct from dictatorships in Latin America and post-colonial dictatorships in Africa and Asia. Leading examples of modern totalitarian dictatorship include:

After World War II, dictators established themselves in the several new states of Africa and Asia, often at the expense or failure of the constitutions inherited from the colonial powers. These constitutions often failed to work without a strong middle class or work against the preexisting autocratic rule. Some elected presidents and prime ministers captured power by suppressing the opposition and installing one-party rule and others established military dictatorships through their armies. Whatever their form, these dictatorships had an adverse impact on economic growth and the quality of political institutions. Dictators who stayed in office for a long period of time found it increasingly difficult to carry out sound economic policies.

The often-cited exploitative dictatorship is the regime of Mobutu Sese Seko, who ruled Zaire from 1965 to 1997, embezzling over $5 billion from his country.

The global dynamics of democratization has been a central question for political scientists. The Third Wave Democracy was said to turn some dictatorships into democracies (see also the contrast between the two figures of the Democracy-Dictatorship Index in 1988 and 2008).

Mancur Olson suggests that the emergence of dictatorships can be linked to the concept of "roving bandits", individuals in an atomic system who move from place to place extracting wealth from individuals. These bandits provide a disincentive for investment and production. Olson states that a community of individuals would be better served if that bandit were to establish himself as a stationary bandit to monopolize theft in the form of taxes. Except from the community, the bandits themselves will be better served, according to Olson, by transforming themselves into "stationary bandits". By settling down and making themselves the rulers of a territory, they will be able to make more profits through taxes than they used to obtain through plunder. By maintaining order and providing protection to the community, the bandits will create a peaceful environment in which their people can maximize their surplus which means a greater taxable base. Thus a potential dictator will have a greater incentive to provide security to a given community from which he is extracting taxes and conversely, the people from whom he extracts the taxes are more likely to produce because they will be unconcerned with potential theft by other bandits. This is the rationality that bandits use in order to justify their transformation from "roving bandits" into "stationary bandits".




</doc>
<doc id="9039" url="https://en.wikipedia.org/wiki?curid=9039" title="Django Reinhardt">
Django Reinhardt

Jean Reinhardt ( or ; 23 January 1910 – 16 May 1953) stage name Django Reinhardt, was a Belgian-born Romani-French jazz guitarist, musician and composer, regarded as one of the greatest musicians of the twentieth century. He was the first jazz talent to emerge from Europe and remains the most significant.

With violinist Stéphane Grappelli, Reinhardt formed the Paris-based Quintette du Hot Club de France in 1934. The group was among the first to play jazz that featured the guitar as a lead instrument. Reinhardt recorded in France with many visiting American musicians, including Coleman Hawkins and Benny Carter, and briefly toured the United States with Duke Ellington's orchestra in 1946. He died suddenly of a stroke at the age of 43.

Reinhardt's most popular compositions have become standards within gypsy jazz, including "Minor Swing", "Daphne", "Belleville", "Djangology", "Swing '42", and "Nuages". Jazz guitarist Frank Vignola claims that nearly every major popular-music guitarist in the world has been influenced by Reinhardt. Over the last few decades, annual Django festivals have been held throughout Europe and the U.S., and a biography has been written about his life. In February 2017, the Berlin International Film Festival held the world premiere of the French film, "Django".

Reinhardt was born on 23 January 1910 in Liberchies, Pont-à-Celles, Belgium, into a Belgian family of Manouche Romani descent. His father was Jean Eugene Weiss, but domiciled in Paris with his wife, he went by Jean-Baptiste Reinhardt, his wife's surname, to avoid French military conscription. His mother, Laurence Reinhardt, was a dancer. The birth certificate refers to "Jean Reinhart, son of Jean Baptiste Reinhart, artist, and Laurence Reinhart, housewife, domiciled in Paris".

A number of authors have repeated the claim that Reinhardt's nickname, Django, is Romani for "I awake"; however, it may also simply have been a diminutive, or local Walloon version, of "Jean". Reinhardt spent most of his youth in Romani encampments close to Paris, where he started playing the violin, banjo, and guitar. He became adept at stealing chickens, which was viewed as a noble skill by the Romani, because part of their means of survival on the road was to steal from the non-Gypsy world around them. His father reportedly played music in a family band comprising himself and seven brothers; a surviving photograph shows this band including his father on piano.

Reinhardt was attracted to music at an early age, first playing the violin. At the age of 12 he received a banjo-guitar as a gift. He quickly learned to play, mimicking the fingerings of musicians he watched, who would have included local virtuoso players of the day such as Jean "Poulette" Castro and Auguste "Gusti" Malha, as well as from his uncle Guiligou, who played violin, banjo and guitar. Reinhardt was able to make a living playing music by the time he was 15. He received little formal education and acquired the rudiments of literacy only in adult life.

At the age of 17 Reinhardt married Florine "Bella" Mayer, a girl from the same gypsy settlement, according to gypsy custom (although not an official marriage under French law). The following year he recorded for the first time. On these recordings, made in 1928, Reinhardt plays the "banjo" (actually the banjo-guitar) accompanying the accordionists Maurice Alexander, Jean Vaissade and Victor Marceau, and the singer Maurice Chaumel. His name was now drawing international attention, such as from British bandleader Jack Hylton, who came to France just to hear him play. He offered him a job on the spot, and Reinhardt accepted.

Before he had a chance to start with the band, however, he nearly lost his life when the caravan he and his wife lived in caught fire when he knocked over a candle on his way to bed. His wife made artificial flowers from extremely flammable celluloid. They caught fire, engulfing the wagon in flames almost immediately. Reinhardt dragged himself and his wife through the fire to safety, but suffered extensive burns on his left hand and other areas. He received first- and second-degree burns over half his body. His right leg was paralyzed, and the fourth and fifth fingers of his left hand were badly burned. Doctors believed that he would never play guitar again, and they intended to amputate one of his legs. Reinhardt refused to have the surgery and left the hospital after a short time; he was able to walk within a year with the aid of a cane.

Two of his fingers remained paralyzed. By sheer will, he taught himself to overcome his now permanent handicap by using only his thumb and two fingers. His brother, Joseph Reinhardt, also an accomplished guitarist, bought Reinhardt a new guitar. With rehabilitation and practice, he re-learned his craft in a completely new way. He played all his guitar solos with only the index and middle fingers and used the two injured fingers only for chord work.

In 1929, his wife gave birth to a son, Henri "Lousson" Reinhardt. As a result of the trauma and injuries, he and Bella parted company soon after. His son later took the surname of his mother's new husband, Baumgartner. He later recorded with Django.

The years between 1925 and 1933 were formative for Reinhardt, personally and musically. He had parted with his wife and had formed a relationship with one of his distant cousins, Sophie Ziegler, nicknamed "Naguine." They traveled throughout France with Reinhardt getting occasional jobs playing at small clubs. He had no definite goals, living a hand-to-mouth existence. The concept of money and saving was foreign to him, and he spent his earnings as quickly as he made them.

One change during this period was his abandonment of the banjo in favor of the guitar. He was playing all types of music previously but began to appreciate American jazz a little during this period, when an acquaintance, Émile Savitry, played him a number of records from his collection. It was the first time Reinhardt heard leading American jazz musicians, such as Louis Armstrong and Duke Ellington. The new sounds gave Reinhardt a vision and goal of becoming a jazz professional.

He later met Stéphane Grappelli, a young violinist with similar musical interests. In the absence of paid work in their radical new music, the two would jam together, along with a loose circle of other musicians. Finally, Reinhardt acquired his first Selmer guitar in the mid-1930s. He used the volume and expressiveness of the instrument as integral elements of his style.

From 1934 until the outbreak of World War II in 1939, Reinhardt and Grappelli worked together as the principal soloists of their newly formed Hot Club, in Paris. It became the most accomplished and innovative European jazz group of the period.

Reinhardt's brother Joseph and Roger Chaput also played on guitar, and Louis Vola was on bass. The Quintette was one of the few well-known jazz ensembles composed only of stringed instruments.

In Paris on 14 March 1933, Reinhardt recorded two takes each of "Parce-que je vous aime" and "Si, j'aime Suzy", vocal numbers with lots of guitar fills and guitar support. He used three guitarists along with an accordion lead, violin, and bass. In August 1934, he made other recordings with more than one guitar (Joseph Reinhardt, Roger Chaput, and Reinhardt), including the first recording by the Quintette. In both years the great majority of their recordings featured a wide variety of horns, often in multiples, piano, and other instruments, but the all-string instrumentation is the one most often adopted by emulators of the Hot Club sound.

Decca Records in the United States released three records of Quintette songs with Reinhardt on guitar, and one other, credited to "Stephane Grappelli & His Hot 4 with Django Reinhardt", in 1935.

Reinhardt also played and recorded with many American jazz musicians, such as Adelaide Hall, Coleman Hawkins, Benny Carter, and Rex Stewart (who later stayed in Paris). He participated in a jam session and radio performance with Louis Armstrong. Later in his career, Reinhardt played with Dizzy Gillespie in France. Also in the neighborhood was the artistic salon R-26, at which Reinhardt and Grappelli performed regularly as they developed their unique musical style.

In 1938 Reinhardt's quintet played to thousands at an all-star show held in London's Kilburn State auditorium. While playing, he noticed American film actor Eddie Cantor in the front row. When their set ended, Cantor rose to his feet, then went up on stage and kissed Reinhardt's hand, paying no concern to the audience. A few weeks later the quintet played at the London Palladium.

When World War II broke out, the original quintet was on tour in the United Kingdom. Reinhardt returned to Paris at once, leaving his wife in the UK. Grappelli remained in the United Kingdom for the duration of the war. Reinhardt re-formed the quintet, with Hubert Rostaing on clarinet replacing Grappelli.

In 1943, Reinhardt married Sophie "Naguine" Ziegler in Salbris. They had a son, Babik Reinhardt, who later became a respected guitarist in his own right. Thanks to his superior music talent, Reinhardt would survive the war unscathed, unlike many Gypsies who were interned and killed in the Porajmos, the Nazi regime's systematic murder of several hundred thousand European Gypsies.

In addition, the German attitude toward jazz from the time of World War I had been one of general hostility. Between 1916 and 1920 all jazz was banned in Germany. From 1922 on, jazz was mostly suppressed, and after 1933 Hitler banned most jazz, which he and his minister, Goebbels, felt was part of an international conspiracy to undermine Germany's greatness. It would not be until the mid-1950s that Germany reopened itself to European jazz.

But beginning in 1933, all German Gypsies were doomed, states Dregni. They were barred from living in cities and were herded into settlement camps. Nazi doctors began sterilizing them, and like the yellow Stars of David that Jews had to subsequently wear, Gypsies were required to wear a brown Gypsy ID triangle sewn on their chest. By 1942, Gypsies and Jews were systematically being killed at new camps such as Auschwitz. Other Gypsies, such as those in France, were used as slave labor on farms and factories. Some 600,000 Gypsies throughout Europe were eventually killed.

Because Reinhardt and his family were Gypsies, and he was also a jazz musician, he tried to escape from occupied France with his family. After his first attempt, he survived when a secretly jazz-loving German, Luftwaffe officer Dietrich Schulz-Köhn, let him go back to France after he was captured. But still desperate to get out of France, knowing that Gypsies were being rounded up and killed in concentration camps, he tried again to cross into Switzerland a few days later, this time in the dead of night. But he was stopped by Swiss border guards who forced him to return to Paris.

During the occupation of France, Reinhardt continued playing and composing. One of his songs, "Nuages," became an unofficial anthem in Paris to signify hope for liberation. During a concert at the Salle Pleyel, the popularity of the song was such that the crowd made him replay the song three times in a row. The 78 of the song sold over 100,000 copies.

Since the Nazis officially disapproved of jazz, Reinhardt tried to develop other musical directions. He tried to write a Mass for the Gypsies and a symphony (he worked with an assistant to notate what he was improvising). His modernist piece "Rhythm Futur" was intended to be acceptable. 

After the war, Reinhardt rejoined Grappelli in the UK. In the autumn of 1946, he made his first tour in the United States, debuting at Cleveland Music Hall as a special guest soloist with Duke Ellington and His Orchestra. He played with many notable musicians and composers, such as Maury Deutsch. At the end of the tour, Reinhardt played two nights at Carnegie Hall in New York City; he received a great ovation and took six curtain calls on the first night.

Despite his pride in touring with Ellington (one of two letters to Grappelli relates his excitement), he was not fully integrated into the band. He played a few tunes at the end of the show, backed by Ellington, with no special arrangements written for him. After the tour, Reinhardt secured an engagement at Café Society Uptown, where he played four solos a day, backed by the resident band. These performances drew large audiences. Having failed to bring his usual Selmer Modèle Jazz, he played on a borrowed electric guitar, which he felt hampered the delicacy of his style. He had been promised jobs in California, but they failed to develop. Tired of waiting, Reinhardt returned to France in February 1947.

After his return, Reinhardt re-immersed himself in Gypsy life, finding it difficult to adjust to the postwar world. He sometimes showed up for scheduled concerts without a guitar or amplifier, or wandered off to the park or beach. On a few occasions he refused to get out of bed. Reinhardt developed a reputation among his band, fans, and managers as extremely unreliable. He skipped sold-out concerts to "walk to the beach" or "smell the dew." During this period he continued to attend the R-26 artistic salon in Montmartre, improvising with his devoted collaborator, Stéphane Grappelli.

In Rome in 1949, Reinhardt recruited three Italian jazz players (on bass, piano, and snare drum) and recorded over 60 tunes in an Italian studio. He united with Grappelli, and used his acoustic Selmer-Maccaferri. The recording was issued for the first time in the late 1950s.

Back in Paris, in June 1950, Reinhardt was invited to join an entourage to welcome the return of Benny Goodman. He also attended a reception for Goodman, who after the war ended had asked Reinhardt to join him in the U.S. He asked him again, and out of politeness, Reinhardt agreed. But he later had second thoughts about what role he could play alongside Goodman, who was the "King of Swing," and remained in France.

In 1951, Reinhardt retired to Samois-sur-Seine, near Fontainebleau, where he lived until his death. He continued to play in Paris jazz clubs and began playing electric guitar. (He often used a Selmer fitted with an electric pickup, despite his initial hesitation about the instrument.) In his final recordings, made with his Nouvelle Quintette in the last few months of his life, he had begun moving in a new musical direction, in which he assimilated the vocabulary of bebop and fused it with his own melodic style.

While walking from the Avon railway station after playing in a Paris club, he collapsed outside his house from a brain hemorrhage.
It was a Saturday and it took a full day for a doctor to arrive. Reinhardt was declared dead on arrival at the hospital in Fontainebleau, at the age of 43.

Reinhardt's second son, Babik, became a guitarist in the contemporary jazz style. His first son, Lousson, was more of a traditionalist. He followed the Romani lifestyle and rarely performed in public.
After Reinhardt died, his brother Joseph at first swore to abandon music, but he was persuaded to perform and record again. Joseph's son Markus Reinhardt is a violinist in the Romani style.

A third generation of direct descendants has developed as musicians: David Reinhardt, Reinhardt's grandson (by his son Babik), leads his own trio. Dallas Baumgartner, a great-grandson by Lousson, is a guitarist who travels with the Romani and keeps a low public profile. A slightly younger distant relative, violinist Schnuckenack Reinhardt, became famous in Germany as a performer of gypsy music and gypsy jazz up to his death in 2006, and also assisted in keeping Reinhardt's legacy alive through the period following his death.

Reinhardt is regarded as one of the greatest guitar players of all time, and the first important European jazz musician to make a major contribution with jazz guitar. During his career he wrote nearly 100 songs, according to jazz guitarist Frank Vignola.

Using a Selmer Guitar in the mid-1930s, his style took on new volume and expressiveness. Despite his physical handicap, he played mainly using his index and middle fingers, and invented a distinctive style of jazz guitar.

For about a decade after Reinhardt's death, interest in his musical style was minimal. In the fifties, bebop superseded swing in jazz, rock and roll took off, and electric instruments became dominant in popular music. Since the mid-sixties, there has been a revival of interest in Reinhardt's music, a revival that has extended into the 21st century, with annual festivals and periodic tribute concerts. His devotees included classical guitarist Julian Bream and country guitarist Chet Atkins, who considered him one of the ten greatest guitarists of the twentieth century.

The Allman Brothers Band song "Jessica" was written by Dickey Betts in tribute to Reinhardt. Woody Allen's 1999 film "Sweet and Lowdown", the story of a Django Reinhardt-like character, mentions Reinhardt and includes actual recordings in the film. "Django was the definitive genius on the guitar, and the depth of his gift was so spectacular," says Allen.

Jazz guitarists in the U.S., such as Charlie Byrd and Wes Montgomery, were influenced by his style. In fact, Byrd, who lived from 1925 to 1999, said that Reinhardt was his primary influence. Guitarist Mike Peters notes that "the word 'genius' is bantered about too much. But in jazz, Louis Armstrong was a genius, Duke Ellington was another one, and Reinhardt was also." Grisman adds, "As far as I'm concerned, no one since has come anywhere close to Django Reinhardt as an improviser or technician."
The popularity of gypsy jazz has generated an increasing number of festivals, such as the Festival Django Reinhardt held every last weekend of June since 1983 in Samois-sur-Seine (France), the various DjangoFests held throughout Europe and the USA, and Django in June, an annual camp for Gypsy jazz musicians and aficionados.

In February 2017, the Berlin International Film Festival held the world premiere of "Django", a French film directed by Etienne Comar. The movie covers Django's escape from Nazi-occupied Paris in 1943 and the fact that even under "constant danger, flight and the atrocities committed against his family", he continued composing and performing. Reinhardt's music was re-recorded for the film by the Dutch jazz band Rosenberg Trio with lead guitarist Stochelo Rosenberg.

The documentary film, "Djangomania!" was released in 2005. The hour-long film was directed and written by Jamie Kastner, who traveled throughout the world to show the influence of Django's music in various countries.

In 1984 the Kool Jazz Festival, held in Carnegie Hall and Avery Fisher Hall, was dedicated entirely to Reinhardt. Performers included Grappelli, Benny Carter, and Mike Peters with his group of seven musicians. The festival was organized by George Wein. Reinhardt is celebrated annually in the village of Liberchies, his birthplace.

Numerous musicians have written and recorded tributes to Reinhardt.

Many guitar players and other musicians have expressed admiration for Reinhardt or have cited him as a major influence. Jeff Beck described Reinhardt as "by far the most astonishing guitar player ever" and "quite superhuman".

Grateful Dead's Jerry Garcia and Black Sabbath's Tony Iommi, both of whom lost fingers in accidents, were inspired by Reinhardt's example of becoming an accomplished guitar player despite his injuries. Garcia was quoted in June 1985 in "Frets Magazine": 
His technique is awesome! Even today, nobody has really come to the state that he was playing at. As good as players are, they haven't gotten to where he is. There's a lot of guys that play fast and a lot of guys that play clean, and the guitar has come a long way as far as speed and clarity go, but nobody plays with the whole fullness of expression that Django has. I mean, the combination of incredible speed – all the speed you could possibly want – but also the thing of every note have a specific personality. You don't hear it. I really haven't heard it anywhere but with Django.
Denny Laine and Jimmy McCulloch, members of Paul McCartney's band Wings, have mentioned him as an inspiration.

Andrew Latimer, of the band Camel, has stated that he was influenced by Reinhardt. 

Willie Nelson has been a life-long Reinhardt fan, stating in his memoir, "This was a man who changed my musical life by giving me a whole new perspective on the guitar and, on an even more profound level, on my relationship with sound...During my formative years, as I listened to Django's records, especially songs like 'Nuages' that I would play for the rest of my life, I studied his technique. Even more, I studied his gentleness. I love the human sound he gave his acoustic guitar."


Reinhardt recorded over 900 sides in his recording career, from 1928 to 1953, the majority as sides of the then-prevalent 78-RPM records, with the remainder as acetates, transcription discs, private and off-air recordings (of radio broadcasts), and part of a film soundtrack. Only one session (eight tracks) from March 1953 was ever recorded specifically for album release by Norman Granz in the then-new LP format, but Reinhardt died before the album could be released. In his earliest recordings Reinhardt played banjo (or, more accurately, banjo-guitar) accompanying accordionists and singers on dances and popular tunes of the day, with no jazz content, whereas in the last recordings before his death he played amplified guitar in the bebop idiom with a pool of younger, more modern French musicians. 

A full chronological listing of his lifetime recorded output is available from the source cited here, and an index of individual tunes is available from the source cited here. A few fragments of film performance (without original sound) also survive, as does one complete performance with sound, of the tune "J'Attendrai" performed with the Quintet in 1938 for the short film "Le Jazz Hot".

Reinhardt's recorded output has been re-released on a large number of LPs, cassettes and CDs since his death and also the start of the LP era. Of particular mention is "Intégrale Django Reinhardt", volumes 1–20 (40 CDs), released by the French company Frémeaux from 2002 to 2005, which strove to include every known track on which he played.

The following list of reissues is only a selection; as at December 2015, www.discogs.com listed more than 560 such albums; a full listing is available from the source cited here.


A small number of waltzes composed by Reinhardt in his youth were never recorded by the composer, but were retained in the repertoire of his associates and several are still played today. They came to light via recordings by Matelo Ferret in 1960 (the waltzes "Montagne Sainte-Genevieve", "Gagoug", "Chez Jacquet" and "Choti"; Disques Vogue (F)EPL7740) and 1961 ("Djalamichto" and "En Verdine"; Disques Vogue (F)EPL7829). The first four are now available on Matelo's CD "Tziganskaïa and Other Rare Recordings", released by Hot Club Records (subsequently reissued as "Tziganskaïa: The Django Reinhardt Waltzes"); "Chez Jacquet" was also recorded by Baro Ferret in 1966. 

The names "Gagoug" and "Choti" were reportedly conferred by Django's widow Naguine on request from Matelo, who had learned the tunes without names. Django also worked on composing a Mass for use by the gypsies, which was not completed although an 8-minute extract exists, played by the organist Léo Chauliac for Reinhardt's benefit, via a 1944 radio broadcast; this can be found on the CD release "Gipsy Jazz School" and also on volume 12 of the "Intégrale Django Reinhardt" CD compilation.


</doc>
<doc id="9041" url="https://en.wikipedia.org/wiki?curid=9041" title="Digit">
Digit

Digit may refer to:






</doc>
<doc id="9048" url="https://en.wikipedia.org/wiki?curid=9048" title="Dana Plato">
Dana Plato

Dana Michelle Plato (born Dana Michelle Strain; November 7, 1964 – May 8, 1999) was an American actress who was notable for having played the role of Kimberly Drummond on the U.S. television sitcom "Diff'rent Strokes", from 1978 to 1986. After leaving the cast of "Diff'rent Strokes", Plato attempted to establish herself as a working actress, with mixed success: she worked sporadically in made-for-TV movies and in independent films, and did voice-over work. At the age of 34, after years of struggling with poverty and substance abuse, Plato died from an overdose of prescription drugs.

Plato was born Dana Michelle Strain on November 7, 1964 in Maywood, California, to Linda Strain, an unwed teenager who was already caring for an 18-month-old child. In June 1965, the seven-month-old Dana was adopted by Dean Plato, who owned a trucking company, and his wife Florine "Kay" Plato. Plato was raised in the San Fernando Valley. When she was three, her adoptive parents divorced, and she lived with her mother.

When she was very young, Plato began attending auditions with her mother and from age seven began appearing in television commercials; she reportedly appeared in over 100 commercials for companies as diverse as Kentucky Fried Chicken, Dole, and Atlantic Richfield. Plato made her film debut at age 13, in the horror film "Return to Boggy Creek" (1977). Other early credits included "" (1977) and "California Suite" (1978).

In addition to acting, Plato was an accomplished figure skater; at one point she trained for a possible Olympic team spot. It was during this time that she made a brief appearance on TV's "The Gong Show" and was spotted by a producer who helped her secure what became her most famous acting role, Kimberly Drummond on "Diff'rent Strokes." According to Plato, her mother decided she should cut back on her skating in order to focus on the TV role.

"Diff'rent Strokes" debuted on NBC in 1978, becoming an immediate hit. The show features Phillip Drummond (Conrad Bain), a wealthy white widower in New York City who adopts two black boys after their parents' deaths. Plato played Kimberly, Drummond's teenage daughter, who at the start of the show becomes the adoptive sister of the two boys, Willis (Todd Bridges) and Arnold (Gary Coleman). Plato appeared on the show from 1978 until 1984 and again from 1985 to 1986; during her tenure the show appeared on two different networks.

During her years on "Diff'rent Strokes", Plato struggled with drug and alcohol problems. She admitted to drinking alcohol, and using cannabis and cocaine, and she suffered an overdose of diazepam when she was 14.

In December 1983, Plato moved in with her boyfriend, rock guitarist Lanny Lambert. The couple married on April 24, 1984, and their only child, Tyler Edward Lambert, was born on July 2, 1984. During this time, Plato was let go from "Diff'rent Strokes" because the producers did not feel that a pregnancy would fit the show's wholesome image. Although rumors of drug use and other problems on the set surrounded her dismissal, the producers were adamant that Plato's pregnancy was the only reason her character was written out.

She returned for six guest appearances during the show's seventh and eighth seasons. In the season 8 episode that aired on January 17, 1986—Plato's final appearance on the show—Kimberly suffers from the effects of bulimia.

After leaving "Diff'rent Strokes", Plato attempted to establish herself as a serious actress but found it difficult to achieve success outside of her sitcom career. She had breast implants and modeled for a June 1989 "Playboy" pictorial, but her career remained in stagnation, and she started taking roles in such B-movies as "Bikini Beach Race" (1989) and "Lethal Cowboy" (1992).

Plato separated from Lambert in January 1988, the same week her mother died of scleroderma. In desperation over these traumatic events, she signed over power of attorney to an accountant who disappeared with the majority of her money, leaving her with no more than $150,000. She claimed the accountant was never found nor prosecuted, despite an exhaustive search, and that he had also stolen more than $11 million of other people's money.

During her March 1990 divorce, Plato lost custody of her son to Lambert and was given visitation rights. She moved to Las Vegas, where she struggled with poverty and unemployment. At one point she worked at a dry-cleaning store, where customers reported being impressed by her lack of airs.

On February 28, 1991, she entered a video store, produced a pellet gun, and demanded the money in the cash register. The clerk called 911, saying, "I've just been robbed by the girl who played Kimberly on "Diff'rent Strokes"." Approximately 15 minutes after the robbery, Plato returned to the scene and was immediately arrested. The robbery netted Plato $164. Las Vegas entertainer Wayne Newton posted her $13,000 bail, and Plato was given five years' probation. Plato made headlines and became a subject of the national debate surrounding troubled child stars, particularly given the difficulties of her "Diff'rent Strokes" co-stars Gary Coleman and Todd Bridges.

In January 1992, she was arrested again, this time for forging a prescription for Diazepam. She served 30 days in jail for violating the terms of her probation and entered a drug rehabilitation program immediately thereafter.

In 1992, Plato was one of the first celebrities to star in a video game. The game, "Night Trap", was not a great success (the majority of the game's video content was actually filmed in 1987 then shelved), but is considered a pioneering title because it was the first game to use live actors, specifically a well-known personality. It was one of the first video game titles to have mature content and attracted controversy due to its depiction of violence. The controversy, along with that surrounding "Mortal Kombat", eventually led to the creation of the ESRB.

Toward the end of her career, Plato chose roles that could be considered erotic, softcore pornography. She appeared nude in "Prime Suspect" (1989) and "Compelling Evidence" (1995), and in the softcore erotic drama "Different Strokes: The Story of Jack and Jill...and Jill" (1998), whose title was changed after filming in order to tie it to Plato's past. Following her appearance in the film, in the same year, Plato appeared in a cover story of the lesbian lifestyle magazine "Girlfriends", in which she came out as a lesbian, although she later recanted.

She became engaged to Fred Potts, a filmmaker and close friend of Johnny Whitaker, but the romance soon broke up after Plato's manipulation of Potts, and she returned to drugs. Just before her death, she was engaged to her manager Robert Menchaca, with whom she lived in a motor home in Navarre, Florida.

On May 7, 1999, the day before she died, Plato appeared on "The Howard Stern Show". She spoke about her life, discussing her financial problems and past run-ins with the law. She admitted to being a recovering alcoholic and drug addict, but claimed she had been sober for more than 10 years by that point, and was not using any drugs, with the exception of prescribed painkillers due to the recent extraction of two molars. Many callers insulted her and questioned her sobriety, which provoked a defiant Plato who offered to take a drug test on the air. Some callers, as well as host Howard Stern, came to Plato's defense. Although she allowed a hair to be cut for the test, Stern later claimed she asked for it back after the interview.

The next day, Plato and Menchaca were returning to California and stopped at Menchaca's mother's home in Moore, Oklahoma, for a Mother's Day visit. Plato went to lie down inside her Winnebago motor home parked outside the house, where she died of an overdose of the painkiller Lortab and the muscle-relaxant Soma. Her death was eventually ruled a suicide. Her body was cremated and her ashes were scattered in the Pacific Ocean.

Almost exactly 11 years to the day after Plato's death, on May 6, 2010, her son Tyler Lambert died at the age of 25 in Tulsa, Oklahoma, of a self-inflicted gunshot wound to the head. He had reportedly been experimenting with drugs and alcohol. Johnny Whitaker, Plato's former manager and a family friend, said that Lambert always said he "wanted to be with Mom."



</doc>
<doc id="9051" url="https://en.wikipedia.org/wiki?curid=9051" title="Drop kick">
Drop kick

A drop kick is a type of kick in various codes of football. It involves a player dropping the ball and then kicking it when it bounces off the ground.

Drop kicks are most importantly used as a method of restarting play and scoring points in rugby union and rugby league. Association football goalkeepers also often return the ball to play with drop kicks. The kick was once in wide use in both Australian rules football and gridiron football, but is today rarely seen in either sport.

The drop kick technique in rugby codes is usually to hold the ball with one end pointing downwards in two hands above the kicking leg. The ball is dropped onto the ground in front of the kicking foot, which makes contact at the moment or fractionally after the ball touches the ground, called the "half-volley". The kicking foot usually makes contact with the ball slightly on the instep.

In a rugby union kick-off, or drop out, the kicker usually aims to kick the ball high but not a great distance, and so usually strikes the ball after it has started to bounce off the ground, so the contact is made close to the bottom of the ball.

In rugby league, drop kicks are mandatory to restart play from the goal line (called a goal line drop-out) after the defending team is tackled or knocks on in the in-goal area or the defending team causes the ball to go dead or into touch-in-goal. Drop kicks are also mandatory to restart play from the 20 metre line after an unsuccessful penalty goal attempt goes dead or into touch-in-goal and to score a drop goal (sometimes known as a field goal) in open play, which is worth one point.

Drop kicks are optional for a penalty kick to score a penalty goal (this being done rarely, as place kicks are generally used) and when kicking for touch (the sideline) from a penalty, although the option of a punt kick is usually taken instead.

In rugby union, a drop kick is used for the kick-off and restarts and to score a drop goal (sometimes called a field goal). Originally, it was one of only two ways to score points, along with the place kick.

Drop kicks are mandatory from the centre spot to start a half (a kick-off), from the centre spot to restart the game after points have been scored, to restart play from the 22-metre line (called a drop-out) after the ball is touched down or made dead in the in-goal area by the defending team when the attacking team kicked or took the ball into the in-goal area, and to score a drop goal (sometimes called a field goal) in open play, which is worth three points.

Drop kicks are optional for a conversion kick after a try has been scored.

The usage of drop kicks in rugby sevens is the same as in rugby union, except that drop kicks are used for all conversion attempts and for penalty kicks, both of which must be taken within 40 seconds of the try being scored or the award of the penalty.

In both American and Canadian football, one method of scoring a field goal or extra point is by drop-kicking the football through the goal.

It contrasts to a punt, wherein the player kicks the ball without letting it hit the ground first; and a placekick, wherein the player kicks a stationary ball off the ground: "from placement". A drop kick is significantly more difficult; as Jim Thorpe once explained, "I regard the place kick as almost two to one safer than the drop kick in attempting a goal from the field."

The drop kick was often used in early football as a surprise tactic. The ball would be snapped or lateraled to a back, who would fake a run or pass, but then would kick the field goal instead. This method of scoring worked well in the 1920s and early 1930s, when the football was rounder at the ends (similar to a modern rugby ball). Early football stars such as Charles Brickley, Frank Hudson, Jim Thorpe, Paddy Driscoll, and Al Bloodgood were skilled drop-kickers; Driscoll in 1925 and Bloodgood in 1926 hold a tied NFL record of four drop kicked field goals in a single game. Driscoll's 55 yard drop kick in 1924 stood as the unofficial record for field goal range until Bert Rechichar kicked a 56-yard field goal (by placekick) in 1953.

In 1934, the ball was made more pointed at the ends. The creation of the pointed football is generally credited to Shorty Ray, at the time a college football official and later the NFL's head of officiating. This made passing the ball easier, as was its intent, but made the drop kick obsolete, as the more pointed ball did not bounce up from the ground reliably. The drop kick was supplanted by the place kick, which cannot be attempted out of a formation generally used as a running or passing set. The drop kick remains in the rules, but is seldom seen, and rarely effective when attempted.

In Canadian football the drop kick can be taken from any point on the field, unlike placekicks which must be attempted behind the line of scrimmage.

Before the NFL–AFL merger, the last successful drop kick in the NFL was executed by Scooter McLean of the Chicago Bears in their 37–9 victory over the New York Giants on December 21, 1941, in the NFL Championship game at Chicago's Wrigley Field. Though it was not part of the NFL at the time, the All-America Football Conference saw its last drop kick November 28, 1948, when Joe Vetrano of the San Francisco 49ers drop kicked an extra point after a muffed snap against the Cleveland Browns.
The only successful drop kick in the NFL since the 1940s was by Doug Flutie, the backup quarterback of the New England Patriots, against the Miami Dolphins on January 1, 2006, for an extra point after a touchdown. Flutie had estimated "an 80 percent chance" of making the drop kick, which was called to give Flutie, 43 at the time, the opportunity to make a historic kick in his final NFL game; the drop kick was his last play in the NFL. After the game, New England coach Bill Belichick said, "I think Doug deserves it," and Flutie said, "I just thanked him for the opportunity."

Dallas Cowboys punter Mat McBriar attempted a maneuver similar to a drop kick during the 2010 Thanksgiving Day game after a botched punt attempt, but the ball bounced several times before the kick and the sequence of events is officially recorded as a fumble, followed by an illegal kick, with the fumble being recovered by the New Orleans Saints 29 yards downfield from the spot of the kick. The Saints declined the illegal kick penalty.

New England Patriots kicker Stephen Gostkowski attempted an onside drop kick on a free kick after a safety against the Pittsburgh Steelers on October 30, 2011. The kick went out of bounds.

New Orleans Saints quarterback Drew Brees, a former teammate of Flutie's, executed a drop kick late on an extra point attempt in the fourth quarter of the 2012 Pro Bowl; the kick fell short.

New England Patriots special teams player Nate Ebner attempted an onside drop kick on a kickoff after a Patriots touchdown against the Philadelphia Eagles on December 6, 2015. The kick was recovered by the Eagles at their own 41 yard line. Two weeks later, on December 20, Buffalo Bills punter Colton Schmidt executed what is believed to be an unintentional drop kick after a botched punt against the Washington Redskins; because the Redskins recovered the kick, it was treated as a punt (and not as a field goal attempt, which would have pushed the ball back to the spot of the kick).

The last successful drop kick extra point in the NCAA was by Jason Millgan of Hartwick College on December 11, 1998, St. Lawrence University. Frosty Peters of Montana State College made 17 drop kicks in one game in 1924.

In the Canadian game, the drop kick can be attempted at any time by either team. Any player on the kicking team behind the kicker, and including the kicker, can recover the kick. When a drop kick goes out of bounds, possession on the next scrimmage goes to the non-kicking team.

On September 8, 1974, Tom Wilkinson, quarterback for the Edmonton Eskimos, unsuccessfully attempted a drop kick field goal in the final seconds of a 24–2 romp over the Winnipeg Blue Bombers. This may have been the last time the play was deliberately attempted in the CFL.

During one game in the 1980s, Hamilton Tiger-Cats wide receiver Earl Winfield was unable to field a punt properly; in frustration, he kicked the ball out of bounds. The kick was considered a drop kick and led to a change of possession, with the punting team regaining possession of the ball.

In Arena football, a drop-kicked extra point counts for two points rather than one and a drop-kicked field goal counts for four points rather than three. The most recent conversion of a drop kick was by Geoff Boyer of the Pittsburgh Power on June 16, 2012; it was the first successful conversion in the Arena Football League since 1997.In 2018, Maine Mammoth kicker Nell converted a drop kick as a PAT against the Massachusetts Pirates in the National Arena League.

Once the preferred method of conveying the ball over long distances, the drop-kick has been superseded by the drop punt as a more accurate means of delivering the ball to a fellow player..



</doc>
<doc id="9053" url="https://en.wikipedia.org/wiki?curid=9053" title="Diaeresis">
Diaeresis

Diaeresis (dieresis, diëresis) may refer to:



</doc>
<doc id="9055" url="https://en.wikipedia.org/wiki?curid=9055" title="Derry">
Derry

Derry, officially Londonderry (), is the second-largest city in Northern Ireland and the fourth-largest city on the island of Ireland. The name Derry is an anglicisation of the Old Irish name "Daire" (modern Irish: "Doire") meaning "oak grove". In 1613, the city was granted a Royal Charter by King James I and gained the "London" prefix to reflect the funding of its construction by the London guilds. While the city is more usually known colloquially as Derry, Londonderry is also commonly used and remains the legal name.

The old walled city lies on the west bank of the River Foyle, which is spanned by two road bridges and one footbridge. The city now covers both banks (Cityside on the west and Waterside on the east). The population of the city was 83,652 at the 2001 Census, while the Derry Urban Area had a population of 90,736. The district administered by Derry City and Strabane District Council contains both Londonderry Port and City of Derry Airport.

Derry is close to the border with County Donegal, with which it has had a close link for many centuries. The person traditionally seen as the founder of the original Derry is Saint Colmcille, a holy man from Tír Chonaill, the old name for almost all of modern County Donegal, of which the west bank of the Foyle was a part before 1610.

In 2013, Derry was the inaugural UK City of Culture, having been awarded the title in 2010.

According to the city's Royal Charter of 10 April 1662, the official name is "Londonderry". This was reaffirmed in a High Court decision in 2007 when Derry City Council sought guidance on the procedure for effecting a name change. The council had changed its name from "Londonderry City Council" to "Derry City Council" in 1984; the court case was seeking clarification as to whether this had also changed the name of the city. The decision of the court was that it had not but it was clarified that the correct procedure to do so was via a petition to the Privy Council. Derry City Council since started this process and were involved in conducting an equality impact assessment report (EQIA). Firstly it held an opinion poll of district residents in 2009, which reported that 75% of Catholics and 77% of Nationalists found the proposed change acceptable, compared to 6% of Protestants and 8% of Unionists. Then the EQIA held two consultative forums, and solicited comments from the general public on whether or not the city should have its name changed to Derry. A total of 12,136 comments were received, of which 3,108 were broadly in favour of the proposal, and 9,028 opposed to it. On 23 July 2015, the council voted in favour of a motion to change the official name of the city to Derry and to write to Mark H. Durkan, Northern Ireland Minister of the Environment, to ask how the change could be effected.

Despite the official name, the city is more usually known as "Derry", which is an anglicisation of the Irish "Daire" or "Doire", and translates as "oak-grove/oak-wood". The name derives from the settlement's earliest references, "Daire Calgaich" ("oak-grove of Calgach"). The name was changed from Derry in 1613 during the Plantation of Ulster to reflect the establishment of the city by the London guilds.

The name "Derry" is preferred by nationalists and it is broadly used throughout Northern Ireland's Catholic community, as well as that of the Republic of Ireland, whereas many unionists prefer "Londonderry"; however in everyday conversation Derry is used by most Protestant residents of the city. Linguist Kevin McCafferty argues that "It is not, strictly speaking, correct that Northern Ireland Catholics call it Derry, while Protestants use the Londonderry form, although this pattern has become more common locally since the mid-1980s, when the city council changed its name by dropping the prefix". In McCafferty's survey of language use in the city, "only very few interviewees—all Protestants—use the official form".

Apart from the name of Derry City Council, the city is usually known as Londonderry in official use within the UK. In the Republic of Ireland, the city and county are almost always referred to as Derry, on maps, in the media and in conversation. In April 2009, however, the Republic of Ireland's Minister for Foreign Affairs, Micheál Martin, announced that Irish passport holders who were born there could record either Derry or Londonderry as their place of birth. Whereas official road signs in the Republic use the name Derry, those in Northern Ireland bear Londonderry (sometimes abbreviated to "L'Derry"), although some of these have been defaced with the reference to London obscured. Usage varies among local organisations, with both names being used. Examples are City of Derry Airport, City of Derry Rugby Club, Derry City FC and the Protestant Apprentice Boys of Derry, as opposed to Londonderry Port, Londonderry YMCA Rugby Club and Londonderry Chamber of Commerce. The bishopric has always remained that of Derry, both in the (Protestant, formerly-established) Church of Ireland (now combined with the bishopric of Raphoe), and in the Roman Catholic Church. Most companies within the city choose local area names such as Pennyburn, Rosemount or "Foyle" from the River Foyle to avoid alienating the other community. Londonderry railway station is often referred to as Waterside railway station within the city but is called Derry/Londonderry at other stations. The council changed the name of the local government district covering the city to Derry on 7 May 1984, consequently renaming itself Derry City Council. This did not change the name of the city, although the city is coterminous with the district, and in law the city council is also the "Corporation of Londonderry" or, more formally, the "Mayor, Aldermen and Citizens of the City of Londonderry". The form "Londonderry" is used for the post town by the Royal Mail, however use of Derry will still ensure delivery.

The city is also nicknamed the Maiden City by virtue of the fact that its walls were never breached despite being besieged on three separate occasions in the 17th century, the most notable being the Siege of Derry of 1688-89. It is also nicknamed "Stroke City" by local broadcaster, Gerry Anderson, due to the 'politically correct' use of the oblique notation Derry/Londonderry (which appellation has itself been used by BBC Television). A recent addition to the landscape has been the erection of several large stone columns on main roads into the city welcoming drivers, euphemistically, to "the walled city".

The name Derry is very much in popular use throughout Ireland for the naming of places, and there are at least six towns bearing that name and at least a further 79 places. The word Derry often forms part of the place name, for example Derrybeg, Derryboy, Derrylea and Derrymore.

The names Derry and Londonderry are not limited to Ireland. There is a town called Derry situated right beside another town called Londonderry in New Hampshire in the United States. There are also Londonderrys in Yorkshire, England, in Vermont, United States, in Nova Scotia, Canada, and in northern and eastern Australia. Londonderry Island is situated off Tierra del Fuego in Chile.

Derry is also a fictional town in Maine, United States, used in some Stephen King novels.

Derry is the only remaining completely intact walled city in Ireland and one of the finest examples of a walled city in Europe. The walls constitute the largest monument in State care in Northern Ireland and, as the last walled city to be built in Europe, stands as the most complete and spectacular.

The Walls were built in 1613–1619 by The Honourable The Irish Society as defences for early 17th century settlers from England and Scotland. The Walls, which are approximately in circumference and which vary in height and width between , are completely intact and form a walkway around the inner city. They provide a unique promenade to view the layout of the original town which still preserves its Renaissance style street plan. The four original gates to the Walled City are Bishop's Gate, Ferryquay Gate, Butcher Gate and Shipquay Gate. Three further gates were added later, Magazine Gate, Castle Gate and New Gate, making seven gates in total. Historic buildings within the walls include the 1633 Gothic cathedral of St Columb, the Apprentice Boys Memorial Hall and the courthouse.

It is one of the few cities in Europe that never saw its fortifications breached, withstanding several sieges including one in 1689 which lasted 105 days, hence the city's nickname, The Maiden City.

Derry is one of the oldest continuously inhabited places in Ireland. The earliest historical references date to the 6th century when a monastery was founded there by St Columba or Colmcille, a famous saint from what is now County Donegal, but for thousands of years before that people had been living in the vicinity.

Before leaving Ireland to spread Christianity elsewhere, Colmcille founded a monastery at Derry (which was then called "Doire Calgach"), on the west bank of the Foyle. According to oral and documented history, the site was granted to Colmcille by a local king. The monastery then remained in the hands of the federation of Columban churches who regarded Colmcille as their spiritual mentor. The year 546 is often referred to as the date that the original settlement was founded. However, it is now accepted by historians that this was an erroneous date assigned by medieval chroniclers. It is accepted that between the 6th century and the 11th century, Derry was known primarily as a monastic settlement.

The town became strategically more significant during the Tudor conquest of Ireland and came under frequent attack. During O'Doherty's Rebellion in 1608 it was attacked by Sir Cahir O'Doherty, Irish chieftain of Inishowen, who burnt much of the town and killed the governor George Paulet. The soldier and statesman Sir Henry Docwra made vigorous efforts to develop the town, earning the reputation of being " the founder of Derry"; but he was accused of failing to prevent the O'Doherty attack, and returned to England.

What became the City of Derry was part of the relatively new County Donegal up until 1610. In that year, the west bank of the future city was transferred by the English Crown to The Honourable The Irish Society and was combined with County Coleraine, part of County Antrim and a large portion of County Tyrone to form County Londonderry. Planters organised by London livery companies through The Honourable The Irish Society arrived in the 17th century as part of the Plantation of Ulster, and rebuilt the town with high walls to defend it from Irish insurgents who opposed the plantation. The aim was to settle Ulster with a population supportive of the Crown. It was then renamed "Londonderry".

This city was the first planned city in Ireland: it was begun in 1613, with the walls being completed in 1619, at a cost of £10,757. The central diamond within a walled city with four gates was thought to be a good design for defence. The grid pattern chosen was subsequently much copied in the colonies of British North America. The charter initially defined the city as extending three Irish miles (about 6.1 km) from the centre.

The modern city preserves the 17th century layout of four main streets radiating from a central Diamond to four gateways  – Bishop's Gate, Ferryquay Gate, Shipquay Gate and Butcher's Gate. The city's oldest surviving building was also constructed at this time: the 1633 Plantation Gothic cathedral of St Columb. In the porch of the cathedral is a stone that records completion with the inscription: "If stones could speake, then London's prayse should sound, Who built this church and cittie from the grounde."

During the 1640s, the city suffered in the Wars of the Three Kingdoms, which began with the Irish Rebellion of 1641, when the Gaelic Irish insurgents made a failed attack on the city. In 1649 the city and its garrison, which supported the republican Parliament in London, were besieged by Scottish Presbyterian forces loyal to King Charles I. The Parliamentarians besieged in Derry were relieved by a strange alliance of Roundhead troops under George Monck and the Irish Catholic general Owen Roe O'Neill. These temporary allies were soon fighting each other again however, after the landing in Ireland of the New Model Army in 1649. The war in Ulster was finally brought to an end when the Parliamentarians crushed the Irish Catholic Ulster army at the Battle of Scarrifholis, near Letterkenny in nearby County Donegal, in 1650.

During the Glorious Revolution, only Derry and nearby Enniskillen had a Protestant garrison by November 1688. An army of around 1,200 men, mostly ""Redshanks"" (Highlanders), under Alexander Macdonnell, 3rd Earl of Antrim, was slowly organised (they set out on the week William of Orange landed in England). When they arrived on 7 December 1688 the gates were closed against them and the Siege of Derry began. In April 1689, King James came to the city and summoned it to surrender. The King was rebuffed and the siege lasted until the end of July with the arrival of a relief ship.

The city was rebuilt in the 18th century with many of its fine Georgian style houses still surviving. The city's first bridge across the River Foyle was built in 1790. During the 18th and 19th centuries the port became an important embarkation point for Irish emigrants setting out for North America. Some of these founded the colonies of Derry and Londonderry in the state of New Hampshire.

Also during the 19th century, it became a destination for migrants fleeing areas more severely affected by the Irish Potato Famine. One of the most notable shipping lines was the McCorkell Line operated by Wm. McCorkell & Co. Ltd. from 1778. The McCorkell's most famous ship was the Minnehaha, which was known as the "Green Yacht from Derry".

During World War I the city contributed over 5,000 men to the British Army from Catholic and Protestant families.

During the Irish War of Independence, the area was rocked by sectarian violence, partly prompted by the guerilla war raging between the Irish Republican Army and British forces, but also influenced by economic and social pressures. By mid-1920 there was severe sectarian rioting in the city. Many lives were lost and in addition many Catholics and Protestants were expelled from their homes during this communal unrest. After a week's violence, a truce was negotiated by local politicians on both unionist and republican sides.

In 1921, following the Anglo-Irish Treaty and the Partition of Ireland, it unexpectedly became a 'border city', separated from much of its traditional economic hinterland in County Donegal.

During World War II, the city played an important part in the Battle of the Atlantic.
Ships from the Royal Navy, the Royal Canadian Navy, and other Allied navies were stationed in the city and the United States military established a base. Over 20,000 Royal Navy, 10,000 Royal Canadian Navy, and 6,000 American Navy personnel were stationed in the city during the war.
The establishment of the American presence in the city was the result of a secret agreement between the Americans and the British before the Americans entered the war. It was the first American naval base in Europe and the terminal for American convoys en route to Europe.

The reason for such a high degree of military and naval activity was self-evident: Derry was the United Kingdom's westernmost port; indeed, the city was the westernmost Allied port in Europe: thus, Derry was a crucial jumping-off point, together with Glasgow and Liverpool, for the shipping convoys that ran between Europe and North America. The large numbers of military personnel in Derry substantially altered the character of the city, bringing in some outside colour to the local area, as well as some cosmopolitan and economic buoyancy during these years. Several airfields were built in the outlying regions of the city at this time, Maydown, Eglinton and Ballykelly. RAF Eglinton went on to become City of Derry Airport.

The city contributed significant number of men to the war effort throughout the services, most notably the 500 men in the 9th (Londonderry) Heavy Anti-Aircraft Regiment, known as the 'Derry Boys'. This regiment served in North Africa, the Sudan, Italy and mainland UK. Many others served in the Merchant Navy taking part in the convoys that supplied the UK and Russia during the war.

The border location of the city, and influx of trade from the military convoys allowed for significant smuggling operations to develop in the city.

At the conclusion of the Second World War, eventually some 60 U-boats of the German Kriegsmarine ended in the city's harbour at Lisahally after their surrender. The initial surrender was attended by Admiral Sir Max Horton, Commander-in-Chief of the Western Approaches, and Sir Basil Brooke, third Prime Minister of Northern Ireland.

The city languished after the second world war, with unemployment and development stagnating. A large campaign, led by the University for Derry Committee, to have Northern Ireland's second university located in the city, ended in failure.

Derry was a focal point for the nascent civil rights movement in Northern Ireland.
Catholics were discriminated against under Unionist government in Northern Ireland, both politically and economically. In the late 1960s the city became the flashpoint of disputes about institutional gerrymandering. Political scientist John Whyte explains that:
All the accusations of gerrymandering, practically all the complaints about housing and regional policy, and a disproportionate amount of the charges about public and private employment come from this area. The area – which consisted of Counties Tyrone and Fermanagh, Londonderry County Borough, and portions of Counties Londonderry and Armagh – had less than a quarter of the total population of Northern Ireland yet generated not far short of three-quarters of the complaints of discrimination...The unionist government must bear its share of responsibility. It put through the original gerrymander which underpinned so many of the subsequent malpractices, and then, despite repeated protests, did nothing to stop those malpractices continuing. The most serious charge against the Northern Ireland government is not that it was directly responsible for widespread discrimination, but that it allowed discrimination on such a scale over a substantial segment of Northern Ireland.
A civil rights demonstration in 1968 led by the Northern Ireland Civil Rights Association was banned by the Government and blocked using force by the Royal Ulster Constabulary. The events that followed the August 1969 Apprentice Boys parade resulted in the Battle of the Bogside, when Catholic rioters fought the police, leading to widespread civil disorder in Northern Ireland and is often dated as the starting point of the Troubles.

On Sunday 30 January 1972, 13 unarmed civilians were shot dead by British paratroopers during a civil rights march in the Bogside area. Another 13 were wounded and one further man later died of his wounds. This event came to be known as Bloody Sunday.

The conflict which became known as the Troubles is widely regarded as having started in Derry with the Battle of the Bogside. The Civil Rights movement had also been very active in the city. In the early 1970s the city was heavily militarised and there was widespread civil unrest. Several districts in the city constructed barricades to control access and prevent the forces of the state from entering.

Violence eased towards the end of the Troubles in the late 1980s and early 1990s. Irish journalist Ed Maloney claims in "The Secret History of the IRA" that republican leaders there negotiated a "de facto" ceasefire in the city as early as 1991. Whether this is true or not, the city did see less bloodshed by this time than Belfast or other localities.

The city was visited by a killer whale in November 1977 at the height of the Troubles; it was dubbed Dopey Dick by the thousands who came from miles around to see him.

From 1613 the city was governed by the Londonderry Corporation. In 1898 this became Londonderry County Borough Council, until 1969 when administration passed to the unelected Londonderry Development Commission. In 1973 a new district council with boundaries extending to the rural south-west was established under the name Londonderry City Council, renamed in 1984 to Derry City Council, consisting of five electoral areas: Cityside, Northland, Rural, Shantallow and Waterside. The council of 30 members was re-elected every four years. As of the 2011 election, 14 Social Democratic and Labour Party (SDLP) members, ten Sinn Féin, five Democratic Unionist Party (DUP), and one Ulster Unionist Party (UUP) made up the council. The mayor and deputy mayor were elected annually by councillors. The local authority boundaries corresponded to the Foyle constituency of the Parliament of the United Kingdom and the Foyle constituency of the Northern Ireland Assembly. In European Parliament elections, it was part of the Northern Ireland constituency.

The council merged with Strabane District Council in April 2015 under local government reorganisation to become Derry and Strabane District Council.

The councillors elected in 2014 for the city are:

The devices on the city's arms are a skeleton and a three-towered castle on a black field, with the "chief" or top third of the shield depicting the arms of the City of London: a red cross and sword on white. In the centre of the cross is a gold harp.
The blazon of the arms is as follows:

"Sable, a human skeleton Or seated upon a mossy stone proper and in dexter chief a castle triple towered argent on a chief also argent a cross gules thereon a harp or and in the first quarter a sword erect gules"

According to documents in the College of Arms in London and the Office of the Chief Herald of Ireland in Dublin, the arms of the city were confirmed in 1613 by Daniel Molyneux, Ulster King of Arms. The College of Arms document states that the original arms of the City of Derry were "ye picture of death (or a skeleton) on a moissy stone & in ye dexter point a castle" and that upon grant of a charter of incorporation and the renaming of the city as Londonderry in that year the first mayor had requested the addition of a "chief of London".

Theories have been advanced as to the meaning of the "old" arms of Derry, before the addition of the chief bearing the arms of the City of London:

In 1979, Londonderry City Council, as it was then known, commissioned a report into the city's arms and insignia, as part of the design process for an heraldic badge. The published report found that there was no basis for any of the popular explanations for the skeleton and that it was "purely symbolic and does not refer to any identifiable person".

The 1613 records of the arms depicted a harp in the centre of the cross, but this was omitted from later depictions of the city arms, and in the Letters Patent confirming the arms to Londonderry Corporation in 1952. In 2002 Derry City Council applied to the College of Arms to have the harp restored to the city arms, and Garter and Norroy & Ulster Kings of Arms accepted the 17th century evidence, issuing letters patent to that effect in 2003.

The motto attached to the coat of arms reads in Latin, "Vita, Veritas, Victoria". This translates into English as, "Life, Truth, Victory".

Derry is characterised by its distinctively hilly topography. The River Foyle forms a deep valley as it flows through the city, making Derry a place of very steep streets and sudden, startling views. The original walled city of Londonderry lies on a hill on the west bank of the River Foyle. In the past, the river branched and enclosed this wooded hill as an island; over the centuries, however, the western branch of the river dried up and became a low-lying and boggy district that is now called the Bogside.

Today, modern Derry extends considerably north and west of the city walls and east of the river. The half of the city the west of the Foyle is known as the Cityside and the area east is called the Waterside. The Cityside and Waterside are connected by the Craigavon Bridge and Foyle Bridge, and by a foot bridge in the centre of the city called Peace Bridge. The district also extends into rural areas to the southeast of the city.

This much larger city, however, remains characterised by the often extremely steep hills that form much of its terrain on both sides of the river. A notable exception to this lies on the north-eastern edge of the city, on the shores of Lough Foyle, where large expanses of sea and mudflats were reclaimed in the middle of the 19th century. Today, these slob lands are protected from the sea by miles of sea walls and dikes. The area is an internationally important bird sanctuary, ranked among the top 30 wetland sites in the UK.

Other important nature reserves lie at Ness Country Park, east of Derry; and at Prehen Wood, within the city's south-eastern suburbs.

Derry has, like most of Ireland, a temperate maritime climate according to the Köppen climate classification system. The nearest official Met Office Weather Station for which climate data is available is Carmoney, just west of City of Derry Airport and about north east of the city centre. However, observations ceased in 2004 and the nearest Weather Station is currently Ballykelly, due east north east. Typically, 27 nights of the year will report an air frost at Ballykelly, and at least 1 mm of precipitation will be reported on 170 days (1981–2010 averages).

The lowest temperature recorded at Carmoney was on 27 December 1995.

Derry Urban Area (DUA), including the city and the neighbouring settlements of Culmore, Newbuildings and Strathfoyle, is classified as a city by the Northern Ireland Statistics and Research Agency (NISRA) since its population exceeds 75,000. On census day (27 March 2011) there were 105,066 people living in Derry Urban Area. Of these, 27% were aged under 16 years and 14% were aged 60 and over; 49% of the population were male and 51% were female; 75% were from a Roman Catholic background and 23% (up three per cent from 2001) were from a Protestant background.

The mid-2006 population estimate for the wider Derry City Council area was 107,300. Population growth in 2005/06 was driven by natural change, with net out-migration of approximately 100 people.

The city was one of the few in Ireland to experience an increase in population during the Irish Potato Famine as migrants came to it from other, more heavily affected areas.

Concerns have been raised by both communities over the increasingly divided nature of the city. There were about 17,000 Protestants on the west bank of the River Foyle in 1971. The proportion rapidly declined during the 1970s; the 2011 census recorded 3,169 Protestants on the west bank, compared to 54,976 Catholics, and it is feared that the city could become permanently divided.

However, concerted efforts have been made by local community, church and political leaders from both traditions to redress the problem. A conference to bring together key actors and promote tolerance was held in October 2006. The Rt Rev. Dr Ken Good, the Church of Ireland Bishop of Derry and Raphoe, said he was happy living on the cityside. "I feel part of it. It is my city and I want to encourage other Protestants to feel exactly the same", he said.

Support for Protestants in the district has been strong from the former SDLP city Mayor Helen Quigley. Cllr Quigley has made inclusion and tolerance key themes of her mayoralty. The Mayor Helen Quigley said it is time for "everyone to take a stand to stop the scourge of sectarian and other assaults in the city."

The economy of the district was based significantly on the textile industry until relatively recently. For many years women were commonly the sole wage earners working in the shirt factories while the men in comparison had high levels of unemployment. This led to significant male emigration. The history of shirt making in the city dates to 1831, said to have been started by William Scott and his family who first exported shirts to Glasgow. Within 50 years, shirt making in the city was the most prolific in the UK with garments being exported all over the world. It was known so well that the industry received a mention in "Das Kapital" by Karl Marx, when discussing the factory system:

The industry reached its peak in the 1920s employing around 18,000 people. In modern times however the textile industry declined due largely to lower Asian wages.

A long-term foreign employer in the area is Du Pont, which has been based at Maydown since 1958, its first European production facility. Originally Neoprene was manufactured at Maydown and subsequently followed by Hypalon. More recently Lycra and Kevlar production units were active. Thanks to a healthy worldwide demand for Kevlar which is made at the plant, the facility recently undertook a £40 million upgrade to expand its global Kevlar production. Du Pont has stated that contributing factors to its continued commitment to Maydown are "low labour costs, excellent communications, and tariff-free, easy access to the Britain and European continent."

In the last 15 years there has been a drive to increase inward investment in the city, more recently concentrating on digital industries. Currently the three largest private-sector employers are American firms. Economic successes have included call centres and a large investment by Seagate, which has operated a factory in the Springtown Industrial Estate since 1993. Seagate currently employs over 1,000 people, producing more than half of the company's total requirement for hard drive read-write heads.

A controversial new employer in the area was Raytheon Systems Limited, a software division of the American defence contractor, which was set up in Derry in 1999. Although some of the local people welcomed the jobs boost, others in the area objected to the jobs being provided by a firm involved heavily in the arms trade. Following four years of protest by the Foyle Ethical Investment Campaign, in 2004 Derry City Council passed a motion declaring the district a "A 'No – Go' Area for the Arms Trade", and in 2006 its offices were briefly occupied by anti-war protestors who became known as the Raytheon 9. In 2009, the company announced that it was not renewing its lease when it expired in 2010 and was looking for a new location for its operations.

Other significant multinational employers in the region include Firstsource of India, INVISTA, Stream International, Perfecseal, NTL, Northbrook Technology of the United States, Arntz Belting and Invision Software of Germany, and Homeloan Management of the UK. Major local business employers include Desmonds, Northern Ireland's largest privately owned company, manufacturing and sourcing garments, E&I Engineering, St. Brendan's Irish Cream Liqueur and McCambridge Duffy, one of the largest insolvency practices in the UK.

Even though the city provides cheap labour by standards in Western Europe, critics have noted that the grants offered by the Northern Ireland Industrial Development Board have helped land jobs for the area that only last as long as the funding lasts. This was reflected in questions to the Parliamentary Under-Secretary of State for Northern Ireland, Richard Needham, in 1990. It was noted that it cost £30,000 to create one job in an American firm in Northern Ireland.

Critics of investment decisions affecting the district often point to the decision to build a new university building in nearby (predominantly Protestant) Coleraine rather than developing the Ulster UniversityMagee Campus. Another major government decision affecting the city was the decision to create the new town of Craigavon outside Belfast, which again was detrimental to the development of the city. Even in October 2005, there was perceived bias against the comparatively impoverished North West of the province, with a major civil service job contract going to Belfast. Mark Durkan, the Social Democratic and Labour Party (SDLP) leader and Member of Parliament (MP) for Foyle was quoted in the "Belfast Telegraph" as saying:

In July 2005, the Irish Minister for Finance, Brian Cowen, called for a joint task force to drive economic growth in the cross border region. This would have implications for Counties Londonderry, Tyrone, and Donegal across the border.

The city is the north west's foremost shopping district, housing two large shopping centres along with numerous shop packed streets serving much of the greater county, as well as Tyrone and Donegal.

The city centre has two main shopping centres; the Foyleside Shopping Centre which has 45 stores and 1,430 parking spaces, and the Richmond Centre, which has 39 retail units. The Quayside Shopping Centre also serves the city-side and there is also Lisnagelvin Shopping Centre in the Waterside. These centres, as well as local-run businesses, feature numerous national and international stores. A recent addition was the Crescent Link Retail Park located in the Waterside with many international chain stores, including Homebase, Currys & PC World (stores combined), Carpet Right, Maplin, Argos Extra, Toys R Us, Halfords, DW Sports (formerly JJB Sports), Pets at Home, Next Home, Starbucks, McDonald's, Tesco Express and M&S Simply Food. In the short period of time that this site has been operational, it has quickly grown to become the second largest retail park in Northern Ireland (second only to Sprucefield in Lisburn). Plans have also been approved for Derry's first Asda store, which will be located at the retail park sharing a unit with Homebase. Sainsbury's also applied for planning permission for a store at Crescent Link, but Environment Minister Alex Attwood turned it down.

Until the store's closure in March 2016, the city was also home to the world's oldest independent department store, Austins. Established in 1830, Austins predates Jenners of Edinburgh by 5 years, Harrods of London by 15 years and Macy's of New York by 25 years. The store's five-story Edwardian building is located within the walled city in the area known as The Diamond.

Derry is renowned for its architecture. This can be primarily ascribed to the formal planning of the historic walled city of Derry at the core of the modern city. This is centred on the Diamond with a collection of late Georgian, Victorian and Edwardian buildings maintaining the gridlines of the main thoroughfares (Shipquay Street, Ferryquay Street, Butcher Street and Bishop Street) to the City Gates. St Columb's Cathedral does not follow the grid pattern reinforcing its civic status. This Church of Ireland Cathedral was the first post-Reformation Cathedral built for an Anglican church. The construction of the Roman Catholic St Eugene's Cathedral in the Bogside in the 19th-century was another major architectural addition to the city. The Townscape Heritage Initiative has funded restoration works to key listed buildings and other older structures.

In the three centuries since their construction, the city walls have been adapted to meet the needs of a changing city. The best example of this adaptation is the insertion of three additional gates – Castle Gate, New Gate and Magazine Gate – into the walls in the course of the 19th century. Today, the fortifications form a continuous promenade around the city centre, complete with cannon, avenues of mature trees and views across Derry. Historic buildings within the city walls include St Augustine's Church, which sits on the city walls close to the site of the original monastic settlement; the copper-domed Austin's department store, which claims to the oldest such store in the world; and the imposing Greek Revival Courthouse on Bishop Street. The red-brick late-Victorian Guildhall, also crowned by a copper dome, stands just beyond Shipquay Gate and close to the river front.

There are many museums and sites of interest in and around the city, including the Foyle Valley Railway Centre, the Amelia Earhart Centre And Wildlife Sanctuary, the Apprentice Boys Memorial Hall, Ballyoan Cemetery, The Bogside, numerous murals by the Bogside Artists, Derry Craft Village, Free Derry Corner, O'Doherty Tower (now home to part of the Tower Museum), the Harbour Museum, the Museum of Free Derry, Chapter House Museum, the Workhouse Museum, the Nerve Centre, St. Columb's Park and Leisure Centre, Creggan Country Park, The Millennium Forum and the Foyle and Craigavon bridges.

Attractions include museums, a vibrant shopping centre and trips to the Giant's Causeway, which is approximately away, though poorly connected by public transport. Lonely Planet called Derry the fourth best city in the world to see in 2013.

In 2011, on the 25th of June, the Peace Bridge opened. It is a cycle and foot bridge that begins from the Guild Hall in the city centre of Derry City to Ebrington Square and St Columb’s Park on the far side of the River Foyle. It symbolizes the unity of the Protestant community and the Nationalist community who are settled on either sides of the Foyle River. "The Derry Peace Bridge has become an integral part of Derry City’s infrastructure and has changed the way local people use and view their city with over 3 million people having crossed it so far and many of the locals using it daily".

Future projects include the Walled City Signature Project, which intends to ensure that the city's walls become a world class tourist experience. The Ilex Urban Regeneration Company is charged with delivering several landmark redevelopments. It has taken control of two former British Army barracks in the centre of the city. The Ebrington site is nearing completion and is linked to the city centre by the new Peace Bridge.

The transport network is built out of a complex array of old and modern roads and railways throughout the city and county. The city's road network also makes use of two bridges to cross the River Foyle, the Craigavon Bridge and the Foyle Bridge, the longest bridge in Ireland. Derry also serves as a major transport hub for travel throughout nearby County Donegal.

In spite of it being the second city of Northern Ireland (and it being the second-largest city in all of Ulster), road and rail links to other cities are below par for its standing. Many business leaders claim that government investment in the city and infrastructure has been badly lacking. Some have stated that this is due to its outlying border location whilst others have cited a sectarian bias against the region west of the River Bann due to its high proportion of Catholics. There is no direct motorway link with Dublin or Belfast. The rail link to Belfast has been downgraded over the years so that, presently, it is not a viable alternative to the roads for industry to rely on. There are currently plans for £1 billion worth of transport infrastructure investment in and around the district. Planned upgrades to the A5 Dublin road agreed as part of the Good Friday Agreement and St. Andrews Talks fell through when the government of the Republic of Ireland reneged on its funding citing the recent economic crisis.

Most public transport in Northern Ireland is operated by the subsidiaries of Translink. Originally the city's internal bus network was run by Ulsterbus, which still provides the city's connections with other towns in Northern Ireland. The city's buses are now run by Ulsterbus Foyle, just as Translink Metro now provides the bus service in Belfast. The Ulsterbus Foyle network offers 13 routes across the city into the suburban areas, excluding an Easibus link which connects to the Waterside and Drumahoe, and a free Rail Link Bus runs from the Waterside Railway Station to the city centre. All buses leave from the Foyle Street Bus Station in the city centre.

Long-distance buses depart from Foyle Street Bus Station to destinations throughout Ireland. Buses are operated by both Ulsterbus and Bus Éireann on cross-border routes. Lough Swilly formerly operated buses to Co. Donegal, but the company entered liquidation and is no longer in operation. There is a half-hourly service to Belfast every day, called the Maiden City Flyer, which is the Goldline Express flagship route. There are hourly services to Strabane, Omagh, Coleraine, Letterkenny and Buncrana, and up to twelve services a day to bring people to Dublin. There is a daily service to Sligo, Galway, Shannon Airport and Limerick.

City of Derry Airport, the council-owned airport near Eglinton, has been growing in recent years with new investment in extending the runway and plans to redevelop the terminal. It is hoped that the new investment will add to the airport's currently limited array of domestic and international flights and reduce the annual subsidy of £3.5 million from the local council.

The A2 from Maydown to Eglinton, serving airport, has recently been turned into a dual carriageway. City of Derry airport is the main regional airport for County Donegal, County Londonderry and west County Tyrone as well as Derry City itself.

The airport is served by Ryanair with scheduled flights to Glasgow Airport and Liverpool, all year round with a summer schedule to Alicante and Faro.

Northern Ireland Railways (N.I.R.) has a single route from Londonderry railway station (also known as Waterside Station) on the Waterside to Belfast Great Victoria Street via , , , , Mossley West and Belfast Central. The service, which had been allowed to deteriorate in the 1990s, has since been improved by increased investment.

In 2008 the Department for Regional Development announced plans to have the track re-laid between Derry and Coleraine by 2013, add a passing loop to increase traffic capacity and increase the number of trains by introducing two additional diesel multiple units. The £86 million plan will reduce the journey time to Belfast by 30 minutes and allow commuter trains to arrive before 9 a.m. for the first time. Many still do not use the train, because, at over two hours, it is slower centre-to-centre than the 100-minute Ulsterbus Goldline Express service.

Throughout the first half of the 20th century the city was served by four different railways that between them linked the city with much of the province of Ulster, plus a harbour railway network that linked the other four lines. There was also a tramway on the City side of the Foyle.

Derry's first railway was the Irish gauge () Londonderry and Enniskillen Railway (L&ER). Construction began in 1845 from a temporary station at Cow Market on the City side of the Foyle, reached Strabane in 1847 and was extended from Cow Market to its permanent terminus at Foyle Road in 1850. The L&ER reached Omagh in 1852 and Enniskillen in 1854, and was absorbed into the Great Northern Railway (Ireland) in 1883.

The Londonderry and Coleraine Railway (L&CR), also Irish gauge, reached the city in 1852 and opened its terminus at Waterside. The Belfast and Northern Counties Railway leased the line from 1861 and took it over in 1871.

The Londonderry and Lough Swilly Railway opened between Farland Point on Lough Swilly and a temporary terminus at Pennyburn in 1863. In 1866 it extended from Pennyburn to its permanent terminus at Graving Dock. The L&LSR was Irish gauge until 1885, when it was converted to narrow gauge for through running with the Letterkenny Railway.

The Londonderry Port and Harbour Commissioners (LPHC) linked Graving Dock and Foyle Road stations with a railway through Middle Quay in 1867, and linked this line with Waterside station by a railway over the new Carlisle Bridge in 1868. The bridge was replaced in 1933 with the double-deck Craigavon Bridge, with the LPHC railway on its lower deck.

In 1900 the gauge Donegal Railway extended from Strabane to Derry, establishing a terminus at Victoria Road. This was next to Carlisle Bridge and had a junction with the LPHC railway. The LPHC line was altered to dual gauge which allowed gauge traffic between the Donegal Railway and L&LSR as well as Irish gauge traffic between the GNR and B&NCR. In 1906 the Northern Counties Committee (NCC, successor to the B&NCR) and the GNR jointly took over the Donegal Railway, making it the County Donegal Railways Joint Committee (CDRJC).

The United Kingdom Government subsidised both the L&LSR and the Donegal Railway to build long extensions into remote parts of County Donegal. By 1905 these served much of the county, making Derry (and also Strabane) a key rail hub for the county.

The City of Derry Tramways was opened in 1897. This was a standard gauge () line served by horse trams and was never electrified. The tramway had only one line, was long, and ran along the City side of the Foyle parallel to the LPHC's line on that side of the river. It was closed in 1919.

The partition of Ireland in 1922 turned the boundary with County Donegal into an international frontier. This changed trade patterns to the railways' detriment and placed border posts on every line to and from Derry except the NCC route to . The L&LSR crossed the border between Pennyburn and Bridge End, the CDRJC crossed just beyond Strabane, and the GNR line crossed twice between Derry and Strabane. Stops for customs inspections greatly delayed trains and disrupted timekeeping.

Over the next few years customs agreements between the two states enabled GNR trains to and from Derry to pass through the Free State without inspection unless they were scheduled to serve local stations on the west bank of the Foyle, and for goods on all railways to be carried between different parts of the Free State to pass through Northern Ireland under customs bond. However, local passenger and goods traffic continued to be delayed by customs examinations.

In the 1920s and 30s and again after the Second World War the railways also faced increasing road competition. The L&LSR closed its line in 1953, followed by the CDRJC in 1954. The Ulster Transport Authority took over the NCC in 1949 and the GNR's lines in Northern Ireland in 1958. The UTA also took over the LPHC railway, which it closed in 1962. In accordance with The Benson Report submitted to the Northern Ireland Government in 1963, the UTA closed the former GNR line to Derry in 1965.

Since 1965 the former L&CR line has been Derry's sole railway link. As such it has carried not only passenger services between Derry and Belfast but also CIÉ freight services using Derry as a railhead for Donegal.

The largest road investment in the north west's history is now (2010) taking place with building of the 'A2 Broadbridge Maydown to City of Derry Airport dualling' project and announcement of the 'A6 Londonderry to Dungiven Dualling Scheme' which will help to reduce the travel time to Belfast. The latter project brings a dual-carriageway link between Northern Ireland's two largest cities one step closer. The project is costing £320 million and is expected to be completed in 2016.

In October 2006 the Government of Ireland announced that it was to invest €1 billion in Northern Ireland; and one of the planned projects will be 'The A5 Western Transport Corridor', the complete upgrade of the A5 Derry – Omagh – Aughnacloy (– Dublin) road, around long, to dual carriageway standard.

It is not yet known if these two separate projects will connect at any point, although there have been calls for some form of connection between the two routes. In June 2008 Conor Murphy, Minister for Regional Development, announced that there will be a study into the feasibility of connecting the A5 and A6. Should it proceed, the scheme would most likely run from Drumahoe to south of Prehen along the south east of the City.

Londonderry Port at Lisahally is the United Kingdom's most westerly port and has capacity for 30,000-ton vessels. The Londonderry Port and Harbour Commissioners (LPHC) announced record turnover, record profits and record tonnage figures for the year ended March 2008. The figures are the result of a significant capital expenditure programme for the period 2000 to 2007 of about £22 million. Tonnage handled by LPHC increased almost 65% between 2000 and 2007, according to the latest annual results.

The port gave vital Allied service in the longest running campaign of the Second World War, the Battle of the Atlantic, and saw the surrender of the German U-Boat fleet at Lisahally on 8 May 1945.

The tidal River Foyle is navigable from the coast at Derry to approximately inland. In 1796, the Strabane Canal was opened, continuing the navigation a further southwards to Strabane. The canal was closed in 1962.

Derry is home to the Magee Campus of Ulster University, formerly Magee College. However, Lockwood's 1960s decision to locate Northern Ireland's second university in Coleraine rather than Derry helped contribute to the formation of the civil rights movement that ultimately led to The Troubles. Derry was the town more closely associated with higher learning, with Magee College already more than a century old by that time. In the mid-1980s a half-hearted attempt was made at rectifying this mistake by forming Magee College as a campus of the Ulster University but this has failed to stifle calls for the establishment of an independent University in Derry that can grow to it full potential. The campus has never thrived and currently only has 3,500 students out of a total Ulster University student population of 27,000. Ironically, although Coleraine is blamed by many in the city for 'stealing the University', it has only 5,000 students, the remaining 19,000 being based in Belfast.

The North West Regional College is also based in the city. In recent years it has grown to almost 30,000 students.

One of the two oldest secondary schools in Northern Ireland is located in Derry, Foyle and Londonderry College. It was founded in 1616 by the merchant taylors and remains a popular choice. Other secondary schools include St. Columb's College, Oakgrove Integrated College, St Cecilia's College, St Mary's College, St. Joseph's Boys' School, Lisneal College, Thornhill College, Lumen Christi College and St. Brigid's College. There are also numerous primary schools.

The city is home to sports clubs and teams. Both association football and Gaelic football are popular in the area.

In association football, the city's most prominent clubs include Derry City who play in the national league of the Republic of Ireland; Institute of the NIFL Championship and Oxford United Stars and Trojans, both of the Northern Ireland Intermediate League.
In addition to these clubs, who all play in national leagues, other clubs are based in the city. The local football league governed by the IFA is the North-West Junior League, which contains many clubs from the city, such as BBOB (Boys Brigade Old Boys) and Lincoln Courts. The city's other junior league is the Derry and District League and teams from the city and surrounding areas participate, including Don Boscos and Creggan Swifts. The Foyle Cup youth soccer tournament is held annually in the city. It has attracted many notable teams in the past, including Werder Bremen, IFK Göteborg and Ferencváros.

In Gaelic football Derry GAA are the county team and play in the Gaelic Athletic Association's National Football League, Ulster Senior Football Championship and All-Ireland Senior Football Championship. They also field hurling teams in the equivalent tournaments. There are many Gaelic games clubs in and around the city, for example Na Magha CLG, Steelstown GAC, Doire Colmcille CLG, Seán Dolans GAC, Na Piarsaigh CLG Doire Trasna and Slaughtmanus GAC.

There are many boxing clubs, the most well-known being "The Ring Boxing Club", which is based on the City side, and associated with Charlie Nash and John Duddy, amongst others.

A recent development has seen the formation of Rochester's Amateur Boxing club, bringing boxing to the residents of the city's Waterside.

Rugby Union is also quite popular in the city, with the City of Derry Rugby Club situated not far from the city centre. City of Derry won both the Ulster Towns Cup and the Ulster Junior Cup in 2009.
Londonderry YMCA RFC is another rugby club and is based in the village of Drumahoe which is in the outskirts of the city.

The city's only basketball club is North Star Basketball Club which has teams in the Basketball Northern Ireland senior and junior Leagues.

Cricket is also a popular sport in the city, particularly in the Waterside. The city is home to two cricket clubs, Brigade Cricket Club and Glendermott Cricket Club, both of whom play in the North West Senior League.

Golf is also a sport which is popular with many in the city. There are two golf clubs situated in the city, City of Derry Golf Club and Foyle International Golf Centre.

In recent years the city and surrounding countryside have become well known for their artistic legacy, producing Nobel Prize-winning poet Seamus Heaney, poet Seamus Deane, playwright Brian Friel, writer and music critic Nik Cohn, artist Willie Doherty, socio-political commentator and activist Eamonn McCann and bands such as The Undertones. The large political gable-wall murals of Bogside Artists, Free Derry Corner, the Foyle Film Festival, the Derry Walls, St Eugene's and St Columb's Cathedrals and the annual Halloween street carnival are popular tourist attractions. In 2010, Derry was named the UK's tenth 'most musical' city by PRS for Music.
In May 2013 a perpetual Peace Flame Monument was unveiled by Martin Luther King III and Presbyterian minister Rev. David Latimer. The flame was lit by children from both traditions in the city and is one of only 15 such flames across the world.

The local papers the "Derry Journal" (known as the "Londonderry Journal" until 1880) and the "Londonderry Sentinel" reflect the divided history of the city: the "Journal" was founded in 1772 and is Ireland's second oldest newspaper; the "Sentinel" newspaper was formed in 1829 when new owners of the "Journal" embraced Catholic Emancipation, and the editor left the paper to set up the "Sentinel".

There are numerous radio stations receivable: the largest stations based in the city are BBC Radio Foyle and the commercial station Q102.9.

There was a locally based television station, C9TV, one of only two local or 'restricted' television services in Northern Ireland, which ceased broadcasts in 2007.

The city's night-life is mainly focused on the weekends, with several bars and clubs providing "student nights" during the weekdays. Waterloo Street and Strand Road provide the main venues. Waterloo Street, a steep street lined with both Irish traditional and modern pubs, frequently has live rock and traditional music at night.


Notable people who were born or have lived in Derry include:



</doc>
<doc id="9058" url="https://en.wikipedia.org/wiki?curid=9058" title="European influence in Afghanistan">
European influence in Afghanistan

The European influence in Afghanistan refers to political, social, and mostly imperialistic influence several European nations and colonial powers have had on the historical development of Afghanistan.

After the decline of the Durrani dynasty in 1823, Dost Mohammad Khan established the Barakzai dynasty after becoming the next Emir of Afghanistan. It was not until 1826 that the energetic Dost Mohammad Khan was able to exert sufficient control over his brothers to take over the throne in Kabul, where he proclaimed himself the Shah.

Dost Mohammad achieved prominence among his brothers through clever use of the support of his mother's Qizilbash tribesmen and his own youthful apprenticeship under his brother, Fateh Khan. Among the many problems he faced was repelling Sikh encroachment on the Pashtun areas east of the Khyber Pass. After working assiduously to establish control and stability in his domains around Kabul, the Shah next chose to confront the warring Sikhs.

In 1834 Dost Mohammad defeated an invasion by the former ruler, Shuja Shah Durrani, but his absence from Kabul gave the Sikhs the opportunity to expand westward. Ranjit Singh's forces occupied Peshawar, moving from there into territory ruled directly by Kabul. In 1836 Dost Mohammad's forces, under the command of his son Akbar Khan, defeated the Sikhs at the Battle of Jamrud, a post fifteen kilometres west of Peshawar. This was a pyrrhic victory and they failed to fully dislodge the Sikhs from Jamrud. The Afghan leader did not follow up this triumph by retaking Peshawar, however, but instead contacted Lord Auckland, the new British governor general in British India, for help in dealing with the Sikhs. With this letter, Dost Mohammad formally set the stage for British intervention in Afghanistan. At the heart of the Great Game lay the willingness of Britain and Russia to subdue, subvert, or subjugate the small independent states that lay between Russia and British India.

The British became the major power in the Indian subcontinent after the Treaty of Paris (1763) and began to show interest in Afghanistan as early as their 1809 treaty with Shuja Shah Durrani. It was the threat of the expanding Russian Empire beginning to push for an advantage in the Afghanistan region that placed pressure on British India, in what became known as the "Great Game". The Great Game set in motion the confrontation of the British and Russian empires, whose spheres of influence moved steadily closer to one another until they met in Afghanistan. It also involved Britain's repeated attempts to impose a puppet government in Kabul. The remainder of the 19th century saw greater European involvement in Afghanistan and her surrounding territories and heightened conflict among the ambitious local rulers as Afghanistan's fate played out globally.

The débâcle of the Afghan civil war left a vacuum in the Hindu Kush area that concerned the British, who were well aware of the many times in history it had been employed as the invasion route to South Asia. In the early decades of the 19th century, it became clear to the British that the major threat to their interests in India would not come from the fragmented Afghan empire, the Iranians, or the French, but from the Russians, who had already begun a steady advance southward from the Caucasus winning decisive wars against the Ottoman Turks and Qajar Persians.

At the same time, the Russians feared permanent British occupation in Central Asia as the British encroached northward, taking the Punjab, Sindh, and Kashmir; later to become Pakistan. The British viewed Russia's absorption of the Caucasus, the Kyrgyz and Turkmen lands, the Khanate of Khiva, and the Emirate of Bukhara with equal suspicion as a threat to their interests in the Asian subcontinent.
In addition to this rivalry between Britain and Russia, there were two specific reasons for British concern over Russia's intentions. First was the Russian influence at the Iranian court, which prompted the Russians to support Iran in its attempt to take Herat, historically the western gateway to Afghanistan and northern India. In 1837 Iran advanced on Herat with the support and advice of Russian officers. The second immediate reason was the presence in Kabul in 1837 of a Russian agent, Yan Vitkevich, who was ostensibly there, as was the British agent Alexander Burnes, for commercial discussions.

The British demanded that Dost Mohammad sever all contact with the Iranians and Russians, remove Vitkevich from Kabul, surrender all claims to Peshawar, and respect Peshawar's independence as well as that of Kandahar, which was under the control of his brothers at the time. In return, the British government intimated that it would ask Ranjit Singh to reconcile with the Afghans. When Auckland refused to put the agreement in writing, Dost Mohammad turned his back on the British and began negotiations with Vitkevich.

In 1838 Auckland, Ranjit Singh, and Shuja signed an agreement stating that Shuja would regain control of Kabul and Kandahar with the help of the British and Sikhs; he would accept Sikh rule of the former Afghan provinces already controlled by Ranjit Singh, and that Herat would remain independent. In practice, the plan replaced Dost Mohammad with a British figurehead whose autonomy would be as limited as that of other Indian princes.

It soon became apparent to the British that Sikh participation, advancing toward Kabul through the Khyber Pass while Shuja and the British advanced through Kandahar, would not be forthcoming. Auckland's plan in the spring of 1838 was for the Sikhs to place Shuja on the Afghan throne, with British support. By the end of the summer however, the plan had changed; now the British alone would impose the pliant Shuja Shah.

To justify his plan, the Governor-General of India Lord Auckland issued the Simla Manifesto in October 1838, setting forth the necessary reasons for British intervention in Afghanistan. The manifesto stated that in order to ensure the welfare of India, the British must have a trustworthy ally on India's western frontier. The British pretense that their troops were merely supporting Shah Shujah's small army in retaking what was once his throne fooled no one. Although the Simla Manifesto stated that British troops would be withdrawn as soon as Shuja was installed in Kabul, Shuja's rule depended entirely on British arms to suppress rebellion and on British funds to buy the support of tribal chiefs. The British denied that they were invading Afghanistan, instead claiming they were merely supporting its legitimate Shuja government "against foreign interference and factious opposition".

In November 1841 insurrection and massacre flared up in Kabul. The British vacillated and disagreed and were beleaguered in their inadequate cantonments. The British negotiated with the most influential sirdars, cut off as they were by winter and insurgent tribes from any hope of relief. Mohammad Akbar Khan, son of the captive Dost Mohammad, arrived in Kabul and became effective leader of the sirdars. At a conference with them Sir William MacNaghten was killed, but in spite of this, the sirdars' demands were agreed to by the British and they withdrew. During the withdrawal they were attacked by Ghilzai tribesmen and in running battles through the snowbound passes nearly the entire column of 4,500 troops and 12,000 camp followers were killed. Of the British only one, Dr. William Brydon, reached Jalalabad, while a few others were captured.

Afghan forces loyal to Akbar Khan besieged the remaining British contingents at Kandahar, Ghazni and Jalalabad. Ghazni fell, but the other garrisons held out, and with the help of reinforcements from India their besiegers were defeated. While preparations were under way for a renewed advance on Kabul, the new Governor-General Lord Ellenborough ordered British forces to leave Afghanistan after securing the release of the prisoners from Kabul and taking reprisals. The forces from Kandahar and Jalalabad again defeated Akbar Khan, retook Ghazni and Kabul, inflicted widespread devastation and rescued the prisoners before withdrawing through the Khyber Pass.

After months of chaos in Kabul, Mohammad Akbar Khan secured local control and in April 1843 his father Dost Mohammad, who had been released by the British, returned to the throne in Afghanistan. In the following decade, Dost Mohammad concentrated his efforts on reconquering Mazari Sharif, Konduz, Badakhshan, and Kandahar. Mohammad Akbar Khan died in 1845. During the Second Anglo-Sikh War (1848–49), Dost Mohammad's last effort to take Peshawar failed.

By 1854 the British wanted to resume relations with Dost Mohammad, whom they had essentially ignored in the intervening twelve years. The 1855 Treaty of Peshawar reopened diplomatic relations, proclaimed respect for each side's territorial integrity, and pledged both sides as friends of each other's friends and enemies of each other's enemies.

In 1857 an addendum to the 1855 treaty permitted a British military mission to become a presence in Kandahar (but not Kabul) during a conflict with the Persians, who had attacked Herat in 1856. During the Indian Rebellion of 1857, some British officials suggested restoring Peshawar to Dost Mohammad, in return for his support against the rebellious sepoys of the Bengal Army, but this view was rejected by British political officers on the North West frontier, who believed that Dost Mohammad would see this as a sign of weakness and turn against the British.

In 1863 Dost Mohammad retook Herat with British acquiescence. A few months later, he died. Sher Ali Khan, his third son, and proclaimed successor, failed to recapture Kabul from his older brother, Mohammad Afzal (whose troops were led by his son, Abdur Rahman) until 1868, after which Abdur Rahman retreated across the Amu Darya and bided his time.

In the years immediately following the First Anglo-Afghan War, and especially after the Indian rebellion of 1857 against the British in India, Liberal Party governments in London took a political view of Afghanistan as a buffer state. By the time Sher Ali had established control in Kabul in 1868, he found the British ready to support his regime with arms and funds, but nothing more. Over the next ten years, relations between the Afghan ruler and Britain deteriorated steadily. The Afghan ruler was worried about the southward encroachment of Russia, which by 1873 had taken over the lands of the khan, or ruler, of Khiva. Sher Ali sent an envoy seeking British advice and support. The previous year the British had signed an agreement with the Russians in which the latter agreed to respect the northern boundaries of Afghanistan and to view the territories of the Afghan Emir as outside their sphere of influence. The British, however, refused to give any assurances to the disappointed Sher Ali.

After tension between Russia and Britain in Europe ended with the June 1878 Congress of Berlin, Russia turned its attention to Central Asia. That same summer, Russia sent an uninvited diplomatic mission to Kabul. Sher Ali tried, but failed, to keep them out. Russian envoys arrived in Kabul on 22 July 1878 and on 14 August, the British demanded that Sher Ali accept a British mission too.
The amir not only refused to receive a British mission but threatened to stop it if it were dispatched. Lord Lytton, the viceroy, ordered a diplomatic mission to set out for Kabul in September 1878 but the mission was turned back as it approached the eastern entrance of the Khyber Pass, triggering the Second Anglo-Afghan War. A British force of about 40,000 fighting men was distributed into military columns which penetrated Afghanistan at three different points. An alarmed Sher Ali attempted to appeal in person to the tsar for assistance, but unable to do so, he returned to Mazari Sharif, where he died on 21 February 1879.

With British forces occupying much of the country, Sher Ali's son and successor, Mohammad Yaqub Khan, signed the Treaty of Gandamak in May 1879 to prevent a British invasion of the rest of the country. According to this agreement and in return for an annual subsidy and vague assurances of assistance in case of foreign aggression, Yaqub relinquished control of Afghan foreign affairs to the British. British representatives were installed in Kabul and other locations, British control was extended to the Khyber and Michni Passes, and Afghanistan ceded various frontier areas and Quetta to Britain. The British army then withdrew. Soon afterwards, an uprising in Kabul led to the slaughter of Britain's Resident in Kabul, Sir Pierre Cavagnari and his guards and staff on 3 September 1879, provoking the second phase of the Second Afghan War. Major General Sir Frederick Roberts led the Kabul Field Force over the Shutargardan Pass into central Afghanistan, defeated the Afghan Army at Char Asiab on 6 October 1879 and occupied Kabul. Ghazi Mohammad Jan Khan Wardak staged an uprising and attacked British forces near Kabul in the Siege of the Sherpur Cantonment in December 1879, but his defeat there resulted in the collapse of this rebellion.
Yaqub Khan, suspected of complicity in the massacre of Cavagnari and his staff, was obliged to abdicate. The British considered a number of possible political settlements, including partitioning Afghanistan between multiple rulers or placing Yaqub's brother Ayub Khan on the throne, but ultimately decided to install his cousin Abdur Rahman Khan as emir instead. Ayub Khan, who had been serving as governor of Herat, rose in revolt, defeated a British detachment at the Battle of Maiwand in July 1880 and besieged Kandahar. Roberts then led the main British force from Kabul and decisively defeated Ayub Khan in September at the Battle of Kandahar, bringing his rebellion to an end. Abdur Rahman had confirmed the Treaty of Gandamak, leaving the British in control of the territories ceded by Yaqub Khan and ensuring British control of Afghanistan's foreign policy in exchange for protection and a subsidy. Abandoning the provocative policy of maintaining a British resident in Kabul, but having achieved all their other objectives, the British withdrew.

As far as British interests were concerned, Abdur Rahman answered their prayers: a forceful, intelligent leader capable of welding his divided people into a state; and he was willing to accept limitations to his power imposed by British control of his country's foreign affairs and the British buffer state policy. His twenty-one-year reign was marked by efforts to modernize and establish control of the kingdom, whose boundaries were delineated by the two empires bordering it. Abdur Rahman turned his considerable energies to what evolved into the creation of the modern state of Afghanistan.

He achieved this consolidation of Afghanistan in three ways. He suppressed various rebellions and followed up his victories with harsh punishment, execution, and deportation. He broke the stronghold of Pashtun tribes by forcibly transplanting them. He transplanted his most powerful Pashtun enemies, the Ghilzai, and other tribes from southern and south-central Afghanistan to areas north of the Hindu Kush with predominantly non-Pashtun populations. The last non-Muslim Afghans of Kafiristan north of Kabul were forcefully converted to Islam. Finally, he created a system of provincial governorates different from old tribal boundaries. Provincial governors had a great deal of power in local matters, and an army was placed at their disposal to enforce tax collection and suppress dissent. Abdur Rahman kept a close eye on these governors, however, by creating an effective intelligence system. During his reign, tribal organization began to be eroded as provincial government officials allowed land to change hands outside the traditional clan and tribal limits.

The Pashtuns battled and conquered the Uzbeks and forced them into the status of ruled people who were discriminated against. Out of anti-Russian strategic interests, the British assisted the Afghan conquest of the Uzbek Khanates, giving weapons to the Afghans and backing the Pashtun colonization of northern Afghanistan, which involved sending massive amounts of Pashtun colonists onto Uzbek land; British literature from the period demonized the Uzbeks.

In addition to forging a nation from the splintered regions making up Afghanistan, Abdur Rahman tried to modernize his kingdom by forging a regular army and the first institutionalized bureaucracy. Despite his distinctly authoritarian personality, Abdur Rahman called for a loya jirga, an assemblage of royal princes, important notables, and religious leaders. According to his autobiography, Abdur Rahman had three goals: subjugating the tribes, extending government control through a strong, visible army, and reinforcing the power of the ruler and the royal family.
During his visit to Rawalpindi in 1885, the Amir requested the Viceroy of India to depute a Muslim Envoy to Kabul who was noble birth and of ruling family background. Mirza Atta Ullah Khan, Sardar Bahadur s/o Khan Bahadur Mirza Fakir Ullah Khan (Saman Burj Wazirabad), a direct descendent of Jarral Rajput Rajas of Rajauri, was selected and approved by the Amir to be the British Envoy to Kabul.

Abdur Rahman also paid attention to technological advance. He brought foreign physicians, engineers (especially for mining), geologists, and printers to Afghanistan. He imported European machinery and encouraged the establishment of small factories to manufacture soap, candles, and leather goods. He sought European technical advice on communications, transport, and irrigation. Local Afghan tribes strongly resisted this modernization. Workmen making roads had to be protected by the army against local warriors. Nonetheless, despite these sweeping internal policies, Abdur Rahman's foreign policy was completely in foreign hands.

The first important frontier dispute was the Panjdeh crisis of 1885, precipitated by Russian encroachment into Central Asia. Having seized the Merv (now Mary) Oasis by 1884, Russian forces were directly adjacent to Afghanistan. Claims to the Panjdeh Oasis were in debate, with the Russians keen to take over all the region's Turkoman domains. After battling Afghan forces in the spring of 1885, the Russians seized the oasis. Russian and British troops were quickly alerted, but the two powers reached a compromise; Russia was in possession of the oasis, and Britain believed it could keep the Russians from advancing any farther. Without an Afghan say in the matter, the Joint Anglo-Russian Boundary Commission agreed that the Russians would relinquish the farthest territory captured in their advance but retain Panjdeh. This agreement on these border sections delineated for Afghanistan a permanent northern frontier at the Amu Darya, but also involved the loss of much territory, especially around Panjdeh.

The second section of Afghan border demarcated during Abdur Rahman's reign was in the Wakhan. The British insisted that Abdur Rahman accept sovereignty over this remote region, where unruly Kyrgyz held sway; he had no choice but to accept Britain's compromise. In 1895 and 1896, another Joint Anglo-Russian Boundary Commission agreed on the frontier boundary to the far northeast of Afghanistan, which bordered Chinese territory (although the Chinese did not formally accept this as a boundary between the two countries until 1964.)

For Abdur Rahman, delineating the boundary with India (through the Pashtun area) was far more significant, and it was during his reign that the Durand Line was drawn. Under pressure, Abdur Rahman agreed in 1893 to accept a mission headed by the British Indian foreign secretary, Sir Mortimer Durand, to define the limits of British and Afghan control in the Pashtun territories. Boundary limits were agreed on by Durand and Abdur Rahman before the end of 1893, but there is some question about the degree to which Abdur Rahman willingly ceded certain regions. There were indications that he regarded the Durand Line as a delimitation of separate areas of political responsibility, not a permanent international frontier, and that he did not explicitly cede control over certain parts (such as Kurram and Chitral) that were already in British control under the Treaty of Gandamak.

The Durand Line cut through tribes and bore little relation to the realities of demography or military strategy. The line laid the foundation not for peace between the border regions, but for heated disagreement between the governments of Afghanistan and British India, and later, Afghanistan and Pakistan over what came to be known as the issue of Pashtunistan or 'Land of the Pashtuns'. (See Siege of Malakand).

The clearest manifestation that Abdur Rahman had established control in Afghanistan was the peaceful succession of his eldest son, Habibullah Khan, to the throne on his father's death in October 1901. Although Abdur Rahman had fathered many children, he groomed Habibullah to succeed him, and he made it difficult for his other sons to contest the succession by keeping power from them and sequestering them in Kabul under his control.

Habibullah Khan, Abdur Rahman Khan's eldest son and child of a slave mother, kept a close watch on the palace intrigues revolving around his father's more distinguished wife (a granddaughter of Dost Mohammad), who sought the throne for her own son. Although made secure in his position as ruler by virtue of support from the army which was created by his father, Habibullah was not as domineering as Abdur Rahman. Consequently, the influence of religious leaders as well as that of Mahmud Tarzi, a cousin of the king, increased during his reign.

Mahmud Tarzi, a highly educated, well-traveled poet and journalist, founded an Afghan nationalist newspaper with Habibullah's agreement, and until 1919 he used the newspaper as a platform for rebutting clerical criticism of Western-influenced changes in government and society, for espousing full Afghan independence, and for other reforms. Tarzi's passionate Afghan nationalism influenced a future generation of Asian reformers.

The boundary with Iran was firmly delineated in 1904, replacing the ambiguous line made by a British commission in 1872. Agreement could not be reached, however, on sharing the waters of the Helmand River.

Like all foreign policy developments of this period affecting Afghanistan, the conclusion of the "Great Game" between Russia and Britain occurred without the Afghan ruler's participation. The 1907 Anglo-Russian Convention (the Convention of St. Petersburg) not only divided the region into separate areas of Russian and British influence but also established foundations for Afghan neutrality. The convention provided for Russian acquiescence that Afghanistan was now outside this sphere of influence, and for Russia to consult directly with Britain on matters relating to Russian-Afghan relations. Britain, for its part, would not occupy or annex Afghan territory, or interfere in Afghanistan's internal affairs.

During World War I, Afghanistan remained neutral despite pressure to support Turkey when its sultan proclaimed his nation's participation in what it considered a holy war. Habibullah did, however, entertain an Indo-German–Turkish mission in Kabul in 1915 that had as its titular head the Indian nationalist Mahendra Pratap and was led by Oskar Niedermayer and the German legate Werner Otto von Hentig. After much procrastination, he won an agreement from the Central Powers for a huge payment and arms provision in exchange for attacking British India. But the crafty Afghan ruler clearly viewed the war as an opportunity to play one side off against the other, for he also offered the British to resist a Central Powers attack on India in exchange for an end to British control of Afghan foreign policy.

Amanullah's ten years of reign initiated a period of dramatic change in Afghanistan in both foreign and domestic politics. Amanullah declared full independence and sparked the Third Anglo-Afghan War. Amanullah altered foreign policy in his new relations with external powers and transformed domestic politics with his social, political, and economic reforms. Although his reign ended abruptly, he achieved some notable successes, and his efforts failed as much due to the centrifugal forces of tribal Afghanistan and the machinations of Russia and Britain as to any political folly on his part.

Amanullah came to power just as the entente between Russia and Britain broke down following the Russian Revolution of 1917. Once again Afghanistan provided a stage on which the great powers played out their schemes against one another. Keen to modernise his country and free it from foreign domination, Amanullah, sought to shore up his powerbase. Amidst intrigue in the Afghan court, and political and civil unrest in India, he sought to divert attention from the internal divisions of Afghanistan and unite all faction behind him by attacking the British.

Using the civil unrest in India as an excuse to move troops to the Durand Line, Afghan troops crossed the border at the western end of the Khyber Pass on 3 May 1919 and occupied the village of Bagh, the scene of an earlier uprising in April. In response, the Indian government ordered a full mobilisation and on 6 May 1919 declared war. For the British it had come at a time when they were still recovering from the First World War. The troops that were stationed in India were mainly reserves and Territorials, who were awaiting demobilisation and keen to return to Britain, whilst the few regular regiments that were available were tired and depleted from five years of fighting.

Afghan forces achieved success in the initial days of the war, taking the British and Indians by surprise in two main thrusts as the Afghan regular army was joined by large numbers of Pashtun tribesmen from both sides of the border. A series of skirmishes then followed as the British and Indians recovered from their initial surprise. As a counterbalance to deficiencies in manpower and morale, the British had a considerable advantage in terms of equipment, possessing machine guns, armoured cars, motor transport, wireless communications and aircraft and it was the latter that would prove decisive.

British forces used airpower to shock the Afghans, and the King's home was directly attacked in what is the first case of aerial bombardment in Afghanistan's history. The attacks played a key role in forcing an armistice but brought an angry rebuke from King Amanullah. He wrote: "It is a matter of great regret that the throwing of bombs by zeppelins on London was denounced as a most savage act and the bombardment of places of worship and sacred spots was considered a most abominable operation. While we now see with our own eyes that such operations were a habit which is prevalent among all civilized people of the west"

The fighting concluded in August 1919 and Britain virtually dictated the terms of the Anglo-Afghan Treaty of 1919, a temporary armistice that provided, on one somewhat ambiguous interpretation, for Afghan self-determination in foreign affairs. Before final negotiations were concluded in 1921, however, Afghanistan had already begun to establish its own foreign policy without repercussions anyway, including diplomatic relations with the new government in the Soviet Union in 1919. During the 1920s, Afghanistan established diplomatic relations with most major countries.

On 20 February 1919, Habibullah Khan was assassinated on a hunting trip. He had not declared a succession, but left his third son, Amanullah Khan, in charge in Kabul. Amanullah did have an older brother, Nasrullah Khan. But, because Amanullah controlled both the national treasury and the army, Amanullah was well situated to seize power. The army's support allowed Amanullah to suppress other claims and imprison those relatives who would not swear loyalty to him. Within a few months, the new amir had gained the allegiance of most tribal leaders and established control over the cities.
Amanullah Khan's reforms were heavily influenced by Europe. This came through the influence of Mahmud Tarzi, who was both Amanullah Khan's father-in-law and Foreign Minister. Mahmud Tarzi, a highly educated, well-traveled poet, journalist, and diplomat, was a key figure that brought Western dress and etiquette to Afghanistan. He also fought for progressive reforms such as woman's rights, educational rights, and freedom of press. All of these influences, brought by Tarzi and others, were welcomed by Amanullah Khan.

In 1926, Amanullah ended the Emirate of Afghanistan and proclaimed the Kingdom of Afghanistan with himself as king. In 1927 and 1928, King Amanullah Khan and his wife Soraya Tarzi visited Europe. On this trip they were honored and feted. In fact, in 1928 the King and Queen of Afghanistan received honorary degrees from Oxford University. This was an era when other Muslim nations, like Turkey and Egypt were also on the path to modernization. King Amanullah was so impressed with the social progress of Europe that he tried to implement them right away, this met with heavy resistance from the conservative sect and eventually led to his demise.

Amanullah enjoyed early popularity within Afghanistan and he used his power to modernize the country. Amanullah created new cosmopolitan schools for both boys and girls in the region and overturned centuries-old traditions such as strict dress codes for women. He created a new capital city and increased trade with Europe and Asia. He also advanced a modernist constitution that incorporated equal rights and individual freedoms. This rapid modernization though, created a backlash, and a reactionary uprising known as the "Khost rebellion" which was suppressed in 1924.

After Amanullah travelled to Europe in late 1927, opposition to his rule increased. An uprising in Jalalabad culminated in a march to the capital, and much of the army deserted rather than resist. On 14 January 1929, Amanullah abdicated in favor of his brother, King Inayatullah Khan. On 17 January, Inayatullah abdicated and Habibullah Kalakani became the next ruler of Afghanistan and restored the emirate. However, his rule was short lived and, on 17 October 1929, Habibullah Kalakani was overthrown and replaced by King Nadir Khan.

After his abdication in 1929, Amanullah went into temporary exile in India. When he attempted to return to Afghanistan, he had little support from the people. From India, the ex-king traveled to Europe and settled in Italy, and later in Switzerland. Meanwhile, Nadir Khan made sure his return to Afghanistan was impossible by engaging in a propaganda war. Nadir Khan accused Amanullah Khan of kufr with his pro western policies.

In 1933, after the assassination of Nadir Khan, Mohammed Zahir Shah became king.





</doc>
<doc id="9059" url="https://en.wikipedia.org/wiki?curid=9059" title="Dementia praecox">
Dementia praecox

Dementia praecox (a "premature dementia" or "precocious madness") is a disused psychiatric diagnosis that originally designated a chronic, deteriorating psychotic disorder characterized by rapid cognitive disintegration, usually beginning in the late teens or early adulthood. Over the years, the term "dementia praecox" was gradually replaced by "schizophrenia", which remains in current diagnostic use.

The term "dementia praecox" was first used in 1891 by Arnold Pick (1851–1924), a professor of psychiatry at Charles University in Prague. His brief clinical report described the case of a person with a psychotic disorder resembling hebephrenia. German psychiatrist Emil Kraepelin (1856–1926) popularised it in his first detailed textbook descriptions of a condition that eventually became a different disease concept and relabeled as schizophrenia. Kraepelin reduced the complex psychiatric taxonomies of the nineteenth century by dividing them into two classes: manic-depressive psychosis and dementia praecox. This division, commonly referred to as the Kraepelinian dichotomy, had a fundamental impact on twentieth-century psychiatry, though it has also been questioned.

The primary disturbance in dementia praecox was seen to be a disruption in cognitive or mental functioning in attention, memory, and goal-directed behaviour. Kraepelin contrasted this with manic-depressive psychosis, now termed bipolar disorder, and also with other forms of mood disorder, including major depressive disorder. He eventually concluded that it was not possible to distinguish his categories on the basis of cross-sectional symptoms.

Kraepelin viewed dementia praecox as a progressively deteriorating disease from which no one recovered. However, by 1913, and more explicitly by 1920, Kraepelin admitted that while there may be a residual cognitive defect in most cases, the prognosis was not as uniformly dire as he had stated in the 1890s. Still, he regarded it as a specific disease concept that implied incurable, inexplicable madness.

"Dementia" is an ancient term which has been in use since at least the time of Lucretius in 50 B.C.E. where it meant "being out of one's mind". Until the seventeenth century dementia referred to states of cognitive and behavioural deterioration leading to psychosocial incompetence. This condition could be innate or acquired and the concept had no reference to a necessarily irreversible condition. It is the concept in this popular notion of psychosocial incapacity that forms the basis for the idea of legal incapacity. By the eighteenth century, at the period when the term entered into European medical discourse, clinical concepts were added to the vernacular understanding such that dementia was now associated with intellectual deficits arising from any cause and at any age. By the end of the nineteenth century the modern 'cognitive paradigm' of dementia was taking root. This holds that dementia is understood in terms of criteria relating to aetiology, age and course which excludes former members of the family of the demented such as adults with acquired head trauma or children with cognitive deficits. Moreover, it was now understood as an irreversible condition and a particular emphasis was placed on memory loss in regard to the deterioration of intellectual functions.

The term "démence précoce" was used in passing to describe the characteristics of a subset of young mental patients by the French physician Bénédict Augustin Morel in 1852 in the first volume of his "Études cliniques." and the term is used more frequently in his textbook "Traité des maladies mentales" which was published in 1860. Morel, whose name will be forever associated with religiously inspired concept of degeneration theory in psychiatry, used the term in a descriptive sense and not to define a specific and novel diagnostic category. It was applied as a means of setting apart a group of young men and women who were suffering from "stupor." As such their condition was characterised by a certain torpor, enervation, and disorder of the will and was related to the diagnostic category of melancholia. He did not conceptualise their state as irreversible and thus his use of the term dementia was equivalent to that formed in the eighteenth century as outlined above.

While some have sought to interpret, if in a qualified fashion, the use by Morel of the term "démence précoce" as amounting to the "discovery" of schizophrenia, others have argued convincingly that Morel's descriptive use of the term should not be considered in any sense as a precursor to Kraepelin's dementia praecox disease concept. This is due to the fact that their concepts of dementia differed significantly from each other, with Kraepelin employing the more modern sense of the word and that Morel was not describing a diagnostic category. Indeed, until the advent of Pick and Kraepelin, Morel's term had vanished without a trace and there is little evidence to suggest that either Pick or indeed Kraepelin were even aware of Morel's use of the term until long after they had published their own disease concepts bearing the same name. As Eugène Minkowski succinctly stated, 'An abyss separates Morel's démence précoce from that of Kraepelin.'

Morel described several psychotic disorders that ended in dementia, and as a result he may be regarded as the first alienist or psychiatrist to develop a diagnostic system based on presumed outcome rather than on the current presentation of signs and symptoms. Morel, however, did not conduct any long-term or quantitative research on the course and outcome of dementia praecox (Kraepelin would be the first in history to do that) so this prognosis was based on speculation. It is impossible to discern whether the condition briefly described by Morel was equivalent to the disorder later called dementia praecox by Pick and Kraepelin.

Psychiatric nosology in the nineteenth-century was chaotic and characterised by a conflicting mosaic of contradictory systems. Psychiatric disease categories were based upon short-term and cross-sectional observations of patients from which were derived the putative characteristic signs and symptoms of a given disease concept. The dominant psychiatric paradigms which gave a semblance of order to this fragmentary picture were Morelian degeneration theory and the concept of "unitary psychosis" ("Einheitspsychose"). This latter notion, derived from the Belgian psychiatrist Joseph Guislain (1797–1860), held that the variety of symptoms attributed to mental illness were manifestations of a single underlying disease process. While these approaches had a diachronic aspect they lacked a conception of mental illness that encompassed a coherent notion of change over time in terms of the natural course of the illness and based upon an empirical observation of changing symptomatology.

In 1863, the Danzig-based psychiatrist Karl Ludwig Kahlbaum (1828–1899) published his text on psychiatric nosology "Die Gruppierung der psychischen Krankheiten" ("The Classification of Psychiatric Diseases"). Although with the passage of time this work would prove profoundly influential, when it was published it was almost completely ignored by German academia despite the sophisticated and intelligent disease classification system which it proposed. In this book Kahlbaum categorized certain typical forms of psychosis ("vesania typica") as a single coherent type based upon their shared progressive nature which betrayed, he argued, an ongoing degenerative disease process. For Kahlbaum the disease process of "vesania typica" was distinguished by the passage of the sufferer through clearly defined disease phases: a melancholic stage; a manic stage; a confusional stage; and finally a demented stage.

In 1866 Kahlbaum became the director of a private psychiatric clinic in Görlitz (Prussia, today Saxony, a small town near Dresden). He was accompanied by his younger assistant, Ewald Hecker (1843–1909), and during a ten-year collaboration they conducted a series of research studies on young psychotic patients that would become a major influence on the development of modern psychiatry.

Together Kahlbaum and Hecker were the first to describe and name such syndromes as dysthymia, cyclothymia, paranoia, catatonia, and hebephrenia. Perhaps their most lasting contribution to psychiatry was the introduction of the "clinical method" from medicine to the study of mental diseases, a method which is now known as psychopathology.

When the element of time was added to the concept of diagnosis, a diagnosis became more than just a description of a collection of symptoms: diagnosis now also defined by prognosis (course and outcome). An additional feature of the clinical method was that the characteristic symptoms that define syndromes should be described without any prior assumption of brain pathology (although such links would be made later as scientific knowledge progressed). Karl Kahlbaum made an appeal for the adoption of the clinical method in psychiatry in his 1874 book on catatonia. Without Kahlbaum and Hecker there would be no dementia praecox.

Upon his appointment to a full professorship in psychiatry at the University of Dorpat (now Tartu, Estonia) in 1886, Kraepelin gave an inaugural address to the faculty outlining his research programme for the years ahead. Attacking the "brain mythology" of Meynert and the positions of Griesinger and Gudden, Kraepelin advocated that the ideas of Kahlbaum, who was then a marginal and little known figure in psychiatry, should be followed. Therefore, he argued, a research programme into the nature of psychiatric illness should look at a large number of patients over time to discover the course which mental disease could take. It has also been suggested that Kraepelin's decision to accept the Dorpat post was informed by the fact that there he could hope to gain experience with chronic patients and this, it was presumed, would facilitate the longitudinal study of mental illness.

Understanding that objective diagnostic methods must be based on scientific practice, Kraepelin had been conducting psychological and drug experiments on patients and normal subjects for some time when, in 1891, he left Dorpat and took up a position as professor and director of the psychiatric clinic at Heidelberg University. There he established a research program based on Kahlbaum's proposal for a more exact qualitative clinical approach, and his own innovation: a quantitative approach involving meticulous collection of data over time on each new patient admitted to the clinic (rather than only the interesting cases, as had been the habit until then).

Kraepelin believed that by thoroughly describing all of the clinic's new patients on index cards, which he had been using since 1887, researcher bias could be eliminated from the investigation process. He described the method in his posthumously published memoir:
The fourth edition of his textbook, "Psychiatrie", published in 1893, two years after his arrival at Heidelberg, contained some impressions of the patterns Kraepelin had begun to find in his index cards. Prognosis (course and outcome) began to feature alongside signs and symptoms in the description of syndromes, and he added a class of psychotic disorders designated "psychic degenerative processes", three of which were borrowed from Kahlbaum and Hecker: "dementia paranoides" (a degenerative type of Kahlbaum's paranoia, with sudden onset), "catatonia" (per Kahlbaum, 1874) and "dementia praecox", (Hecker's hebephrenia of 1871). Kraepelin continued to equate dementia praecox with hebephrenia for the next six years.

In the March 1896 fifth edition of "Psychiatrie", Kraepelin expressed confidence that his clinical method, involving analysis of both qualitative and quantitative data derived from long term observation of patients, would produce reliable diagnoses including prognosis:

In this edition dementia praecox is still essentially hebephrenia, and it, dementia paranoides and catatonia are described as distinct psychotic disorders among the "metabolic disorders leading to dementia".

In the 1899 (6th) edition of "Psychiatrie", Kraepelin established a paradigm for psychiatry that would dominate the following century, sorting most of the recognized forms of insanity into two major categories: dementia praecox and manic-depressive illness. Dementia praecox was characterized by disordered intellectual functioning, whereas manic-depressive illness was principally a disorder of affect or mood; and the former featured constant deterioration, virtually no recoveries and a poor outcome, while the latter featured periods of exacerbation followed by periods of remission, and many complete recoveries. The class, dementia praecox, comprised the paranoid, catatonic and hebephrenic psychotic disorders, and these forms were found in the Diagnostic and Statistical Manual of Mental Disorders until the fifth edition was released, in May 2013. These terms, however, are still found in general psychiatric nomenclature. The ICD-10 still uses "hebephrenic" to designate the third type.

In the seventh, 1904, edition of "Psychiatrie", Kraepelin accepted the possibility that a small number of patients may recover from dementia praecox. Eugen Bleuler reported in 1908 that in many cases there was no inevitable progressive decline, there was temporary remission in some cases, and there were even cases of near recovery with the retention of some residual defect. In the eighth edition of Kraepelin's textbook, published in four volumes between 1909 and 1915, he described eleven forms of dementia, and dementia praecox was classed as one of the "endogenous dementias". Modifying his previous more gloomy prognosis in line with Bleuler's observations, Kraepelin reported that about 26% of his patients experienced partial remission of symptoms. Kraepelin died while working on the ninth edition of "Psychiatrie" with Johannes Lange (1891–1938), who finished it and brought it to publication in 1927.

Though his work and that of his research associates had revealed a role for heredity, Kraepelin realized nothing could be said with certainty about the aetiology of dementia praecox, and he left out speculation regarding brain disease or neuropathology in his diagnostic descriptions. Nevertheless, from the 1896 edition onwards Kraepelin made clear his belief that poisoning of the brain, "auto-intoxication," probably by sex hormones, may underlie dementia praecox – a theory also entertained by Eugen Bleuler. Both theorists insisted dementia praecox is a biological disorder, not the product of psychological trauma. Thus, rather than a disease of hereditary degeneration or of structural brain pathology, Kraepelin believed dementia praecox was due to a systemic or "whole body" disease process, probably metabolic, which gradually affected many of the tissues and organs of the body before affecting the brain in a final, decisive cascade. Kraepelin, recognizing dementia praecox in Chinese, Japanese, Tamil and Malay patients, suggested in the eighth edition of "Psychiatrie" that, "we must therefore seek the real cause of dementia praecox in conditions which are spread all over the world, which thus do not lie in race or in climate, in food or in any other general circumstance of life..."

Kraepelin had experimented with hypnosis but found it wanting, and disapproved of Freud's and Jung's introduction, based on no evidence, of psychogenic assumptions to the interpretation and treatment of mental illness. He argued that, without knowing the underlying cause of dementia praecox or manic-depressive illness, there could be no disease-specific treatment, and recommended the use of long baths and the occasional use of drugs such as opiates and barbiturates for the amelioration of distress, as well as occupational activities, where suitable, for all institutionalized patients. Based on his theory that dementia praecox is the product of autointoxication emanating from the sex glands, Kraepelin experimented, without success, with injections of thyroid, gonad and other glandular extracts.

Kraepelin noted the dissemination of his new disease concept when in 1899 he enumerated the term's appearance in almost twenty articles in the German-language medical press. In the early years of the twentieth century the twin pillars of the Kraepelinian dichotomy, dementia praecox and manic depressive psychosis, were assiduously adopted in clinical and research contexts among the Germanic psychiatric community. German-language psychiatric concepts were always introduced much faster in America (than, say, Britain) where émigré German, Swiss and Austrian physicians essentially created American psychiatry. Swiss-émigré Adolf Meyer (1866–1950), arguably the most influential psychiatrist in America for the first half of the 20th century, published the first critique of dementia praecox in an 1896 book review of the 5th edition of Kraepelin's textbook. But it was not until 1900 and 1901 that the first three American publications regarding dementia praecox appeared, one of which was a translation of a few sections of Kraepelin's 6th edition of 1899 on dementia praecox.

Adolf Meyer was the first to apply the new diagnostic term in America. He used it at the Worcester Lunatic Hospital in Massachusetts in the fall of 1896. He was also the first to apply Eugen Bleuler's term "schizophrenia" (in the form of "schizophrenic reaction") in 1913 at the Henry Phipps Psychiatric Clinic of the Johns Hopkins Hospital.

The dissemination of Kraepelin's disease concept to the Anglophone world was facilitated in 1902 when Ross Diefendorf, a lecturer in psychiatry at Yale, published an adapted version of the sixth edition of the "Lehrbuch der Psychiatrie". This was republished in 1904 and with a new version, based on the seventh edition of Kraepelin's "Lehrbuch" appearing in 1907 and reissued in 1912. Both dementia praecox (in its three classic forms) and "manic-depressive psychosis" gained wider popularity in the larger institutions in the eastern United States after being included in the official nomenclature of diseases and conditions for record-keeping at Bellevue Hospital in New York City in 1903. The term lived on due to its promotion in the publications of the National Committee on Mental Hygiene (founded in 1909) and the Eugenics Records Office (1910). But perhaps the most important reason for the longevity of Kraepelin's term was its inclusion in 1918 as an official diagnostic category in the uniform system adopted for comparative statistical record-keeping in all American mental institutions, "The Statistical Manual for the Use of Institutions for the Insane". Its many revisions served as the official diagnostic classification scheme in America until 1952 when the first edition of the "Diagnostic and Statistical Manual: Mental Disorders", or DSM-I, appeared. Dementia praecox disappeared from official psychiatry with the publication of DSM-I, replaced by the Bleuler/Meyer hybridization, "schizophrenic reaction".

Schizophrenia was mentioned as an alternate term for dementia praecox in the 1918 "Statistical Manual". In both clinical work as well as research, between 1918 and 1952 five different terms were used interchangeably: dementia praecox, schizophrenia, dementia praecox (schizophrenia), schizophrenia (dementia praecox) and schizophrenic reaction. This made the psychiatric literature of the time confusing since, in a strict sense, Kraepelin's disease was not Bleuler's disease. They were defined differently, had different population parameters, and different concepts of prognosis.

The reception of dementia praecox as an accepted diagnosis in British psychiatry came more slowly, perhaps only taking hold around the time of World War I. There was substantial opposition to the use of the term "dementia" as misleading, partly due to findings of remission and recovery. Some argued that existing diagnoses such as "delusional insanity" or "adolescent insanity" were better or more clearly defined. In France a psychiatric tradition regarding the psychotic disorders predated Kraepelin, and the French never fully adopted Kraepelin's classification system. Instead the French maintained an independent classification system throughout the 20th century. From 1980, when DSM-III totally reshaped psychiatric diagnosis, French psychiatry began to finally alter its views of diagnosis to converge with the North American system. Kraepelin thus finally conquered France via America.

Due to the influence of alienists such as Adolf Meyer, August Hoch, George Kirby, Charles Macphie Campbell, Smith Ely Jelliffe and William Alanson White, psychogenic theories of dementia praecox dominated the American scene by 1911. In 1925 Bleuler's schizophrenia rose in prominence as an alternative to Kraepelin's dementia praecox. When Freudian perspectives became influential in American psychiatry in the 1920s schizophrenia became an attractive alternative concept. Bleuler corresponded with Freud and was connected to Freud's psychoanalytic movement, and the inclusion of Freudian interpretations of the symptoms of schizophrenia in his publications on the subject, as well as those of C.G. Jung, eased the adoption of his broader version of dementia praecox (schizophrenia) in America over Kraepelin's narrower and prognostically more negative one.

The term "schizophrenia" was first applied by American alienists and neurologists in private practice by 1909 and officially in institutional settings in 1913, but it took many years to catch on. It is first mentioned in "The New York Times" in 1925. Until 1952 the terms dementia praecox and schizophrenia were used interchangeably in American psychiatry, with occasional use of the hybrid terms "dementia praecox (schizophrenia)" or "schizophrenia (dementia praecox)".

Editions of the Diagnostic and Statistical Manual of Mental Disorders since the first in 1952 had reflected views of schizophrenia as "reactions" or "psychogenic" (DSM-I), or as manifesting Freudian notions of "defense mechanisms" (as in DSM-II of 1969 in which the symptoms of schizophrenia were interpreted as "psychologically self-protected"). The diagnostic criteria were vague, minimal and wide, including either concepts that no longer exist or that are now labeled as personality disorders (for example, schizotypal personality disorder). There was also no mention of the dire prognosis Kraepelin had made. Schizophrenia seemed to be more prevalent and more psychogenic and more treatable than either Kraepelin or Bleuler would have allowed.

As a direct result of the effort to construct Research Diagnostic Criteria (RDC) in the 1970s that were independent of any clinical diagnostic manual, Kraepelin's idea that categories of mental disorder should reflect discrete and specific disease entities with a biological basis began to return to prominence. Vague dimensional approaches based on symptoms—so highly favored by the Meyerians and psychoanalysts—were overthrown. For research purposes, the definition of schizophrenia returned to the narrow range allowed by Kraepelin's dementia praecox concept. Furthermore, after 1980 the disorder was a progressively deteriorating one once again, with the notion that recovery, if it happened at all, was rare. This revision of schizophrenia became the basis of the diagnostic criteria in DSM-III (1980). Some of the psychiatrists who worked to bring about this revision referred to themselves as the "neo-Kraepelinians".





</doc>
<doc id="9061" url="https://en.wikipedia.org/wiki?curid=9061" title="Dolphin">
Dolphin

Dolphins are a widely distributed and diverse group of aquatic mammals. They are an informal grouping within the order Cetacea, excluding whales and porpoises, so to zoologists the grouping is paraphyletic. The dolphins comprise the extant families Delphinidae (the oceanic dolphins), Platanistidae (the Indian river dolphins), Iniidae (the new world river dolphins), and Pontoporiidae (the brackish dolphins), and the extinct Lipotidae (baiji or Chinese river dolphin). There are 40 extant species of dolphins. Dolphins, alongside other cetaceans, belong to the clade Cetartiodactyla with even-toed ungulates. Cetaceans' closest living relatives are the hippopotamuses, having diverged about 40 million years ago.

Dolphins range in size from the long and Maui's dolphin to the and killer whale. Several species exhibit sexual dimorphism, in that the males are larger than females. They have streamlined bodies and two limbs that are modified into flippers. Though not quite as flexible as seals, some dolphins can travel at . Dolphins use their conical shaped teeth to capture fast moving prey. They have well-developed hearing which is adapted for both air and water and is so well developed that some can survive even if they are blind. Some species are well adapted for diving to great depths. They have a layer of fat, or blubber, under the skin to keep warm in the cold water.

Although dolphins are widespread, most species prefer the warmer waters of the tropic zones, but some, like the right whale dolphin, prefer colder climates. Dolphins feed largely on fish and squid, but a few, like the killer whale, feed on large mammals, like seals. Male dolphins typically mate with multiple females every year, but females only mate every two to three years. Calves are typically born in the spring and summer months and females bear all the responsibility for raising them. Mothers of some species fast and nurse their young for a relatively long period of time. Dolphins produce a variety of vocalizations, usually in the form of clicks and whistles.

Dolphins are sometimes hunted in places like Japan, in an activity known as dolphin drive hunting. Besides drive hunting, they also face threats from bycatch, habitat loss, and marine pollution. Dolphins have been depicted in various cultures worldwide. Dolphins occasionally feature in literature and film, as in the film series "Free Willy". Dolphins are sometimes kept in captivity and trained to perform tricks. The most common dolphin species kept is the bottlenose dolphin, while there are around 60 captive killer whales.

The name is originally from Greek ("delphís"), "dolphin", which was related to the Greek ("delphus"), "womb". The animal's name can therefore be interpreted as meaning "a 'fish' with a womb". The name was transmitted via the Latin "delphinus" (the romanization of the later Greek δελφῖνος – "delphinos"), which in Medieval Latin became "dolfinus" and in Old French "daulphin", which reintroduced the "ph" into the word. The term mereswine (that is, "sea pig") has also historically been used.
The term 'dolphin' can be used to refer to, under the parvorder Odontoceti, all the species in the family Delphinidae (oceanic dolphins) and the river dolphin families Iniidae (South American river dolphins), Pontoporiidae (La Plata dolphin), Lipotidae (Yangtze river dolphin) and Platanistidae (Ganges river dolphin and Indus river dolphin).
This term has often been misused in the US, mainly in the fishing industry, where all small cetaceans (dolphins and porpoises) are considered porpoises, while the fish "dorado" is called dolphin fish. In common usage the term 'whale' is used only for the larger cetacean species, while the smaller ones with a beaked or longer nose are considered 'dolphins'. The name 'dolphin' is used casually as a synonym for bottlenose dolphin, the most common and familiar species of dolphin. There are six species of dolphins commonly thought of as whales, collectively known as blackfish: the killer whale, the melon-headed whale, the pygmy killer whale, the false killer whale, and the two species of pilot whales, all of which are classified under the family Delphinidae and qualify as dolphins. Though the terms 'dolphin' and 'porpoise' are sometimes used interchangeably, porpoises are not considered dolphins and have different physical features such as a shorter beak and spade-shaped teeth; they also differ in their behavior. Porpoises belong to the family Phocoenidae and share a common ancestry with the Delphinidae.

A group of dolphins is called a "school" or a "pod". Male dolphins are called "bulls", females "cows" and young dolphins are called "calves".


In 1933, three strange dolphins beached off the Irish coast; they appeared to be hybrids between Risso's and bottlenose dolphins. This mating was later repeated in captivity, producing a hybrid calf. In captivity, a bottlenose and a rough-toothed dolphin produced hybrid offspring. A common-bottlenose hybrid lives at SeaWorld California. Other dolphin hybrids live in captivity around the world or have been reported in the wild, such as a bottlenose-Atlantic spotted hybrid. The best known hybrid is the wolphin, a false killer whale-bottlenose dolphin hybrid. The wolphin is a fertile hybrid. Two wolphins currently live at the Sea Life Park in Hawaii; the first was born in 1985 from a male false killer whale and a female bottlenose. Wolphins have also been observed in the wild.

Dolphins are descendants of land-dwelling mammals of the artiodactyl order (even-toed ungulates). They are related to the "Indohyus", an extinct chevrotain-like ungulate, from which they split approximately 48 million years ago.

The primitive cetaceans, or archaeocetes, first took to the sea approximately 49 million years ago and became fully aquatic by 5–10 million years later.

Archaeoceti is a parvorder comprising ancient whales. These ancient whales are the predecessors of modern whales, stretching back to their first ancestor that spent their lives near (rarely in) the water. Likewise, the archaeocetes can be anywhere from near fully terrestrial, to semi-aquatic to fully aquatic, but what defines an archaeocete is the presence of visible legs or asymmetrical teeth. Their features became adapted for living in the marine environment. Major anatomical changes include the hearing set-up that channeled vibrations from the jaw to the earbone which occurred with "Ambulocetus" 49 million years ago, a streamlining of the body and the growth of flukes on the tail which occurred around 43 million years ago with "Protocetus", the migration of the nasal openings toward the top of the cranium and the modification of the forelimbs into flippers which occurred with "Basilosaurus" 35 million years ago, and the shrinking and eventual disappearance of the hind limbs which took place with the first odontocetes and mysticetes 34 million years ago. The modern dolphin skeleton has two small, rod-shaped pelvic bones thought to be vestigial hind limbs. In October 2006, an unusual bottlenose dolphin was captured in Japan; it had small fins on each side of its genital slit, which scientists believe to be an unusually pronounced development of these vestigial hind limbs.

Today, the closest living relatives of cetaceans are the hippopotamuses; these share a semi-aquatic ancestor that branched off from other artiodactyls some 60 million years ago. Around 40 million years ago, a common ancestor between the two branched off into cetacea and anthracotheres; anthracotheres became extinct at the end of the Pleistocene two-and-a-half million years ago, eventually leaving only one surviving lineage: the hippo.

Dolphins have torpedo shaped bodies with generally non-flexible necks, limbs modified into flippers, non-existent external ear flaps, a tail fin, and bulbous heads. Dolphin skulls have small eye orbits, long snouts, and eyes placed on the sides of its head. Dolphins range in size from the long and Maui's dolphin to the and killer whale. Overall, however, they tend to be dwarfed by other Cetartiodactyls. Several species have female-biased sexual dimorphism, with the females being larger than the males.

Dolphins have conical teeth, as opposed to porpoises' spade-shaped teeth. These conical teeth are used to catch swift prey such as fish, squid or large mammals, such as seal.

Breathing involves expelling stale air from their blowhole, forming an upward, steamy spout, followed by inhaling fresh air into the lungs, however this only occurs in the polar regions of the oceans. Dolphins have rather small, unidentifiable spouts.

All dolphins have a thick layer of blubber, thickness varying on climate. This blubber can help with buoyancy, protection to some extent as predators would have a hard time getting through a thick layer of fat, and energy for leaner times; the primary usage for blubber is insulation from the harsh climate. Calves, generally, are born with a thin layer of blubber, which develops at different paces depending on the habitat.

Dolphins have a two-chambered stomach that is similar in structure to terrestrial carnivores. They have fundic and pyloric chambers.

Dolphins' reproductive organs are located inside the body, with genital slits on the ventral (belly) side. Males have two slits, one concealing the dolphin penis and one further behind for the anus. Females have one genital slit, housing the vagina and the anus, with a mammary slit on either side.

Dolphins have two pectoral flippers, containing four digits, a boneless dorsal fin for stability, and a tail fin for propulsion. Although dolphins do not possess external hind limbs, some possess discrete rudimentary appendages, which may contain feet and digits. Dolphins are fast swimmers in comparison to seals which typically cruise at ; the killer whale (orca), in comparison, can travel at speeds up to . The fusing of the neck vertebrae, while increasing stability when swimming at high speeds, decreases flexibility, which means they are unable to turn their heads. River dolphins, however, have non-fused neck vertebrae and are able to turn their head up to 90°. Dolphins swim by moving their tail fin and rear body vertically, while their flippers are mainly used for steering. Some species log out of the water, which may allow them to travel faster. Their skeletal anatomy allows them to be fast swimmers. All species have a dorsal fin to prevent themselves from involuntarily spinning in the water.

Some dolphins are adapted for diving to great depths. In addition to their streamlined bodies, some can slow their heart rate to conserve oxygen. Some can also re-route blood from tissue tolerant of water pressure to the heart, brain and other organs. Their hemoglobin and myoglobin store oxygen in body tissues and they have twice the concentration of myoglobin than hemoglobin.

The dolphin ear has specific adaptations to the marine environment. In humans, the middle ear works as an impedance equalizer between the outside air's low impedance and the cochlear fluid's high impedance. In dolphins, and other marine mammals, there is no great difference between the outer and inner environments. Instead of sound passing through the outer ear to the middle ear, dolphins receive sound through the throat, from which it passes through a low-impedance fat-filled cavity to the inner ear. The dolphin ear is acoustically isolated from the skull by air-filled sinus pockets, which allow for greater directional hearing underwater. Dolphins send out high frequency clicks from an organ known as a melon. This melon consists of fat, and the skull of any such creature containing a melon will have a large depression. This allows dolphins to produce biosonar for orientation. Though most dolphins do not have hair, they do have hair follicles that may perform some sensory function. Beyond locating an object, echolocation also provides the animal with an idea on an object's shape and size, though how exactly this works is not yet understood. The small hairs on the rostrum of the boto are believed to function as a tactile sense, possibly to compensate for the boto's poor eyesight.

The dolphin eye is relatively small for its size, yet they do retain a good degree of eyesight. As well as this, the eyes of a dolphin are placed on the sides of its head, so their vision consists of two fields, rather than a binocular view like humans have. When dolphins surface, their lens and cornea correct the nearsightedness that results from the refraction of light; they contain both rod and cone cells, meaning they can see in both dim and bright light, but they have far more rod cells than they do cone cells. Dolphins do, however, lack short wavelength sensitive visual pigments in their cone cells indicating a more limited capacity for color vision than most mammals. Most dolphins have slightly flattened eyeballs, enlarged pupils (which shrink as they surface to prevent damage), slightly flattened corneas and a tapetum lucidum; these adaptations allow for large amounts of light to pass through the eye and, therefore, a very clear image of the surrounding area. They also have glands on the eyelids and outer corneal layer that act as protection for the cornea.

The olfactory lobes are absent in dolphins, suggesting that they have no sense of smell.

Dolphins are not thought to have a good sense of taste, as their taste buds are atrophied or missing altogether. However, some have preferences between different kinds of fish, indicating some sort of attachment to taste.

Dolphins are often regarded as one of Earth's most intelligent animals, though it is hard to say just how intelligent. Comparing species' relative intelligence is complicated by differences in sensory apparatus, response modes, and nature of cognition. Furthermore, the difficulty and expense of experimental work with large aquatic animals has so far prevented some tests and limited sample size and rigor in others. Compared to many other species, however, dolphin behavior has been studied extensively, both in captivity and in the wild. See cetacean intelligence for more details.

Dolphins are highly social animals, often living in pods of up to a dozen individuals, though pod sizes and structures vary greatly between species and locations. In places with a high abundance of food, pods can merge temporarily, forming a "superpod"; such groupings may exceed 1,000 dolphins. Membership in pods is not rigid; interchange is common. Dolphins can, however, establish strong social bonds; they will stay with injured or ill individuals, even helping them to breathe by bringing them to the surface if needed. This altruism does not appear to be limited to their own species. The dolphin "Moko" in New Zealand has been observed guiding a female Pygmy Sperm Whale together with her calf out of shallow water where they had stranded several times. They have also been seen protecting swimmers from sharks by swimming circles around the swimmers or charging the sharks to make them go away.

Dolphins communicate using a variety of clicks, whistle-like sounds and other vocalizations. Dolphins also use nonverbal communication by means of touch and posturing.

Dolphins also display culture, something long believed to be unique to humans (and possibly other primate species). In May 2005, a discovery in Australia found Indo-Pacific bottlenose dolphins ("Tursiops aduncus") teaching their young to use tools. They cover their snouts with sponges to protect them while foraging. This knowledge is mostly transferred by mothers to daughters, unlike simian primates, where knowledge is generally passed on to both sexes. Using sponges as mouth protection is a learned behavior. Another learned behavior was discovered among river dolphins in Brazil, where some male dolphins use weeds and sticks as part of a sexual display.

Forms of care-giving between fellows and even for members of different species (see Moko (dolphin)) are recorded in various species - such as trying to save weakened fellows or female pilot whales holding up dead calves for long periods.

Dolphins engage in acts of aggression towards each other. The older a male dolphin is, the more likely his body is to be covered with bite scars. Male dolphins can get into disputes over companions and females. Acts of aggression can become so intense that targeted dolphins sometimes go into exile after losing a fight.

Male bottlenose dolphins have been known to engage in infanticide. Dolphins have also been known to kill porpoises for reasons which are not fully understood, as porpoises generally do not share the same diet as dolphins and are therefore not competitors for food supplies. The Cornwall Wildlife Trust records about one such death a year. Possible explanations include misdirected infanticide, misdirected sexual aggression or play behaviour.

Dolphin copulation happens belly to belly; though many species engage in lengthy foreplay, the actual act is usually brief, but may be repeated several times within a short timespan. The gestation period varies with species; for the small Tucuxi dolphin, this period is around 11 to 12 months, while for the orca, the gestation period is around 17 months. Typically dolphins give birth to a single calf, which is, unlike most other mammals, born tail first in most cases. They usually become sexually active at a young age, even before reaching sexual maturity. The age of sexual maturity varies by species and gender.

Dolphins are known to display non-reproductive sexual behavior, engaging in masturbation, stimulation of the genital area of other individuals using the rostrum or flippers, and homosexual contact.

Various species of dolphin have been known to engage in sexual behavior up to and including copulation with dolphins of other species. Sexual encounters may be violent, with male dolphins sometimes showing aggressive behavior towards both females and other males. Male dolphins may also work together and attempt to herd females in estrus, keeping the females by their side by means of both physical aggression and intimidation, to increase their chances of reproductive success. Occasionally, dolphins behave sexually towards other animals, including humans.

Various methods of feeding exist among and within species, some apparently exclusive to a single population. Fish and squid are the main food, but the false killer whale and the orca also feed on other marine mammals. Orcas on occasion also hunt whale species larger than themselves.

One common feeding method is herding, where a pod squeezes a school of fish into a small volume, known as a bait ball. Individual members then take turns plowing through the ball, feeding on the stunned fish. Coralling is a method where dolphins chase fish into shallow water to catch them more easily. Orcas and bottlenose dolphins have also been known to drive their prey onto a beach to feed on it, a behaviour known as beach or strand feeding. Some species also whack fish with their flukes, stunning them and sometimes knocking them out of the water.

Reports of cooperative human-dolphin fishing date back to the ancient Roman author and natural philosopher Pliny the Elder. A modern human-dolphin partnership currently operates in Laguna, Santa Catarina, Brazil. Here, dolphins drive fish towards fishermen waiting along the shore and signal the men to cast their nets. The dolphins' reward is the fish that escape the nets.

Dolphins are capable of making a broad range of sounds using nasal airsacs located just below the blowhole. Roughly three categories of sounds can be identified: frequency modulated whistles, burst-pulsed sounds and clicks. Dolphins communicate with whistle-like sounds produced by vibrating connective tissue, similar to the way human vocal cords function, and through burst-pulsed sounds, though the nature and extent of that ability is not known. The clicks are directional and are for echolocation, often occurring in a short series called a click train. The click rate increases when approaching an object of interest. Dolphin echolocation clicks are amongst the loudest sounds made by marine animals.

Bottlenose dolphins have been found to have signature whistles, a whistle that is unique to a specific individual. These whistles are used in order for dolphins to communicate with one another by identifying an individual. It can be seen as the dolphin equivalent of a name for humans. These signature whistles are developed during a dolphin's first year; it continues to maintain the same sound throughout its lifetime. In order to obtain each individual whistle sound, dolphins undergo vocal production learning. This consists of an experience with other dolphins that modifies the signal structure of an existing whistle sound. An auditory experience influences the whistle development of each dolphin. Dolphins are able to communicate to one another by addressing another dolphin through mimicking their whistle. The signature whistle of a male bottlenose dolphin tends to be similar to that of his mother, while the signature whistle of a female bottlenose dolphin tends to be more distinguishing. Bottlenose dolphins have a strong memory when it comes to these signature whistles, as they are able to relate to a signature whistle of an individual they have not encountered for over twenty years. Research done on signature whistle usage by other dolphin species is relatively limited. The research on other species done so far has yielded varied outcomes and inconclusive results.

Because dolphins are generally associated in groups, communication is necessary. Signal masking is when other similar sounds (conspecific sounds) interfere with the original acoustic sound. In larger groups, individual whistle sounds are less prominent. Dolphins tend to travel in pods, upon which there are groups of dolphins that range from a few to many. Although they are traveling in these pods, the dolphins do not necessarily swim right next to each other. Rather, they swim within the same general vicinity. In order to prevent losing one of their pod members, there are higher whistle rates. Because their group members were spread out, this was done in order to continue traveling together.

Dolphins frequently leap above the water surface, this being done for various reasons. When travelling, jumping can save the dolphin energy as there is less friction while in the air. This type of travel is known as porpoising. Other reasons include orientation, social displays, fighting, non-verbal communication, entertainment and attempting to dislodge parasites.

Dolphins show various types of playful behavior, often including objects, self-made bubble rings, other dolphins or other animals. When playing with objects or small animals, common behavior includes carrying the object or animal along using various parts of the body, passing it along to other members of the group or taking it from another member, or throwing it out of the water. Dolphins have also been observed harassing animals in other ways, for example by dragging birds underwater without showing any intent to eat them. Playful behaviour that involves another animal species with active participation of the other animal can also be observed however. Playful human interaction with dolphins being the most obvious example, however playful interactions have been observed in the wild with a number of other species as well, such as humpback whales and dogs.

Juvenile dolphins off the coast of Western Australia have been observed chasing, capturing, and chewing on blowfish. While some reports state that the dolphins are becoming intoxicated on the tetrodotoxin in the fishes' skin, other reports have characterized this behavior as the normal curiosity and exploration of their environment in which dolphins engage.

Dolphins are known to teach, learn, cooperate, scheme, and grieve. The neocortex of many species is home to elongated spindle neurons that, prior to 2007, were known only in hominids. In humans, these cells are involved in social conduct, emotions, judgment, and theory of mind. Cetacean spindle neurons are found in areas of the brain that are homologous to where they are found in humans, suggesting that they perform a similar function.

Brain size was previously considered a major indicator of the intelligence of an animal. Since most of the brain is used for maintaining bodily functions, greater ratios of brain to body mass may increase the amount of brain mass available for more complex cognitive tasks. Allometric analysis indicates that mammalian brain size scales at approximately the ⅔ or ¾ exponent of the body mass. Comparison of a particular animal's brain size with the expected brain size based on such allometric analysis provides an encephalization quotient that can be used as another indication of animal intelligence. Killer whales have the second largest brain mass of any animal on earth, next to the sperm whale. The brain to body mass ratio in some is second only to humans.

Self-awareness is seen, by some, to be a sign of highly developed, abstract thinking. Self-awareness, though not well-defined scientifically, is believed to be the precursor to more advanced processes like meta-cognitive reasoning (thinking about thinking) that are typical of humans. Research in this field has suggested that cetaceans, among others, possess self-awareness.
The most widely used test for self-awareness in animals is the mirror test in which a temporary dye is placed on an animal's body, and the animal is then presented with a mirror; they then see if the animal shows signs of self-recognition.

Some disagree with these findings, arguing that the results of these tests are open to human interpretation and susceptible to the Clever Hans effect. This test is much less definitive than when used for primates, because primates can touch the mark or the mirror, while cetaceans cannot, making their alleged self-recognition behavior less certain. Skeptics argue that behaviors that are said to identify self-awareness resemble existing social behaviors, and so researchers could be misinterpreting self-awareness for social responses to another individual. The researchers counter-argue that the behaviors shown are evidence of self-awareness, as they are very different from normal responses to another individual. Whereas apes can merely touch the mark on themselves with their fingers, cetaceans show less definitive behavior of self-awareness; they can only twist and turn themselves to observe the mark.

In 1995, Marten and Psarakos used television to test dolphin self-awareness. They showed dolphins real-time footage of themselves, recorded footage, and another dolphin. They concluded that their evidence suggested self-awareness rather than social behavior. While this particular study has not been repeated since then, dolphins have since passed the mirror test.

Generally, dolphins sleep with only one brain hemisphere in slow-wave sleep at a time, thus maintaining enough consciousness to breathe and to watch for possible predators and other threats. Earlier sleep stages can occur simultaneously in both hemispheres.
In captivity, dolphins seemingly enter a fully asleep state where both eyes are closed and there is no response to mild external stimuli. In this case, respiration is automatic; a tail kick reflex keeps the blowhole above the water if necessary. Anesthetized dolphins initially show a tail kick reflex. Though a similar state has been observed with wild sperm whales, it is not known if dolphins in the wild reach this state. The Indus river dolphin has a sleep method that is different from that of other dolphin species. Living in water with strong currents and potentially dangerous floating debris, it must swim continuously to avoid injury. As a result, this species sleeps in very short bursts which last between 4 and 60 seconds.

Dolphins have few marine enemies. Some species or specific populations have none, making them apex predators. For most of the smaller species of dolphins, only a few of the larger sharks, such as the bull shark, dusky shark, tiger shark and great white shark, are a potential risk, especially for calves. Some of the larger dolphin species, especially orcas (killer whales), may also prey on smaller dolphins, but this seems rare. Dolphins also suffer from a wide variety of diseases and parasites. The Cetacean morbillivirus in particular has been known to cause regional epizootics often leaving hundreds of animals of various species dead. Symptoms of infection are often a severe combination of pneumonia, encephalitis and damage to the immune system, which greatly impair the cetacean's ability to swim and stay afloat unassisted. A study at the U.S. National Marine Mammal Foundation revealed that dolphins, like humans, develop a natural form of type 2 diabetes which may lead to a better understanding of the disease and new treatments for both humans and dolphins.

Dolphins can tolerate and recover from extreme injuries such as shark bites although the exact methods used to achieve this are not known. The healing process is rapid and even very deep wounds do not cause dolphins to hemorrhage to death. Furthermore, even gaping wounds restore in such a way that the animal's body shape is restored, and infection of such large wounds seems rare.

The study, published in the journal Marine Mammal Science, suggests that at least some dolphins are up to the shark challenge with research finding that many dolphins survive attacks using everything from sophisticated combat moves to teaming up against the shark.

Some dolphin species are at risk of extinction, especially some river dolphin species such as the Amazon river dolphin, and the Ganges and Yangtze river dolphin, which are critically or seriously endangered. A 2006 survey found no individuals of the Yangtze river dolphin, which now appears to be functionally extinct.

Pesticides, heavy metals, plastics, and other industrial and agricultural pollutants that do not disintegrate rapidly in the environment concentrate in predators such as dolphins. Injuries or deaths due to collisions with boats, especially their propellers, are also common.

Various fishing methods, most notably purse seine fishing for tuna and the use of drift and gill nets, unintentionally kill many dolphins. Accidental by-catch in gill nets and incidental captures in antipredator nets that protect marine fish farms are common and pose a risk for mainly local dolphin populations. In some parts of the world, such as Taiji in Japan and the Faroe Islands, dolphins are traditionally considered food and are killed in harpoon or drive hunts. Dolphin meat is high in mercury and may thus pose a health danger to humans when consumed.

Dolphin safe labels attempt to reassure consumers that fish and other marine products have been caught in a dolphin-friendly way. The earliest campaigns with "Dolphin safe" labels were initiated in the 1980s as a result of cooperation between marine activists and the major tuna companies, and involved decreasing incidental dolphin kills by up to 50% by changing the type of nets used to catch tuna. The dolphins are netted only while fishermen are in pursuit of smaller tuna. Albacore are not netted this way, making albacore the only truly dolphin-safe tuna.
Loud underwater noises, such as those resulting from naval sonar use, live firing exercises, and certain offshore construction projects such as wind farms, may be harmful to dolphins, increasing stress, damaging hearing, and causing decompression sickness by forcing them to surface too quickly to escape the noise.

Dolphins and other smaller cetaceans are also hunted in an activity known as dolphin drive hunting. This is accomplished by driving a pod together with boats and usually into a bay or onto a beach. Their escape is prevented by closing off the route to the ocean with other boats or nets. Dolphins are hunted this way in several places around the world, including the Solomon Islands, the Faroe Islands, Peru, and Japan, the most well-known practitioner of this method. By numbers, dolphins are mostly hunted for their meat, though some end up in dolphinariums. Despite the controversial nature of the hunt resulting in international criticism, and the possible health risk that the often polluted meat causes, thousands of dolphins are caught in drive hunts each year.

Dolphins have long played a role in human culture. Dolphins are sometimes used as symbols, for instance in heraldry.

In Greek myths, dolphins were seen invariably as helpers of humankind. Dolphins also seem to have been important to the Minoans, judging by artistic evidence from the ruined palace at Knossos. During the 2009 excavations of a major Mycenaean city at Iklaina, a striking fragment of a wall-paintings came to light, depicting a ship with three human figures and dolphins. Dolphins are common in Greek mythology, and many coins from ancient Greece have been found which feature a man, a boy or a deity riding on the back of a dolphin. The Ancient Greeks welcomed dolphins; spotting dolphins riding in a ship's wake was considered a good omen. In both ancient and later art, Cupid is often shown riding a dolphin. A dolphin rescued the poet Arion from drowning and carried him safe to land, at Cape Matapan, a promontory forming the southernmost point of the Peloponnesus. There was a temple to Poseidon and a statue of Arion riding the dolphin.

The Greeks reimagined the Phoenician god Melqart as Melikertês (Melicertes) and made him the son of Athamas and Ino. He drowned but was transfigured as the marine deity Palaemon, while his mother became Leucothea. ("cf" Ino.) At Corinth, he was so closely connected with the cult of Poseidon that the Isthmian Games, originally instituted in Poseidon's honor, came to be looked upon as the funeral games of Melicertes. Phalanthus was another legendary character brought safely to shore (in Italy) on the back of a dolphin, according to Pausanias.

Dionysus was once captured by Etruscan pirates who mistook him for a wealthy prince they could ransom. After the ship set sail Dionysus invoked his divine powers, causing vines to overgrow the ship where the mast and sails had been. He turned the oars into serpents, so terrifying the sailors that they jumped overboard, but Dionysus took pity on them and transformed them into dolphins so that they would spend their lives providing help for those in need. Dolphins were also the messengers of Poseidon and sometimes did errands for him as well. Dolphins were sacred to both Aphrodite and Apollo.

When heraldry developed in the Middle Ages, not much was known about the biology of the dolphin and it was often depicted as a sort of fish. Traditionally, the stylised dolphins in heraldry still may take after this notion, sometimes showing the dolphin skin covered with fish scales.

Dolphins are present in the coat of arms of Anguilla and the coat of arms of Romania, and the coat of arms of Barbados has a dolphin supporter.

A well-known historical example of a dolphin in heraldry, was the arms for the former province of the Dauphiné in southern France, from which were derived the arms and the title of the Dauphin of France, the heir to the former throne of France (the title literally means "The Dolphin of France").

"Dolfin" was the name of an aristocratic family in the maritime Republic of Venice, whose most prominent member was the 13th Century Doge Giovanni Dolfin.

In Hindu mythology the Ganges River Dolphin is associated with Ganga, the deity of the Ganges river. The dolphin is said to be among the creatures which heralded the goddess' descent from the heavens and her mount, the Makara, is sometimes depicted as a dolphin.

The Boto, a species of river dolphin that resides in the Amazon River, are believed to be shapeshifters, or "encantados", who are capable of having children with human women.

There are comparatively few surviving myths of dolphins in Polynesian cultures, in spite of their maritime traditions and relevance of other marine animals such as sharks and seabirds; unlike these, they are more often perceived as food than as totemic symbols. Dolphins are most clearly represented in Rapa Nui Rongorongo, and in the traditions of the Caroline Islands they are depicted similarly to the Boto, being sexually active shapeshifters.

The renewed popularity of dolphins in the 1960s resulted in the appearance of many dolphinaria around the world, making dolphins accessible to the public. Criticism and animal welfare laws forced many to close, although hundreds still exist around the world. In the United States, the best known are the SeaWorld marine mammal parks.
In the Middle East the best known are Dolphin Bay at Atlantis, The Palm and the Dubai Dolphinarium.
Various species of dolphins are kept in captivity. These small cetaceans are more often than not kept in theme parks, such as SeaWorld, commonly known as a dolphinarium. Bottlenose dolphins are the most common species of dolphin kept in dolphinariums as they are relatively easy to train, have a long lifespan in captivity and have a friendly appearance. Hundreds if not thousands of bottlenose dolphins live in captivity across the world, though exact numbers are hard to determine. Other species kept in captivity are spotted dolphins, false killer whales and common dolphins, Commerson's dolphins, as well as rough-toothed dolphins, but all in much lower numbers than the bottlenose dolphin. There are also fewer than ten pilot whales, Amazon river dolphins, Risso's dolphins, spinner dolphins, or tucuxi in captivity. An unusual and very rare hybrid dolphin, known as a wolphin, is kept at the Sea Life Park in Hawaii, which is a cross between a bottlenose dolphin and a false killer whale.

The number of killer whales kept in captivity is very small, especially when compared to the number of bottlenose dolphins, with 60 captive killer whales being held in aquaria . The killer whale's intelligence, trainability, striking appearance, playfulness in captivity and sheer size have made it a popular exhibit at aquaria and aquatic theme parks. From 1976 to 1997, 55 whales were taken from the wild in Iceland, 19 from Japan, and three from Argentina. These figures exclude animals that died during capture. Live captures fell dramatically in the 1990s, and by 1999, about 40% of the 48 animals on display in the world were captive-born.

Organizations such as the Mote Marine Laboratory rescue and rehabilitate sick, wounded, stranded or orphaned dolphins while others, such as the Whale and Dolphin Conservation Society and Hong Kong Dolphin Conservation Society, work on dolphin conservation and welfare. India has declared the dolphin as its national aquatic animal in an attempt to protect the endangered Ganges River Dolphin. The Vikramshila Gangetic Dolphin Sanctuary has been created in the Ganges river for the protection of the animals.

There is debate over the welfare of cetaceans in captivity, and often welfare can vary greatly dependent on the levels of care being provided at a particular facility. In the United States, facilities are regularly inspected by federal agencies to ensure that a high standard of welfare is maintained. Additionally, facilities can apply to become accredited by the Association of Zoos and Aquariums(AZA), which (for accreditation) requires "the highest standards of animal care and welfare in the world" to be achieved. Facilities such as SeaWorld and the Georgia Aquarium are accredited by the AZA. Organizations such as World Animal Protection and the Whale and Dolphin Conservation Society campaign against the practice of keeping them in captivity. In captivity, they often develop pathologies, such as the dorsal fin collapse seen in 60–90% of male killer whales. Captives have vastly reduced life expectancies, on average only living into their 20s, although there are examples of killer whales living longer, including several over 30 years old, and two captive orcas, Corky II and Lolita, are in their mid-40s. In the wild, females who survive infancy live 46 years on average, and up to 70–80 years in rare cases. Wild males who survive infancy live 31 years on average, and up to 50–60 years. Captivity usually bears little resemblance to wild habitat, and captive whales' social groups are foreign to those found in the wild. Critics claim captive life is stressful due to these factors and the requirement to perform circus tricks that are not part of wild killer whale behavior. Wild killer whales may travel up to in a day, and critics say the animals are too big and intelligent to be suitable for captivity. Captives occasionally act aggressively towards themselves, their tankmates, or humans, which critics say is a result of stress.

Although dolphins generally interact well with humans, some attacks have occurred, most of them resulting in small injuries. Orcas, the largest species of dolphin, have been involved in fatal attacks on humans in captivity. The record-holder of documented orca fatal attacks is a male named Tilikum, who has lived at SeaWorld since 1992. Tilikum has played a role in the death of three people in three different incidents (1991, 1999 and 2010). Tilikum's behaviour sparked the production of the documentary "Blackfish", which focuses on the consequences of keeping orcas in captivity. There are documented incidents in the wild, too, but none of them fatal.

Fatal attacks from other species are less common, but there is a registered occurrence off the coast of Brazil in 1994, when a man died after being attacked by a bottlenose dolphin named Tião. Tião had suffered harassment by human visitors, including attempts to stick ice cream sticks down her blowhole. Non-fatal incidents occur more frequently, both in the wild and in captivity.

While dolphin attacks occur far less frequently than attacks by other sea animals, such as sharks, some scientists are worried about the careless programs of human-dolphin interaction. Dr. Andrew J. Read, a biologist at the Duke University Marine Laboratory who studies dolphin attacks, points out that dolphins are large and wild predators, so people should be more careful when they interact with them.

Several scientists who have researched dolphin behaviour have proposed that dolphins' unusually high intelligence in comparison to other animals means that dolphins should be seen as non-human persons who should have their own specific rights and that it is morally unacceptable to keep them captive for entertainment purposes or to kill them either intentionally for consumption or unintentionally as by-catch. Four countries – Chile, Costa Rica, Hungary, and India – have declared dolphins to be "non-human persons" and have banned the capture and import of live dolphins for entertainment.

A military dolphin is a dolphin trained for military uses. A number of militaries have employed dolphins for various purposes from finding mines to rescuing lost or trapped humans. The military use of dolphins, however, drew scrutiny during the Vietnam War when rumors circulated that the United States Navy was training dolphins to kill Vietnamese divers. The United States Navy denies that at any point dolphins were trained for combat. Dolphins are still being trained by the United States Navy for other tasks as part of the U.S. Navy Marine Mammal Program. The Russian military is believed to have closed its marine mammal program in the early 1990s. In 2000 the press reported that dolphins trained to kill by the Soviet Navy had been sold to Iran.

Dolphins are an increasingly popular choice of animal-assisted therapy for psychological problems and developmental disabilities. For example, a 2005 study found dolphins an effective treatment for mild to moderate depression. However, this study was criticized on several grounds. For example, it is not known whether dolphins are more effective than common pets. Reviews of this and other published dolphin-assisted therapy (DAT) studies have found important methodological flaws and have concluded that there is no compelling scientific evidence that DAT is a legitimate therapy or that it affords more than fleeting mood improvement.

In some parts of the world, such as Taiji, Japan and the Faroe Islands, dolphins are traditionally considered as food, and are killed in harpoon or drive hunts.
Dolphin meat is consumed in a small number of countries worldwide, which include Japan and Peru (where it is referred to as "chancho marino", or "sea pork"). While Japan may be the best-known and most controversial example, only a very small minority of the population has ever sampled it.

Dolphin meat is dense and such a dark shade of red as to appear black. Fat is located in a layer of blubber between the meat and the skin. When dolphin meat is eaten in Japan, it is often cut into thin strips and eaten raw as "sashimi", garnished with onion and either horseradish or grated garlic, much as with "sashimi" of whale or horse meat ("basashi"). When cooked, dolphin meat is cut into bite-size cubes and then batter-fried or simmered in a "miso" sauce with vegetables. Cooked dolphin meat has a flavor very similar to beef liver.

There have been human health concerns associated with the consumption of dolphin meat in Japan after tests showed that dolphin meat contained high levels of mercury. There are no known cases of mercury poisoning as a result of consuming dolphin meat, though the government continues to monitor people in areas where dolphin meat consumption is high. The Japanese government recommends that children and pregnant women avoid eating dolphin meat on a regular basis.

Similar concerns exist with the consumption of dolphin meat in the Faroe Islands, where prenatal exposure to methylmercury and PCBs primarily from the consumption of pilot whale meat has resulted in neuropsychological deficits amongst children.


Conservation, research and news:

Photos:


</doc>
<doc id="9067" url="https://en.wikipedia.org/wiki?curid=9067" title="Division ring">
Division ring

In abstract algebra, a division ring, also called a skew field, is a ring in which division is possible. Specifically, it is a nonzero ring in which every nonzero element has a multiplicative inverse, i.e., an element with Stated differently, a ring is a division ring if and only if the group of units equals the set of all nonzero elements. A division ring is a type of noncommutative ring under the looser definition where "noncommutative ring" refers to rings which are not "necessarily" commutative.

Division rings differ from fields only in that their multiplication is not required to be commutative. However, by Wedderburn's little theorem all finite division rings are commutative and therefore finite fields. Historically, division rings were sometimes referred to as fields, while fields were called “commutative fields”. 

All division rings are simple, i.e. have no two-sided ideal besides the zero ideal and itself.

All fields are division rings; more interesting examples are the non-commutative division rings. The best known example is the ring of quaternions H. If we allow only rational instead of real coefficients in the constructions of the quaternions, we obtain another division ring. In general, if "R" is a ring and "S" is a simple module over "R", then, by Schur's lemma, the endomorphism ring of "S" is a division ring; every division ring arises in this fashion from some simple module.

Much of linear algebra may be formulated, and remains correct, for modules over a division ring "D" instead of vector spaces over a field. Doing so it must be specified whether one is considering right or left modules, and some care is needed in properly distinguishing left and right in formulas. Working in coordinates, elements of a finite dimensional right module can be represented by column vectors, which can be multiplied on the right by scalars, and on the left by matrices (representing linear maps); for elements of a finite dimensional left module, row vectors must be used, which can be multiplied on the left by scalars, and on the right by matrices. The dual of a right module is a left module, and vice versa. The transpose of a matrix must be viewed as a matrix over the opposite division ring "D" in order for the rule to remain valid.

Every module over a division ring is free; i.e., has a basis, and all bases of a module have the same number of elements. Linear maps between finite-dimensional modules over a division ring can be described by matrices; the fact that linear maps by definition commute with scalar multiplication is most conveniently represented in notation by writing them on the "opposite" side of vectors as scalars are. The Gaussian elimination algorithm remains applicable. The column rank of a matrix is the dimension of the right module generated by the columns, and the row rank is dimension of the left module generated by the rows; the same proof as for the vector space case can be used to show that these ranks are the same, and define the rank of a matrix.

In fact the converse is also true and this gives a "characterization of division rings" via their module category: A unital ring "R" is a division ring if and only if every R-module is free.

The center of a division ring is commutative and therefore a field. Every division ring is therefore a division algebra over its center. Division rings can be roughly classified according to whether or not they are finite-dimensional or infinite-dimensional over their centers. The former are called "centrally finite" and the latter "centrally infinite". Every field is, of course, one-dimensional over its center. The ring of Hamiltonian quaternions forms a 4-dimensional algebra over its center, which is isomorphic to the real numbers.


Wedderburn's little theorem: All finite division rings are commutative and therefore finite fields. (Ernst Witt gave a simple proof.)

Frobenius theorem: The only finite-dimensional associative division algebras over the reals are the reals themselves, the complex numbers, and the quaternions.

Division rings "used to be" called "fields" in an older usage. In many languages, a word meaning "body" is used for division rings, in some languages designating either commutative or non-commutative division rings, while in others specifically designating commutative division rings (what we now call fields in English). A more complete comparison is found in the article Field (mathematics).

Skew fields have an interesting semantic feature: a modifier (here "skew") "widens" the scope of the base term (here "field"). Thus a field is a particular type of skew field, and not all skew fields are fields.

While division rings and algebras as discussed here are assumed to have associative multiplication, nonassociative division algebras such as the octonions are also of interest.

A near-field is an algebraic structure similar to a division ring, except that it has only one of the two distributive laws.




</doc>
<doc id="9069" url="https://en.wikipedia.org/wiki?curid=9069" title="Dia (software)">
Dia (software)

Dia ()
is free and open source general-purpose diagramming software, developed originally by Alexander Larsson. Dia uses a controlled single document interface (SDI) similar to GIMP and Inkscape.

Dia has a modular design with several shape packages available for different needs: flowchart, network diagrams, circuit diagrams, and more. It does not restrict symbols and connectors from various categories from being placed together.

Dia has special objects to help draw entity-relationship models (obsoleted tedia2sql or newer parsediasql can be used to create the SQL DDL), Unified Modeling Language (UML) diagrams, flowcharts, network diagrams, and simple electrical circuits. It is also possible to add support for new shapes by writing simple XML files, using a subset of Scalable Vector Graphics (SVG) to draw the shape.

Dia loads and saves diagrams in a custom XML format which is, by default, gzipped to save space. It can print large diagrams spanning multiple pages and can also be scripted using the Python programming language.

Dia can export diagrams to various formats including the following:

Dia was originally created by Alexander Larsson but he moved on to work on GNOME and other projects. James Henstridge then took over as the lead developer, but he also moved on to other projects. He was followed by Cyrille Chepelov and Lars Ræder Clausen in turn.

Dia is maintained by a group of developers: Hans Breuer, Steffen Macke, and Sameer Sahasrabuddhe.

Dia is written in C, and has an extension system, which also supports writing extensions in Python.




</doc>
<doc id="9070" url="https://en.wikipedia.org/wiki?curid=9070" title="Deep Space 1">
Deep Space 1

Deep Space 1 (DS1) was a NASA technology demonstration spacecraft which flew by an asteroid and a comet. It was part of the New Millennium Program, dedicated to testing advanced technologies.

Launched on 24 October 1998, the "Deep Space 1" spacecraft carried out a flyby of asteroid 9969 Braille, which was its primary science target. The mission was extended twice to include an encounter with comet 19P/Borrelly and further engineering testing. Problems during its initial stages and with its star tracker led to repeated changes in mission configuration. While the flyby of the asteroid was only a partial success, the encounter with the comet retrieved valuable information. Three of twelve technologies on board had to work within a few minutes of separation from the carrier rocket for the mission to continue.

The Deep Space series was continued by the "Deep Space 2" probes, which were launched in January 1999 piggybacked on the Mars Polar Lander and were intended to strike the surface of Mars (though contact was lost and the mission failed). "Deep Space 1" was the first NASA spacecraft to use ion propulsion rather than the traditional chemical-powered rockets.

The purpose of "Deep Space 1" was technology development and validation for future missions; 12 technologies were tested:

The Autonav system, developed by NASA's Jet Propulsion Laboratory, takes images of known bright asteroids. The asteroids in the inner Solar System move in relation to other bodies at a noticeable, predictable speed. Thus a spacecraft can determine its relative position by tracking such asteroids across the star background, which appears fixed over such timescales. Two or more asteroids let the spacecraft triangulate its position; two or more positions in time let the spacecraft determine its trajectory. Existing spacecraft are tracked by their interactions with the transmitters of the NASA Deep Space Network (DSN), in effect an inverse GPS. However, DSN tracking requires many skilled operators, and the DSN is overburdened by its use as a communications network. The use of Autonav reduces mission cost and DSN demands.

The Autonav system can also be used in reverse, tracking the position of bodies relative to the spacecraft. This is used to acquire targets for the scientific instruments. The spacecraft is programmed with the target's coarse location. After initial acquisition, Autonav keeps the subject in frame, even commandeering the spacecraft's attitude control. The next spacecraft to use Autonav was "Deep Impact".

Primary power for the mission was produced by a new solar array technology, the Solar Concentrator Array with Refractive Linear Element Technology (SCARLET), which uses linear Fresnel lenses made of silicone to concentrate sunlight onto solar cells. ABLE Engineering developed the concentrator technology and built the solar array for DS1, with Entech Inc, who supplied the Fresnel optics, and the NASA Glenn Research Center. The activity was sponsored by the Ballistic Missile Defense Organization. The concentrating lens technology was combined with dual-junction solar cells, which had considerably better performance than the GaAs solar cells that were the state of the art at the time of the mission launch.

The SCARLET arrays generated 2.5 kilowatts at 1 AU, with less size and weight than conventional arrays.

Although ion engines had been developed at NASA since the late 1950s, with the exception of the SERT missions in the 1960s, the technology had not been demonstrated in flight on United States spacecraft, though hundreds of Hall-effect engines had been used on Soviet and Russian spacecraft. This lack of a performance history in space meant that despite the potential savings in propellant mass, the technology was considered too experimental to be used for high-cost missions. Furthermore, unforeseen side effects of ion propulsion might in some way interfere with typical scientific experiments, such as fields and particle measurements. Therefore, it was a primary mission of the "Deep Space 1" demonstration to show long-duration use of an ion thruster on a scientific mission.

The NASA Solar Technology Application Readiness (NSTAR) electrostatic ion thruster, developed at NASA Glenn, achieves a specific impulse of 1000–3000 seconds. This is an order of magnitude higher than traditional space propulsion methods, resulting in a mass savings of approximately half. This leads to much cheaper launch vehicles. Although the engine produces just thrust at maximal power (2,100 W on DS1), the craft achieved high speeds because ion engines thrust continuously for long periods.

The next spacecraft to use NSTAR engines was "Dawn", with three redundant units.

Remote Agent (RAX), remote intelligent self-repair software developed at NASA's Ames Research Center and the Jet Propulsion Laboratory, was the first artificial-intelligence control system to control a spacecraft without human supervision. Remote Agent successfully demonstrated the ability to plan onboard activities and correctly diagnose and respond to simulated faults in spacecraft components through its built-in REPL environment. Autonomous control will enable future spacecraft to operate at greater distances from Earth and to carry out more sophisticated science-gathering activities in deep space. Components of the Remote Agent software have been used to support other NASA missions. Major components of Remote Agent were a robust planner (EUROPA), a plan-execution system (EXEC) and a model-based diagnostic system (Livingstone). EUROPA was used as a ground-based planner for the Mars Exploration Rovers. EUROPA II was used to support the "Phoenix" Mars lander and the Mars Science Laboratory. Livingstone2 was flown as an experiment aboard Earth Observing-1 and on an F/A-18 Hornet at NASA's Dryden Flight Research Center.

Another method for reducing DSN burdens is the Beacon Monitor experiment. During the long cruise periods of the mission, spacecraft operations are essentially suspended. Instead of data, the craft emits a carrier signal on a predetermined frequency. Without data decoding, the carrier can be detected by much simpler ground antennas and receivers. If the spacecraft detects an anomaly, it changes the carrier between four tones, based on urgency. Ground receivers then signal operators to divert DSN resources. This prevents skilled operators and expensive hardware from babysitting an unburdened mission operating nominally. A similar system is used on the "New Horizons" Pluto probe to keep costs down during its ten-year cruise from Jupiter to Pluto.

The Small Deep Space Transponder (SDST) is a compact and lightweight radio-communications system. Aside from using miniaturized components, the SDST is capable of communicating over the K band. Because this band is higher in frequency than bands currently in use by deep-space missions, the same amount of data can be sent by smaller equipment in space and on the ground. Conversely, existing DSN antennas can split time among more missions. At the time of launch, the DSN had a small number of K receivers installed on an experimental basis; K operations and missions are increasing.

The SDST was later used on other space missions such as the Mars Science Laboratory (the Mars rover "Curiosity").

Once at a target, DS1 senses the particle environment with the PEPE (Plasma Experiment for Planetary Exploration) instrument. This instrument measured the flux of ions and electrons as a function of their energy and direction. The composition of the ions was determined by using a time-of-flight mass spectrometer.

The MICAS (Miniature Integrated Camera And Spectrometer) instrument combined visible light imaging with infrared and ultraviolet spectroscopy to determine chemical composition. All channels share a telescope, which uses a silicon carbide mirror.

Both PEPE and MICAS were similar in capabilities to larger instruments or suites of instruments on other spacecraft. They were designed to be smaller and require lower power than those used on previous missions.

Prior to launch, "Deep Space 1" was intended to visit comet 76P/West–Kohoutek–Ikemura and asteroid 3352 McAuliffe. Because of the delayed launch, the targets were changed to asteroid 9969 Braille (at the time called 1992 KD) and comet 107P/Wilson–Harrington. It achieved an impaired flyby of Braille and, due to problems with the star tracker, was re-tasked to fly by comet 19P/Borrelly, which was successful. An August 2002 flyby of asteroid as another extended mission was considered, but ultimately was not advanced due to cost concerns. During the mission, high quality infrared spectra of Mars were also taken.

The ion propulsion engine initially failed after 4.5 minutes of operation. However, it was later restored to action and performed excellently. Early in the mission, material ejected during launch vehicle separation caused the closely spaced ion extraction grids to short-circuit. The contamination was eventually cleared, as the material was eroded by electrical arcing, sublimed by outgassing, or simply allowed to drift out. This was achieved by repeatedly restarting the engine in an engine repair mode, arcing across trapped material.

It was thought that the ion engine exhaust might interfere with other spacecraft systems, such as radio communications or the science instruments. The PEPE detectors had a secondary function to monitor such effects from the engine. No interference was found.

Another failure was the loss of the star tracker. The star tracker determines spacecraft orientation by comparing the star field to its internal charts. The mission was saved when the MICAS camera was reprogrammed to substitute for the star tracker. Although MICAS is more sensitive, its field-of-view is an order of magnitude smaller, creating a greater information processing burden. Ironically, the star tracker was an off-the-shelf component, expected to be highly reliable.

Without a working star tracker, ion thrusting was temporarily suspended. The loss of thrust time forced the cancellation of a flyby past comet 107P/Wilson–Harrington.

The Autonav system required occasional manual corrections. Most problems were in identifying objects that were too dim, or were difficult to identify because of brighter objects causing diffraction spikes and reflections in the camera, causing Autonav to misidentify targets.

The Remote Agent system was presented with three simulated failures on the spacecraft and correctly handled each event.
Overall this constituted a successful demonstration of fully autonomous planning, diagnosis, and recovery.

The MICAS instrument was a design success, but the ultraviolet channel failed due to an electrical fault. Later in the mission, after the star tracker failure, MICAS assumed this duty as well. This caused continual interruptions in its scientific use during the remaining mission, including the Comet Borrelly encounter.

The flyby of the asteroid 9969 Braille was only a partial success. "Deep Space 1" was intended to perform the flyby at at only from the asteroid. Due to technical difficulties, including a software crash shortly before approach, the craft instead passed Braille at a distance of . This, plus Braille's lower albedo, meant that the asteroid was not bright enough for the Autonav to focus the camera in the right direction, and the picture shoot was delayed by almost an hour. The resulting pictures were disappointingly indistinct.

However, the flyby of Comet Borrelly was a great success and returned extremely detailed images of the comet's surface. Such images were of higher resolution than the only previous pictures, of Halley's Comet taken by the "Giotto" spacecraft. The PEPE instrument reported that the comet's fields were offset from the nucleus. This is believed to be due to emission of jets, which were not distributed evenly across the comet's surface.

Despite having no debris shields, the spacecraft survived the comet passage intact. Once again, the sparse comet jets did not appear to point towards the spacecraft. "Deep Space 1" then entered its second extended mission phase, focused on retesting the spacecraft's hardware technologies. The focus of this mission phase was on the ion engine systems. The spacecraft eventually ran out of hydrazine fuel for its attitude control thrusters. The highly efficient ion thruster had a sufficient amount of propellant left to perform attitude control in addition to main propulsion, thus allowing the mission to continue.

During late October and early November 1999, during the spacecraft's post-Braille encounter coast phase, "Deep Space 1" observed Mars with its MICAS instrument. Although this was a very distant flyby, the instrument did succeed in taking multiple infrared spectra of the planet.

"Deep Space 1" succeeded in its primary and secondary objectives, returning valuable science data and images. DS1's ion engines were shut down on 18 December 2001 at approximately 20:00:00 UTC, signaling the end of the mission. On-board communications were commanded to remain active in case the craft is needed in the future. However, attempts to resume contact in March 2002 were unsuccessful. It remains within the Solar System, orbiting the Sun.





</doc>
<doc id="9071" url="https://en.wikipedia.org/wiki?curid=9071" title="King David (disambiguation)">
King David (disambiguation)

David was the second king of the united Kingdom of Israel.

King David may also refer to:





</doc>
<doc id="9072" url="https://en.wikipedia.org/wiki?curid=9072" title="Jacques-Louis David">
Jacques-Louis David

Jacques-Louis David (; 30 August 1748 – 29 December 1825) was a French painter in the Neoclassical style, considered to be the preeminent painter of the era. In the 1780s his cerebral brand of history painting marked a change in taste away from Rococo frivolity toward classical austerity and severity and heightened feeling, harmonizing with the moral climate of the final years of the Ancien Régime.

David later became an active supporter of the French Revolution and friend of Maximilien Robespierre (1758–1794), and was effectively a dictator of the arts under the French Republic. Imprisoned after Robespierre's fall from power, he aligned himself with yet another political regime upon his release: that of Napoleon, The First Consul of France. At this time he developed his Empire style, notable for its use of warm Venetian colours. After Napoleon's fall from Imperial power and the Bourbon revival, David exiled himself to Brussels, then in the United Kingdom of the Netherlands, where he remained until his death. David had , making him the strongest influence in French art of the early 19th century, especially academic Salon painting.

Jacques-Louis David was born into a prosperous family in Paris on 30 August 1748. When he was about nine his father was killed in a duel and his mother left him with his well-off architect uncles. They saw to it that he received an excellent education at the Collège des Quatre-Nations, University of Paris, but he was never a good student: he had a facial tumor that impeded his speech, and he was always preoccupied with drawing. He covered his notebooks with drawings, and he once said, "I was always hiding behind the instructor's chair, drawing for the duration of the class". Soon, he desired to be a painter, but his uncles and mother wanted him to be an architect. He overcame the opposition, and went to learn from François Boucher (1703–1770), the leading painter of the time, who was also a distant relative. Boucher was a Rococo painter, but tastes were changing, and the fashion for Rococo was giving way to a more classical style. Boucher decided that instead of taking over David's tutelage, he would send David to his friend, Joseph-Marie Vien (1716–1809), a painter who embraced the classical reaction to Rococo. There David attended the Royal Academy, based in what is now the Louvre.

Each year the Academy awarded an outstanding student the prestigious Prix de Rome, which funded a three- to five-year stay in the Eternal City. The culmination of the Academy's educational program, the Rome trip provided its winners the opportunity to study the remains of classical antiquity and the works of the Italian Renaissance masters at first hand. Each "pensionnaire" was lodged in the French Academy's Roman outpost, which from the years 1737 to 1793 was the Palazzo Mancini in the Via del Corso. David competed for, and failed to win, the prize for three consecutive years (with "Minerva Fighting Mars", "Diana and Apollo Killing Niobe's Children" and "The Death of Seneca"), each failure contributing to his lifelong grudge against the institution. After his second loss in 1772, David went on a hunger strike, which lasted two and a half days before the faculty encouraged him to continue painting. Confident he now had the support and backing needed to win the prize, he resumed his studies with great zeal—only to fail to win the Prix de Rome again the following year. Finally, in 1774, David was awarded the Prix de Rome on the strength of his painting of "Erasistratus Discovering the Cause of Antiochus' Disease", a subject set by the judges. In October 1775 he made the journey to Italy with his mentor, Joseph-Marie Vien, who had just been appointed director of the French Academy at Rome.

While in Italy, David especially studied the works of 17th-century masters such as Poussin, Caravaggio, and the Carracci. Although he declared, "the Antique will not seduce me, it lacks animation, it does not move", David filled twelve sketchbooks with drawings that he and his studio used as model books for the rest of his life. He was introduced to the painter Raphael Mengs (1728–1779), who opposed the tendency in Rococo painting to sweeten and trivialize ancient subjects, advocating instead the rigorous study of classical sources and close adherence to ancient models. Mengs' principled, historicizing approach to the representation of classical subjects profoundly influenced David's pre-revolutionary painting, such as "The Vestal Virgin", probably from the 1780s. Mengs also introduced David to the theoretical writings on ancient sculpture by Johann Joachim Winckelmann (1717–1768), the German scholar held to be the founder of modern art history. In 1779, David toured the newly excavated ruins of Pompeii, which deepened his belief that the persistence of classical culture was an index of its eternal conceptual and formal power. While in Rome, David also assiduously studied the High Renaissance painters, Raphael making a profound and lasting impression on the young French artist.

David's fellow students at the academy found him difficult to get along with, but they recognized his genius. David's stay at the French Academy in Rome was extended by a year, but in July 1780 he returned to Paris. There, he found people ready to use their influence for him, and he was made a member of the Royal Academy. He sent the Academy two paintings, and both were included in the Salon of 1781, a high honor. He was praised by his famous contemporary painters, but the administration of the Royal Academy was very hostile to this young upstart. After the Salon, the King granted David lodging in the Louvre, an ancient and much desired privilege of great artists. When the contractor of the King's buildings, M. Pécoul, was arranging with David, he asked the artist to marry his daughter, Marguerite Charlotte. This marriage brought him money and eventually four children. David had his own pupils, about 40 to 50, and was commissioned by the government to paint "Horace defended by his Father", but he soon decided, "Only in Rome can I paint Romans." His father-in-law provided the money he needed for the trip, and David headed for Rome with his wife and three of his students, one of whom, Jean-Germain Drouais (1763–1788), was the Prix de Rome winner of that year.

In Rome, David painted his famous "Oath of the Horatii", 1784. In this piece, the artist references Enlightenment values while alluding to Rousseau's social contract. The republican ideal of the general will becomes the focus of the painting with all three sons positioned in compliance with the father. The Oath between the characters can be read as an act of unification of men to the binding of the state. The issue of gender roles also becomes apparent in this piece, as the women in Horatii greatly contrast the group of brothers. David depicts the father with his back to the women, shutting them out of the oath making ritual; they also appear to be smaller in scale than the male figures. The masculine virility and discipline displayed by the men's rigid and confident stances is also severely contrasted to the slouching, swooning female softness created in the other half of the composition. Here we see the clear division of male-female attributes that confined the sexes to specific roles under Rousseau's popularized doctrine of "separate spheres".

These revolutionary ideals are also apparent in the "Distribution of Eagles". While "Oath of the Horatii" and "The Tennis Court Oath" stress the importance of masculine self-sacrifice for one's country and patriotism, the "Distribution of Eagles" would ask for self-sacrifice for one's Emperor (Napoleon) and the importance of battlefield glory.
In 1787, David did not become the Director of the French Academy in Rome, which was a position he wanted dearly. The Count in charge of the appointments said David was too young, but said he would support him in 6 to 12 years. This situation would be one of many that would cause him to lash out at the Academy in years to come.

For the Salon of 1787, David exhibited his famous "Death of Socrates". "Condemned to death, Socrates, strong, calm and at peace, discusses the immortality of the soul. Surrounded by Crito, his grieving friends and students, he is teaching, philosophizing, and in fact, thanking the God of Health, Asclepius, for the hemlock brew which will ensure a peaceful death... The wife of Socrates can be seen grieving alone outside the chamber, dismissed for her weakness. Plato is depicted as an old man seated at the end of the bed." Critics compared the Socrates with Michelangelo's Sistine Ceiling and Raphael's Stanze, and one, after ten visits to the Salon, described it as "in every sense perfect". Denis Diderot said it looked like he copied it from some ancient bas-relief. The painting was very much in tune with the political climate at the time. For this painting, David was not honored by a royal "works of encouragement".

For his next painting, David created "The Lictors Bring to Brutus the Bodies of His Sons". The work had tremendous appeal for the time. Before the opening of the Salon, the French Revolution had begun. The National Assembly had been established, and the Bastille had fallen. The royal court did not want propaganda agitating the people, so all paintings had to be checked before being hung. David's portrait of Lavoisier, who was a chemist and physicist as well as an active member of the Jacobin party, was banned by the authorities for such reasons. When the newspapers reported that the government had not allowed the showing of "The Lictors Bring to Brutus the Bodies of His Sons", the people were outraged, and the royals were forced to give in. The painting was hung in the exhibition, protected by art students. The painting depicts Lucius Junius Brutus, the Roman leader, grieving for his sons. Brutus's sons had attempted to overthrow the government and restore the monarchy, so the father ordered their death to maintain the republic. Thus, Brutus was the heroic defender of the republic, at the cost of his own family. On the right, the Mother holds her two daughters, and the nurse is seen on the far right, in anguish. Brutus sits on the left, alone, brooding, seemingly dismissing the dead bodies of his sons. Knowing what he did was best for his country, but the tense posture of his feet and toes reveals his inner turmoil. The whole painting was a Republican symbol, and obviously had immense meaning during these times in France.

In the beginning, David was a supporter of the Revolution, a friend of Robespierre and a member of the Jacobin Club. While others were leaving the country for new and greater opportunities, David stayed to help destroy the old order; he was a regicide who voted in the National Convention for the Execution of Louis XVI. It is uncertain why he did this, as there were many more opportunities for him under the King than the new order; some people suggest David's love for the classical made him embrace everything about that period, including a republican government.
Others believed that they found the key to the artist's revolutionary career in his personality. Undoubtedly, David's artistic sensibility, mercurial temperament, volatile emotions, ardent enthusiasm, and fierce independence might have been expected to help turn him against the established order but they did not fully explain his devotion to the republican regime. Nor did the vague statements of those who insisted upon his "powerful ambition...and unusual energy of will" actually account for his revolutionary connections. Those who knew him maintained that "generous ardor", high-minded idealism and well-meaning though sometimes fanatical enthusiasm, rather than opportunism and jealousy, motivated his activities during this period.

Soon, David turned his critical sights on the Royal Academy of Painting and Sculpture. This attack was probably caused primarily by the hypocrisy of the organization and their personal opposition against his work, as seen in previous episodes in David's life. The Royal Academy was chock full of royalists, and David's attempt to reform it did not go over well with the members. However, the deck was stacked against this symbol of the old regime, and the National Assembly ordered it to make changes to conform to the new constitution.

David then began work on something that would later hound him: propaganda for the new republic. David's painting of Brutus was shown during the play "Brutus", by the famous Frenchman, Voltaire. The people responded in an uproar of approval.

In 1789, Jacques-Louis David attempted to leave his artistic mark on the historical beginnings of the French Revolution with his painting of "The Oath of the Tennis Court". David undertook this task not out of personal political conviction but rather because he was commissioned to do so. The painting was meant to commemorate the event of the same name but was never completed. A meeting of the Estates General was convened in May to address reforms of the monarchy. Dissent arose over whether the three estates would meet separately, as had been tradition, or as one body. The King's acquiescence with the demands of the upper orders led to the deputies of the Third Estate renaming themselves as the National Assembly on 17 June. They were locked out of the meeting hall three days later when they attempted to meet, and forced to reconvene to the royal indoor tennis court. Presided over by Jean-Sylvain Bailly, they made a 'solemn oath never to separate' until a national constitution had been created. In 1789 this event was seen as a symbol of the national unity against the "ancien regime". Rejecting the current conditions, the oath signified a new transition in human history and ideology. David was enlisted by the Society of Friends of the Constitution, the body that would eventually form the Jacobins, to enshrine this symbolic event.

This instance is notable in more ways than one because it eventually led David to finally become involved in politics as he joined the Jacobins. The picture was meant to be massive in scale; the figures in the foreground were to be life-sized portraits of the counterparts, including Jean-Sylvain Bailly, the President of the Constituent Assembly. Seeking additional funding, David turned to the Society of Friends of the Constitution. The funding for the project was to come from over three thousand subscribers hoping to receive a print of the image. However, when the funding was insufficient, the state ended up financing the project.

David set out in 1790 to transform the contemporary event into a major historical picture which would appear at the Salon of 1791 as a large pen-and-ink drawing. As in the "Oath of the Horatii", David represents the unity of men in the service of a patriotic ideal. The outstretched arms which are prominent in both works betray David's deeply held belief that acts of republican virtue akin to those of the Romans were being played out in France. In what was essentially an act of intellect and reason, David creates an air of drama in this work. The very power of the people appears to be "blowing" through the scene with the stormy weather, in a sense alluding to the storm that would be the revolution.

Symbolism in this work of art closely represents the revolutionary events taking place at the time. The figure in the middle is raising his right arm making the oath that they will never disband until they have reached their goal of creating a "constitution of the realm fixed upon solid foundations." The importance of this symbol is highlighted by the fact that the crowd's arms are angled to his hand forming a triangular shape. Additionally, the open space in the top half contrasted to the commotion in the lower half serves to emphasize the magnitude of the Tennis Court Oath.

In his attempt to depict political events of the Revolution in "real time", David was venturing down a new and untrodden path in the art world. However, Thomas Crow argues that this path "proved to be less a way forward than a cul-de-sac for history painting." Essentially, the history of the demise of David's "The Tennis Court Oath" illustrates the difficulty of creating works of art that portray current and controversial political occurrences. Political circumstances in France proved too volatile to allow the completion of the painting. The unity that was to be symbolized in "The Tennis Court Oath" no longer existed in radicalized 1792. The National Assembly had split between conservatives and radical Jacobins, both vying for political power. By 1792 there was no longer consensus that all the revolutionaries at the tennis court were "heroes". A sizeable number of the heroes of 1789 had become the villains of 1792. In this unstable political climate David's work remained unfinished. With only a few nude figures sketched onto the massive canvas, David abandoned "The Oath of the Tennis Court". To have completed it would have been politically unsound. After this incident, when David attempted to make a political statement in his paintings, he returned to the less politically charged use of metaphor to convey his message.

When Voltaire died in 1778, the church denied him a church burial, and his body was interred near a monastery. A year later, Voltaire's old friends began a campaign to have his body buried in the Panthéon, as church property had been confiscated by the French Government. In 1791 David was appointed to head the organizing committee for the ceremony, a parade through the streets of Paris to the Panthéon. Despite rain, and opposition from conservatives based on the amount of money that was being spent, the procession went ahead. Up to 100,000 people watched the "Father of the Revolution" be carried to his resting place. This was the first of many large festivals organized by David for the republic. He went on to organize festivals for martyrs that died fighting royalists. These funerals echoed the religious festivals of the pagan Greeks and Romans and are seen by many as Saturnalian.

David incorporated many revolutionary symbols into these theatrical performances and orchestrated ceremonial rituals; in effect radicalizing the applied arts, themselves. The most popular symbol for which David was responsible as propaganda minister was drawn from classical Greek images; changing and transforming them with contemporary politics. In an elaborate festival held on the anniversary of the revolt that brought the monarchy to its knees, David's Hercules figure was revealed in a procession following the Goddess of Liberty (Marianne). Liberty, the symbol of Enlightenment ideals was here being overturned by the Hercules symbol; that of strength and passion for the protection of the Republic against disunity and factionalism. In his speech during the procession, David "explicitly emphasized the opposition between people and monarchy; Hercules was chosen, after all, to make this opposition more evident". The ideals that David linked to his Hercules single-handedly transformed the figure from a sign of the old regime into a powerful new symbol of revolution. "David turned him into the representation of a collective, popular power. He took one of the favorite signs of monarchy and reproduced, elevated, and monumentalized it into the sign of its opposite." Hercules, the image, became to the revolutionaries, something to rally around.

In June 1791, the King made an ill-fated attempt to flee the country, but was apprehended short of his goal on the Austrian Netherlands border and was forced to return under guard to Paris. Louis XVI had made secret requests to Emperor Leopold II of Austria, Marie-Antoinette's brother, to restore him to his throne. This was granted and Austria threatened France if the royal couple were hurt. In reaction, the people arrested the King. This led to an Invasion after the trials and execution of Louis and Marie-Antoinette. The Bourbon monarchy was destroyed by the French people in 1792—it would be restored after Napoleon, then destroyed again with the Restoration of the House of Bonaparte. When the new National Convention held its first meeting, David was sitting with his friends Jean-Paul Marat and Robespierre. In the Convention, David soon earned a nickname "ferocious terrorist". Soon, Robespierre's agents discovered a secret vault of the king's proving he was trying to overthrow the government, and demanded his execution. The National Convention held the trial of Louis XVI and David voted for the death of the King, causing his wife, a royalist, to divorce him.

When Louis XVI was executed on 21 January 1793, another man had already died as well—Louis Michel le Peletier de Saint-Fargeau. Le Peletier was killed on the preceding day by a royal bodyguard in revenge for having voted for the death of the King. David was called upon to organize a funeral, and he painted "Le Peletier Assassinated". In it, the assassin's sword was seen hanging by a single strand of horsehair above Le Peletier's body, a concept inspired by the proverbial ancient tale of the sword of Damocles, which illustrated the insecurity of power and position. This underscored the courage displayed by Le Peletier and his companions in routing an oppressive king. The sword pierces a piece of paper on which is written "I vote the death of the tyrant", and as a tribute at the bottom right of the picture David placed the inscription "David to Le Peletier. 20 January 1793". The painting was later destroyed by Le Peletier's royalist daughter, and is known by only a drawing, an engraving, and contemporary accounts. Nevertheless, this work was important in David's career because it was the first completed painting of the French Revolution, made in less than three months, and a work through which he initiated the regeneration process that would continue with "The Death of Marat", David's masterpiece.

On 13 July 1793, David's friend Marat was assassinated by Charlotte Corday with a knife she had hidden in her clothing. She gained entrance to Marat's house on the pretense of presenting him a list of people who should be executed as enemies of France. Marat thanked her and said that they would be guillotined next week upon which Corday immediately fatally stabbed him. She was guillotined shortly thereafter. Corday was of an opposing political party, whose name can be seen in the note Marat holds in David's subsequent painting, "The Death of Marat". Marat, a member of the National Convention and a journalist, had a skin disease that caused him to itch horribly. The only relief he could get was in his bath over which he improvised a desk to write his list of suspect counter-revolutionaries who were to be quickly tried and, if convicted, guillotined. David once again organized a spectacular funeral, and Marat was buried in the Panthéon. Marat's body was to be placed upon a Roman bed, his wound displayed and his right arm extended holding the pen which he had used to defend the Republic and its people. This concept was to be complicated by the fact that the corpse had begun to putrefy. Marat's body had to be periodically sprinkled with water and vinegar as the public crowded to see his corpse prior to the funeral on 15 and 16 July. The stench became so bad however that the funeral had to be brought forward to the evening of 16 July.

"The Death of Marat", perhaps David's most famous painting, has been called the Pietà of the revolution. Upon presenting the painting to the convention, he said "Citizens, the people were again calling for their friend; their desolate voice was heard: David, take up your brushes.., avenge Marat... I heard the voice of the people. I obeyed." David had to work quickly, but the result was a simple and powerful image.

"The Death of Marat", 1793, became the leading image of the Terror and immortalized both Marat and David in the world of the revolution. This piece stands today as "a moving testimony to what can be achieved when an artist's political convictions are directly manifested in his work". A political martyr was instantly created as David portrayed Marat with all the marks of the real murder, in a fashion which greatly resembles that of Christ or his disciples. The subject although realistically depicted remains lifeless in a rather supernatural composition. With the surrogate tombstone placed in front of him and the almost holy light cast upon the whole scene; alluding to an out of this world existence. "Atheists though they were, David and Marat, like so many other fervent social reformers of the modern world, seem to have created a new kind of religion." At the very center of these beliefs, there stood the republic.

After the King's execution, war broke out between the new Republic and virtually every major power in Europe. David, as a member of the Committee of General Security, contributed directly to the Reign of Terror. David organized his last festival: the festival of the Supreme Being. Robespierre had realized what a tremendous propaganda tool these festivals were, and he decided to create a new religion, mixing moral ideas with the Republic and based on the ideas of Rousseau. This process had already begun by confiscating church lands and requiring priests to take an oath to the state. The festivals, called fêtes, would be the method of indoctrination. On the appointed day, 20 Prairial by the revolutionary calendar, Robespierre spoke, descended steps, and with a torch presented to him by David, incinerated a cardboard image symbolizing atheism, revealing an image of wisdom underneath.

Soon, the war began to go well; French troops marched across the southern half of the Netherlands (which would later become Belgium), and the emergency that had placed the Committee of Public Safety in control was no more. Then plotters seized Robespierre at the National Convention and he was later guillotined, in effect ending the Reign of Terror. As Robespierre was arrested, David yelled to his friend "if you drink hemlock, I shall drink it with you." After this, he supposedly fell ill, and did not attend the evening session because of "stomach pain", which saved him from being guillotined along with Robespierre. David was arrested and placed in prison, first from 2 August to 28 December 1794 and then from 29 May to 3 August 1795. There he painted his own portrait, showing him much younger than he actually was, as well as that of his jailer.

After David's wife visited him in jail, he conceived the idea of telling the story of The rape of the Sabine women. "The Sabine Women Enforcing Peace by Running between the Combatants", also called "The Intervention of the Sabine Women" is said to have been painted to honor his wife, with the theme being love prevailing over conflict. The painting was also seen as a plea for the people to reunite after the bloodshed of the revolution.

David conceived a new style for this painting, one which he called the "Grecian style", as opposed to the "Roman style" of his earlier historical paintings. The new style was influenced heavily by the work of art historian Johann Joachim Winckelmann. In David's words, "the most prominent general characteristics of the Greek masterpieces are a noble simplicity and silent greatness in pose as well as in expression."
This work also brought him to the attention of Napoleon. The story for the painting is as follows: "The Romans have abducted the daughters of their neighbors, the Sabines. To avenge this abduction, the Sabines attacked Rome, although not immediately—since Hersilia, the daughter of Tatius, the leader of the Sabines, had been married to Romulus, the Roman leader, and then had two children by him in the interim. Here we see Hersilia between her father and husband as she adjures the warriors on both sides not to take wives away from their husbands or mothers away from their children. The other Sabine Women join in her exhortations." During this time, the martyrs of the Revolution were taken from the Pantheon and buried in common ground, and revolutionary statues were destroyed. When David was finally released to the country, France had changed. His wife managed to get him released from prison, and he wrote letters to his former wife, and told her he never ceased loving her. He remarried her in 1796. Finally, wholly restored to his position, he retreated to his studio, took pupils and for the most part, retired from politics.

In August 1796, David and many other artists signed a petition orchestrated by Quatremère de Quincy which questioned the wisdom of the planned seizure of works of art from Rome. The Director Barras believed that David was "tricked" into signing, although one of David's students recalled that in 1798 his master lamented the fact that masterpieces had been imported from Italy.

David's close association with the Committee of Public Safety during the Terror resulted in his signing of the death warrant for Alexandre de Beauharnais, a minor noble. Beauharnais's widow, Joséphine, went on to marry Napoleon Bonaparte and became his empress; David himself depicted their coronation in the "Coronation of Napoleon and Josephine, 2 December 1804".
David had been an admirer of Napoleon from their first meeting, struck by Bonaparte's classical features. Requesting a sitting from the busy and impatient general, David was able to sketch Napoleon in 1797. David recorded the face of the conqueror of Italy, but the full composition of Napoleon holding the peace treaty with Austria remains unfinished. Bonaparte had high esteem for David, and asked him to accompany him to Egypt in 1798, but David refused, claiming he was too old for adventuring and sending instead his student, Antoine-Jean Gros.

After Napoleon's successful coup d'état in 1799, as First Consul he commissioned David to commemorate his daring crossing of the Alps. The crossing of the St. Bernard Pass had allowed the French to surprise the Austrian army and win victory at the Battle of Marengo on 14 June 1800. Although Napoleon had crossed the Alps on a mule, he requested that he be portrayed "calm upon a fiery steed". David complied with "Napoleon Crossing the Saint-Bernard." After the proclamation of the Empire in 1804, David became the official court painter of the regime. During this period he took students, one of whom was the Belgian painter Pieter van Hanselaere.

One of the works David was commissioned for was "The Coronation of Napoleon in Notre Dame". David was permitted to watch the event. He had plans of Notre Dame delivered and participants in the coronation came to his studio to pose individually, though never the Emperor (the only time David obtained a sitting from Napoleon had been in 1797). David did manage to get a private sitting with the Empress Joséphine and Napoleon's sister, Caroline Murat, through the intervention of erstwhile art patron Marshal Joachim Murat, the Emperor's brother-in-law. For his background, David had the choir of Notre Dame act as his fill-in characters. Pope Pius VII came to sit for the painting, and actually blessed David. Napoleon came to see the painter, stared at the canvas for an hour and said "David, I salute you." David had to redo several parts of the painting because of Napoleon's various whims, and for this painting, he received twenty-four thousand Francs.

On the Bourbons returning to power, David figured in the list of proscribed former revolutionaries and Bonapartists—for having voted execution for the deposed King Louis XVI; and for participating in the death of Louis XVII. Mistreated and starved, the imprisoned Louis XVII was forced into a false confession of incest with his mother, Queen Marie-Antoinette. This was untrue, as the son was separated from his mother early and was not allowed communication with her, nevertheless, the allegation helped earn her the guillotine. The newly restored Bourbon King, Louis XVIII, however, granted amnesty to David and even offered him the position of court painter. David refused, preferring self-exile in Brussels. There, he trained and influenced Brussels artists like François-Joseph Navez and Ignace Brice, painted "Cupid and Psyche" and quietly lived the remainder of his life with his wife (whom he had remarried). In that time, he painted smaller-scale mythological scenes, and portraits of citizens of Brussels and Napoleonic émigrés, such as the Baron Gerard.

David created his last great work, "Mars Being Disarmed by Venus and the Three Graces", from 1822 to 1824. In December 1823, he wrote: "This is the last picture I want to paint, but I want to surpass myself in it. I will put the date of my seventy-five years on it and afterwards I will never again pick up my brush." The finished painting—evoking painted porcelain because of its limpid coloration—was exhibited first in Brussels, then in Paris, where his former students flocked to view it. 
The exhibition was profitable—13,000 francs, after deducting operating costs, thus, more than 10,000 people visited and viewed the painting. In his later years, David remained in full command of his artistic faculties, even after a stroke in the spring of 1825 disfigured his face and slurred his speech. In June 1825, he resolved to embark on an improved version of his "Anger of Achilles" (also known as the "Sacrifice of Iphigenie"); the earlier version was completed in 1819 and is now in the collection of the Kimbell Art Museum, Fort Worth, Texas. David remarked to his friends who visited his studio "this [painting] is what is killing me" such was his determination to complete the work, but by October it must have already been well advanced, as his former pupil Gros wrote to congratulate him, having heard reports of the painting's merits. By the time David died, the painting had been completed and the commissioner Ambroise Firmin-Didot brought it back to Paris to include it in the exhibition "Pour les grecs" that he had organised and which opened in Paris in April 1826.

When David was leaving a theater, a carriage struck him, and he later died, on 29 December 1825. At his death, some portraits were auctioned in Paris, they sold for little; the famous "Death of Marat" was exhibited in a secluded room, to avoid outraging public sensibilities. Disallowed return to France for burial, for having been a regicide of King Louis XVI, the body of the painter Jacques-Louis David was buried in Brussels and moved in 1882 to Brussels Cemetery, while some say his heart was buried with his wife at Père Lachaise Cemetery, Paris.
David was made a Chevalier de la Légion d'honneur in 1803. He was promoted to an Officier in 1808. And, in 1815, he was promoted to a Commandant (now Commandeur) de la Légion d'honneur.

The theme of the oath found in several works like "The Oath of the Tennis Court", "The Distribution of the Eagles", and "Leonidas at Thermopylae", was perhaps inspired by the rituals of Freemasonry. In 1989 during the "David against David" conference Albert Boime was able to prove, on the basis of a document dated in 1787, the painter's membership in the "La Moderation" Masonic Lodge.

Jacques-Louis David's facial abnormalities were traditionally reported to be a consequence of a deep facial sword wound after a fencing incident. These left him with a noticeable asymmetry during facial expression and resulted in his difficulty in eating or speaking (he could not pronounce some consonants such as the letter 'r'). A sword scar wound on the left side of his face is present in his self-portrait and sculptures and corresponds to some of the buccal branches of the facial nerve. An injury to this nerve and its branches are likely to have resulted in the difficulties with his left facial movement.

Furthermore, as a result of this injury, he suffered from a growth on his face that biographers and art historians have defined as a benign tumor. These however may have been a granuloma, or even a post-traumatic neuroma. As Simon Schama has pointed out, witty banter and public speaking ability were key aspects of the social culture of 18th-century France. In light of these cultural keystones, David's tumor would have been a heavy obstacle in his social life. David was sometimes referred to as "David of the Tumor".

In addition to his history paintings, David completed a number of privately commissioned portraits. Warren Roberts, among others, has pointed out the contrast between David's "public style" of painting, as shown in his history paintings, and his "private style", as shown in his portraits. His portraits were characterized by a sense of truth and realism. He focused on defining his subjects’ features and characters without idealizing them. This is different from the style seen in his historical paintings, in which he idealizes his figures’ features and bodies to align with Greek and Roman ideals of beauty. He puts a great deal of detail into his portraits, defining smaller features like hands and fabric. The compositions of his portraits remain simple with blank backgrounds that allow the viewer to focus on the details of the subject.

The portrait he did of his wife (1813) is an example of his typical portrait style. The background is dark and simple without any clues as to the setting, which forces the viewer to focus entirely on her. Her features are un-idealized and truthful to her appearance. There is a great amount of detail that can be seen in his attention to portraying the satin material of the dress she wears, the drapery of the scarf around her, and her hands which rest in her lap. 

In the painting of Brutus (1789), the man and his wife are separated, both morally and physically. Paintings like these, depicting the great strength of patriotic sacrifice, made David a popular hero of the revolution.

In the "Portrait of Antoine-Laurent Lavoisier and his wife" (1788), the man and his wife are tied together in an intimate pose. She leans on his shoulder while he pauses from his work to look up at her. David casts them in a soft light, not in the sharp contrast of Brutus or of the Horatii. Also of interest—Lavoisier was a tax collector, as well as a famous chemist. Though he spent some of his money trying to clean up swamps and eradicate malaria, he was nonetheless sent to the guillotine during the Reign of Terror as an enemy of the people. David, then a powerful member of the National Assembly, stood idly by and watched.

Other portraits include paintings of his sister-in-law and her husband, Madame and Monsieur Seriziat. The picture of Monsieur Seriziat depicts a man of wealth, sitting comfortably with his horse-riding equipment. The picture of the Madame shows her wearing an unadorned white dress, holding her young child's hand as they lean against a bed. David painted these portraits of Madame and Monsieur Seriziat out of gratitude for letting him stay with them after he was in jail.

Towards the end of David's life, he painted a portrait of his old friend "Abbé Sieyès". Both had been involved in the Revolution, both had survived the purging of political radicals that followed the reign of terror. 
The shift in David's perspective played an important role in the paintings of David's later life, including this one of Sieyès. During the height of the reign of terror, David was an ardent supporter of radicals such as Robespierre and Marat, and twice offered up his life in their defense. He organized revolutionary festivals and painted portraits of martyrs of the revolution, such as Lepeletier, who was assassinated for voting for the death of the king. David was an impassioned speaker at times in the National Assembly. In speaking to the Assembly about the young boy named Bara, another martyr of the revolution, David said, "O Bara! O Viala! The blood that you have spread still smokes; it rises toward Heaven and cries for vengeance."

After Robespierre was sent to the guillotine, however, David was imprisoned and changed the attitude of his rhetoric. During his imprisonment he wrote many letters, pleading his innocence. In one he wrote, "I am prevented from returning to my atelier, which, alas, I should never have left. I believed that in accepting the most honorable position, but very difficult to fill, that of legislator, that a righteous heart would suffice, but I lacked the second quality, understanding."

Later, while explaining his developing "Grecian style" for paintings such as "The Intervention of the Sabine Women", David further commented on a shift in attitude: "In all human activity the violent and transitory develops first; repose and profundity appear last. The recognition of these latter qualities requires time; only great masters have them, while their pupils have access only to violent passions."

Jacques-Louis David was, in his time, regarded as the leading painter in France, and arguably all of Western Europe; many of the painters honored by the restored Bourbons following the French Revolution had been David's pupils. David's student Antoine-Jean Gros for example, was made a Baron and honored by Napoleon Bonaparte's court. Another pupil of David's, Jean Auguste Dominique Ingres became the most important artist of the restored Royal Academy and the figurehead of the Neoclassical school of art, engaging the increasingly popular Romantic school of art that was beginning to challenge Neoclassicism. David invested in the formation of young artists for the Rome Prize, which was also a way to pursue his old rivalry with other contemporary painters such as Joseph Suvee, who also had opened teaching studios. To be one of David's students was considered prestigious and earned his students a lifetime of reputation. He also called on the more advanced students, such as Jérôme-Martin Langlois, to help him paint his large canvases.

Despite David's reputation, he was more fiercely criticized right after his death than at any point during his life. His style came under the most serious criticism for being static, rigid, and uniform throughout all his work. David's art was also attacked for being cold and lacking warmth. David, however, made his career precisely by challenging what he saw as the earlier rigidity and conformity of the French Royal Academy's approach to art. David's later works also reflect his growth in the development of the Empire style, notable for its dynamism and warm colors. It is likely that much of the criticism of David following his death came from David's opponents; during his lifetime David made a great many enemies with his competitive and arrogant personality as well as his role in the Terror. David sent many people to the guillotine and personally signed the death warrants for King Louis XVI and Marie Antoinette. One significant episode in David's political career that earned him a great deal of contempt was the execution of Emilie Chalgrin. A fellow painter Carle Vernet had approached David, who was on the Committee of Public Safety, requesting him to intervene on behalf of his sister, Chalgrin. She had been accused of crimes against the Republic, most notably possessing stolen items. David refused to intervene in her favor, and she was executed. Vernet blamed David for her death, and the episode followed him for the rest of his life and after.

In the last 50 years David has enjoyed a revival in popular favor and in 1948 his two-hundredth birthday was celebrated with an exhibition at the Musée de l'Orangerie in Paris and at Versailles showing his life's works. Following World War II, Jacques-Louis David was increasingly regarded as a symbol of French national pride and identity, as well as a vital force in the development of European and French art in the modern era.

The birth of Romanticism is traditionally credited to the paintings of eighteenth century French artists such as Jacques-Louis David. 

"Danton" (Andrzej Wajda, France, 1982) – Historical drama. Many scenes include David as a silent character watching and drawing. The film focuses on the period of the Terror.






</doc>
<doc id="9074" url="https://en.wikipedia.org/wiki?curid=9074" title="Design science license">
Design science license

Design Science License (DSL) is a copyleft license for any type of free content such as text, images, music. Unlike other open source licenses, the DSL was intended to be used on any type of copyrightable work, including documentation and source code. It was the first "generalized copyleft" license. The DSL was written by Michael Stutz.

The DSL came out in the 1990s, before the formation of the Creative Commons. Once the Creative Commons arrived, Stutz considered the DSL experiment "over" and no longer recommended its use.


</doc>
<doc id="9079" url="https://en.wikipedia.org/wiki?curid=9079" title="Drum kit">
Drum kit

A drum kit — also called a drum set, trap set, or simply drums — is a collection of drums and other percussion instruments, typically cymbals, which are set up on stands to be played by a single player, with drumsticks held in both hands, and the feet operating pedals that control the hi-hat cymbal and the beater for the bass drum. A drum kit consists of a mix of drums (categorized classically as membranophones, Hornbostel-Sachs high-level classification 2) and idiophones – most significantly cymbals, but can also include the woodblock and cowbell (classified as Hornbostel-Sachs high-level classification 1). In the 2000s, some kits also include electronic instruments (Hornbostel-Sachs classification 53). Also, both hybrid (mixing acoustic instruments and electronic drums) and entirely electronic kits are used.

A standard modern kit (for a right-handed player), as used in popular music and taught in music schools, contains:


All of these are classed as non-pitched percussion, allowing for the music to be scored using percussion notation, for which a loose semi-standardized form exists for the drum kit. If some or all of them are replaced by electronic drums, the scoring and most often positioning remains the same, allowing a standard teaching approach. The drum kit is usually played while seated on a "drum stool" or "throne". The drum kit differs from instruments that can be used to produce pitched melodies or chords, even though drums are often placed musically alongside others that do, such as the guitar or piano. The drum kit is a part of the standard rhythm section, used in many types of popular and traditional music styles, ranging from rock and pop to blues and jazz. Other standard instruments used in the rhythm section include the piano, electric guitar, electric bass, and keyboards.

Many drummers extend their kits from this basic pattern, adding more drums, more cymbals, and many other instruments including pitched percussion. In some styles of music particular extensions are normal, for example double bass drums in heavy metal music and the enlarged kits used by some progressive drummers, which may include unusual instruments such as gongs. Some performers, such as some rockabilly drummers, use small kits that omit elements from the basic setup. Some drum kit players may have other roles in the band, such as providing backup vocals, or less commonly, lead vocals.

Prior to the development of the drum set, the standard way that drums and cymbals were used in military and orchestral music settings was to have the different drums and cymbals played separately by different percussionists. Thus, in an early 1800s orchestra piece, if the score called for bass drum, triangle and cymbals, three percussionists would be hired to play these three instruments. In the 1840s, percussionists began to experiment with foot pedals as a way to enable them to play more than one instrument. In the 1860s, percussionists started combining multiple drums into a set. The bass drum, snare drum, cymbals, and other percussion instruments were all played using hand-held drum sticks. Drummers in musical theater shows and stage shows, where the budget for pit orchestras was often limited, contributed to the creation of the drum set because they tried to develop ways that one drummer could do the job of multiple percussionists.

Double-drumming was developed to enable one person to play the bass and snare with sticks, while the cymbals could be played by tapping the foot on a "low-boy". With this approach, the bass drum was usually played on beats one and three (in 4/4 time). While the music was first designed to accompany marching soldiers, this simple and straightforward drumming approach eventually led to the birth of ragtime music when the simplistic marching beats became more syncopated. This resulted in a greater 'swing' and dance feel. The drum set was initially referred to as a "trap set", and from the late 1800s to the 1930s, drummers were referred to as "trap drummers". By the 1870s, drummers were using an "overhang pedal". Most drummers in the 1870s preferred to do double drumming without any pedal to play multiple drums, rather than use an overhang pedal. Companies patented their pedal systems such as Dee Dee Chandler of New Orleans 1904–05. Liberating the hands for the first time, this evolution saw the bass drum played with the foot of a standing percussionist (thus the term "kick drum"). The bass drum became the central piece around which every other percussion instrument would later revolve.

William F. Ludwig, Sr., and his brother, Theobald Ludwig, founded the Ludwig & Ludwig Co. in 1909 and patented the first commercially successful bass drum pedal system, paving the way for the modern drum kit. It was the golden age of drum building for many famous drum companies, with Ludwig introducing... "The ornately engraved" Black Beauty Brass Snare drum; Slingerland premiered its Radio King solid-maple shell; Leedy invented the floating drum head & self-aligning lug;& Gretsch originated the three-way tension system of the Gladstone snare drum". Wire brushes for use with drums and cymbals were introduced in 1912. The need for brushes arose due to the problem of the drum sound overshadowing the other instruments on stage. Drummers began using metal fly swatters to reduce the volume on stage next to the other acoustic instruments. Drummers could still play the rudimentary snare figures and grooves with brushes they would normally play with drumsticks.

By World War I, drum kits were often marching band-style military bass drums with many percussion items suspended on and around them. Drum kits became a central part of jazz music, especially Dixieland. The modern drum kit was developed in the Vaudeville era during the 1920s in New Orleans. In 1917, a New Orleans band called "The Original Dixieland Jazz Band " recorded jazz tunes that became hits all over the country. These were the first official jazz recordings. Drummers such as Baby Dodds, "Zutty" Singleton and Ray Bauduc had taken the idea of marching rhythms, combining the bass drum and snare drum and "traps", a term used to refer to the percussion instruments associated with immigrant groups, which included miniature cymbals, tom toms, cowbells and woodblocks. They started incorporating these elements with ragtime, which had been popular for a couple of decades, creating an approach which evolved into a jazz drumming style.

Budget constraints and space considerations in musical theatre pit orchestras led bandleaders to pressure fewer percussionists to cover more percussion parts. Metal consoles were developed to hold Chinese tom-toms, with swing-out stands for snare drums and cymbals. On top of the console was a "contraption" tray (shortened to "trap"), used to hold items like whistles, klaxons, and cowbells, so these drums/kits were dubbed "trap kits". Hi-hat stands became available around 1926.

In 1918 Baby Dodds (Warren "Baby" Dodds, circa 1898–1959), playing on riverboats with Louis Armstrong on the Mississippi, was modifying the military marching set-up and experimenting with playing the drum rims instead of woodblocks, hitting cymbals with sticks (1919), which was not yet common, and adding a side cymbal above the bass drum, what became known as the ride cymbal. Drum maker William Ludwig developed the "sock" or early low-mounted high-hat after observing Dodd's drumming. Ludwig noticed that Dodd tapped his left foot all the time. Dodds had Ludwig raise the newly produced low hats 9 inches higher to make it easier to play, thus creating the modern hi-hat cymbal. Dodds was one of the first drummers to also play the broken-triplet beat that became the standard pulse and roll of modern ride cymbal playing. Dodds also popularized the use of Chinese cymbals.

In 1919, US Congress passed a prohibition law outlawing the manufacturing and transporting of drinking alcohol. When drinking became illegal, it became popular in underground nightclubs. The type of music that was played at these underground establishments that were selling alcohol was jazz. It was not seen as upstanding to listen to or perform jazz music, because it was an African American style and at that time the United States was segregated and racism was an overtly prevalent issue. Because jazz music was seen as great dance music, big band jazz became popular in nightclubs. In the 1920s, freelance drummers emerged. They were hired to play shows, concerts, theaters, clubs and back dancers and artists of various genres. Just as modern drummers have many different roles, so did the drummers of the 1920s. One important role for drummers in the 1920s is what is referred to in modern times as a foley artist. During silent films, an orchestra was hired to accompany the silent film and the drummer was responsible for providing all the sound effects. Drummers played instruments to imitate gun shots, planes flying overhead, a train coming into a train station, and galloping horses etc.

Sheet music from the 1920s provides evidence that the drummer's sets were starting to evolve in size and sound to support the various acts mentioned above. However, the first "talkies" or films with audio, were released circa 1927 and by 1930 most films were released with a soundtrack and the silent film era was over. The downside of the technological breakthrough was that thousands of drummers who served as sound effect specialists were put out of work overnight. A similar panic was felt by drummers in the 1980s, when electronic drum machines were first released.

In 1929, when the stock market crash resulted in a global depression, one of the things that helped people cope with the trying years was swing jazz music. By the early to mid 1930's, big band swing was being embraced throughout the US, becoming the country's most popular form of music. The other contributing factor to the big band's success during the 1930s was the popularity of radio. The drum kit played a key role in the big band swing sound. Throughout the 1930s Chick Webb and Gene Krupa at the Savoy Ballroom in Harlem, increased the visual and musical driving force of the drummer and their equipment by simply being so popular and in demand- and they ensured that their drum kits became not only functionally developed but dazzling and well designed. Jazz drummers were influential in developing the concept of the modern drum kit and extending playing techniques. Gene Krupa was the first drummer to head his own orchestra and thrust the drums into the spotlight with his drum solos. Others would soon follow his lead.

As the music of the world was evolving, so was the drum set. Tom-tom drums, small crash cymbals, Chinese cymbals and hi-hat cymbals were added to the drum set. The hi-hats were the primary way for the drummers of the big band era to keep time. Before 1930, while playing the New Orleans jazz and Chicago styles, drummers would choke the cymbals on the "ands" of eighth note figures as an alternative to playing a buzz roll, the rim of the drums, or on the woodblocks to keep time. This muting method of keeping time by choking the crash and china cymbals proved to be awkward, so the drummers of that time came up with the idea of having a foot-operated cymbal. This resulted in the creation of the snowshoe cymbal, a foot-operated cymbal. It enabled drummers to play the eighth note figures between the right and left foot, improving the ergonomics and facility of drumset playing and helping drummers to keep a more steady rhythm.

Toward the end of the 1920s, variations of the hi-hats were introduced. One of the most popular hand held hi-hat cymbal variations used was called the "hand sock cymbals". The reason for the name "hi-hat" was because earlier versions of the hi-hat were referred to as a "low boy". The evolution that became the "hi-hats" allowed drummers to play the two cymbals with drum sticks while simultaneously controlling how open or closed the two cymbals were with their foot. The pedal could also be used to play the cymbals with the foot alone, while the right hand played other drums. By the 1930s, Ben Duncan and others popularized streamlined trap kits leading to a basic four piece drum set standard: bass, snare, tom-tom, and a larger floor tom. In time, legs were fitted to larger floor toms, and "consolettes" were devised to hold smaller tom-toms (ride toms) on the bass drum.

In the early 1940s, many jazz musicians, especially African American jazz musicians, started to stray from the popular big band dance music of the 1930s. Their experimentation and quest for deeper expression and freedom on the instrument led to the birth of a new style of music based from Harlem called bebop music. Whereas swing was a popular music designed for dancing, bebop was a "musician's music" designed for listening. During the bebop era, given that bands no longer had to accompany dancers, bandleaders could speed up the tempo. Bebop was also much more based on improvisation, in comparison to the heavily arranged big band scores. Bebop musicians would take an old standard and re-write the melody, add more complex chord changes, resulting in a new composition.

Swing drummers such as Max Roach and Kenny Clarke had already deviated from the large marching band-style bass drums, finding that they were too loud and boomy. Bebop drummers continued this trend, and they started trying out smaller bass drum sizes in the drum set. Bebop drummers' experimentations with new drum sizes and new sounds led to the innovative concept of applying the busy "four on the floor" bass drum rhythms to a new larger cymbal called the ride cymbal. By focusing on keeping time on the new ride cymbal instead of the bass drum, the "feel" went from bass drum and hi-hat heavy, to a lighter melodic feel that has been explained as "floating on top of the time". This allowed drummers to express themselves in a more melodic fashion by playing the rhythms used by the guitar, piano and sax players using the new smaller, more focused bass drums and snare. Louie Bellson also assisted in the innovative sizes and sounds of the 1940s drum set by pioneering the use of two bass drums, or the double bass drum kit.

With rock and roll coming into place, a watershed moment occurred between 1962 and 1964 when the Surfaris released "Wipe Out", as well as when Ringo Starr of The Beatles played his Ludwig kit on American television. As rock moved from the nightclubs and bars and into stadiums in the 1960s, there was a trend towards bigger drum kits. The trend towards larger drum kits took momentum in the 1970s with the emergence of progressive rock. By the 1980s, widely popular drummers like Billy Cobham, Carl Palmer, Nicko McBrain, Phil Collins, Stewart Copeland, Simon Phillips and Neil Peart were using large numbers of drums and cymbals. In the 1980s, some drummers began to use electronic drums.

In the 2010s, some drummers use a variety of auxiliary percussion instruments, found objects, and electronics as part of their "drum" kits. Popular electronics include: electronic sound modules; laptop computers used to activate loops, sequences and samples; metronomes and tempo meters; recording devices; and personal sound reinforcement equipment (e.g., a small PA system to amplify electronic drums and provide a monitor).

On early recording media (until 1925) such as wax cylinders and discs carved with an engraving needle, sound balancing meant that musicians had to be literally moved in the room. Drums were often put far from the horn (part of the mechanical transducer) to reduce sound distortion. Since this affected the rendition of cymbals at playback, sound engineers of the time remedied the situation by asking drummers to play the content of the cymbals onto woodblocks, temple blocks, and cowbells for their loudness and short decay.

The drum kit may be loosely divided into four parts:


There are several reasons for this division. When more than one band plays in a single performance, the drum kit is often considered part of the backline (the key rhythm section equipment that stays on stage all night, which often also includes a bass amp and a stage piano), and which is shared between/among the drummers. Often the main "headlining" act will provide the drums, as they are being paid more, possibly have the better gear, and in any case have the prerogative of using their own. However sticks, snare drum and cymbals are commonly swapped, each drummer bringing their own, and sometimes other components. The term "breakables" in this context refers to whatever basic components the "guest" drummer is expected to bring. Similar considerations apply if using a "house kit" (a drum kit owned by the venue, which is rare), even if there is only one band at the performance.

The snare drum and cymbals are the core of the "breakables", as they are particularly critical and individual components of the standard kit, in several related ways.

Much the same considerations apply to bass drum pedals and the stool, but these are not always considered "breakables", particularly if changeover time between bands is very limited. Swapping the snare drum in a standard kit can be done very quickly. Replacing cymbals on stands takes longer, particularly if there are many of them, and cymbals are easily damaged by incorrect mounting, so many drummers prefer to bring their own cymbal stands.

See Common configurations below for typical drum sizes.

Traditionally, in America and the United Kingdom, drum sizes were expressed as "depth x diameter", both in inches, but in The United Kingdom it was stated the other way around. More recently, many drum kit manufacturers have begun to express their sizes in terms of "diameter x depth"; still in the measure of inches.

Manufacturers using the American traditional format in their catalogs include these:
Those using the European measures of diameter x depth include these:

For example, a hanging tom 12 inches in diameter and 8 inches deep would be described by Tama as 8 inches × 12 inches, but by Pearl as 12 inches × 8 inches, and a standard diameter Ludwig snare drum 5 inches deep is a 5-inch × 14-inch, while the UK's Premier Manufacturer offers the same dimensions as: a 14-inch × 5-inch snare.

The snare drum is the heart of the drum kit, particularly in rock, due to the use of the snare to play the backbeat. It provides the strongest regular accents, played by the left hand (if right handed), and the backbone for many fills. It produces its distinctive sound due to the bed of stiff snare wires held under tension to the underside of the lower drum head. When the stiff wire are "engaged" (held under tension), they vibrate with the top (snare-side) drum skin (head) when the head is hit, creating a snappy, staccato buzzing sound, along with the sound of the stick striking the head.

Tom-tom drums, or "toms" for short, are drums without snares and played with sticks (or whatever tools the music style requires), and are the most numerous drums in most kits. They provide the bulk of most drum fills and solos.

They include:

The smallest and largest drums without snares, such as octobans and gong drums, are sometimes considered toms. The naming of common configurations (four-piece, five-piece, etc.) is largely a reflection of the number of toms, as only the drums are conventionally counted, and these configurations all contain one snare and one or more bass drums, (though not regularly any standardized use of 2 bass/kick drums) the balance usually being in toms.

The bass drum (also known as the "kick drum") provides a regular but often-varied foundation to the rhythm. The bass drum is the lowest pitched drum and usually provides the basic beat or timing element with basic pulse patterns. Some drummers may use two or more bass drums or use a double bass drum pedal with a single bass drum. Double bass drumming is an important technique in many heavy metal genres. Using a double bass drum pedal enables a drummer to play a double bass drum style with only one bass drum, saving space in recording/performance areas and reducing time and effort during set-up, taking down, and transportation.

Octobans are smaller toms designed for use in a drum kit, extending the tom range upwards in pitch, primarily by their depth; as well as diameter (typically 6"). Pearl brand octobans are called "rocket toms"; the instruments are also called 

Timbales are tuned much higher than a tom of the same diameter, and normally played with very light, thin, non-tapered sticks. They have relatively thin heads and a very different tone than a tom, but are used by some drummers/percussionists to extend the tom range upwards. Alternatively, they can be fitted with tom heads and tuned as shallow concert toms. Attack timbales and mini timbales are reduced-diameter timbales designed for drum kit usage, the smaller diameter allowing for thicker heads providing the same pitch and head tension. They are recognizable in 2010s genres and in more traditional forms of Latin, reggae & numerous world music styles. Timbales were also used on occasion by Led Zeppelin drummer John Bonham. Gong drums are a rare extension to a drum kit. The single-headed mountable drum appears similar to a bass drum (sizing around 20–24 inches in diameter), but has the same purpose as that of a floor tom.
Similarly, most hand drum percussion cannot be played easily or suitably with drum sticks without risking damage to the head and to the bearing edge, which is not protected by a metal drum rim, like a snare or tom. For use in a drum kit, they may be fitted with a metal drum head and played with care, or played by hand.

In most drum kits and drum/percussion kits cymbals are as important as the drums themselves. The oldest idiophones in music are cymbals, and were used throughout the ancient Near East, very early in the Bronze Age period. Cymbals are most associated with Turkey and Turkish craftsmanship, where Zildjian (the name means cymbal smith) has predominantly made them since 1623.

Beginners cymbal packs normally contain four cymbals: one ride, one crash, and a pair of hi-hats. A few contain only three cymbals, using a crash/ride instead of the separate ride and crash. The sizes closely follow those given in Common configurations below.

Most drummers soon extend this by adding another crash, a splash, a china/effects cymbal; or even all of those last mentioned.

The ride cymbal is most often used for keeping a constant-rhythm pattern, every beat or more often, as the music requires. Development of this ride technique is generally credited to Baby Dodds.

Most drummers have a single main ride, located near their right hand—within easy playing reach, as it is used very regularly—most often a 20" sizing but, 16"-24" diameters are not uncommon. It is most often a heavy, or medium-weighted cymbal that cuts through other instrumental sounds, but some drummers use a swish cymbal, sizzle cymbal or other exotic or lighter metal ride, as the main or only ride in their kit, particularly for jazz, gospel or ballad/folk sounds. In the 1960s Ringo Starr used a sizzle cymbal as a second ride, particularly during guitar solos.

The hi-hat cymbals (nicknamed "hats") consist of two cymbals mounted facing each other on a metal pole with folding support legs that keep a hollow support cylinder standing up. Like the bass drum, the hi-hat has a foot pedal. The bottom cymbal is fixed in place. The top cymbal is mounted on a thin pole which is inserted into the hollow cymbal stand cylinder. The thin pole is connected to a foot pedal. When the foot pedal is pressed down, a mechanism causes the thin pole to move down, causing the cymbals to move together. When the foot is lifted off the pedal, the cymbals move apart, due to the pedal's spring-loaded mechanism. The hi-hats can be sounded by striking the cymbals with one or two sticks or just by opening and closing the cymbals with the footpedal, without striking the cymbals. The ability to create rhythms on the hi-hats with the foot alone enables drummers to use both sticks on other drums or cymbals. Different sounds can be created by striking "open hi-hats" (without the pedal depressed, which creates a noisy sound nicknamed "sloppy hats") or a crisp "closed hi-hats" sound (with the pedal pressed down). As well, the high hats can be played with a partially depressed pedal.

A unique effect can be created by striking an open hi-hat (i.e., in which the two cymbals are apart) and then closing the cymbals with the foot pedal; this effect is widely used in disco and funk. The hi-hat has a similar function to the ride cymbal. The two are rarely played consistently for long periods at the same time, but one or the other is used to keep the faster-moving rhythms (e.g., sixteenth notes) much of the time in a song. The hi-hats are played by the right stick of a right-handed drummer. Changing between ride and hi-hat, or between either and a "leaner" sound with neither, is often used to mark a change from one passage to another, for example; to distinguish between a verse and chorus.

The crash cymbals are usually the strongest accent markers within the kit, marking crescendos and climaxes, vocal entries, and major changes of mood/swells and effects. A crash cymbal is often accompanied by a strong kick on the bass drum pedal, both for musical effect and to support the stroke. It provides a fuller sound and is a commonly taught technique.

In the very smallest kits, in jazz, and at very high volumes, ride cymbals may be played in with the technique and sound of a crash cymbal. Some hi-hats will also give a useful crash, particularly thinner hats or those with an unusually severe taper. At low volumes, producing a good crash from a cymbal not particularly suited to it is a highly skilled art. Alternatively, specialised crash/ride and ride/crash cymbals are specifically designed to combine both functions.

All cymbals other than rides, hi-hats and crashes/splashes are usually called effects cymbals when used in a drum kit, though this is a non-classical or colloquial designation that has become a standardized label. Most extended kits include one or more splash cymbals and at least one china cymbal. Major cymbal makers produce cymbal extension packs consisting of one splash and one china, or more rarely a second crash, a splash and a china, to match some of their starter packs of ride, crash and hi-hats. However any combination of options can be found in the marketplace.

Some cymbals may be considered effects in some kits but "basic" in another set of components. A swish cymbal may, for example serve, as the main ride in some styles of music, but in a larger kit, which includes a conventional ride cymbal as well, it may well be considered an effects cymbal per se. Likewise, Ozone crashes have the same purpose as a standard crash cymbal, but are considered to be effects cymbals due to their rarity, and the holes cut into them, which provide a darker, more resonant attack.

Cymbals of any type used to provide an accent rather than a regular pattern or groove are known as accent cymbals. While any cymbal can be used to provide an accent, the term is applied more correctly to cymbals for which the main purpose is to provide an accent. Accent cymbals include chime cymbals, small-bell domed cymbals or those with a clear sonorous/oriental chime to them like specialized crash and splash cymbals and many china types too, particularly the smaller or thinner ones.

Other instruments that have regularly been incorporated into drum kits include:

See also Extended kits below.

Electronic drums are used for many reasons. Some drummers use electronic drums for playing in small venues such as coffeehouses or church services, where a very low volume for the band is desired. Since fully electronic drums do not create any acoustic sound (apart from the quiet sound of the stick hitting the sensor pads), all of the drum sounds come from a keyboard amplifier or PA system; as such, the volume of electronic drums can be much lower than an acoustic kit. Some drummers use electronic drums as practice instruments, because they can be listened to with headphones, enabling a drummer to practice in an apartment or in the middle of the night without disturbing others. Some drummers use electronic drums to take advantage of the huge range of sounds that modern drum modules can produce, which range from sampled sounds of real drums, cymbals and percussion instruments (including instruments that would be impractical to take to a small gig, such as gongs or tubular bells), to electronic and synthesized sounds, including non-instrument sounds such as ocean waves.

A fully electronic kit is also easier to soundcheck than acoustic drums, assuming that the electronic drum module has levels that the drummer has pre-set in her/his practice room; in contrast, when an acoustic kit is sound checked, most drums and cymbals need to be miked and each mic needs to be tested by the drummer so its level and tone equalization can be adjusted by the sound engineer. As well, even after all the individual drum and cymbal mics are soundchecked, the engineer needs to listen to the drummer play a standard groove, to check that the balance between the kit instruments is right. Finally, the engineer needs to set up the monitor mix for the drummer, which the drummer uses to hear her/his instruments and the instruments and vocals of the rest of the band. With a fully electronic kit, many of these steps could be eliminated.

Drummers' usage of electronic drum equipment can range from adding a single electronic pad to an acoustic kit (e.g., to have access to an instrument that might otherwise be impractical, such as a large gong), to using a mix of acoustic drums/cymbals and electronic pads, to using an acoustic kit in which the drums and cymbals have triggers, which can be used to sound electronic drums and other sounds, to having an exclusively electronic kit, which is often set up with the rubber or mesh drum pads and rubber "cymbals" in the usual drum kit locations. A fully electronic kit weighs much less and takes up less space to transport than an acoustic kit and it can be set up more quickly. One of the disadvantages of a fully electronic kit is that it may not have the same "feel" as an acoustic kit, and the drum sounds, even if they are high-quality samples, may not sound the same as acoustic drums.

Electronic drum pads are the second most widely used type of MIDI performance controllers, after electronic music keyboards. Drum controllers may be built into drum machines, they may be standalone control surfaces (e.g., rubber drum pads), or they may emulate the look and feel of acoustic percussion instruments. The pads built into drum machines are typically too small and fragile to be played with sticks, and they are usually played with fingers. Dedicated drum pads such as the Roland Octapad or the DrumKAT are playable with the hands or with sticks, and are often built to resemble the general form of a drum kit. There are also percussion controllers such as the vibraphone-style MalletKAT, and Don Buchla's Marimba Lumina.

As well as providing an alternative to a conventional acoustic drum kit, electronic drums can be incorporated into an acoustic drum kit to supplement it. MIDI triggers can also be installed into acoustic drum and percussion instruments. Pads that can trigger a MIDI device can be homemade from a piezoelectric sensor and a practice pad or other piece of foam rubber.

This is possible in two ways:

In either case, an electronic control unit (sound module/"brain") with suitable sampled/modeled or synthesized drum sounds, amplification equipment (a PA system, keyboard amp, etc.) and stage monitor speakers are required for the drummer (and other band members and audience) to hear the electronically produced sounds. See Triggered drum kit.

A trigger pad could contain up to four independent sensors, each of them capable of sending information describing the timing and dynamic intensity of a stroke to the drum module/brain. A circular drum pad may have only one sensor for triggering, but a 2016-era cymbal-shaped rubber pad/cymbal will often contain two; one for the body and one for the bell at the centre of the cymbal, and perhaps a cymbal choke trigger, to allow drummers to produce this effect.

Trigger sensors are most commonly used to replace the acoustic drum sounds, but they can often also be used effectively with an acoustic kit to augment or supplement an instrument's sound for the needs of the session or show. For example, in a live performance in a difficult acoustical space, a trigger may be placed on each drum or cymbal, and used to trigger a similar sound on a drum module. These sounds are then amplified through the PA system so the audience can hear them, and they can be amplified to any level without the risks of audio feedback or bleed problems associated with microphones and PAs in certain settings.

The sound of electronic drums and cymbals themselves is heard by the drummer and possibly other musicians in close proximity, but even so, the foldback (audio monitor) system is usually fed from the electronic sounds rather than the live acoustic sounds. The drums can be heavily dampened (made to resonate less or subdue the sound), and their tuning and even quality is less critical in the latter scenario. In this way, much of the atmosphere of the live performance is retained in a large venue, but without some of the problems associated with purely microphone-amplified drums. Triggers and sensors can also be used in conjunction with conventional or built-in microphones. If some components of a kit prove more difficult to "mike" than others (e.g., an excessively "boomy" low tom), triggers may be used on only the more difficult instruments, balancing out a drummer's/band's sound in the mix.

Trigger pads and drums, on the other hand, when deployed in a conventional set-up, are most commonly used to produce sounds not possible with an acoustic kit, or at least not with what is available. Any sound that can be sampled/recorded can be played when the pad is struck, by assigning the recorded sounds to specific triggers . Recordings or samples of barking dogs, sirens, breaking glass and stereo recordings of aircraft taking off and landing have all been used. Along with the more obvious electronically generated sounds there are synthesized human voices or song parts or even movie audio or digital video/pictures that (depending on device used) can also be played/triggered by electronic drums.

Virtual drums are a type of audio software that simulates the sound of a drum kit using synthesized drum kit sounds or digital samples of acoustic drum sounds. Different drum software products offer a recording function, the ability to select from several acoustically distinctive drum kits (e.g., jazz, rock, metal), as well as the option to incorporate different songs into the session. Some software for the personal computer (PC) can turn any hard surface into a virtual drum kit using only one microphone.

"Hardware" is the name given to the metal stands that support the drums, cymbals and other percussion instruments. Generally the term also includes the hi-hat pedal and bass drum pedal or pedals, and the drum stool, but not the drum sticks.

Hardware is carried along with sticks and other accessories in the traps case, and includes:

Many or even all of the stands may be replaced by a drum rack, particularly useful for large drum kits.

Drummers often set up their own drum hardware onstage and adjust to their own comfort level. Major touring bands on tour will often have a drum tech who knows how to set up the drummer's hardware and instruments in the desired location and layout.

Drum kits are traditionally categorised by the number of drums, ignoring cymbals and other instruments. Snare, tom-tom and bass drums are always counted; Other drums such as octobans may or may not be counted.

The sizes of drums and cymbals given below are typical. Many drummers differ slightly or radically from them. Where no size is given, it is because there is too much variety to call a typical size.

A three-piece drum set is the most basic set. A conventional three-piece kit consists of a bass drum, a 14" diameter snare drum, 12"–14" hi-hats, a single 12" diameter hanging tom, 8"–9" in depth, and a suspended cymbal, in the range of 14"–18", both mounted on the bass drum. These kits were common in the 1950s and 1960s and are still used in the 2010s in small acoustic dance bands. It is a common configuration for kits sold through mail order, and, with smaller sized drums and cymbals, for kits for children.

A four-piece kit extends the three-piece by adding one tom, either a second hanging tom mounted on the bass drum (a notable user is Chris Frantz of Talking Heads) and often displacing the cymbal, or by adding a floor tom. Normally another cymbal is added as well, so there are separate ride and crash cymbals, either on two stands, or the ride cymbal mounted on the bass drum to the player's right and the crash cymbal on a separate stand. The standard cymbal sizes are 16" crash and 18"–20" ride, with the 20" ride most common.

When a floor tom is added to make a four-piece kit, the floor tom is usually 14" for jazz, and 16" otherwise. This configuration is usually common in jazz, classic rock and rock and roll. Notable users include Ringo Starr in The Beatles, Mitch Mitchell in the Jimi Hendrix Experience, and John Barbata in the Turtles. For jazz, which normally emphasizes the use of ride cymbal, the lack of second hanging tom in a four-piece kit allows the cymbal to be positioned closer to the drummer, making them easier to be struck.

If a second hanging tom is used, it is 10" diameter and 8" deep for fusion, or 13" diameter and one inch deeper than the 12" diameter tom. Otherwise, a 14" diameter hanging tom is added to the 12", both being 8" deep. In any case, both toms are most often mounted on the bass drum with the smaller of the two next to the hi-hats (on the left for a right-handed drummer). These kits are particularly useful for smaller venues where space is limited, such as coffeehouses and small pubs.

The five-piece kit is the full entry-level kit and the most common configuration. It adds a third tom to the bass drum/snare drum/two toms set, making three toms in all. A fusion kit will normally add a 14" tom, either a floor tom or a hanging tom on a stand to the right of the bass drum; in either case, making the tom lineup 10", 12" and 14". Having three toms enables drummers to have a low-pitched, middle-register and higher-pitched tom, which gives them more options for fills and solos.

Other kits will normally have 12" and 13" hanging toms plus either a 14" hanging tom on a stand, a 14" floor tom, or a 16" floor tom. For depths, see Tom-tom drum#Modern tom-toms. In the 2010s, it is very popular to have 10" and 12" hanging toms, with a 16" floor tom. This configuration is often called a hybrid setup. The bass drum is most commonly 22" in diameter, but rock kits may use 24", fusion 20", jazz 18", and in larger bands up to 26". A second crash cymbal is common, typically an inch or two larger or smaller than the 16", with the larger of the two to the right for a right-handed drummer, but a big band may use crashes up to 20" and ride up to 24" or, very rarely, 26". A rock kit may also substitute a larger ride cymbal or larger hi-hats, typically 22" for the ride and 15" for the hats.

Most five-piece kits, at more than entry level, also have one or more effects cymbals. Adding cymbals beyond the basic ride, hi-hats and one crash configuration requires more stands in addition to the standard drum hardware packs. Because of this, many higher-cost kits for professionals are sold with little or even no hardware, to allow the drummer to choose the stands and also the bass drum pedal he/she prefers. At the other extreme, many inexpensive, entry-level kits are sold as a five-piece kit complete with two cymbal stands, most often one straight and one boom, and some even with a standard cymbal pack, a stool and a pair of 5A drum sticks. In the 2010s, digital kits are often offered in a five-piece kit, usually with one plastic crash cymbal triggers and one ride cymbal trigger. Fully electronic drums do not produce any acoustic sound beyond the quiet tapping of sticks on the plastic or rubber heads. The trigger-pads are wired up to a synth module or sampler.

If the toms are omitted completely, or the bass drum is replaced by a pedal-operated beater on the bottom skin of a floor tom and the hanging toms omitted, the result is a two-piece "cocktail" (lounge) kit. Such kits are particularly favoured in musical genres such as trad jazz, rockabilly and jump blues. Some rockabilly kits and beginners kits for very young players omit the hi-hat stand. In rockabilly, this allows the drummer to play standing rather than seated.

Although these kits may be small with respect to the number of drums used, the drums themselves are most often normal sizes, or even larger in the case of the bass drum. Kits using smaller drums in both smaller and larger configurations are also produced for particular uses, such as "boutique" kits designed to reduce the visual impact that a large kit creates or due space constraints in coffeehouses, "travelling" kits to reduce luggage volume, and junior kits for very young players. Smaller drums also tend to be quieter, again suiting smaller venues, and many of these kits extend this with extra muffling which allows quiet or even silent practice in a hotel room or bedroom. 

Common extensions beyond these standard configurations include:

See also other acoustic instruments above. Another versatile extension becoming increasingly common is the use of some electronic drums in a mainly conventional kit.

Less common extensions found particularly, but not exclusive to very large kits, include:

Sticks were traditionally made from wood (particularly maple, hickory, and oak} but more recently metal, carbon fibre and other exotic materials have been used for high market end sticks. The prototypical wooden drum stick was primarily designed for use with the snare drum, and optimised for playing snare rudiments. Sticks come in a variety of weights and tip designs; 7N is a common jazz stick with a nylon tip, while a 5B is a common wood tipped stick, heavier than a 7N but with a similar profile, and a common standard for beginners. Numbers range from 1 (heaviest) to 10 (lightest).

The meanings of both numbers and letters vary from manufacturer to manufacturer, and some sticks are not described using this system at all, just being known as "Smooth Jazz" (typically a 7N or 9N) or "Speed Rock" (typically a 2B or 3B) for example. Many famous drummers endorse sticks made to their particular preference and sold under their signature. 

Besides drumsticks, drummers will also use brushes and rutes in jazz and similar softer music. More rarely, other beaters such as cartwheel mallets (known to kit drummers as "soft sticks") may be used. It is not uncommon for rock drummers to use the "wrong" (butt) end of a stick for a heavier sound; some makers produce tipless sticks with two butt ends.

A stick bag is the standard way for a drummer to bring drumsticks to a live performance. For easy access, the stick bag is commonly mounted on the side of the floor tom, just within reach of the drummer’s right hand for a right-handed drummer.

Drum muffles are types of mutes that can reduce the ring, boomy overtone frequencies, or overall volume on a snare, bass, or tom. Controlling the ring is useful in studio or live settings when unwanted frequencies can clash with other instruments in the mix. There are internal and external muffling devices which rest on the inside or outside of the drumhead, respectively. Common types of mufflers include muffling rings, gels and duct tape, and improvised methods, such as placing a wallet near the edge of the head. Some drummers muffle the sound of a drum by putting a cloth over the drumhead.

Snare drum and tom-tom
Typical ways to muffle a snare or tom include placing an object on the outer edge of the drumhead. A piece of cloth, a wallet, gel, or fitted rings made of mylar are common objects. Also used are external clip-on muffles that work using the same principle. Internal mufflers that lie on the inside of the drumhead are often built into a drum, but are generally considered less effective than external muffles, as they stifle the initial tone, rather than simply reducing the sustain of it.

Bass drum
Muffling the bass can be achieved with the same muffling techniques as the snare, but bass drums in a drum kit are more commonly muffled by adding pillows, a sleeping bag or another soft filling inside the drum, between the heads. Cutting a small hole in the resonant head can also produce a more muffled tone, and allows manipulation in internally placed muffling. The Evans EQ pad places a pad against the batterhead and, when struck, the pad moves off the head momentarily, then returns to rest against the head, thus reducing the sustain without choking the tone.

Silencers/mutes
Another type of drum muffler is a piece of rubber that fits over the entire drumhead or cymbal. It interrupts contact between the stick and the head which dampens the sound even more. They are typically used in practice settings.

Cymbals are usually muted with the fingers or hand, to reduce the length or volume of ringing (e.g., the cymbal choke technique which is a key part of heavy metal drumming). Cymbals can also be muted with special rubber rings or with DIY approaches such as using duct tape.

Companies with muffle products:

Historical uses
Muffled drums are often associated with funeral ceremonies as well, such as the funerals of John F. Kennedy and Queen Victoria. The use of muffled drums has been written about by such poets as Henry Wadsworth Longfellow, John Mayne, and Theodore O'Hara. Drums have also been used for therapy and learning purposes, such as when an experienced player will sit with a number of students and by the end of the session have all of them relaxed and playing complex rhythms.

There are various types of stick holder accessories, including bags that can be attached to a drum and angled sheath-style stick holders, which can hold a single pair of sticks.

A sizzler is a metal chain or combination of chains that is hung across a cymbal, creating a distinctive metallic sound when the cymbal is struck similar to that of a sizzle cymbal. Using a sizzler is the non-destructive alternative to drilling holes in a cymbal and putting metal rivets in the holes. Another benefit of using a "sizzler" chain is that the chain can be removed and the cymbal will return to its normal sound (in contrast, a cymbal with rivets would have to have the rivets removed).
Some sizzlers feature pivoting arms that allow the chains to be quickly raised from the cymbal, or lowered onto it, allowing the effect to be used for some songs and removed for others.

Three types of protective covers are common for kit drums:

As with all musical instruments, the best protection is a provided by a combination of a hard-shelled case with padding such as foam next to the drums or cymbals.

Microphones ("mics") are used with drums to pick up the sound of the drums and cymbals for a sound recording and/or to pick up the sound of the drum kit so that it can be amplified through a PA system or sound reinforcement system. While most drummers use microphones and amplification in live shows in the 2010s, so that the sound engineer can adjust and balance the levels of the drums and cymbals, some bands that play in quieter genres of music and that play in small venues such as coffeehouses play acoustically, without mics or PA amplification. Small jazz groups such as jazz quartets or organ trios that are playing in a small bar will often just use acoustic drums. Of course if the same small jazz groups play on the mainstage of a big jazz festival, the drums will be mic'ed so that they can be adjusted in the sound system mix. A middle-ground approach is used by some bands that play in small venues; they do not mic every drum and cymbal, but rather mic only the instruments that the sound engineer wants to be able to control in the mix, such as the bass drum and the snare.

In "miking" a drum kit, dynamic microphones, which can handle high sound-pressure levels, are usually used to close-mic drums, which is the predominant way to mic drums for live shows. Condenser microphones are used for overheads and room mics, an approach which is more common with sound recording applications. Close miking of drums may be done using stands or by mounting the microphones on the rims of the drums, or even using microphones built into the drum itself, which eliminates the need for stands for these microphones, reducing both clutter and set-up time, as well as isolating them.

In some styles of music, drummers use electronic effects on drums, such as individual noise gates that mute the attached microphone when the signal is below a threshold volume. This allows the sound engineer to use a higher overall volume for the drum kit by reducing the number of "active" mics which could produce unwanted feedback at any one time. When a drum kit is entirely miked and amplified through the sound reinforcement system, the drummer or the sound engineer can add other electronic effects to the drum sound, such as reverb or digital delay.

Some drummers arrive at the venue with their drum kit and use the mics and mic stands provided by the venue's sound engineer. Other drummers bring their all of their own mics, or selected mics (e.g., a good quality bass drum mic and a good mic for the snare) to ensure that they have good quality mics for each show. In bars and nightclubs, the microphones supplied by the venue can sometimes be in substandard condition, due to the heavy use they experience.
Drummers using electronic drums, drum machines, or hybrid acoustic-electric kits (which blend traditional acoustic drums and cymbals with electronic pads) typically use a monitor speaker, keyboard amplifier or even a small PA system to hear the electronic drum sounds. Even a drummer playing entirely acoustic drums may use a monitor speaker to hear her drums, especially if she is playing in a loud rock or metal band, where there is substantial onstage volume from huge, powerful guitar stacks. Since the drum kit uses the deep bass drum, drummers are often given a large speaker cabinet with a 15" subwoofer to help them monitor their bass drum sound (along with a full-range monitor speaker to hear the rest of their kit). Some sound engineers and drummers prefer to use an electronic vibration system, colloquially known as a "butt shaker" or "throne thumper" to monitor the bass drum, because this lowers the stage volume. With a "butt shaker", the "thump" of each bass drum strike causes a vibration in the drum stool; this way the drummer "feels" their beat on the posterior, rather than hears it.

A number of accessories are designed for the bass drum (also called "kick drum"). Ported tubes for the bass drum are available to take advantage of the bass reflex speaker design, in which a tuned port (a hole and a carefully measured tube) are put in a speaker enclosure to improve the bass response at the lowest frequencies. Bass drumhead patches are available, which protect the drumhead from the impact of the felt beater. Bass drum pillows are fabric bags with filling or stuffing that can be used to alter the tone or resonance of the bass drum. A less expensive alternative to using a specialized bass drum pillow is to use an old sleeping bag.

Some drummers wear special drummer's gloves to improve their grip on the sticks when they play. Drumming gloves often have a textured grip surface made of a synthetic or rubber material and mesh or vents on the parts of the glove not used to hold sticks, to ventilate perspiration.

In some styles or settings, such as country music clubs or churches, small venues, or when a live recording is being made, the drummer may use a transparent perspex or plexiglas "drum screen" (also known as a "drum shield") to dampen the onstage volume of the drums. A screen that completely surrounds the drum kit is known as a "drum booth". In live sound applications, drum shields are used so that the audio engineer can have more control over the volume of drums that the audience hears through the PA system mix or to reduce the overall volume of the drums, as a way to reduce the overall volume of the band in the venue. In some recording studios, foam and fabric baffles are used in addition to or in place of clear panels. The drawback with foam/cloth baffle panels is that the drummer cannot see other performers, the record producer or the audio engineer well.

Drummers often bring a carpet, mats or rugs to venues to prevent the bass drum and hi-hat stand from "crawling" (moving away) on a slippery surface from the drum head striking the bass drum. The carpet also reduces short reverberation (which is generally but not always an advantage), and helps to prevent damage to the flooring or floor coverings. In shows where multiple drummers will bring their kits onstage over the night, it is common for drummers to mark the location of their stands and pedals with tape, to allow for quicker positioning of a kits in a drummer's accustomed position. Bass drums and hi-hat stands commonly have retractable spikes to help them to grip surfaces such as carpet, or stay stationary (on hard surfaces) with rubber feet.

Drummers use a variety of accessories when practicing. Metronomes and beat counters are used to develop a sense of a steady pulse. Drum muffling pads may be used to lessen the volume of drums during practicing. A practice pad, held on the lap, on a leg, or mounted on a stand, is used for near-silent practice with drumsticks. A set of practice pads mounted to simulate an entire drum kit is known as a practice kit. In the 2010s, these have largely been superseded by electronic drums, which can be listened to with headphones for quiet practice and kits with non-sounding mesh heads.

Drummers use a drum key for tuning their drums and adjusting some drum hardware. Besides the basic type of drum key (a T-handled wrench) there are various tuning wrenches and tools. Basic drum keys are divided in three types which allows tuning of three types of tuning screws on drums: square (most used), slotted and hexagonal. Ratchet-type wrenches allow high-tension drums to be tuned easily. Spin keys (utilizing a ball joint) allow rapid head changing. Torque-wrench type keys are available, graphically revealing the torque at each lug. Also, tension gauges, or meters, which are set on the head, aid drummers to achieve a consistent tuning. Drummers can tune drums "by ear" or, in the 2010s, use a digital drum tuner, which "measures tympanic pressure" on the drumhead to provide accurate tuning.

Kit drumming, whether playing accompaniment of voices and other instruments or doing a drum solo, consists of two elements:

A "fill" is a departure from the repetitive rhythm pattern in a song. A drum fill is used to "fill in" the space between the end of one verse and the beginning of another verse or chorus. Fills vary from a simple few strokes on a tom or snare, to a distinctive rhythm played on the hi-hat, to sequences several bars long that are short virtuosic drum solos. As well as adding interest and variation to the music, fills serve an important function in preparing and indicating significant changes of sections in songs and linking sections. A "vocal cue" is a short drum fill that introduces a vocal entry. A fill ending with a cymbal crash on beat one is often used to lead into a chorus or verse.

A drum solo is an instrumental section that highlights the virtuosity, skill and musical creativity of the drummer. While other instrument solos such as guitar solos are typically accompanied by the other rhythm section instruments (e.g., bass guitar and electric guitar), for most drum solos, all the other band members stop playing so that all of the audience's focus will be on the drummer. In some drum solos, the other rhythm section instrumentalists may play "punches" at certain points–sudden, loud chords of a short duration. Drum solos are common in jazz, but they are also used in a number of rock genres, such as heavy metal and progressive rock. During drum solos, drummers have a great deal of creative freedom, and drummers often use the entire drum kit. In live concerts, drummers may be given long drum solos, even in genres where drum solos are rare on singles.

Most drummers hold the drumsticks in one of two types of grip:
Within these two types, there is still considerable variation, and even disagreements as to exactly how the stick is held in a particular method. For example, Jim Chapin, an early and influential exponent of the Moeller method, asserts that the technique does not rely on rebound, while Dave Weckl asserts that it does rely on rebound.

Drum kit music is either written down in music notation (called "drum parts"), learned and played "by ear", improvised or some combination of some or all three of these methods. Professional session musician drummers and Big Band jazz drummers are often required to read drum parts. Drum parts are most commonly written on a standard five-line staff. In 2016, a special "percussion clef" is used, while previously the bass clef was used. However, even if the bass or no clef is used, each line and space is assigned an instrument of the kit, rather than to a pitch. In jazz, traditional music, folk music, rock music and pop music, drummers are expected to be able to learn songs by ear (from a recording or from another musician who is playing or singing the song) and improvise. The degree of improvisation differs in different styles. Jazz and jazz fusion drummers may have lengthy improvised solos in every song. In rock music and blues, there are also drum solos in some songs, although they tend to be shorter than those in jazz. Drummers in all popular music and traditional music styles are expected to be able to improvise accompaniment parts to songs, once they are told the genre or style (e.g., shuffle, ballad, slow blues, etc.).


</doc>
<doc id="9080" url="https://en.wikipedia.org/wiki?curid=9080" title="Dying Earth">
Dying Earth

Dying Earth is a fantasy series by the American author Jack Vance, comprising four books originally published from 1950 to 1984.
Some have been called picaresque. They vary from short story collection to fix-up (novel created from older short stories) perhaps all the way to novel.

The first book in the series, "The Dying Earth", was ranked number 16 of 33 "All Time Best Fantasy Novels" by "Locus" in 1987, based on a poll of subscribers, although it was marketed as a collection and the ISFDB calls it a "loosely connected series of stories".

The stories of the "Dying Earth" series are set in the distant future, at a point when the sun is almost exhausted and magic has asserted itself as a dominant force. The Moon has disappeared and the Sun is in danger of burning out at any time, often flickering as if about to go out, before shining again. The various civilizations of Earth have collapsed for the most part into decadence or religious fanaticism and its inhabitants overcome with a fatalistic outlook. The Earth is mostly barren and cold, and has become infested with various predatory monsters (possibly created by a magician in a former age).

Magic in the Dying Earth is performed by memorizing syllables, and the human brain can only accommodate a certain amount at once. When a spell is used, the syllables vanish from the caster's mind. Creatures called Sandestin can be summoned and used to perform more complex actions, but are considered dangerous to rely upon. Magic has loose links the science of old, and advanced mathematics is treated like arcane lore. 

The Dying Earth exists alongside several Overworlds and Underworlds. These help add a sense of profound longing and entrapment to the series. While humans can, with relative ease, physically travel to the horrific Underworlds (as Cugel does on several occasions, to his dismay) the vast majority of the population are only capable of mentally visiting the wondrous Overworlds through rare artifacts (IE through the "Eyes of the Overworld") or dangerous magic phenomena (such as the ship Cugel encounters in the deserts). Though they can look at the wonders and pretend they are really there, humans can never truly inhabit or escape to these utopia as their physical bodies remain stuck on the Dying Earth and will die with the sun regardless. These siren-like visions of paradise lead to the deaths, insanity, and suffering of many, especially during Cugel's journeys.

While most remaining civilizations on the Dying Earth are utterly unique in their customs and cultures, there are some common threads. Because the moon is gone and wind is often weak (the sun no longer heats the earth as much) the oceans are largely placid bodies of water with no tide and tiny waves. To cross them, boats are propelled by giant sea-worms. These worms are cared for and controlled by "Wormingers". In addition, the manses of magicians, protected by walls and spells and monsters, are relatively common sights in inhabited lands.

Vance wrote the stories of the first book while he served in the United States Merchant Marine during World War II. In the late 1940s several of his other stories were published in magazines.
According to pulp editor Sam Merwin, Vance's earliest magazine submissions in the 1940s were heavily influenced by the style of James Branch Cabell. Fantasy historian Lin Carter has noted several probable lasting influences of Cabell on Vance's work, and suggests that the early "pseudo-Cabell" experiments bore fruit in "The Dying Earth" (1950).

The series comprises four books by Vance and some sequels by other authors that may be or may not have been canonical.

All four books were published with Tables of Contents, the first and fourth as collections. The second and third contained mostly material previously published in short story form but were marketed as novels, the second as a fix-up and the third without acknowledging any previous publication.

1. "The Dying Earth" (the author's preferred title is "Mazirian the Magician") was openly a collection of six stories, all original, although written during Vance's WWII service. ISFDB calls them "slightly connected" and catalogs the last as a novella (17,500 to 40,000 word count).

2. "Eyes of the Overworld" (the author's preferred title is "Cugel the Clever") was a fix-up of six stories, presented as seven. All were novellas by word count (7500 to 17,500). Five were previously published as noted here.

3. "Cugel's Saga" (the author's preferred title is "Cugel: The Skybreak Spatterlight") was marketed as a novel. ISFDB calls it "[t]wice as large and less episodic than "Eyes of the Overworld"" but qualifies that label. "This is marketed as a novel, but there is a table of contents, and some of the parts were previously published (although none are acknowledged thus)." It catalogs previous publication of three chapters without remark on the degree of revision.

4. "Rhialto the Marvellous" was marketed as a collection, a Foreword and three stories, one previously published. The Foreword is non-narrative canonical fiction presenting the general state of the world in the 21st Aeon (a "short story" loosely).

Some sequels have been written by other authors, either with Vance's authorization or as tributes to his work.

Michael Shea's first publication, the novel "A Quest for Simbilis" (DAW Books, 1974, ), was an authorized sequel to "Eyes". However, "When Vance returned to the milieu, his "Cugel's Saga" continued the events of "The Eyes of the Overworld" in a different direction."

The tribute anthology "Songs of the Dying Earth" (2009) contains short fiction set in the world of the Dying Earth by numerous writers alongside tributes to Vance's work and influence.

In 2010 Shea wrote another authorized story belonging to the "Dying Earth" series and featuring Cugel as one of characters: "Hew the Tintmaster", published in the anthology "Swords & Dark Magic: The New Sword and Sorcery", ed. Jonathan Strahan and Lou Anders (Eos, 2010, pp. 323–362).

WorldCat contributing libraries report holding all four books in French, Spanish, and (in omnibus edition) Hebrew translations; and report holding "The Dying Earth" in five other languages: Finnish, German, Japanese, Polish, and Russian.

The whole first volume (six stories) has been translated also into Esperanto together with two Cugel stories and made available on-line as e-books by a long-time fan and Vance Integral Edition co-worker. Permission to translate and distribute (only into Esperanto) was obtained informally direct from the author and, since his death in 2013, continues with ongoing permission from the author's estate. To date these are three: "Mazirian the Magician", "The Sorcerer Pharesm", and "The Bagful of Dreams" available for free download as EPub, Mobi and PDF.

The Dying Earth subgenre of science fiction is named in recognition of Vance's role in standardizing a setting, the entropically dying earth and sun. Its importance was recognized with the publication of "Songs of the Dying Earth", a tribute anthology edited by George R. R. Martin and Gardner Dozois (Subterranean, 2009). Each short story in the anthology is set on the Dying Earth, and concludes with a short acknowledgement by the author of Vance's influence on them.

Some particular works since Vance should be singled out.

Gene Wolfe's "The Book of the New Sun" (1980–83) is set in a slightly similar world, and was written under Vance's influence. Wolfe suggested in "The Castle of the Otter", a collection of essays, that he inserted the book "The Dying Earth" into his fictional world under the title "The Book of Gold" (specifically, Wolfe wrote that the "Book of Gold" mentioned in "The Book of the New Sun" is different for each reader, but for him it was "The Dying Earth.")'. Wolfe has extended the series.

Michael Shea's novel "Nifft the Lean" (1982), his second book eight years after "A Quest for Simbilis", also owes much debt to Vance's creation, since the protagonist of the story is a petty thief (not unlike Cugel the Clever), who travels and struggles in an exotic world. Shea returned to Nifft with 1997 and 2000 sequels.

The Archonate stories by Matthew Hughes — the 1994 novel "Fools Errant" and numerous works in this millennium —
take place in "the penultimate age of Old Earth," a period of science and technology that is on the verge of transforming into the magical era of the time of the Dying Earth.
Booklist has called him Vance's "heir apparent." (Review by Carl Hays of The Gist Hunter and Other Stories, Booklist, August 2005)

The original creators of the "Dungeons & Dragons" games were fans of Jack Vance and incorporated many aspects of the "Dying Earth" series into the game. The magic system, in which a wizard is limited in the number of spells that can be simultaneously remembered and forgets them once they are cast, was based on the magic of Dying Earth. In role-playing game circles, this sort of magic system is called 'Vancian' or 'Vancean'. Some of the spells from "Dungeons & Dragons" are based on spells mentioned in the "Dying Earth" series, such as the "prismatic spray". Magic items from the "Dying Earth" stories such as ioun stones also made their way into "Dungeons & Dragons". One of the deities of magic in "Dungeons & Dragons" is named Vecna (an anagram of Vance).

The Talislanta role-playing game designed by Stephan Michael Sechi and originally published in 1987 by Bard Games was inspired by the works of Jack Vance so much so that the first release, The Chronicles of Talislanta is dedicated to the author.

There is an official "Dying Earth" role-playing game published by Pelgrane Press with an occasional magazine "The Excellent Prismatic Spray" (named after a magic spell). The game situates players in Vance's world populated by desperately extravagant people. Many other role-playing settings pay homage to the series by including fantasy elements he invented such as the darkness-dwelling Grues.




</doc>
<doc id="9082" url="https://en.wikipedia.org/wiki?curid=9082" title="Dispute resolution">
Dispute resolution

Dispute resolution is the process of resolving disputes between parties. The term "dispute resolution" may also be used interchangeably with "conflict resolution", where conflict styles can be used for different scenarios. 

Methods of dispute resolution include:
One could theoretically include violence or even war as part of this spectrum, but dispute resolution practitioners do not usually do so; violence rarely ends disputes effectively, and indeed, often only escalates them. 

Dispute resolution processes fall into two major types:

Not all disputes, even those in which skilled intervention occurs, end in resolution. Such intractable disputes form a special area in dispute resolution studies.

Dispute resolution is an important requirement in international trade, including negotiation, mediation, arbitration and litigation.

The legal system provides resolutions for many different types of disputes. Some disputants will not reach agreement through a collaborative process. Some disputes need the coercive power of the state to enforce a resolution. Perhaps more importantly, many people want a professional advocate when they become involved in a dispute, particularly if the dispute involves perceived legal rights, legal wrongdoing, or threat of legal action against them.

The most common form of judicial dispute resolution is litigation. Litigation is initiated when one party files suit against another. In the United States, litigation is facilitated by the government within federal, state, and municipal courts. The proceedings are very formal and are governed by rules, such as rules of evidence and procedure, which are established by the legislature. Outcomes are decided by an impartial judge and/or jury, based on the factual questions of the case and the application law. The verdict of the court is binding, not advisory; however, both parties have the right to appeal the judgment to a higher court. Judicial dispute resolution is typically adversarial in nature, for example, involving antagonistic parties or opposing interests seeking an outcome most favorable to their position. 

Retired judges or private lawyers often become arbitrators or mediators; however, trained and qualified non-legal dispute resolution specialists form a growing body within the field of alternative dispute resolution (ADR). In the United States, many states now have mediation or other ADR programs annexed to the courts, to facilitate settlement of lawsuits.

Some use the term "dispute resolution" to refer only to alternative dispute resolution (ADR), that is, extrajudicial processes such as arbitration, collaborative law, and mediation used to resolve conflict and potential conflict between and among individuals, business entities, governmental agencies, and (in the public international law context) states. ADR generally depends on agreement by the parties to use ADR processes, either before or after a dispute has arisen. ADR has experienced steadily increasing acceptance and utilization because of a perception of greater flexibility, costs below those of traditional litigation, and speedy resolution of disputes, among other perceived advantages. However, some have criticized these methods as taking away the right to seek redress of grievances in the courts, suggesting that extrajudicial dispute resolution may not offer the fairest way for parties not in an equal bargaining relationship, for example in a dispute between a consumer and a large corporation. In addition, in some circumstances, arbitration and other ADR processes may become as expensive as litigation or more so.

Alés, Javier y Mata, Juan Diego " manual práctico para mediadores: el misterio de la mediacion" éxito Atelier. Barcelona 2016



</doc>
<doc id="9085" url="https://en.wikipedia.org/wiki?curid=9085" title="Catan: Cities &amp; Knights">
Catan: Cities &amp; Knights

Catan: Cities & Knights (), formerly "The Cities and Knights of Catan" is an expansion to the board game "The Settlers of Catan" for three to four players (five to six player play is also possible with the "Settlers" and "Cities & Knights" five to six player extensions; two-player play is possible with the "" expansion). It contains features taken from "The Settlers of Catan", with emphasis on city development and the use of knights, which are used as a method of attacking other players as well as helping opponents defend Catan against a common foe. "Cities & Knights" can also be combined with the "" expansion or with scenarios (again, five to six player play only possible with the applicable five to six player extension(s)).

Because of the new rules introduced in "Cities & Knights", the game is played to 13 victory points, as opposed to 10 as in the base game "The Settlers of Catan".

The following cards are not used in Cities & Knights:

One of the main additions to the game is commodities, which are a type of secondary resource produced only by cities. Like resources, commodities are associated with a type of terrain, can be stolen by the robber (with "Seafarers", also the pirate), count against the resource hand limit, and may not be collected if the robber is on the terrain. Resources may be traded for commodities, and commodities may be traded for resources. Commodities can then be used to build city improvements (provided the player has a city), which provide additional benefits.

The commodities are paper (which comes from forest terrain), coin (from mountain terrain), and cloth (from pasture terrain).

When combining "Cities & Knights" with "Barbarian Attack", the written rules are ambiguous with regards to whether commodities are collected along with normal resources when collecting from a Gold River tile, as well as whether or not commodities can be collected directly from Gold River tiles. However, online rules state that "Gold can only buy you resources, not commodities."

A city on grain or brick gives two of each, as in the original "Settlers". A city on wool, ore, or wood, produces one corresponding resource as well as one corresponding commodity (cloth, coin, or paper). Grain and brick, however, are used for new purchasing options: grain activates knights, and brick can be used to build city walls.

In total there are 36 commodity cards: 12 paper (from forest), 12 cloth (from pasture), and 12 coin (from mountains).

A player with a city may use commodities to build city improvements, which allow several advantages. There are city improvements in five levels, and in three different categories. Each category of improvements requires a different commodity and higher levels require more cards of that commodity. At the third level, players earn a special ability, depending on the type of improvement.

The first player with an improvement at the fourth level can claim any of their cities as a metropolis, worth four victory points instead of two for that city. Each type of improvement has only one associated metropolis, and no city can be a metropolis of two different types (because of this, a player without a non-metropolis city may not build improvements beyond the third level). If a player is the first to build an improvement to the final level (out-building the current holder of the metropolis), they take the metropolis from its current holder.

The other significant concept in "Cities & Knights" is the concept of knights, which replace the concept of soldiers and the largest army. Knights are units that require continuous maintenance through their activation mechanism, but have a wide variety of functions. Knights can be promoted through three ranks, although promotion to the final rank is a special ability granted by the city improvement the Fortress.

Knights are placed on the board in a similar manner to settlements, and can be used to block opposing roads, active or not. However, knights must be activated in order to perform other functions, which immediately deactivate the knight. Knights cannot perform actions on the same turn they are activated, but can be reactivated on the same turn as performing an action. These actions include:

If a knight is promoted or forced to retreat, its active status does not change.

The standard "Cities & Knights" game comes with 24 knights, 6 of each color. The 5/6 player extension adds a further 12 knights, 6 each of two new colors.

"Cities & Knights" introduces a third die, known as the event die, which serves two functions. The first applies to the concept of barbarians, a periodic foe that all players must work together to defend against. Three of the sides of the event die have a picture of a ship on them. The other three sides have a symbol of a city gate, allowing players who have sufficiently built up a city to obtain progress cards (see below).

The barbarians are represented by a ship positioned on a track representing the distance between the ship and Catan (i.e. the board). Each time the event die shows a black ship, the barbarian ship takes one step closer to Catan. When the barbarians arrive at Catan, a special phase is immediately performed before all other actions (including collecting resources). In this special phase, the barbarians' attack strength, corresponding to the combined number of cities and metropolises held by all players, is compared to Catan's defense strength, corresponding to the combined levels (i.e. 1 point for each basic, 2 for each strong, and 3 for each mighty) of all activated knights in play.

If the barbarians are successful in their attack (if they have a strength greater than Catan), then the players must pay the consequence. The player(s) who had the least defense will be attacked, and will have one city reduced to a settlement. If they only have settlements, or metropolises, then they are immune to barbarians and do not count as the player contributing the least defense.

Should Catan prevail, the player who contributes the most to Catan's defense receives a special "Defender of Catan" card, worth a victory point. Regardless of the outcome, all knights are immediately deactivated, and the barbarian ship returns to its starting point on the track. In the event of a tie among the greatest contributors of knights, none of the tied players earn a Defender of Catan card. Instead, each of the tied players draw a progress card (explained below) of the type of their choosing. There are 6 Defender of Catan cards.

As the likelihood of having the barbarian move closer to Catan is very high, a variant in common usage is that the robber (and with "Seafarers", the pirate) does not move until the first barbarian attack, nor can a knight move the robber before that point.

Examples where cities are lost:


The other significant outcome of the event die is Progress cards, which replace development cards. Because of the mechanics of progress cards explained below, one of the two white dice used in "Settlers" is replaced by a red die.

Progress cards are organized into three categories, corresponding to the three types of improvements. Yellow progress cards aid in commercial development, green progress cards aid in technological advancements, and blue progress cards allow for political moves. When a castle appears on the event die, progress cards of the corresponding type may be drawn depending on the value of the red die. Higher levels of city improvements increase the chance that progress cards will be drawn, with the highest level of city improvement allowing progress cards to be drawn regardless of the value on the red die.

Progress cards, unlike the development cards they replace, can be played on the turn that they are drawn, and more than one progress card can be played per turn. However, they can generally only be played after the dice are rolled. Progress cards granting victory points are an exception, being played immediately (without regards to whose turn it is), while the Alchemist progress card, which allows a player to select the roll of the white and red dice, necessitates the card being played instead of rolling the numerical dice. (The event die is still rolled as normal.)

Players are allowed to keep four progress cards (five in a five to six player game), and any additional ones must be discarded on the spot (unless the 5th card is a victory point, which is played immediately and the original progress cards remain). The only exception to this rule is if a player receives a 5th non-victory point progress card on his or her turn, in which case he or she has the option of playing any one of the five progress cards in his or her hand during the turn, thus bringing the progress card count back down to four. While this clarification is not overtly stated in the Cities & Knights rule book, it is enforced in the online version of the game.

In total, there are 54 progress cards: 18 science, 18 politics, and 18 trade.

City walls are a minor addition to "Cities & Knights" that increase the number of resource and commodity cards a player is allowed in their hand before having to discard on a roll of 7. However, they do not protect the player from the robber or barbarians. Only cities and metropolises may have walls, and each city or metropolis can only have one wall, up to three walls per player. Each wall that the player has deployed permits the player to hold two more cards before being required to discard on a roll of seven. This results in a maximum of 13 cards.

If the barbarians pillage your city, then the city wall is also destroyed and the wall is removed from the board.

The game comes with 12 city walls, 3 of each color.

The merchant is another addition to "Cities & Knights". Like the robber, the merchant is placed on a single land hex. Unlike the robber, the merchant has a beneficial effect.

The merchant can only be deployed through the use of a Merchant progress card (of which there are six), on a land hex near a city or a settlement. The player with the control of the merchant can trade the resource (not commodity) of that type at a two-to-one rate, as if the player had a control of a corresponding two-to-one harbor.

The player with the control of the merchant also earns a victory point. Both the victory point and the trade privilege are lost if another player takes control of the merchant.

In place of "The Settlers of Catan" standard improvement cost card, "Cities & Knights" gives a calendar type flip-chart to each player, of course still matching that players color. The top of the chart has the standard costs from the "Settlers" game (for settlements, upgrade to city, and roads). It does not include the Development Card cost as those cards are not used in a "Cities & Knights" game. It does include the costs of hiring a knight, upgrading a knight's level or strength, and the cost to activate a knight. It also includes the cost of a ship, which are not used in a regular game of "Cities & Knights", but presumably this is to cater for players who have combined "Cities & Knights" and "Seafarers".

Those are only the rudimentary costs of the game however. The calendar also shows the costs of the next city improvement in each of the three categories — as a city is improved in a category, that segment has its card flipped down calendar style to reveal the newly built improvement, any advantages gained by the improvement, and the updated cost of upgrading to the next level in that category. Each segment, as it is flipped down, also shows the updated dice pattern needed to earn the player a progress card in that category.


The official website for the world of Catan. (2016). Retrieved May 17, 2016, from <nowiki>http://www.catan.com/service/game-rules</nowiki>


</doc>
<doc id="9086" url="https://en.wikipedia.org/wiki?curid=9086" title="Catan: Seafarers">
Catan: Seafarers

Catan: Seafarers, or Seafarers of Catan in older editions, () is an expansion of the board game "The Settlers of Catan" for three to four players (five-to-six-player play is also possible with both of the respective five-to-six-player extensions). The main feature of this expansion is the addition of ships, gold rivers, and the pirate to the game, allowing play between multiple islands. The expansion also provides numerous scenarios, some of which have custom rules. The "Seafarers" rules and scenarios are also, for the most part, compatible with "" and "".

The concepts introduced in "Seafarers" were part of designer Klaus Teuber's original design for "Settlers".

"Seafarers" introduces the concept of ships, which serve as roads over water or along the coast. Each ship costs one lumber and one wool to create (lumber for the hull and wool for the sails). A settlement must first be built before a player can switch from building roads to building ships, or vice versa. Thus, a chain of ships is always anchored at a settlement on the coast. A shipping line that is not anchored at both ends by different settlements can also move the last ship at the open end, although this can only be done once per turn and may not be done with any ships that were created on the same turn.

The "Longest Road" card is now renamed the "Longest Trade Route" since this is now calculated by counting the number of contiguous ships "plus" roads that a player has. A settlement or city is necessary between a road and a ship for the two to be considered continuous for the purposes of this card.

The Road Building card allows a player to build 2 roads, 2 ships, or one of each when used.

Along with the concept of ships, "Seafarers" also introduces the notion of the pirate, which acts as a waterborne robber which steals from nearby ships (similar to how the robber steals from nearby settlements). The pirate can also prevent ships from being built or moved nearby, but it does not interfere with harbors.

When a seven is rolled or a Knight card is played, the player may move either the robber OR the pirate.

"Seafarers" also introduces the "Gold River" terrain, which grants nearby players one resource of their choice for every settlement adjacent to a gold tile and 2 resources for every city. Since being able to choose any resource type allows more building power, gold rivers are either marked with number token of only 2 or 3 dots and/or are far away from starting positions to offset this.

When combined with "Cities & Knights", the rules state that you are not allowed to take commodities instead of resources if a city is nearby.

Some scenarios have extra rules encompassing the concept of exploration, which is done by having the hex tiles placed face down. Should a player build next to unexplored terrain, the terrain tile is turned face up, and the player is rewarded with a resource should the tile revealed be resource-producing. In other scenarios, the board is divided into islands, and if the player builds a settlement on an island other than the ones they begin on, the settlement is worth extra victory points.

The "Cities and Knights" manual recommends that players not use the "Cities & Knights" rules in scenarios where exploration is a factor.

Unlike "The Settlers of Catan" and "Catan: Cities & Knights", in which the only random element of setup is the placement of land tiles, number tokens, and harbors in an identically-shaped playing area, "Catan: Seafarers" has a number of different scenarios or maps from which to choose. Each map uses a different selection of tiles laid out in a specific pattern, which may not use all of the tiles. Other attributes also set each map apart, for example, restrictions on the placement of initial settlements, whether tiles are distributed randomly, the number of victory points needed to win, and special victory point awards, usually for building on islands across the sea.

"Seafarers" provides scenarios for three or four players (the older fourth edition used the same maps for three- and four-player versions of the scenarios), while the extension provides scenarios for six players (the older third edition also included separate maps for five- and six-player scenarios). The scenarios between the older editions of "Seafarers" and the newest are generally incompatible, knowing the different frames included with the game. (In particular, older editions of "Settlers" did not come with a frame for their board; a separate add-on was made available for players of the older-edition "Settlers" games, containing the newer edition frames, so as to make them compatible with the newer edition of "Seafarers"; the older edition of "Seafarers" included a square frame, and while both older and newer editions of the frames have the same width across, the newer editions are not square-shaped, and are longer down the middle of the board compared to the sides.)

"Heading to New Shores" ("New Shores" in older editions) is the scenario resembling Teuber's original design for the game. The game board consists of the main "Settlers" island as well as a few smaller islands, which award a special victory point to each player for their first settlements on them. This scenario is meant for players new to "Seafarers", with elements of "Seafarers" incorporated into the more familiar main board.

"The Four Islands" is the first scenario introduced where new mechanics introduced to "Seafarers" is brought into the forefront. In this scenario, the map is split up into four islands of roughly equal size and resource distribution. (The six-player version found in the extension has the map split into six islands; the scenario is titled "The Six Islands", but is played identically. Older editions of the extension had a five-player version with five islands, called "The Five Islands".) Players may claim up to two of the islands as their home islands, and settling on any of the other islands awards a special victory point.

"The Fog Island" ("Oceans" in older editions) is the first scenario where exploration is used. The board starts off with a portion of the map left blank: when players expand into the blank region, terrain hexes are drawn at random from a supply and placed in the empty space, and, if a land hex is "discovered", a number token may be assigned. As a reward for discovering land, the player making the discovery is rewarded with a bonus resource card corresponding to the type of land hex discovered.

"Through the Desert" ("Into the Desert" in older edition) is similar to "The Four Islands", but consists of a large continent and smaller outlying islands. On the large island, there exists a "wall of deserts" that separates the island into a large main area and separate smaller strips of land. As the name of the scenario implies, expanding through the desert into these smaller strips of land, or by sea to the outlying islands, award bonus victory points.

"The Forgotten Tribe", originally titled "Friendly Neighbors", was a downloadable scenario (but only in the German language) which was incorporated into newer editions of "Seafarers".

The map consists of a main island and smaller outlying islands, where the namesake forgotten tribe resides. Players may not expand into the outlying islands, but by building ships so that they border the outlying islands, players may be awarded with victory points, development cards, or harbors that players may place on the coast of the main island at a later time.

Introduced in the newer editions, "Cloth for Catan" continues the adventures with the Forgotten Tribe. The scenario was previously available for older editions as a downloadable scenario (but only in German), titled "Coffee for Catan".

Players begin with settlements on the outside of the map, but may build ships to reach the Forgotten Tribe's islands, which are in the center. By connecting to the Forgotten Tribe's settlements (represented by number tokens), players may earn cloth tokens when the number token for the Forgotten Tribe's villages are rolled. Cloth tokens, in turn, are worth one victory point for each pair obtained.

"The Pirate Island", introduced in newer editions, is the first scenario which changes the mechanics of new gameplay elements introduced in "Seafarers". "The Pirate Island" had previously been available as a downloadable scenario (but only in German) suitable for the older editions.

In this scenario, players begin with a pre-placed settlement on a main island. Ships may only be built in one single line, which must pass through a fixed waypoint (different for each player) en route to a pirate fortress (each player has their own pirate fortress). Once ships connect to the pirate fortress, they may attempt to attack the pirate fortress once per turn. Ships may be lost if the attack is unsuccessful, but after three successful attacks, the pirate fortress is converted into a settlement. Players must convert their pirate fortresses and have 10 victory points before being able to claim victory.

Furthermore, the pirate mechanics have also changed: the pirate moves through the middle of the map in a fixed path every turn, and attacks the owner of any nearby settlements. Players win resources if they are able to fend off the pirate attack (which depends on the number rolled by the dice, as well as the number of warships in the defending player's possession; warships are created from using Knight cards on existing ships), but lose resources if they are unsuccessful. Maritime expansion is only permitted by building a settlement at the waypoint, however, this increases the chances of a pirate attack.

"The Wonders of Catan" was a downloadable scenario for older editions of "Seafarers" in both German and English, and was incorporated into "Seafarers" in newer editions.

In this scenario, there are a number of "wonders", each with a large cost of building as well as a prerequisite. If a player meets the prerequisite for a wonder, they may claim the wonder for themselves. A player may only claim one wonder, and each wonder may only be claimed by one player. Wonders must be built in four parts, and each wonder has a different build cost. The winner is the first player to complete their wonder, or the first player to have 10 victory points and have more parts of their wonder complete than any other player.

"The Great Crossing" was a scenario in the older editions of "Seafarers", which has been dropped in newer editions. The map is divided into two islands, Catan and Transcatania. Players begin with both settlements on one of the islands, and must build ships connecting settlements between the two islands. Players earn victory points for connecting their settlements with settlements (not necessarily theirs) from the opposite island using ships, or to another player's shipping lines which connect two settlements together.

"Greater Catan" was a scenario included in the older editions of "Seafarers" but is not included in newer editions. Due to the sheer amounts of equipment needed, two copies of "Settlers" and "Seafarers" are required to set up this scenario. The map consists of a standard "Settlers" island, along with a smaller chain of outlying islands. Only the main island initially has number tokens: number tokens are assigned to the outlying islands as they are expanded. However, the supply of number tokens is smaller than the number of hexes in the scenario: when the number tokens run out and players expand into a new part of the outlying islands, number tokens are moved from the main island to the outlying islands.

Hexes on the main island for which there are no number tokens do not produce resources, but number tokens are moved in such a way so as to avoid rendering a city unproductive; furthermore, whenever possible number tokens must be reassigned from hexes bordering a player's own settlements and cities, so as to prevent harming another player's economy without harming a player's own economy at the same time.

"New World" is a scenario that blankets all other scenarios that may be created from the parts of "Settlers" and "Seafarers". This scenario uses an entirely random map, and players are encouraged to try and create a tile layout that plays well. The only difference between versions in "Seafarers", the extension, and the older editions therein is the size of the frames.



</doc>
<doc id="9087" url="https://en.wikipedia.org/wiki?curid=9087" title="Dynamical system">
Dynamical system

In mathematics, a dynamical system is a system in which a function describes the time dependence of a point in a geometrical space. Examples include the mathematical models that describe the swinging of a clock pendulum, the flow of water in a pipe, and the number of fish each springtime in a lake.

At any given time, a dynamical system has a state given by a tuple of real numbers (a vector) that can be represented by a point in an appropriate state space (a geometrical manifold). The "evolution rule" of the dynamical system is a function that describes what future states follow from the current state. Often the function is deterministic, that is, for a given time interval only one future state follows from the current state. However, some systems are stochastic, in that random events also affect the evolution of the state variables.

In physics, a dynamical system is described as a "particle or ensemble of particles whose state varies over time and thus obeys differential equations involving time derivatives." In order to make a prediction about the system’s future behavior, an analytical solution of such equations or their integration over time through computer simulation is realized.

The study of dynamical systems is the focus of dynamical systems theory, which has applications to a wide variety of fields such as mathematics, physics, biology, chemistry, engineering, economics, and medicine. Dynamical systems are a fundamental part of chaos theory, logistic map dynamics, bifurcation theory, the self-assembly process, and the edge of chaos concept.

The concept of a dynamical system has its origins in Newtonian mechanics. There, as in other natural sciences and engineering disciplines, the evolution rule of dynamical systems is an implicit relation that gives the state of the system for only a short time into the future. (The relation is either a differential equation, difference equation or other time scale.) To determine the state for all future times requires iterating the relation many times—each advancing time a small step. The iteration procedure is referred to as "solving the system" or "integrating the system". If the system can be solved, given an initial point it is possible to determine all its future positions, a collection of points known as a "trajectory" or "orbit".

Before the advent of computers, finding an orbit required sophisticated mathematical techniques and could be accomplished only for a small class of dynamical systems. Numerical methods implemented on electronic computing machines have simplified the task of determining the orbits of a dynamical system.

For simple dynamical systems, knowing the trajectory is often sufficient, but most dynamical systems are too complicated to be understood in terms of individual trajectories. The difficulties arise because:

Many people regard Henri Poincaré as the founder of dynamical systems. Poincaré published two now classical monographs, "New Methods of Celestial Mechanics" (1892–1899) and "Lectures on Celestial Mechanics" (1905–1910). In them, he successfully applied the results of their research to the problem of the motion of three bodies and studied in detail the behavior of solutions (frequency, stability, asymptotic, and so on). These papers included the Poincaré recurrence theorem, which states that certain systems will, after a sufficiently long but finite time, return to a state very close to the initial state.

Aleksandr Lyapunov developed many important approximation methods. His methods, which he developed in 1899, make it possible to define the stability of sets of ordinary differential equations. He created the modern theory of the stability of a dynamic system.

In 1913, George David Birkhoff proved Poincaré's "Last Geometric Theorem", a special case of the three-body problem, a result that made him world-famous. In 1927, he published his "Dynamical Systems"Birkhoff's most durable result has been his 1931 discovery of what is now called the ergodic theorem. Combining insights from physics on the ergodic hypothesis with measure theory, this theorem solved, at least in principle, a fundamental problem of statistical mechanics. The ergodic theorem has also had repercussions for dynamics.

Stephen Smale made significant advances as well. His first contribution is the Smale horseshoe that jumpstarted significant research in dynamical systems. He also outlined a research program carried out by many others.

Oleksandr Mykolaiovych Sharkovsky developed Sharkovsky's theorem on the periods of discrete dynamical systems in 1964. One of the implications of the theorem is that if a discrete dynamical system on the real line has a periodic point of period 3, then it must have periodic points of every other period.

A dynamical system is a manifold "M" called the phase (or state) space endowed with a family of smooth evolution functions Φ that for any element of "t" ∈ "T", the time, map a point of the phase space back into the phase space. The notion of smoothness changes with applications and the type of manifold. There are several choices for the set "T". When "T" is taken to be the reals, the dynamical system is called a "flow"; and if "T" is restricted to the non-negative reals, then the dynamical system is a "semi-flow". When "T" is taken to be the integers, it is a "cascade" or a "map"; and the restriction to the non-negative integers is a "semi-cascade".

The evolution function Φ is often the solution of a "differential equation of motion"

The equation gives the time derivative, represented by the dot, of a trajectory "x"("t") on the phase space starting at some point "x". The vector field "v"("x") is a smooth function that at every point of the phase space "M" provides the velocity vector of the dynamical system at that point. (These vectors are not vectors in the phase space "M", but in the tangent space "TM" of the point "x".) Given a smooth Φ, an autonomous vector field can be derived from it.

There is no need for higher order derivatives in the equation, nor for time dependence in "v"("x") because these can be eliminated by considering systems of higher dimensions. Other types of differential equations can be used to define the evolution rule:

is an example of an equation that arises from the modeling of mechanical systems with complicated constraints.

The differential equations determining the evolution function Φ are often ordinary differential equations; in this case the phase space "M" is a finite dimensional manifold. Many of the concepts in dynamical systems can be extended to infinite-dimensional manifolds—those that are locally Banach spaces—in which case the differential equations are partial differential equations. In the late 20th century the dynamical system perspective to partial differential equations started gaining popularity.

Linear dynamical systems can be solved in terms of simple functions and the behavior of all orbits classified. In a linear system the phase space is the "N"-dimensional Euclidean space, so any point in phase space can be represented by a vector with "N" numbers. The analysis of linear systems is possible because they satisfy a superposition principle: if "u"("t") and "w"("t") satisfy the differential equation for the vector field (but not necessarily the initial condition), then so will "u"("t") + "w"("t").

For a flow, the vector field Φ("x") is an affine function of the position in the phase space, that is,
with "A" a matrix, "b" a vector of numbers and "x" the position vector. The solution to this system can be found by using the superposition principle (linearity).
The case "b" ≠ 0 with "A" = 0 is just a straight line in the direction of "b":

When "b" is zero and "A" ≠ 0 the origin is an equilibrium (or singular) point of the flow, that is, if "x" = 0, then the orbit remains there.
For other initial conditions, the equation of motion is given by the exponential of a matrix: for an initial point "x",

When "b" = 0, the eigenvalues of "A" determine the structure of the phase space. From the eigenvalues and the eigenvectors of "A" it is possible to determine if an initial point will converge or diverge to the equilibrium point at the origin.

The distance between two different initial conditions in the case "A" ≠ 0 will change exponentially in most cases, either converging exponentially fast towards a point, or diverging exponentially fast. Linear systems display sensitive dependence on initial conditions in the case of divergence. For nonlinear systems this is one of the (necessary but not sufficient) conditions for chaotic behavior.

A discrete-time, affine dynamical system has the form of a matrix difference equation:
with "A" a matrix and "b" a vector. As in the continuous case, the change of coordinates "x" → "x" + (1 − "A")"b" removes the term "b" from the equation. In the new coordinate system, the origin is a fixed point of the map and the solutions are of the linear system "A""x".
The solutions for the map are no longer curves, but points that hop in the phase space. The orbits are organized in curves, or fibers, which are collections of points that map into themselves under the action of the map.

As in the continuous case, the eigenvalues and eigenvectors of "A" determine the structure of phase space. For example, if "u" is an eigenvector of "A", with a real eigenvalue smaller than one, then the straight lines given by the points along "α" "u", with "α" ∈ R, is an invariant curve of the map. Points in this straight line run into the fixed point.

There are also many other discrete dynamical systems.

The qualitative properties of dynamical systems do not change under a smooth change of coordinates (this is sometimes taken as a definition of qualitative): a "singular point" of the vector field (a point where "v"("x") = 0) will remain a singular point under smooth transformations; a "periodic orbit" is a loop in phase space and smooth deformations of the phase space cannot alter it being a loop. It is in the neighborhood of singular points and periodic orbits that the structure of a phase space of a dynamical system can be well understood. In the qualitative study of dynamical systems, the approach is to show that there is a change of coordinates (usually unspecified, but computable) that makes the dynamical system as simple as possible.

A flow in most small patches of the phase space can be made very simple. If "y" is a point where the vector field "v"("y") ≠ 0, then there is a change of coordinates for a region around "y" where the vector field becomes a series of parallel vectors of the same magnitude. This is known as the rectification theorem.

The "rectification theorem" says that away from singular points the dynamics of a point in a small patch is a straight line. The patch can sometimes be enlarged by stitching several patches together, and when this works out in the whole phase space "M" the dynamical system is "integrable". In most cases the patch cannot be extended to the entire phase space. There may be singular points in the vector field (where "v"("x") = 0); or the patches may become smaller and smaller as some point is approached. The more subtle reason is a global constraint, where the trajectory starts out in a patch, and after visiting a series of other patches comes back to the original one. If the next time the orbit loops around phase space in a different way, then it is impossible to rectify the vector field in the whole series of patches.

In general, in the neighborhood of a periodic orbit the rectification theorem cannot be used. Poincaré developed an approach that transforms the analysis near a periodic orbit to the analysis of a map. Pick a point "x" in the orbit γ and consider the points in phase space in that neighborhood that are perpendicular to "v"("x"). These points are a Poincaré section "S"("γ", "x"), of the orbit. The flow now defines a map, the Poincaré map "F" : "S" → "S", for points starting in "S" and returning to "S". Not all these points will take the same amount of time to come back, but the times will be close to the time it takes "x".

The intersection of the periodic orbit with the Poincaré section is a fixed point of the Poincaré map "F". By a translation, the point can be assumed to be at "x" = 0. The Taylor series of the map is "F"("x") = "J" · "x" + O("x"), so a change of coordinates "h" can only be expected to simplify "F" to its linear part

This is known as the conjugation equation. Finding conditions for this equation to hold has been one of the major tasks of research in dynamical systems. Poincaré first approached it assuming all functions to be analytic and in the process discovered the non-resonant condition. If "λ", ..., "λ" are the eigenvalues of "J" they will be resonant if one eigenvalue is an integer linear combination of two or more of the others. As terms of the form "λ" – ∑ (multiples of other eigenvalues) occurs in the denominator of the terms for the function "h", the non-resonant condition is also known as the small divisor problem.

The results on the existence of a solution to the conjugation equation depend on the eigenvalues of "J" and the degree of smoothness required from "h". As "J" does not need to have any special symmetries, its eigenvalues will typically be complex numbers. When the eigenvalues of "J" are not in the unit circle, the dynamics near the fixed point "x" of "F" is called "hyperbolic" and when the eigenvalues are on the unit circle and complex, the dynamics is called "elliptic".

In the hyperbolic case, the Hartman–Grobman theorem gives the conditions for the existence of a continuous function that maps the neighborhood of the fixed point of the map to the linear map "J" · "x". The hyperbolic case is also "structurally stable". Small changes in the vector field will only produce small changes in the Poincaré map and these small changes will reflect in small changes in the position of the eigenvalues of "J" in the complex plane, implying that the map is still hyperbolic.

The Kolmogorov–Arnold–Moser (KAM) theorem gives the behavior near an elliptic point.

When the evolution map Φ (or the vector field it is derived from) depends on a parameter μ, the structure of the phase space will also depend on this parameter. Small changes may produce no qualitative changes in the phase space until a special value "μ" is reached. At this point the phase space changes qualitatively and the dynamical system is said to have gone through a bifurcation.

Bifurcation theory considers a structure in phase space (typically a fixed point, a periodic orbit, or an invariant torus) and studies its behavior as a function of the parameter "μ". At the bifurcation point the structure may change its stability, split into new structures, or merge with other structures. By using Taylor series approximations of the maps and an understanding of the differences that may be eliminated by a change of coordinates, it is possible to catalog the bifurcations of dynamical systems.

The bifurcations of a hyperbolic fixed point "x" of a system family "F" can be characterized by the eigenvalues of the first derivative of the system "DF"("x") computed at the bifurcation point. For a map, the bifurcation will occur when there are eigenvalues of "DF" on the unit circle. For a flow, it will occur when there are eigenvalues on the imaginary axis. For more information, see the main article on Bifurcation theory.

Some bifurcations can lead to very complicated structures in phase space. For example, the Ruelle–Takens scenario describes how a periodic orbit bifurcates into a torus and the torus into a strange attractor. In another example, Feigenbaum period-doubling describes how a stable periodic orbit goes through a series of period-doubling bifurcations.

In many dynamical systems, it is possible to choose the coordinates of the system so that the volume (really a ν-dimensional volume) in phase space is invariant. This happens for mechanical systems derived from Newton's laws as long as the coordinates are the position and the momentum and the volume is measured in units of (position) × (momentum). The flow takes points of a subset "A" into the points Φ("A") and invariance of the phase space means that
In the Hamiltonian formalism, given a coordinate it is possible to derive the appropriate (generalized) momentum such that the associated volume is preserved by the flow. The volume is said to be computed by the Liouville measure.

In a Hamiltonian system, not all possible configurations of position and momentum can be reached from an initial condition. Because of energy conservation, only the states with the same energy as the initial condition are accessible. The states with the same energy form an energy shell Ω, a sub-manifold of the phase space. The volume of the energy shell, computed using the Liouville measure, is preserved under evolution.

For systems where the volume is preserved by the flow, Poincaré discovered the recurrence theorem: Assume the phase space has a finite Liouville volume and let "F" be a phase space volume-preserving map and "A" a subset of the phase space. Then almost every point of "A" returns to "A" infinitely often. The Poincaré recurrence theorem was used by Zermelo to object to Boltzmann's derivation of the increase in entropy in a dynamical system of colliding atoms.

One of the questions raised by Boltzmann's work was the possible equality between time averages and space averages, what he called the ergodic hypothesis. The hypothesis states that the length of time a typical trajectory spends in a region "A" is vol("A")/vol(Ω).

The ergodic hypothesis turned out not to be the essential property needed for the development of statistical mechanics and a series of other ergodic-like properties were introduced to capture the relevant aspects of physical systems. Koopman approached the study of ergodic systems by the use of functional analysis. An observable "a" is a function that to each point of the phase space associates a number (say instantaneous pressure, or average height). The value of an observable can be computed at another time by using the evolution function φ. This introduces an operator "U", the transfer operator,

By studying the spectral properties of the linear operator "U" it becomes possible to classify the ergodic properties of Φ. In using the Koopman approach of considering the action of the flow on an observable function, the finite-dimensional nonlinear problem involving Φ gets mapped into an infinite-dimensional linear problem involving "U".

The Liouville measure restricted to the energy surface Ω is the basis for the averages computed in equilibrium statistical mechanics. An average in time along a trajectory is equivalent to an average in space computed with the Boltzmann factor exp(−β"H"). This idea has been generalized by Sinai, Bowen, and Ruelle (SRB) to a larger class of dynamical systems that includes dissipative systems. SRB measures replace the Boltzmann factor and they are defined on attractors of chaotic systems.

Simple nonlinear dynamical systems and even piecewise linear systems can exhibit a completely unpredictable behavior, which might seem to be random, despite the fact that they are fundamentally deterministic. This seemingly unpredictable behavior has been called "chaos". Hyperbolic systems are precisely defined dynamical systems that exhibit the properties ascribed to chaotic systems. In hyperbolic systems the tangent space perpendicular to a trajectory can be well separated into two parts: one with the points that converge towards the orbit (the "stable manifold") and another of the points that diverge from the orbit (the "unstable manifold").

This branch of mathematics deals with the long-term qualitative behavior of dynamical systems. Here, the focus is not on finding precise solutions to the equations defining the dynamical system (which is often hopeless), but rather to answer questions like "Will the system settle down to a steady state in the long term, and if so, what are the possible attractors?" or "Does the long-term behavior of the system depend on its initial condition?"

Note that the chaotic behavior of complex systems is not the issue. Meteorology has been known for years to involve complex—even chaotic—behavior. Chaos theory has been so surprising because chaos can be found within almost trivial systems. The logistic map is only a second-degree polynomial; the horseshoe map is piecewise linear.

A dynamical system is the tuple formula_10, with formula_11 a manifold (locally a Banach space or Euclidean space), formula_12 the domain for time (non-negative reals, the integers, ...) and "f" an evolution rule "t" → "f" (with formula_13) such that "f" is a diffeomorphism of the manifold to itself. So, f is a mapping of the time-domain formula_14 into the space of diffeomorphisms of the manifold to itself. In other terms, "f"("t") is a diffeomorphism, for every time "t" in the domain formula_14 .

A dynamical system may be defined formally, as a measure-preserving transformation of a sigma-algebra, the quadruplet ("X", Σ, μ, τ). Here, "X" is a set, and Σ is a sigma-algebra on "X", so that the pair ("X", Σ) is a measurable space. μ is a finite measure on the sigma-algebra, so that the triplet ("X", Σ, μ) is a probability space. A map τ: "X" → "X" is said to be Σ-measurable if and only if, for every σ ∈ Σ, one has formula_16. A map τ is said to preserve the measure if and only if, for every σ ∈ Σ, one has formula_17. Combining the above, a map τ is said to be a measure-preserving transformation of "X" , if it is a map from "X" to itself, it is Σ-measurable, and is measure-preserving. The quadruple ("X", Σ, μ, τ), for such a τ, is then defined to be a dynamical system.

The map τ embodies the time evolution of the dynamical system. Thus, for discrete dynamical systems the iterates formula_18 for integer "n" are studied. For continuous dynamical systems, the map τ is understood to be a finite time evolution map and the construction is more complicated.

Dynamical systems are defined over a single independent variable, usually thought of as time. A more general class of systems are defined over multiple independent variables and are therefore called multidimensional systems. Such systems are useful for modeling, for example, image processing.

Works providing a broad coverage:

Introductory texts with a unique perspective:

Textbooks

Popularizations:





</doc>
<doc id="9090" url="https://en.wikipedia.org/wiki?curid=9090" title="Dhimmi">
Dhimmi

A ( "", , collectively "/" "the people of the "dhimma"") is a historical term referring to non-Muslims living in an Islamic state with legal protection. The word literally means "protected person". Dhimmis had their rights fully protected in their communities, but as citizens in the Islamic state, had certain restrictions, and it was obligatory for them to pay the jizya tax, which complemented the zakat, or alms, paid by the Muslim subjects. Dhimmis were exempt from certain duties assigned specifically to Muslims, and did not enjoy certain political rights reserved for Muslims, but were otherwise equal under the laws of property, contract, and obligation.

Under sharia, the dhimmi communities were usually subjected to their own special laws, and exempt from some laws applicable to the Muslim community. For example, the Jewish community in Medina was allowed to have its own Halakhic courts, and the Ottoman millet system allowed its various dhimmi communities to rule themselves under separate legal courts. These courts did not cover cases that involved religious groups outside of their own community, or capital offences. Dhimmi communities were also allowed to engage in certain practices that were usually forbidden for the Muslim community, such as the consumption of alcohol and pork.

Historically, dhimmi status was originally applied to Jews, Christians, and Sabians. This status later also came to be applied to Zoroastrians, Hindus, Jains and Buddhists.

Moderate Muslims generally reject the dhimma system as inappropriate for the age of nation-states and democracies. There is a range of opinions among 20th century and contemporary theologians about whether the notion of dhimma is appropriate for modern times, and, if so, what form it should take in an Islamic state.

Based on Quranic verses and Islamic traditions, classical sharia distinguishes between Muslims, followers of other Abrahamic religions, and pagans or people belonging to other polytheistic religions. As monotheists, Jews and Christians have traditionally been considered "People of the Book," and afforded a special status known as "dhimmi" derived from a theoretical contract—"dhimma" or "residence in return for taxes". In Yemenite Jewish sources, a treaty was drafted between Muhammad and his Jewish subjects, known as "kitāb ḏimmat al-nabi", written in the 17th year of the Hijra (638 CE), and which gave express liberty unto Jews living in Arabia to observe the Sabbath and to grow-out their side-locks, but were required to pay the jizya (poll-tax) annually for their protection by their patrons. There are parallels for this in Roman and Jewish law. Muslim governments in the Indus basin readily extended the dhimmi status to the Hindus and Buddhists of India. Eventually, the largest school of Islamic scholarship applied this term to all non-Muslims living in Islamic lands outside the sacred area surrounding Mecca, Saudi Arabia.

Classical sharia incorporated the religious laws and courts of Christians, Jews and Hindus, as seen in the early caliphate, Al-Andalus, Indian subcontinent, and the Ottoman Millet system. Quoting the Qur'anic statement, "Let Christians judge according to what We have revealed in the Gospel", Muhammad Hamidullah writes that Islam has decentralized and "communalized" law and justice. In medieval Islamic societies, the "qadi" (Islamic judge) usually could not interfere in the matters of non-Muslims unless the parties voluntarily chose to be judged according to Islamic law, thus the dhimmi communities living in Islamic states usually had their own laws independent from the sharia law, as with the Jews who would have their own rabbinical courts. These courts did not cover cases that involved other religious groups, or capital offences or threats to public order. By the 18th century, however, dhimmis frequently attended the Ottoman Muslim courts, where cases were taken against them by Muslims, or they took cases against Muslims or other dhimmis. Oaths sworn by dhimmis in these courts were tailored to their beliefs.

Non-Muslims were allowed to engage in certain practices (such as the consumption of alcohol and pork) that were usually forbidden by Islamic law, in point of fact, any Muslim who pours away their wine or forcibly appropriates it is liable to pay compensation. Zoroastrian "self-marriages", that were considered incestuous under sharia, were also tolerated. Ibn Qayyim Al-Jawziyya (1292–1350) opined that non-Muslims were entitled to such practices since they could not be presented to sharia courts and the religious minorities in question held it permissible. This ruling was based on the precedent that the Islamic prophet Muhammad did not forbid such self-marriages among Zoroastrians despite coming into contact with Zoroastrians and knowing about this practice. Religious minorities were also free to do as they wished in their own homes, provided they did not publicly engage in illicit sexual activity in ways that could threaten public morals.

However, the classical dhimma contract is no longer enforced. Western influence has been instrumental in eliminating the restrictions and protections of the dhimma contract.

According to law professor H. Patrick Glenn of McGill University, "[t]oday it is said that the dhimmi are 'excluded from the specifically Muslim privileges, but on the other hand they are excluded from the specifically Muslim duties' while (and here there are clear parallels with western public and private law treatment of aliens—Fremdenrecht, la condition de estrangers), '[f]or the rest, the Muslim and the dhimmi are equal in practically the whole of the law of property and of contracts and obligations'."

The dhimma contract is an integral part of traditional Islamic sharia. From the 9th century AD, the power to interpret and refine law in traditional Islamic societies was in the hands of the scholars ("ulama"). This separation of powers served to limit the range of actions available to the ruler, who could not easily decree or reinterpret law independently and expect the continued support of the community. Through succeeding centuries and empires, the balance between the ulema and the rulers shifted and reformed, but the balance of power was never decisively changed. At the beginning of the 19th century, the Industrial Revolution and the French Revolution introduced an era of European world hegemony that included the domination of most of the lands of Islam. At the end of the Second World War, the European powers found themselves too weakened to maintain their empires. The wide variety in forms of government, systems of law, attitudes toward modernity and interpretations of sharia are a result of the ensuing drives for independence and modernity in the Muslim world.

Muslim states, sects, schools of thought and individuals differ as to exactly what sharia law entails. In addition, Muslim states today utilize a spectrum of legal systems. Most states have a mixed system that implements certain aspects of sharia while acknowledging the supremacy of a constitution. A few, such as Turkey, have declared themselves secular. Local and customary laws may take precedence in certain matters, as well. Islamic law is therefore polynormative, and despite several cases of regression in recent years, the trend is towards modernization and liberalization. Questions of human rights and the status of minorities cannot be generalized with regards to the Muslim world. They must instead be examined on a case-by-case basis, within specific political and cultural contexts, using perspectives drawn from the historical framework.

The status of the dhimmi "was for long accepted with resignation by the Christians and with gratitude by the Jews" but the rising power of Christendom and the radical ideas of the French Revolution caused a wave of discontent among Christian dhimmis. The continuing and growing pressure from the European powers combined with pressure from Muslim reformers gradually relaxed the inequalities between Muslims and non-Muslims.

On 18 February 1856, the Ottoman Reform Edict of 1856 (Hatt-i Humayan) was issued, building upon the 1839 edict. It came about partly as a result of pressure from and the efforts of the ambassadors of Great Britain, France, and Austria, whose respective countries were needed as allies in the Crimean War. It again proclaimed the principle of equality between Muslims and non-Muslims, and produced many specific reforms to this end. For example, the jizya tax was abolished and non-Muslims were allowed to join the army.


Jews and Christians living under early Muslim rule were considered dhimmis, a status that was later also extended to other non-Muslims like Hindus. They were allowed to "practise their religion, subject to certain conditions, and to enjoy a measure of communal autonomy" and guaranteed their personal safety and security of property, in return for paying tribute and acknowledging Muslim rule. Islamic law and custom prohibited the enslavement of free dhimmis within lands under Islamic rule. Taxation from the perspective of dhimmis who came under the Muslim rule, was "a concrete continuation of the taxes paid to earlier regimes" (but lower under the Muslim rule). They were also exempted from the zakat tax paid by Muslims. The dhimmi communities living in Islamic states had their own laws independent from the Sharia law, such as the Jews who had their own Halakhic courts. The dhimmi communities had their own leaders, courts, personal and religious laws, and "generally speaking, Muslim tolerance of unbelievers was far better than anything available in Christendom, until the rise of secularism in the 17th century". "Muslims guaranteed freedom of worship and livelihood, provided that they remained loyal to the Muslim state and paid a poll tax". "Muslim governments appointed Christian and Jewish professionals to their bureaucracies", and thus, Christians and Jews "contributed to the making of the Islamic civilization".

However, dhimmis faced social and symbolic restrictions, and a pattern of stricter, then more lax, enforcement developed over time. Marshall Hodgson, a historian of Islam, writes that during the era of the High Caliphate (7th–13th Centuries), zealous Shariah-minded Muslims gladly elaborated their code of symbolic restrictions on the dhimmis.

From an Islamic legal perspective, the pledge of protection granted dhimmis the freedom to practice their religion and spared them forced conversions. The dhimmis also served a variety of useful purposes, mostly economic, which was another point of concern to jurists. Religious minorities were free to do whatever they wished in their own homes, but could not "publicly engage in illicit sex in ways that threaten public morals". In some cases, religious practices that Muslims found repugnant were allowed. One example was the Zoroastrian practice of incestuous "self-marriage" where a man could marry his mother, sister or daughter. According to the famous Islamic legal scholar Ibn Qayyim Al-Jawziyya (1292–1350), non-Muslims had the right to engage in such religious practices even if it offended Muslims, under the conditions that such cases not be presented to Islamic Sharia courts and that these religious minorities believed that the practice in question is permissible according to their religion. This ruling was based on the precedent that Muhammad did not forbid such self-marriages among Zoroastrians despite coming in contact with them and having knowledge of their practices.

The Arabs generally established garrisons outside towns in the conquered territories, and had little interaction with the local dhimmi populations for purposes other than the collection of taxes. The conquered Christian, Jewish, Mazdean and Buddhist communities were otherwise left to lead their lives as before.

According to historians Lewis and Stillman, local Christians in Syria, Iraq, and Egypt were non-Chalcedonians and many may have felt better off under early Muslim rule than under that of the Byzantine Orthodox of Constantinople. In 1095, Pope Urban II urged western European Christians to come to the aid of the Christians of Palestine. The subsequent Crusades brought Roman Catholic Christians into contact with Orthodox Christians whose beliefs they discovered to differ from their own perhaps more than they had realized, and whose position under the rule of the Muslim Fatimid Caliphate was less uncomfortable than had been supposed. Consequently, the Eastern Christians provided perhaps less support to the Crusaders than had been expected. When the Arab East came under Ottoman rule in the 16th century, Christian populations and fortunes rebounded significantly. The Ottomans had long experience dealing with Christian and Jewish minorities, and were more tolerant towards religious minorities than the former Muslim rulers, the Mamluks of Egypt.

However, Christians living under Islamic rule have suffered certain legal disadvantages and at times persecution. In the Ottoman Empire, in accordance with the "dhimmi" system implemented in Muslim countries, they, like all other Christians and also Jews, were accorded certain freedoms. The dhimmi system in the Ottoman Empire was largely based upon the Pact of Umar. The client status established the rights of the non-Muslims to property, livelihood and freedom of worship but they were in essence treated as second-class citizens in the empire and referred to in Turkish as "gavours", a pejorative word meaning "infidel" or "unbeliever". The clause of the Pact of Umar which prohibited non-Muslims from building new places of worship was historically imposed on some communities of the Ottoman Empire and ignored in other cases, at discretion of the local authorities. Although there were no laws mandating religious ghettos, this led to non-Muslim communities being clustered around existing houses of worship.

In addition to other legal limitations, Christians were not considered equals to Muslims and several prohibitions were placed on them. Their testimony against Muslims by Christians and Jews was inadmissible in courts of law wherein a Muslim could be punished; this meant that their testimony could only be considered in commercial cases. They were forbidden to carry weapons or ride atop horses and camels. Their houses could not overlook those of Muslims; and their religious practices were severely circumscribed (e.g., the ringing of church bells was strictly forbidden).

Because the early Islamic conquests initially preserved much of the existing administrative machinery and culture, in many territories they amounted to little more than a change of rulers for the subject populations, which "brought peace to peoples demoralized and disaffected by the casualties and heavy taxation that resulted from the years of Byzantine-Persian warfare".

María Rosa Menocal, argues that the Jewish dhimmis living under the caliphate, while allowed fewer rights than Muslims, were still better off than in the Christian parts of Europe. Jews from other parts of Europe made their way to al-Andalus, where in parallel to Christian sects regarded as heretical by Catholic Europe, they were not just tolerated, but where opportunities to practice faith and trade were open without restriction save for the prohibitions on proselytization.

Bernard Lewis states:

Professor of Jewish medieval history at Hebrew University of Jerusalem, Hayim Hillel Ben-Sasson, notes:

According to the French historian Claude Cahen, Islam has "shown more toleration than Europe towards the Jews who remained in Muslim lands."

Comparing the treatment of Jews in the medieval Islamic world and medieval Christian Europe, Mark R. Cohen notes that, in contrast to Jews in Christian Europe, the "Jews in Islam were well integrated into the economic life of the larger society", and that they were allowed to practice their religion more freely than they could do in Christian Europe.

According to the scholar Mordechai Zaken, tribal chieftains (also known as aghas) in tribal Muslim societies such as the Kurdish society in Kurdistan would tax their Jewish subjects. The Jews were in fact civilians protected by their chieftains in and around their communities; in return they paid part of their harvest as dues, and contributed their skills and services to their patron chieftain.

By the 10th century, the Turks of Central Asia had brought Islam to the mountains north of the Indic plains. At the end of the 12th century, the Muslims advanced quickly into the Ganges Plain. In one decade, a Muslim army led by Turkic slaves consolidated resistance around Lahore and brought northern India, as far as Bengal, under Muslim rule. From these Turkic slaves would come sultans, including the founder of the sultanate of Delhi. Muslims and dhimmis alike participated in urbanization and urban prosperity.

By the 15th century, Islamic and Hindu civilization had evolved in a complementary manner, with the Muslims taking the role of a ruling caste in Hindu society. Nevertheless, the Muslims retained their Islamic identities, and were in some ways regarded by Hindus in much the same light as their own lowest castes.

In the 16th century, India came under the influence of the Mughals (Mongols). Babur, a ruler of the Mongol Timuri empire, established a foothold in the north which paved the way for further expansion by his successors. Until it was eclipsed by European hegemony in the 18th century, the Timuri Moghul emperors oversaw a period of coexistence and tolerance between Hindus and Muslims. The emperor Akbar has been described as a universalist. He sought to establish tolerance and equality between all communities and religions, and instituted far reaching social and religious reforms. Not all the Mughal emperors endorsed the ideals espoused by Akbar, indeed Aurangzeb was inclined towards a more fundamentalist approach.

There were a number of restrictions on dhimmis. In a modern sense the dhimmis would be described as second-class citizens.

Although "dhimmis" were allowed to perform their religious rituals, they were obliged to do so in a manner not conspicuous to Muslims. Display of non-Muslim religious symbols, such as crosses or icons, was prohibited on buildings and on clothing (unless mandated as part of distinctive clothing). Loud prayers were forbidden, as were the ringing of church bells and the blowing of the shofar. They were also not allowed to build or repair churches without Muslim consent. Moreover, dhimmis were not allowed to seek converts among Muslims. In the Mamluk Egypt, where non-Mamluk Muslims were not allowed to ride horses and camels, dhimmis were prohibited even from riding donkeys inside cities. Sometimes, Muslim rulers issued regulations requiring dhimmis to attach distinctive signs to their houses.

Most of the restrictions were social and symbolic in nature, and a pattern of stricter, then more lax, enforcement developed over time. The major financial disabilities of the dhimmi were the jizya poll tax and the fact dhimmis and Muslims could not inherit from each other. That would create an incentive to convert if someone from the family had already converted. Ira M. Lapidus states that the "payment of the poll tax seems to have been regular, but other obligations were inconsistently enforced and did not prevent many non-Muslims from being important political, business, and scholarly figures. In the late ninth and early tenth centuries, Jewish bankers and financiers were important at the 'Abbasid court." The jurists and scholars of Islamic sharia law called for humane treatment of the dhimmis.

Payment of the "jizya" obligated Muslim authorities to protect dhimmis in civil and military matters. Sura 9 (At-Tawba), verse 29 stipulates that "jizya" be exacted from non-Muslims as a condition required for jihad to cease. Failure to pay the "jizya" could result in the pledge of protection of a dhimmi's life and property becoming void, with the dhimmi facing the alternatives of conversion, enslavement, death or imprisonment, as advocated by Abu Yusuf, the chief qadi (Islamic judge) of Abbasid caliph Harun al-Rashid who ruled over much of modern-day Iraq.

Lewis states there are varying opinions among scholars as to how much of a burden jizya was. According to Norman Stillman: ""jizya" and "kharaj" were a "crushing burden for the non-Muslim peasantry who eked out a bare living in a subsistence economy." Both agree that ultimately, the additional taxation on non-Muslims was a critical factor that drove many dhimmis to leave their religion and accept Islam. However, in some regions the jizya on populations was significantly lower than the zakat, meaning dhimmi populations maintained an economic advantage. According to Cohen, taxation, from the perspective of dhimmis who came under Muslim rule, was "a concrete continuation of the taxes paid to earlier regimes". Lewis observes that the change from Byzantine to Arab rule was welcomed by many among the dhimmis who found the new yoke far lighter than the old, both in taxation and in other matters, and that some, even among the Christians of Syria and Egypt, preferred the rule of Islam to that of Byzantines. Montgomery Watt states, "the Christians were probably better off as dhimmis under Muslim-Arab rulers than they had been under the Byzantine Greeks." In some places, for example Egypt, the jizya was a tax incentive for Christians to convert to Islam.

The importance of dhimmis as a source of revenue for the Rashidun Caliphate is illustrated in a letter ascribed to Umar I and cited by Abu Yusuf: "if we take dhimmis and share them out, what will be left for the Muslims who come after us? By God, Muslims would not find a man to talk to and profit from his labors."

Islamic jurists required adult, free, healthy males among the dhimma community to pay the jizya, while exempting women, children, the elderly, slaves, those affected by mental or physical handicaps, and travelers who did not settle in Muslim lands.

The early Islamic scholars took a relatively humane and practical attitude towards the collection of "jizya", compared to the 11th century commentators writing when Islam was under threat both at home and abroad.

The jurist Abu Yusuf, the chief judge of the caliph Harun al-Rashid, rules as follows regarding the manner of collecting the jizya 

In the border provinces, dhimmis were sometimes recruited for military operations. In such cases, they were exempted from jizya for the year of service.

Religious pluralism existed in medieval Islamic law and ethics. The religious laws and courts of other religions, including Christianity, Judaism and Hinduism, were usually accommodated within the Islamic legal framework, as exemplified in the Caliphate, Al-Andalus, Ottoman Empire and Indian subcontinent. In medieval Islamic societies, the qadi (Islamic judge) usually could not interfere in the matters of non-Muslims unless the parties voluntarily chose to be judged according to Islamic law. The dhimmi communities living in Islamic states usually had their own laws independent from the Sharia law, such as the Jews who had their own Halakha courts.

Dhimmis were allowed to operate their own courts following their own legal systems. However, dhimmis frequently attended the Muslim courts in order to record property and business transactions within their own communities. Cases were taken out against Muslims, against other dhimmis and even against members of the dhimmi's own family. Dhimmis often took cases relating to marriage, divorce or inheritance to the Muslim courts so these cases would be decided under sharia law. Oaths sworn by dhimmis in the Muslim courts were sometimes the same as the oaths taken by Muslims, sometimes tailored to the dhimmis' beliefs.

Muslim men could generally marry dhimmi women who are considered People of the Book, however Islamic jurists rejected the possibility any non-Muslim man might marry a Muslim woman. Bernard Lewis notes that "similar position existed under the laws of Byzantine Empire, according to which a Christian could marry a Jewish woman, but a Jew could not marry a Christian woman under pain of death".

Lewis states

A hadith by Muhammad, "Whoever killed a "Mu'ahid" (a person who is granted the pledge of protection by the Muslims) shall not smell the fragrance of Paradise though its fragrance can be smelt at a distance of forty years (of traveling).", is considered to be a foundation for the protection of the People of the Book in Muslim ruled countries. Anwar Shah Kashmiri writes in his commentary on Sahih al-Bukhari "Fayd al-Bari" on this hadith: "You know the gravity of sin for killing a Muslim, for its odiousness has reached the point of disbelief, and it necessitates that [the killer abides in Hell] forever. As for killing a non-Muslim citizen [mu'ahid], it is similarly no small matter, for the one who does it will not smell the fragrance of Paradise."

A similar hadith in regard to the status of the dhimmis: "Whoever wrongs one with whom a compact (treaty) has been made "[i.e., a dhimmi]" and lays on him a burden beyond his strength, I will be his accuser."

A precedent for the dhimma contract was established with the agreement between Muhammad and the Jews after the Battle of Khaybar, an oasis near Medina. Khaybar was the first territory attacked and conquered by Muslims. When the Jews of Khaybar surrendered to Muhammad after a siege, Muhammad allowed them to remain in Khaybar in return for handing over to the Muslims one half their annual produce.

After Mecca was brought under Islamic rule, deputations from tribes across Arabia came to make terms with Muhammad and the Muslims. The Constitution of Medina, a formal agreement between Muhammad and all the significant tribes and families of Medina (including Muslims, Jews and pagans), declared that non-Muslims in the Ummah had the following rights:

The Pact of Umar, traditionally believed to be between caliph Umar and the conquered Jerusalem Christians in the seventh century, was another source of regulations pertaining to dhimmis. However, Western orientalists doubt the authenticity of the pact, arguing it is usually the victors and not the vanquished who impose rather than propose, the terms of peace, and that it is highly unlikely that the people who spoke no Arabic and knew nothing of Islam could draft such a document. Academic historians believe the Pact of Umar in the form it is known today was a product of later jurists who attributed it to Umar in order to lend greater authority to their own opinions. The similarities between the Pact of Umar and the Theodosian and Justinian Codes of the Eastern Roman Empire suggest that perhaps much of the Pact of Umar was borrowed from these earlier codes by later Islamic jurists. At least some of the clauses of the pact mirror the measures first introduced by the Umayyad caliph Umar II or by the early Abbasid caliphs.

During the Middle Ages, local associations known as "futuwwa" clubs developed across the Islamic lands. There were usually several futuwwah in each town. These clubs catered to varying interests, primarily sports, and might involve distinctive manners of dress and custom. They were known for their hospitality, idealism and loyalty to the group. They often had a militaristic aspect, purportedly for the mutual protection of the membership. These clubs commonly crossed social strata, including among their membership local notables, dhimmi and slaves – to the exclusion of those associated with the local ruler, or amir.

Muslims and Jews were sometimes partners in trade, with the Muslim taking days off on Fridays and Jews taking off on Saturdays.

Andrew Wheatcroft describes how some social customs such as different conceptions of dirt and cleanliness made it difficult for the religious communities to live close to each other, either under Muslim or under Christian rule.

The dhimma and the jizya poll tax are no longer imposed in Muslim majority countries. In the 21st century, jizya is widely regarded as being at odds with contemporary secular conceptions of citizen's civil rights and equality before the law, although there have been occasional reports of religious minorities in conflict zones and areas subject to political instability being forced to pay jizya.

In 2009 it was claimed that a group of militants that referred to themselves as the Taliban imposed the "jizya" on Pakistan's minority Sikh community after occupying some of their homes and kidnapping a Sikh leader.

As late as 2013, in Egypt "jizya" was reportedly being imposed by the Muslim Brotherhood on 15,000 Christian Copts of Dalga village.

In February 2014, the Islamic State of Iraq and the Levant (ISIL) announced that it intended to extract jizya from Christians in the city of Raqqa, Syria, which it controls. Christians who refused to accept the dhimma contract and pay the tax would have to either convert to Islam or die. Wealthy Christians would have to pay half an ounce of gold, the equivalent of USD 664 twice a year; middle-class Christians would have to pay half that amount and poorer ones would be charged one-fourth that amount. In June, the Institute for the Study of War reported that ISIL claims to have collected jizya and fay. On 18 July 2014 the ISIL ordered the Christians in Mosul to accept the dhimma contract and pay the Jizya or convert to Islam. If they refused to accept either of the options they would be killed.





</doc>
<doc id="9091" url="https://en.wikipedia.org/wiki?curid=9091" title="Doctor V64">
Doctor V64

The Doctor V64 (also referred to simply as the V64) is a development and backup device made by Bung Enterprises Ltd that is used in conjunction with the Nintendo 64. The Doctor V64 also had the ability to play Video CDs, audio CDs and had an option for applying stereo 3D effects to the audio.

The Doctor V64 came out in 1996 and was priced around $450 USD. Many third party developers used the V64 instead of the PC64 Development Kit sold by Nintendo; the V64 was considered an attractive, low-cost alternative to the expensive N64 development machine, which was manufactured by Silicon Graphics at the time. The CPU of the V64 is a 6502 chip (the CPU from the Nintendo Entertainment System); the operating system is stored in the BIOS chip. It is likely that Bung reused most of the design of their earlier NES clones in the Doctor V64.

The Doctor V64 unit contains a CD-ROM drive which sits underneath the Nintendo 64 and plugs into the expansion slot on the underside of the Nintendo 64. The expansion slot is essentially a mirror image of the cartridge slot on the top of the unit, with the same electrical connections, thus the Nintendo 64 reads data from the Doctor V64 in the same manner as it would from a cartridge plugged into the normal slot.

Using the Doctor V64 involved the solution of two problems: how to boot a game and how to save. In order to get around Nintendo's lockout chip, when using the Doctor V64 a game cartridge is plugged into the Nintendo 64 through an adaptor which connects only the lockout chip. The game cart used for operation had to contain the same lockout chip used by the game backup. The second problem concerned saving progress. Most N64 games saved to the cart itself instead of external memory cards. If the player wanted to keep their progress then the cartridge used had to have the same type of non-volatile memory hardware.

Following the Doctor V64's success, Bung released the Doctor V64 Jr. in December 1998. This was a cost-efficient condensed version of the original V64. The V64jr had no CD drive and plugged into the normal cartridge slot on the top of the Nintendo 64. Data was loaded into the V64jr's battery-backed RAM from a PC via a parallel port connection. The V64Jr had up to 512 megabits (64 MB) of memory storage. At the time this was done to provide for future Nintendo 64 carts that employed larger memory storage. The prohibitive high costs associated with ordering large storage carts kept this occurrence at a minimum. Only a handful of 512 megabit games were released for the Nintendo 64 system.

During the N64's lifetime, Nintendo made one model revision which made the serial port area smaller. This slight change in the N64's plastic casing made the connection to the Doctor V64 difficult to achieve without user modification. This revision may have been a direct reaction of Nintendo to discourage the use of V64 devices. It also explains why Bung decided to drop the use of this port in the later V64Jr models.

The Doctor V64 could be used to read the data from a game cartridge and transfer the data to a PC via the parallel port. This allowed developers and homebrew programmers to upload their game images to the Doctor V64 without having to create a CD backup each time. It also allowed users to upload game images taken from the Internet.

In 1998 and 1999, there was a homebrew competition known as "Presence of Mind" (POM), an N64 demo competition led by dextrose.com. The contest consisted in submitting a user developed N64 program, game or utility. Bung Enterprises promoted the event and supplied prizes (usually Doctor V64 related accessories). Though a contest was planned for 2000 the interest in the N64 was already fading and so did the event. POM contest demo entries can still be found on the Internet.

The Doctor V64 unit was the first commercially available backup device for the Nintendo 64 unit. Though the unit was sold as a development machine it could be modified to enable the creation and use of commercial game backups. Unlike official development units, the purchase of V64s was not restricted to software companies only. For this reason the unit became a popular choice among those looking to proliferate unlicensed copies of games.

Original Doctor V64 units sold by Bung did not allow the playing of backups. A person would have to modify the unit by themselves in order to make it backup friendly. This usually required a user to download and install a modified Doctor V64 BIOS. Additionally the cartridge adapter had to be opened and soldered in order to allow the operational procedure described early on this article. Though Bung never sold backup enabled V64s many re-sellers would modify the units themselves.

Nintendo made many legal efforts worldwide in order to stop the sale of Doctor V64 units. They sued Bung directly as well as specific store retailers in Europe and North America for copyright infringement. Eventually Nintendo managed to have the courts prohibit the sale of Doctor V64 units in the United States.

As with many backup devices of its time, the Doctor V64 implemented text based menu driven screens. The menus were spartan and purely functional in nature. Utilizing the buttons on the V64 unit a user would navigate the menus and issue commands. It was mainly designed for game developers even though it is possible to backup cartridges with it (through the use of an unofficial V64 bios). Some of the menu items related to game backups were removed from the V64's BIOS near the end of its life due to pressure from Nintendo. These items are only available by obtaining a patched V64 BIOS.




</doc>
<doc id="9093" url="https://en.wikipedia.org/wiki?curid=9093" title="De Havilland Mosquito">
De Havilland Mosquito

The de Havilland DH.98 Mosquito is a British twin-engine shoulder-winged multi-role combat aircraft. The crew of two, pilot and navigator, sat side by side. It served during and after the Second World War. It was one of few operational front-line aircraft of the era whose frame was constructed almost entirely of wood and was nicknamed "The Wooden Wonder". The Mosquito was also known affectionately as the "Mossie" to its crews. Originally conceived as an unarmed fast bomber, the Mosquito was adapted to roles including low to medium-altitude daytime tactical bomber, high-altitude night bomber, pathfinder, day or night fighter, fighter-bomber, intruder, maritime strike aircraft, and fast photo-reconnaissance aircraft. It was also used by the British Overseas Airways Corporation (BOAC) as a fast transport to carry small high-value cargoes to, and from, neutral countries, through enemy-controlled airspace. A single passenger could ride in the aircraft's bomb bay when it was adapted for the purpose.

When Mosquito production began in 1941 it was one of the fastest operational aircraft in the world. Entering service in late 1941, the first Mosquito variant was an unarmed high-speed, high-altitude photo-reconnaissance aircraft. Subsequent versions continued in this role throughout the war. The first Mk. B.IV bomber, serial no. W4064, entered service with No. 105 Squadron on 15 November 1941. From mid-1942 to mid-1943, Mosquito bombers flew high-speed, medium or low-altitude daylight missions against factories, railways and other pinpoint targets in Germany and German-occupied Europe. From June 1943, Mosquito bombers were formed into the Light Night Strike Force and used as pathfinders for RAF Bomber Command heavy-bomber raids. They were also used as "nuisance" bombers, often dropping Blockbuster bombs – "cookies" – in high-altitude, high-speed raids that German night fighters were almost powerless to intercept.

As a night fighter from mid-1942, the Mosquito intercepted "Luftwaffe" raids on Britain, notably those of Operation Steinbock in 1944. Starting in July 1942, Mosquito night-fighter units raided "Luftwaffe" airfields. As part of 100 Group, it was flown as a night fighter and as an intruder supporting Bomber Command heavy bombers that reduced losses during 1944 and 1945.

The Mosquito fighter-bomber served as a strike aircraft in the Second Tactical Air Force (2TAF) from its inception on 1 Jun 1943. The main objective was to prepare for the invasion of occupied Europe a year later. In Operation Overlord three Mosquito FBVI Wings flew close air support for the Allied armies in co-operation with other RAF units equipped with the North American B-25 Mitchell medium bomber. In the months between the foundation of 2TAF and its duties from D day onwards, vital training was interspersed with attacks on V-1 flying bomb launch sites. 

The Mosquito FBVI was often flown in special raids. One of the best-known was Operation Jericho – an attack on Amiens Prison in early 1944. Other 2TAF operations included precision attacks against military intelligence, security and police facilities (such as Gestapo headquarters).

From 1943, Mosquitos with RAF Coastal Command attacked "Kriegsmarine" U-boats and intercepted transport ship concentrations. After Operation Overlord, the U-boat threat in the Western Approaches decreased fairly quickly, but correspondingly the Norwegian and Danish waters posed greater dangers. Hence the RAF Coastal Command Mosquitos were moved to Scotland to counter this threat. The Strike Wing at Banff stood up in September 1944 and comprised Mosquito aircraft of No’s 143, 144, 235 and 248 Squadrons Royal Air Force and No.333 Squadron Royal Norwegian Air Force.
The Mosquito flew with the Royal Air Force (RAF) and other air forces in the European, Mediterranean and Italian theatres. The Mosquito was also operated by the RAF in the South East Asian theatre and by the Royal Australian Air Force (RAAF) based in the Halmaheras and Borneo during the Pacific War. During the 1950s, the RAF replaced the Mosquito with the jet-powered English Electric Canberra.

By the early-mid-1930s, de Havilland had a reputation for innovative high-speed aircraft with the DH.88 Comet racer. The later DH.91 Albatross airliner pioneered the composite wood construction used for the Mosquito. The 22-passenger Albatross could cruise at at , faster than the Handley Page H.P.42 and other biplanes it was replacing. The wooden monocoque construction not only saved weight and compensated for the low power of the de Havilland Gipsy Twelve engines used by this aircraft, but simplified production and reduced construction time.

On 8 September 1936, the British Air Ministry issued Specification P.13/36, which called for a twin-engine medium bomber capable of carrying a bomb load of for with a maximum speed of at ; a maximum bomb load of that could be carried over shorter ranges was also specified. Aviation firms entered heavy designs with new high-powered engines and multiple defensive turrets, leading to the production of the Avro Manchester and Handley Page Halifax.

In May 1937, as a comparison to P.13/36, George Volkert, the chief designer of Handley Page, put forward the concept of a fast unarmed bomber. In 20 pages, Volkert planned an aerodynamically clean medium bomber to carry of bombs at a cruising speed of . There was support in the RAF and Air Ministry; Captain R N Liptrot, Research Director Aircraft 3 (RDA3), appraised Volkert's design, calculating that its top speed would exceed that of the new Supermarine Spitfire. There were, however, counter-arguments that, although such a design had merit, it would not necessarily be faster than enemy fighters for long. The ministry was also considering using non-strategic materials for aircraft production, which, in 1938, had led to specification B.9/38 and the Armstrong Whitworth Albemarle medium bomber, largely constructed from spruce and plywood attached to a steel-tube frame. The idea of a small, fast bomber gained support at a much earlier stage than is sometimes acknowledged though it was likely that the Air Ministry envisaged it using light alloy components.

 Based on his experience with the Albatross, Geoffrey de Havilland believed that a bomber with a good aerodynamic design and smooth, minimal skin area, would exceed the P.13/36 specification. Furthermore, adapting the Albatross principles could save time. In April 1938, performance estimates were produced for a twin Rolls-Royce Merlin-powered DH.91, with the Bristol Hercules (radial engine) and Napier Sabre (H-engine) as alternatives. On 7 July 1938, Geoffrey de Havilland wrote to Air Marshal Wilfrid Freeman, the Air Council's member for Research and Development, discussing the specification and arguing that in war there would be shortages of aluminium and steel but supplies of wood-based products were "adequate." Although inferior in tension, the strength to weight ratio of wood is equal to or better than light alloys or steel, hence this approach was feasible.

A follow-up letter to Freeman on 27 July said that the P.13/36 specification could not be met by a twin Merlin-powered aircraft and either the top speed or load capacity would be compromised, depending on which was paramount. For example, a larger, slower, turret armed aircraft would have a range of carrying a 4,000 lb bomb load, with a maximum of at , and a cruising speed of at . De Havilland believed that a compromise, including eliminating surplus equipment, would improve matters. On 4 October 1938, de Havilland projected the performance of another design based on the Albatross, powered by two Merlin Xs, with a three-man crew and six or eight forward-firing guns, plus one or two manually operated guns and a tail turret. Based on a total loaded weight of it would have a top speed of and cruising speed of at .

Still believing this could be improved, and after examining more concepts based on the Albatross and the new all-metal DH.95 Flamingo, de Havilland settled on designing a new aircraft that would be aerodynamically clean, wooden and powered by the Merlin, which offered substantial future development. The new design would be faster than foreseeable enemy fighter aircraft, and could dispense with a defensive armament, which would slow it and make interception or losses to anti-aircraft guns more likely. Instead, high speed and good manoeuvrability would make it easier to evade fighters and ground fire. The lack of turrets simplified production and reduced production time, with a delivery rate far in advance of competing designs. Without armament, the crew could be reduced to a pilot and navigator. Whereas contemporary RAF design philosophy favoured well-armed heavy bombers, this mode of design was more akin to the German philosophy of the "schnellbomber.". At a meeting in early October 1938 with Geoffrey de Havilland and Charles Walker (de Havilland's chief engineer), the Air Ministry showed little interest, and instead asked de Havilland to build wings for other bombers as a sub-contractor.

By September 1939 de Havilland had produced preliminary estimates for single- and twin-engined variations of light-bomber designs using different engines, speculating on the effects of defensive armament on their designs. One design, completed on 6 September, was for an aircraft powered by a single Napier Sabre, with a wingspan of and capable of carrying a bomb load . On 20 September, in another letter to Wilfred Freeman, de Havilland wrote "...we believe that we could produce a twin-engine bomber which would have a performance so outstanding that little defensive equipment would be needed." By 4 October work had progressed to a twin-engine light bomber with a wingspan of , and powered by Merlin or Griffon engines, the Merlin favoured because of availability. On 7 October 1939, a month into the war, the nucleus of a design team under Eric Bishop moved to the security and secrecy of Salisbury Hall to work on what was later known as the DH.98. For more versatility, Bishop made provision for four 20 mm cannon in the forward half of the bomb bay, under the cockpit, firing via blast tubes and troughs under the fuselage.

The DH.98 was too radical for the Ministry, which wanted a heavily armed, multi-role aircraft, combining medium bomber, reconnaissance and general purpose roles, as well as capable of carrying torpedoes. With outbreak of war, the Ministry became more receptive, but still sceptical about an unarmed bomber. It thought the Germans would produce fighters faster than expected. It suggested two forward and two rear-firing machine guns for defence. The Ministry also opposed a two-man bomber, wanting at least a third crewman to reduce the work of the others on long flights. The Air Council added further requirements, such as remotely controlled guns, a top speed of at 15,000 ft on two-thirds engine power, and a range of with a 4,000 lb bomb load. To appease the Ministry, de Havilland built mock-ups with a gun turret just aft of the cockpit but, apart from this compromise, de Havilland made no changes.

On 12 November, at a meeting considering fast bomber ideas put forward by de Havilland, Blackburn, and Bristol, Air Marshal Freeman directed de Havilland to produce a fast aircraft, powered initially by Merlin engines, with options of using progressively more powerful engines, including the Rolls-Royce Griffon and the Napier Sabre. Although estimates were presented for a slightly larger Griffon-powered aircraft, armed with a four-gun tail turret, Freeman got the requirement for defensive weapons dropped, and a draft requirement was raised calling for a high-speed light reconnaissance bomber capable of at 18,000 ft.

On 12 December, the Vice-Chief of the Air Staff, Director General of Research and Development, and the Air Officer Commanding-in-Chief (AOC-in-C) of RAF Bomber Command met to finalise the design and decide how to fit it in the RAF's aims. The AOC-in-C would not accept an unarmed bomber, but insisted on its suitability for reconnaissance missions with F8 or F24 cameras. After company representatives, the Ministry and the RAF's operational commands examined a full-scale mock-up at Hatfield on 29 December 1939, the project received backing. This was confirmed on 1 January 1940, when Freeman chaired a meeting with Geoffrey de Havilland, John Buchanan, (Deputy of Aircraft Production) and John Connolly (Buchanan's chief of staff). de Havilland claimed the DH.98 was the "fastest bomber in the world...it must be useful". Freeman supported it for RAF service, ordering a single prototype for an unarmed bomber to specification B.1/40/dh, which called for a light bomber/reconnaissance aircraft powered by two Rolls-Royce RM3SM (an early designation for the Merlin 21) with ducted radiators, capable of carrying a bomb load. The aircraft was to have a speed of at and a cruising speed of at with a range of at on full tanks. Maximum service ceiling was to be .

On 1 March 1940, Air Marshal Roderic Hill issued a contract under Specification B.1/40, for 50 bomber-reconnaissance variants of the DH.98: this contract included the prototype, which was given the factory serial "E0234". In May 1940, specification F.21/40 was issued, calling for a long-range fighter armed with four 20 mm cannon and four .303 machine guns in the nose, after which de Havilland was authorised to build a prototype of a fighter version of the DH.98. It was decided after debate that this prototype, given the military serial number "W4052", would carry Airborne Interception (AI) Mk IV equipment as a day and night fighter. By June 1940, the DH.98 had been named "Mosquito". Having the fighter variant kept the Mosquito project alive as there was still criticism within the government and Air Ministry of the usefulness of an unarmed bomber, even after the prototype had shown its capabilities.

With design of the DH.98 started, mock-ups were built, the most detailed at Salisbury Hall, where "E0234" was later constructed. Initially, the concept was for the crew to be enclosed in the fuselage behind a transparent nose (similar to the Bristol Blenheim or Heinkel He 111H), but this was quickly altered to a more solid nose with a conventional canopy.

The construction of the prototype began in March 1940, but work was cancelled again after the Battle of Dunkirk, when Lord Beaverbrook, as Minister of Aircraft Production, decided there was no production capacity for aircraft like the DH.98, which was not expected to be in service until early 1942.

Beaverbrook told Air Vice-Marshal Freeman that work on the project should stop, but he did not issue a specific instruction, and Freeman ignored the request. In June 1940, however, Lord Beaverbrook and the Air Staff ordered that production should focus on five existing types, namely the Supermarine Spitfire, Hawker Hurricane fighter, Vickers Wellington, Armstrong-Whitworth Whitley, and Bristol Blenheim bombers. Work on the DH.98 prototype stopped. Apparently the project shut down when the design team were denied materials for the prototype.

The Mosquito was only reinstated as a priority in July 1940, after de Havilland's general manager L.C.L Murray, promised Lord Beaverbrook 50 Mosquitos by December 1941. This was only after Beaverbrook was satisfied that Mosquito production would not hinder de Havilland's primary work of producing Tiger Moth and Airspeed Oxford trainers, repairing Hurricanes and manufacturing Merlin engines under licence. In promising Beaverbrook such a number by the end of 1941, de Havilland was taking a gamble, because it was unlikely that they could be built in such a limited time. As it transpired, only 20 aircraft were built in 1941, but the other 30 were delivered by mid-March 1942. During the Battle of Britain, interruptions to production due to air raid warnings caused nearly a third of de Havilland's factory time to be lost. Nevertheless, work on the prototype went ahead quickly at Salisbury Hall since "E0234" was completed by November 1940.

In the aftermath of the Battle of Britain, the original order was changed to 20 bomber variants and 30 fighters. It was still uncertain whether the fighter version should have dual or single controls, or should carry a turret, so three prototypes were built: "W4052", "W4053" and "W4073". The second and third, both turret armed, were later disarmed, to become the prototypes for the T.III trainer. This caused some delays since half-built wing components had to be strengthened for the required higher combat loading. The nose sections also had to be changed from a design with a clear perspex bomb-aimer's position, to one with a solid nose housing four .303 machine guns and their ammunition.

On 3 November 1940, the aircraft, painted in "prototype yellow" and still coded "E0234", was disassembled and transported by road to Hatfield, and placed in a small blast-proof assembly building. Two Merlin 21 two-speed single-stage supercharged engines were installed, driving three-bladed de Havilland Hydromatic constant-speed, controllable-pitch propellers. Successful engine runs were made on 19 November. On 24 November, successful taxiing trials were carried out by Geoffrey de Havilland Jr., who was the company's chief test pilot. Then on 25 November, the prototype made its first flight, piloted by Geoffrey de Havilland Jr., accompanied by John E. Walker, the chief engine installation designer. 

For this maiden flight, "E0234", weighing , took off from the grass airstrip at the Hatfield site. The takeoff was reported as "straightforward and easy" and the undercarriage was not retracted until a considerable height was attained. The aircraft reached , with the only problem being the undercarriage doors – which were operated by bungee cords attached to the main undercarriage legs – that remained open by some at that speed. This problem persisted for some time. The left wing of "E0234" also had a tendency to drag to port slightly, so a rigging adjustment, i.e., a slight change in the angle of the wing, was carried out before further flights.
On 5 December 1940, the prototype, now with its military serial number "W4050", experienced tail buffeting at speeds between and . The pilot noticed this most in the control column, with handling becoming more difficult. During testing on 10 December, wool tufts were attached to suspect areas to investigate the direction of airflow. The conclusion was that the airflow separating from the rear section of the inner engine nacelles was disturbed, leading to a localised stall and the disturbed airflow was striking the tailplane, causing buffeting. In an attempt to smooth the air flow and deflect it from forcefully striking the tailplane, non-retractable slots fitted to the inner engine nacelles and to the leading edge of the tailplane were experimented with. These slots, and wing root fairings fitted to the forward fuselage and leading edge of the radiator intakes, stopped some of the vibration experienced but did not cure the tailplane buffeting.

In February 1941, buffeting was eliminated by incorporating triangular fillets on the trailing edge of the wings and lengthening the nacelles, the trailing edge of which curved up to fair into the fillet some behind the wing's trailing edge: this meant the flaps had to be divided into inboard and outboard sections. With the buffeting problems largely resolved, John Cunningham flew "W4050" on 9 February 1941. He was greatly impressed by the "lightness of the controls and generally pleasant handling characteristics". Cunningham concluded that when the type was fitted with AI equipment, it might completely replace the Bristol Beaufighter in the night fighter role.

During its trials on 16 January 1941, "W4050" outpaced a Spitfire at . The original estimates were that as the Mosquito prototype had twice the surface area and over twice the weight of the Spitfire Mk II, but also with twice its power, the Mosquito would end up being faster. Over the next few months, "W4050" surpassed this estimate, easily beating the Spitfire Mk II in testing at RAF Boscombe Down in February 1941, reaching a top speed of at altitude, compared to a top speed of at for the Spitfire.

On 19 February, official trials began at the Aeroplane and Armament Experimental Establishment (A&AEE) based at Boscombe Down, although de Havilland's representative was surprised by a delay in starting the tests. On 24 February, as "W4050" taxied across the rough airfield, the tailwheel jammed leading to the fuselage fracturing. Repairs were made by early March, using part of the fuselage of the photo-reconnaissance prototype "W4051". In spite of this setback, the "Initial Handling Report 767" issued by the A&AEE stated "The aeroplane is pleasant to fly ... aileron control light and effective ..." The maximum speed reached was at , with an estimated maximum ceiling of and a maximum rate of climb of at .

"W4050" continued to be used for various test programmes, essentially as the experimental "workhorse" for the Mosquito family. In late October 1941, it returned to the factory to be fitted with Merlin 61s, the first production Merlins fitted with a two-speed, two-stage supercharger. The first flight with the new engines was on 20 June 1942. "W4050" recorded a maximum speed of at (fitted with straight-through air intakes with snow guards, engines in F.S. gear) and at without snow guards. In October 1942, in connection with development work on the NF Mk XV, "W4050" was fitted with extended wingtips increasing the span to , first flying in this configuration on 8 December. Finally, fitted with high-altitude rated two-stage, two-speed Merlin 77s, it reached in December 1943.

Soon after these flights, "W4050" was grounded and scheduled to be scrapped, but instead served as an instructional airframe at Hatfield. In September 1958, "W4050" was returned to the Salisbury Hall hangar where it was built, restored to its original configuration, and became one of the primary exhibits of the de Havilland Aircraft Heritage Centre.

"W4051", which was designed from the outset to be the prototype for the photo-reconnaissance versions of the Mosquito, was slated to make its first flight in early 1941. However, the fuselage fracture in "W4050" meant that "W4051's" fuselage was used as a replacement; "W4051" was then rebuilt using a production standard fuselage and first flew on 10 June 1941. This prototype continued to use the short engine nacelles, single-piece trailing edge flaps and the "No. 1" tailplane used by "W4050", but had production standard wings and, thus configured, became the only Mosquito prototype to fly operationally.

Construction of the fighter prototype, "W4052" was also carried out at the secret Salisbury Hall facility. This differed from its bomber brethren in a number of ways: it was powered by Merlin 21s; had an altered canopy structure with a flat bullet-proof windscreen; a solid nose mounted four .303 British Browning machine guns and their ammunition boxes, accessed by a single large, sideways hinged panel.; four 20 mm Hispano Mk II cannon were housed in a compartment under the cockpit floor, with the breeches projecting into the bomb bay; and the automatic bomb bay doors were replaced by manually operated bay doors, which incorporated cartridge ejector chutes.

In accordance with its role as a day and night fighter prototype "W4052" was equipped with AI Mk IV equipment, complete with an "arrowhead"-shaped transmission aerial mounted between the central Brownings, and receiving aerials through each outer wingtip, and it was painted in overall black RDM2a "Special Night" finish. It was also the first prototype constructed with the extended engine nacelles. "W4052" was later tested with other modifications including bomb racks, drop tanks, barrage balloon cable cutters in the leading edge of the wings, Hamilton airscrews and braking propellers, as well as drooping aileron systems that enabled steep approaches, and a larger rudder tab.

The prototype continued to serve as a test machine until it was scrapped on 28 January 1946.

"4055" flew the first operational Mosquito flight on 17 September 1941.

During flight testing, the Mosquito prototypes were modified to test a number of experimental configurations. "W4050" was fitted with a turret behind the cockpit for drag tests, after which the idea was abandoned in July 1941. "W4052" had the first version of the Youngman Frill airbrake fitted to the fighter prototype. The frill was mounted around the fuselage behind the wing and was opened by bellows and venturi effect to provide rapid deceleration during interceptions and was tested between January – August 1942, but was also abandoned when it was discovered that lowering the undercarriage had the same effect with less buffeting.
The Air Ministry authorised mass production plans on 21 June 1941, by which time the Mosquito had become one of the world's fastest operational aircraft. It ordered 19 photo-reconnaissance (PR) models and 176 fighters. A further 50 were unspecified; in July 1941, it was confirmed these would be unarmed fast bombers. By the end of January 1942, contracts had been awarded for 1,378 Mosquitos of all variants, including 20 T.III trainers and 334 FB.VI bombers. Another 400 were to be built by de Havilland Canada.

On 20 April 1941, "W4050" was demonstrated to Lord Beaverbrook, the Minister of Aircraft Production. The Mosquito made a series of flights, including one rolling climb on one engine. Also present were US General Henry H. Arnold and his aide Major Elwood Quesada, who wrote "I ... recall the first time I saw the Mosquito as being impressed by its performance, which we were aware of. We were impressed by the appearance of the airplane that looks fast usually is fast, and the Mosquito was, by the standards of the time, an extremely well streamlined airplane, and it was highly regarded, highly respected."

The trials set up future production plans between Britain, Australia and Canada. Six days later Arnold returned to America with a full set of manufacturer's drawings. As a result of his report five companies (Beech; Curtiss-Wright; Fairchild; Fleetwings; and Hughes) were asked to evaluate the de Havilland data. The report by Beech Aircraft summed up the general view: "It appears as though this airplane has sacrificed serviceability, structural strength, ease of construction and flying characteristics in an attempt to use construction material which is not suitable for the manufacture of efficient airplanes." The Americans did not pursue the proposal for licensed production, the consensus arguing that the Lockheed P-38 Lightning could fulfill the same duties. However, Arnold urged the United States Army Air Forces to evaluate the design even if they would not adopt it. On 12 December 1941, after the attack on Pearl Harbor, the USAAF requested one airframe for this purpose.

While timber construction was considered outmoded by some, de Havilland claimed that their successes with techniques used for the DH 91 Albatross could lead to a fast light bomber using monocoque sandwich shell construction. Arguments in favor of this included speed of prototyping, rapid development, minimisation of jig building time, and employment of a separate category of workforce. At the same time, they had to fight conservative Air Ministry views on defensive armament. Guns and gun turrets would spoil the streamlining, losing speed and manoeuvrability. The ply-balsa-ply monocoque fuselage and one-piece wings with doped fabric covering gave smooth aerodynamic performance and low weight, combined with strength and stiffness. Whilst submitting these arguments, Geoffrey de Havilland funded his private venture until the eleventh hour. It was a success beyond all expectations. The initial bomber and photo reconnaissance versions were extremely fast, whilst the armament of subsequent variants might be regarded as primarily offensive.

The most-produced variant, designated the FB Mk VI (Fighter-bomber Mark 6), was powered by two Merlin Mk 23 or Mk 25 engines driving three-bladed de Havilland hydromatic propellers. The typical fixed armament for an FB Mk VI was four Browning .303 machine guns and four 20 mm Hispano cannon while the offensive load consisted of up to of bombs, or eight RP-3 unguided rockets.

The design was noted for light and effective control surfaces that provided good manoeuvrability but required that the rudder not be used aggressively at high speeds. Poor aileron control at low speeds when landing and taking off was also a problem for inexperienced crews. For flying at low speeds, the flaps had to be set at 15°, speed reduced to and rpm set to 2,650. The speed could be reduced to an acceptable for low speed flying. For cruising, the maximum speed for obtaining maximum range was at weight.

The Mosquito had a low stalling speed of with undercarriage and flaps raised. When both were lowered, the stalling speed decreased to . Stall speed at normal approach angle and conditions was . Warning of the stall was given by buffeting and would occur before stall was reached. The conditions and impact of the stall were not severe. The wing did not drop unless the control column was pulled back. The nose drooped gently and recovery was easy.

Early on in the Mosquito's operational life, the intake shrouds that were to cool the exhausts on production aircraft overheated. Flame dampers prevented exhaust glow on night operations, but they had an effect on performance. Multiple ejector and open-ended exhaust stubs helped solve the problem and were used in the PR.VIII, B.IX and B.XVI variants. This increased speed performance in the B.IX alone by .

The oval-section fuselage was a frameless monocoque shell built in two vertically-separate halves formed over a mahogany or concrete mould. Pressure was applied with band clamps. The shell sandwich skins comprised birch three-ply outers, with cores of Ecuadorean balsa. In many generally smaller but vital areas, such as around apertures and attachment zones, stronger timbers, including aircraft-quality spruce, replaced the balsa core. The main areas of the sandwich skin were only thick. Together with various forms of wood reinforcement, often of laminated construction, the sandwich skin gave great stiffness and torsional resistance. The separate fuselage halves speeded construction, permitting access by personnel working in parallel with others, as the work progressed.

Work on the separate half-fuselages included installation of control mechanisms and cabling. Screwed inserts into the inner skins that would be under stress in service were reinforced using round shear plates made from a fabric-Bakelite composite.
Transverse bulkheads were also compositely built-up with several species of timber, plywood and balsa. Seven vertically-halved bulkheads were installed within each moulded fuselage shell before the main "boxing up" operation. Bulkhead number seven was especially strongly built, since it carried the fitments and transmitted the aerodynamic loadings for the tailplane and rudder. The fuselage had a large ventral section cut-out, strongly reinforced, that allowed the fuselage to be lowered onto the wing centre-section at a later stage of assembly.

For early production aircraft, the structural assembly adhesive was Casein-based. At a later stage, this was replaced by "Aerolite", a synthetic urea-formaldehyde type, which was more durable. To provide for the edge joints for the fuselage halves, zones near the outer edges of the shells had their balsa sandwich cores replaced by much stronger inner laminations of birch plywood. For the bonding together of the two halves ("boxing up"), a longitudinal cut was machined into these edges. The profile of this cut was a form of V-groove. Part of the edge bonding process also included adding further longitudinal plywood lap-strips on the outside of the shells. The half-bulkheads of each shell were bonded to their corresponding pair in a similar way.
Two laminated wooden clamps were used in the after portion of the fuselage to provide supports during this complex gluing work. The resulting large structural components had to be kept completely still and held in the correct environment until the glue cured.

For finishing, a covering of doped Madapolam (a fine plain woven cotton) fabric was stretched tightly over the shell and several coats of red, followed by silver dope were added, followed by the final camouflage paint.

The all-wood wing pairs comprised a single structural unit throughout the wingspan, with no central longitudinal joint. Instead, the spars ran from wingtip to wingtip. There was a single continuous main spar and another continuous rear spar. Because of the combination of dihedral with the forward sweep of the trailing edges of the wings, this rear spar was one of the most complex units to laminate and to finish machining after the bonding and curing. It had to produce the correct 3D tilt in each of two planes. Also it was designed and made to taper from the wing roots towards the wingtips. Both principal spars were of ply box construction, using in general 0.25 in. plywood webs with laminated spruce flanges, plus a number of additional reinforcements and special details.

Spruce and plywood ribs were connected with gusset joints. Some heavy-duty ribs contained pieces of ash and walnut as well as the special five ply that included veneers laid up at 45 degrees. The upper skin construction was in two layers of 0.25 in. five ply birch, separated by Douglas fir stringers running in the span-wise direction.

The wings were covered with Madapolam fabric and doped in a similar manner to the fuselage. The wing was installed into the roots by means of four large attachment points. The engine radiators were fitted in the inner wing, just outboard of the fuselage on either side. These gave less drag. The radiators themselves were split into three sections: an oil cooler section outboard, the middle section forming the coolant radiator and the inboard section serving the cabin heater.

The wing contained metal framed and skinned ailerons, but the flaps were made of wood and were hydraulically controlled. The nacelles were mostly wood, although, for strength, the engine mounts were all metal as were the undercarriage parts. Engine mounts of welded steel tube were added, along with simple landing gear oleos filled with rubber blocks. Wood was used to carry only in-plane loads, with metal fittings used for all triaxially loaded components such as landing gear, engine mounts, control surface mounting brackets, and the wing-to-fuselage junction. The outer leading wing edge had to be brought further forward to accommodate this design. The main tail unit was all wood built. The control surfaces, the rudder and elevator, were aluminium framed and fabric covered. The total weight of metal castings and forgings used in the aircraft was only .

In November 1944, several crashes occurred in the Far East. At first, it was thought these were as a result of wing structure failures. The casein glue, it was said, cracked when exposed to extreme heat and/or monsoon conditions. This caused the upper surfaces to "lift" from the main spar. An investigating team led by Major Hereward de Havilland travelled to India and produced a report in early December 1944 stating that "the accidents were not caused by the deterioration of the glue but by shrinkage of the airframe during the wet monsoon season". However, a later inquiry by Cabot & Myers definitely attributed the accidents to faulty manufacture and this was confirmed by a further investigation team by the Ministry of Aircraft Production at Defford, which found faults in six Mosquito marks (all built at de Havilland's Hatfield and Leavesden plants). The defects were similar, and none of the aircraft had been exposed to monsoon conditions or termite attack.

The investigators concluded that there were construction defects at the two plants. They found that the "...standard of glueing...left much to be desired.” Records at the time showed that accidents caused by "loss of control" were three times more frequent on Mosquitos than on any other type of aircraft. The Air Ministry forestalled any loss of confidence in the Mosquito by holding to Major de Havilland's initial investigation in India that the accidents were caused "largely by climate" To solve the problem of seepage into the interior a strip of plywood was set along the span of the wing to seal the entire length of the skin joint.

The fuel systems gave the Mosquito good range and endurance, using up to nine fuel tanks. Two outer wing tanks each contained of fuel. These were complemented by two inner wing fuel tanks, each containing , located between the wing root and engine nacelle. In the central fuselage were twin fuel tanks mounted between bulkhead number two and three aft of the cockpit. In the FB.VI, these tanks contained each, while in the B.IV and other unarmed Mosquitos each of the two centre tanks contained . Both the inner wing, and fuselage tanks are listed as the "main tanks" and the total internal fuel load of was initially deemed appropriate for the type. In addition, the FB Mk VI could have larger fuselage tanks, increasing the capacity to . Drop tanks of or could be mounted under each wing, increasing the total fuel load to .

The design of the Mark VI allowed for a provisional long-range fuel tank to increase range for action over enemy territory, for the installation of bomb release equipment specific to depth charges for strikes against enemy shipping, or for the simultaneous use of rocket projectiles along with a drop tank under each wing supplementing the main fuel cells. The FB.VI had a wingspan of , a length (over guns) of . It had a maximum speed of at . Maximum take-off weight was and the range of the aircraft was with a service ceiling of .

To reduce fuel vaporisation at the high altitudes of photographic reconnaissance variants, the central and inner wing tanks were pressurised. The pressure venting cock located behind the pilot's seat controlled the pressure valve. As the altitude increased, the valve increased the volume applied by a pump. This system was extended to include field modifications of the fuel tank system.

The engine oil tanks were in the engine nacelles. Each nacelle contained a oil tank, including a air space. The oil tanks themselves had no separate coolant controlling systems. The coolant header tank was in the forward nacelle, behind the propeller. The remaining coolant systems were controlled by the coolant radiators shutters in the forward inner wing compartment, between the nacelle and the fuselage and behind the main engine cooling radiators, which were fitted in the leading edge. Electric-pneumatic operated radiator shutters directed and controlled airflow through the ducts and into the coolant valves, to predetermined temperatures.

Electrical power came from a 24 volt DC generator on the starboard (No. 2) engine and an alternator on the port engine, which also supplied AC power for radios. The radiator shutters, supercharger gear change, gun camera, bomb bay, bomb/rocket release and all the other crew controlled instruments were powered by a 24 V battery. The radio communication devices included VHF and HF communications, GEE navigation, and IFF and G.P. devices. The electric generators also powered the fire extinguishers. Located on the starboard side of the cockpit, the switches would operate automatically in the event of a crash. In flight, a warning light would flash to indicate a fire, should the pilot not already be aware of it. In later models, to save liquids and engine clean up time in case of belly landing, the fire extinguisher was changed to semi-automatic triggers.

The main landing gear, housed in the nacelles behind the engines, were raised and lowered hydraulically. The main landing gear shock absorbers were de Havilland manufactured and used a system of rubber in compression, rather than hydraulic oleos, with twin pneumatic brakes for each wheel. The Dunlop-Marstrand anti-shimmy tailwheel was also retractable.

The de Havilland Mosquito operated in many roles, performing medium bomber, reconnaissance, tactical strike, anti-submarine warfare and shipping attacks and night fighter duties, both defensive and offensive, until the end of the war.

In July 1941, the first production Mosquito "W4051" (a production fuselage combined with some prototype flying surfaces – see section of article "Prototypes and test flights") was sent to No. 1 Photographic Reconnaissance Unit (PRU), at RAF Benson. Consequently, the secret reconnaissance flights of this aircraft were the first active service missions of the Mosquito. In 1944, the journal "Flight" gave 19 September 1941 as date of the first PR mission, at an altitude "of some 20 000 ft." 

On 15 November 1941, 105 Squadron, RAF, took delivery at RAF Swanton Morley, Norfolk, of the first operational Mosquito Mk. B.IV bomber, serial no. "W4064". Throughout 1942, 105 Squadron, based next at RAF Horsham St. Faith, then from 29 September, RAF Marham, undertook daylight low-level and shallow dive attacks. Apart from the Oslo and Berlin raids, the strikes were mainly on industrial and infrastructure targets in occupied Netherlands and Norway, France and northern and western Germany. The crews faced deadly flak and fighters, particularly Focke-Wulf Fw 190s, which they called "snappers". Germany still controlled continental airspace, and the Fw 190s were often already airborne and at an advantageous altitude. Collisions within the formations also caused casualties. It was the Mosquito’s excellent handling capabilities, rather than pure speed, that facilitated those evasions that were successful.
The Mosquito was first announced publicly on 26 September 1942 after the Oslo Mosquito raid of 25 September. It was featured in "The Times" on the 28 September, and the next day the newspaper published two captioned photographs illustrating the bomb strikes and damage. On 6 December 1942 Mosquitos from Nos. 105 and 139 Squadrons made up part of the bomber force used in Operation Oyster, the large No. 2 Group raid against the Philips works at Eindhoven.

In another example of the daylight precision raids carried out by the Mosquitos of Nos. 105 and 139 Squadrons, on 30 January 1943, the 10th anniversary of the Nazis' seizure of power, a morning Mosquito attack knocked out the main Berlin broadcasting station while Commander in Chief Reichsmarschall Hermann Göring was speaking, putting his speech off the air. A second sortie in the afternoon inconvenienced another speech, by Goebbels. Lecturing a group of German aircraft manufacturers, Göring said: 
During this daylight-raiding phase, Nos. 105 and 139 Squadrons flew 139 combat operations and aircrew losses were high. Even the losses incurred in the squadrons' dangerous Blenheim era were exceeded in percentage terms. The Roll of Honour shows 51 aircrew deaths from the end of May 1942 to April 1943. In the corresponding period, crews gained three Mentions in Despatches, two DFMs and three DFCs. The low-level daylight attacks finished on 27 May 1943 with strikes on the Schott glass and Zeiss instrument works, both in Jena. Subsequently, when low-level precision attacks required Mosquitos, they were allotted to squadrons operating the FB.IV version. Examples include the Aarhus air raid and Operation Jericho.

Since the beginning of the year, the German fighter force had become seriously overstretched. In April 1943, in response to "political humiliation" caused by the Mosquito, Göring ordered the formation of special "Luftwaffe" units ("Jagdgeschwader 25", commanded by "Oberstleutnant" Herbert Ihlefeld and "Jagdgeschwader 50", under "Major" Hermann Graf) to combat the Mosquito attacks, though these units, which were "little more than glorified squadrons", were unsuccessful against the elusive RAF aircraft. Post-war German histories also indicate that there was a belief within the Luftwaffe that Mosquito aircraft "gave only a weak radar signal.".

The first Mosquito Squadron to be equipped with Oboe (navigation) was No. 109, based at RAF Wyton, after working as an experimental unit at RAF Boscombe Down. They used Oboe in anger for the first time on 31 December 1942 and 1 January 1943, target marking for a force of heavy bombers attacking Düsseldorf.. On the 1st. June, the two pioneering Squadrons joined No. 109 Squadron in the re-formed No. 8 Group RAF(Bomber Command). Initially they were engaged in moderately high altitude (about ) night bombing, with 67 trips during that summer, mainly to Berlin. Soon after, Nos. 105 and 139 Squadron bombers were widely used by the RAF Pathfinder Force, marking targets for the main night-time strategic bombing force.

In what were, initially, diversionary "nuisance raids," Mosquito bombers dropped 4,000 lb Blockbuster bombs or "Cookies." Particularly after the introduction of H2S (radar) in some Mosquitos, these raids carrying larger bombs succeeded to the extent that they provided a significant additional form of attack to the large formations of "heavies." Latterly in the war, there were a significant number of all-Mosquito raids on big German cities involving up to 100 or more aircraft. On the night of 20/21 February 1945, for example, Mosquitos of No. 8 Group mounted the first of 36 consecutive night raids on Berlin.

Despite an initially high loss rate, the Mosquito bomber variants ended the war with the lowest losses of any of the types in RAF Bomber Command service.

The Mosquito also proved a very capable night fighter. Some of the most successful RAF pilots flew these variants. For example, Wing Commander Branse Burbridge claimed 21 kills, and Wing Commander John Cunningham claimed 19 of his 20 victories at night on Mosquitos.

Mosquitos of No. 100 Group RAF acted as night intruders operating at high level in support of the Bomber Command "heavies", to counter the enemy tactic of merging into the bomber stream, which, towards the end of 1943, was causing serious allied losses. These RCM (radio countermeasures) aircraft were fitted with a device called "Serrate" to allow them to track down German night fighters from their "Lichtenstein B/C" (low-UHF-band) and "Lichtenstein SN-2" (lower end of the VHF FM broadcast band) radar emissions, as well as a device named "Perfectos" that tracked German IFF signals. These methods were responsible for the destruction of 257 German aircraft from December 1943 to April 1945. Mosquito fighters from all units accounted for 487 German aircraft during the war, the vast majority of which were night fighters.

One Mosquito is listed as belonging to German secret operations unit "Kampfgeschwader 200", which tested, evaluated and sometimes clandestinely operated captured enemy aircraft during the war. The aircraft was listed on the order of battle of "Versuchsverband OKL"s, "2 Staffel", "Stab Gruppe" on 10 November and 31 December 1944. However, on both lists, the Mosquito is listed as unserviceable.

The Mosquito flew its last official European war mission on 21 May 1945, when Mosquitos of 143 Squadron and 248 Squadron RAF were ordered to continue to hunt German submarines that might be tempted to continue the fight; instead of submarines all the Mosquitos encountered were passive E-boats.

The last operational RAF Mosquitos were the Mosquito TT.35's, which were finally retired from No. 3 Civilian Anti-Aircraft Co-Operation Unit (CAACU) in May 1963.

Until the end of 1942 the RAF always used Roman numerals (I, II, ...) for mark numbers; 1943–1948 was a transition period during which new aircraft entering service were given Arabic numerals (1, 2, ...) for mark numbers, but older aircraft retained their Roman numerals. From 1948 onwards, Arabic numerals were used exclusively.

Three prototypes were built, each with a different configuration. The first to fly was "W4050" on 25 November 1940, followed by the fighter "W4052" on 15 May 1941 and the photo-reconnaissance prototype "W4051" on 10 June 1941. "W4051" later flew operationally with 1 Photographic Reconnaissance Unit (1 PRU).

A total of 10 Mosquito PR Mk Is were built, four of them "long range" versions equipped with a overload fuel tank in the fuselage. The contract called for 10 of the PR Mk I airframes to be converted to B Mk IV Series 1s. All of the PR Mk Is, and the B Mk IV Series 1s, had the original short engine nacelles and short span (19 ft 5.5 in) tailplanes. Their engine cowlings incorporated the original pattern of integrated exhaust manifolds, which, after relatively brief flight time, had a troublesome habit of burning and blistering the cowling panels. The first operational sortie by a Mosquito was made by a PR Mk I, W4055, on 17 September 1941; during this sortie the unarmed Mosquito PR.I evaded three Messerschmitt Bf 109s at . Powered by two Merlin 21s, the PR Mk I had a maximum speed of , a cruise speed of , a ceiling of , a range of , and a climb rate of per minute.

Over 30 Mosquito B Mk IV bombers were converted into the PR Mk IV photo-reconnaissance aircraft. The first operational flight by a PR Mk IV was made by "DK284" in April 1942.

The Mosquito PR Mk VIII, built as a stopgap pending the introduction of the refined PR Mk IX, was the next photo-reconnaissance version. The five VIIIs were converted from B Mk IVs and became the first operational Mosquito version to be powered by two-stage, two-speed supercharged engines, using Rolls-Royce Merlin 61 engines in place of Merlin 21/22s. The first PR Mk VIII, "DK324" first flew on 20 October 1942. The PR Mk VIII had a maximum speed of , an economical cruise speed of at 20,000 ft, and at 30,000 ft, a ceiling of , a range of , and a climb rate of 2,500 ft per minute (760 m).

The Mosquito PR Mk IX, 90 of which were built, was the first Mosquito variant with two-stage, two-speed engines to be produced in quantity; the first of these, "LR405", first flew in April 1943. The PR Mk IX was based on the Mosquito B Mk IX bomber and was powered by two Merlin 72/73 or 76/77 engines. It could carry either two , two or two droppable fuel tanks.

The Mosquito PR Mk XVI had a pressurised cockpit and, like the Mk IX, was powered by two Rolls-Royce Merlin 72/73 or 76/77 piston engines. This version was equipped with three overload fuel tanks, totalling in the bomb bay, and could also carry two or drop tanks. A total of 435 of the PR Mk XVI were built. The PR Mk XVI had a maximum speed of , a cruise speed of , ceiling of , a range of , and a climb rate of 2,900 feet per minute (884 m).

The Mosquito PR Mk 32 was a long-range, high-altitude, pressurised photo-reconnaissance version. It was powered by a pair of two-stage supercharged Rolls-Royce Merlin 113 and Merlin 114 piston engines, the Merlin 113 on the starboard side and the Merlin 114 on the port. First flown in August 1944, only five were built and all were conversions from PR.XVIs.

The Mosquito PR Mk 34 and PR Mk 34A was a very long-range unarmed high altitude photo-reconnaissance version. The fuel tank and cockpit protection armour were removed. Additional fuel was carried in a bulged bomb bay: 1,192 gallons—the equivalent of . A further two 200-gallon (910-litre) drop tanks under the outer wings gave a range of cruising at . Powered by two Merlin 114s first used in the PR.32. The port Merlin 114 drove a Marshal cabin supercharger. A total of 181 were built, including 50 built by Percival Aircraft Company at Luton. The PR.34's maximum speed (TAS) was at sea level, at and at .
All PR.34s were installed with four split F52 vertical cameras, two forward, two aft of the fuselage tank and one F24 oblique camera. Sometimes a K-17 camera was used for air surveys. In August 1945, the PR.34A was the final photo-reconnaissance variant with one Merlin 113A and 114A each delivering .

Colonel Roy M. Stanley II, USAF (RET) wrote: "I consider the Mosquito the best photo-reconnaissance aircraft of the war".

After the end of World War II Spartan Air Services used 10 ex-RAF Mosquitoes, mostly B.35's plus one of only six PR.35's built, for high-altitude photographic survey work in Canada.

On 21 June 1941 the Air Ministry ordered that the last 10 Mosquitos, ordered as photo-reconnaissance aircraft, should be converted to bombers. These 10 aircraft were part of the original 1 March 1940 production order and became the B Mk IV Series 1. "W4052" was to be the prototype and flew for the first time on 8 September 1941.

The bomber prototype led to the B Mk IV, of which 273 were built: apart from the 10 Series 1s, all of the rest were built as Series 2s with extended nacelles, revised exhaust manifolds, with integrated flame dampers, and larger tailplanes. Series 2 bombers also differed from the Series 1 in having a larger bomb bay to increase the payload to four bombs, instead of the four bombs of Series 1. This was made possible by shortening the tail of the bomb so that these four larger weapons could be carried (or a 2,000 lb (920;kg) total load). The B Mk IV entered service in May 1942 with 105 Squadron.

In April 1943 it was decided to convert a B Mk IV to carry a Blockbuster bomb (nicknamed a Cookie). The conversion, including modified bomb bay suspension arrangements, bulged bomb bay doors and fairings, was relatively straightforward and 54 B.IVs were modified and distributed to squadrons of the Light Night Striking Force. 27 B Mk IVs were later converted for special operations with the Highball anti-shipping weapon, and were used by 618 Squadron, formed in April 1943 specifically to use this weapon. A B Mk IV, "DK290" was initially used as a trials aircraft for the bomb, followed by "DZ471,530 and 533". The B Mk IV had a maximum speed of , a cruising speed of , ceiling of , a range of , and a climb rate of 2,500 ft per minute (762 m).
Other bomber variants of the Mosquito included the Merlin 21 powered B Mk V high-altitude version. Trials with this configuration were made with "W4057", which had strengthened wings and two additional fuel tanks, or alternatively, two bombs. This design was not produced in Britain, but formed the basic design of the Canadian-built B.VII. Only "W4057" was built in prototype form. The Merlin 31 powered B Mk VII was built by de Havilland Canada and first flown on 24 September 1942. It only saw service in Canada, 25 were built. Six were handed over to the United States Army Air Forces.

B Mk IX (54 built) was powered by the Merlin 72,73, 76 or 77. The two-stage Merlin variant was based on the PR.IX. The prototype "DK 324" was converted from a PR.VIII and first flew on 24 March 1943. In October 1943 it was decided that all B Mk IVs and all B Mk IXs then in service would be converted to carry the "Cookie", and all B Mk IXs built after that date were designed to allow them to be converted to carry the weapon. The B Mk IX had a maximum speed of , an economical cruise speed of at 20,000 ft, and at 30,000 ft, ceiling of , a range of , and a climb rate of 2,850 feet per minute (869 m). The IX could carry a maximum load of of bombs. A Mosquito B Mk IX holds the record for the most combat operations flown by an Allied bomber in the Second World War. "LR503", known as "F for Freddie" (from its squadron code letters, GB*F), first served with No. 109 and subsequently, No. 105 RAF squadrons. It flew 213 sorties during the war, only to crash at Calgary airport during the Eighth Victory Loan Bond Drive on 10 May 1945, two days after Victory in Europe Day, killing both the pilot, Flt. Lt. Maurice Briggs, DSO, DFC, DFM and navigator Fl. Off. John Baker, DFC and Bar.

The B Mk XVI was powered by the same variations as the B.IX. All B Mk XVIs were capable of being converted to carry the "Cookie". The two-stage powerplants were added along with a pressurised cabin. "DZ540" first flew on 1 January 1944. The prototype was converted from a IV (402 built). The next variant, the B Mk XX, was powered by Packard Merlins 31 and 33s. It was the Canadian version of the IV. Altogether, 245 were built. The B Mk XVI had a maximum speed of , an economical cruise speed of at 20,000 ft, and at 30,000 ft, ceiling of , a range of , and a climb rate of 2,800 ft per minute (853 m). The type could carry of bombs.

The B.35 was powered by Merlin 113 and 114As. Some were converted to TT.35s (Target Tugs) and others were used as PR.35s (photo-reconnaissance). The B.35 had a maximum speed of , a cruising speed of , ceiling of , a range of , and a climb rate of 2,700 ft per minute (823 m). A total of 174 B.35s were delivered up to the end of 1945. A further 100 were delivered from 1946 for a grand total of 274, 65 of which were built by Airspeed Ltd.

Developed during 1940, the first prototype of the Mosquito F Mk II was completed on 15 May 1941. These Mosquitos were fitted with four Hispano cannon in the fuselage belly and four .303 (7.7 mm) Browning machine guns mounted in the nose. On production Mk IIs the machine guns and ammunition tanks were accessed via two centrally hinged, sideways opening doors in the upper nose section. To arm and service the cannon the bomb bay doors were replaced by manually operated bay doors: the F and NF Mk IIs could not carry bombs. The type was also fitted with a gun camera in a compartment above the machine guns in the nose and was fitted with exhaust flame dampers to reduce the glare from the Merlin XXs.

In the summer of 1942, Britain experienced day-time incursions of the high-altitude reconnaissance bomber, the Junkers Ju 86P. Although the Ju 86P only carried a light bomb load, it overflew sensitive areas, including Bristol, Bedfordshire and Hertfordshire. Bombs were dropped on Luton and elsewhere, and this particular aircraft was seen from the main de Havilland offices and factory at Hatfield. An attempt to intercept it with a Spitfire from RAF Manston was unsuccessful. As a result of the potential threat, a decision was quickly taken to develop a high-altitude Mosquito interceptor, using the "MP469" prototype.

"MP469" entered the experimental shop on the 7 September and made its initial flight on the 14 September, piloted by John de Havilland. The bomber nose was altered using a normal fighter nose, armed with four standard .303 (7.7 mm) Browning machine guns. The low pressure cabin retained a bomber canopy structure and a two-piece windscreen. The control wheel was replaced with a fighter control stick. The wingspan was increased to . The airframe was lightened by removing armour plating, some fuel tanks and other fitments. Smaller-diameter main wheels were fitted after the first few flights. At a loaded weight of this HA Mk XV was lighter than a standard Mk II. For this first conversion, the engines were a pair of Merlin 61s. On 15 September, John de Havilland reached an altitude of in this version. The aircraft was delivered to a High Altitude Flight which had been formed at RAF Northolt. However, the high-level German daylight intruders were no longer to be seen. It was subsequently revealed that only five Ju 86P aircraft had been built and they had only flown 12 sorties. Nevertheless, the general need for high altitude interceptors was recognised - but now the emphasis was to be upon night fighters.

The A&AEE tested the climb and speed of night fighter conversion of MP469 in January 1943 for the Ministry of Aircraft Production. Wingspan had been increased to , the Brownings had been moved to a fairing below the fuselage. According to Birtles, an AI radar was mounted in the nose and the Merlins were upgraded to Mk76 type, although Boscombe Down reported Merlin 61s. In addition to MP469, four more B Mk IVs were converted into NF MK XVs. The Fighter Interception Unit at RAF Ford carried out service trials, March 1943, and then these five aircraft went to 85 Squadron, Hunsdon, where they were flown from April until August of that year. The greatest height reached in service was .

Apart from the F Mk XV, all Mosquito fighters and fighter bombers featured a modified canopy structure incorporating a flat, single piece armoured windscreen, and the crew entry/exit door was moved from the bottom of the forward fuselage to the right side of the nose, just forward of the wing leading edge.

At the end of 1940, the Air Staff's preferred turret-equipped night fighter design to Operational Requirement O.R. 95 was the Gloster F.18/40 (derived from their F.9/37). However, although in agreement as to the quality of the Gloster company's design, the Ministry of Aircraft Production was concerned that Gloster would not be able to work on the F.18/40 and also the jet fighter design, considered the greater priority. Consequently, in mid-1941 the Air Staff and MAP agreed that the Gloster aircraft would be dropped and the Mosquito, when fitted with a turret would be considered for the night fighter requirement.

The first production night fighter Mosquitos - minus turrets - were designated NF Mk II. A total of 466 were built with the first entering service with No. 157 Squadron in January 1942, replacing the Douglas Havoc. These aircraft were similar to the F Mk II, but were fitted with the AI Mk IV metric wavelength radar. The herring-bone transmitting antenna was mounted on the nose and the dipole receiving antennae were carried under the outer wings. A number of NF IIs had their radar equipment removed and additional fuel tanks installed in the bay behind the cannon for use as night intruders. These aircraft, designated NF II (Special) were first used by 23 Squadron in operations over Europe in 1942. 23 Squadron was then deployed to Malta on 20 December 1942, and operated against targets in Italy.

Ninety-seven NF Mk IIs were upgraded with 3.3 GHz frequency, low-SHF-band AI Mk VIII radar and these were designated NF Mk XII. The NF Mk XIII, of which 270 were built, was the production equivalent of the Mk XII conversions. These "centimetric" radar sets were mounted in a solid "thimble" (Mk XII / XIII) or universal "bull nose" (Mk XVII / XIX) radome, which required the machine guns to be dispensed with.
Four F Mk XVs were converted to the NF Mk XV. These were fitted with AI Mk VIII in a "thimble" radome, and the .303 Brownings were moved into a gun pack fitted under the forward fuselage.

The NF Mk XIX was an improved version of the NF XIII. It could be fitted with American or British AI radars; 220 were built.

The NF Mk 30 was the final wartime variant and was a high-altitude version, powered by two Rolls-Royce Merlin 76s. The NF Mk 30 had a maximum speed of at . It also carried early electronic countermeasures equipment. 526 were built.

Other Mosquito night fighter variants planned but never built included the NF Mk X and NF Mk XIV (the latter based on the NF Mk XIII), both of which were to have two-stage Merlins. The NF Mk 31 was a variant of the NF Mk 30, but powered by Packard Merlins.

After the war, two more night fighter versions were developed:
The NF Mk 36 was similar to the Mosquito NF Mk 30, but fitted with the American-built AI.Mk X radar. Powered by two Rolls-Royce Merlin 113/114 piston engines; 266 built. Max level speeds (TAS) with flame dampers fitted were at sea level, at , and at .

The NF Mk 38, 101 of which were built, was also similar to the Mosquito NF Mk 30, but fitted with the British-built AI Mk IX radar. This variant suffered from stability problems and did not enter RAF service: 60 were eventually sold to Yugoslavia. According to the Pilot's Notes and Air Ministry 'Special Flying Instruction TF/487', which posted limits on the Mosquito's maximum speeds, the NF Mk 38 had a VNE of 370 knots (425 mph), without under-wing stores, and within the altitude range of sea level to . However, from 10,000 to the maximum speed was 348 knots (400 mph). As the height increased other recorded speeds were; 15,000 to 320 knots (368 mph); 20,000 to , 295 knots (339 mph); 25,000 to , 260 knots (299 mph); 30,000 to , 235 knots (270 mph). With two added 100-gallon fuel tanks this performance fell; between sea level and 15,000 feet 330 knots (379 mph); between 15,000 and 320 knots (368 mph); 20,000 to , 295 knots (339 mph); 25,000 to , 260 knots (299 mph); 30,000 to , 235 knots (270 mph). Little difference was noted above .

The FB Mk VI, which first flew on 1 June 1942, was powered by two, single-stage two-speed, Merlin 21s or Merlin 25s, and introduced a re-stressed and reinforced "basic" wing structure capable of carrying single bombs on racks housed in streamlined fairings under each wing, or up to eight RP-3 25lb or 60 lb rockets. In addition fuel lines were added to the wings to enable single or drop tanks to be carried under each wing. The usual fixed armament was four 20 mm Hispano Mk.II cannon and four .303 (7.7 mm) Browning machine guns, while two bombs could be carried in the bomb bay.

Unlike the F Mk II, the ventral bay doors were split into two pairs, with the forward pair being used to access the cannon, while the rear pair acted as bomb bay doors. The maximum fuel load was distributed between internal fuel tanks, plus two overload tanks, each of capacity, which could be fitted in the bomb bay, and two drop tanks. All-out level speed is often given as , although this speed applies to aircraft fitted with saxophone exhausts. The test aircraft ("HJ679") fitted with stub exhausts was found to be performing below expectations. It was returned to de Havilland at Hatfield where it was serviced. Its top speed was then tested and found to be , in line with expectations. 2,298 FB Mk VIs were built, nearly one-third of Mosquito production. Two were converted to TR.33 carrier-borne, maritime strike prototypes.

The FB Mk VI proved capable of holding its own against fighter aircraft, in addition to strike/bombing roles. For example, on 15 January 1945 Mosquito FB Mk VIs of 143 Squadron were engaged by 30 Focke-Wulf Fw 190s from "Jagdgeschwader 5": the Mosquitos sank an armed trawler and two merchant ships, but five Mosquitos were lost (two reportedly to flak), while shooting down five Fw 190s. 
However, the loss of a two-seat, twin engine aircraft like the Mosquito represented a more costly loss than a single-seat, single-engine fighter (like the Fw 190). The loss of two Mosquitos to flak also underlined the vulnerability of the type to AA artillery – a flaw that contributed to the Bristol Beaufighter constituting the backbone of Commonwealth strike and ground attack squadrons around the world, until the end of the war. 

Another fighter-bomber variant was the Mosquito FB Mk XVIII (sometimes known as the "Tsetse") of which one was converted from a FB Mk VI to serve as prototype and 17 were purpose-built. The Mk XVIII was armed with a Molins "6-pounder Class M" cannon: this was a modified QF 6-pounder (57 mm) anti-tank gun fitted with an auto-loader to allow both semi- or fully automatic fire. 25 rounds were carried, with the entire installation weighing . In addition, of armour was added within the engine cowlings, around the nose and under the cockpit floor to protect the engines and crew from heavily armed U-boats, the intended primary target of the Mk XVIII. Two or four .303 (7.7 mm) Browning machine guns were retained in the nose and were used to "sight" the main weapon onto the target.

The Air Ministry initially suspected that this variant would not work, but tests proved otherwise. Although the gun provided the Mosquito with yet more anti-shipping firepower for use against U-boats, it required a steady approach run to aim and fire the gun, making its wooden construction an even greater liability, in the face of intense anti-aircraft fire. The gun had a muzzle velocity of and an excellent range of some . It was sensitive to sidewards movement; an attack required a dive from at a 30° angle with the turn and bank indicator on centre. A move during the dive could jam the gun. The prototype "HJ732" was converted from a FB.VI and was first flown on 8 June 1943.

The effect of the new weapon was demonstrated on 10 March 1944 when Mk XVIIIs from 248 Squadron (escorted by four Mk VIs) engaged a German convoy of one U-boat and four destroyers, protected by 10 Ju 88s. Three of the Ju 88s were shot down. Pilot Tony Phillips destroyed one Ju 88 with four shells, one of which tore an engine off the Ju 88. The U-boat was damaged. On 25 March, was sunk by Molins-equipped Mosquitos. On 10 June, was abandoned in the face of intense air attack from No. 248 Squadron, and was later sunk by a Liberator of No. 206 Squadron. On 5 April 1945 Mosquitos with Molins attacked five German surface ships in the Kattegat and again demonstrated their value by setting them all on fire and sinking them. A German "Sperrbrecher" ("minefield breaker") was lost with all hands, with some 200 bodies being recovered by Swedish vessels. Some 900 German soldiers died in total. On 9 April, German U-boats , and were spotted in formation heading for Norway. All were sunk with rockets. and followed on 19 April and 2 May 1945, also sunk by rockets.
Despite the preference for rockets, a further development of the large gun idea was carried out using the even larger, 96 mm calibre QF 32-pounder, a gun based on the QF 3.7 inch AA gun designed for tank use, the airborne version using a novel form of muzzle brake. Developed to prove the feasibility of using such a large weapon in the Mosquito, this installation was not completed until after the war, when it was flown and fired in a single aircraft without problems, then scrapped.

Designs based on the Mk VI were the FB Mk 26, built in Canada, and the FB Mk 40, built in Australia, powered by Packard Merlins. The FB.26 improved from the FB.21 using single stage Packard Merlin 225s. Some 300 were built and another 37 converted to T.29 standard. 212 FB.40s were built by de Havilland Australia. Six were converted to PR.40; 28 to PR.41s, one to FB.42 and 22 to T.43 trainers. Most were powered by Packard-built Merlin 31 or 33s.

The Mosquito was also built as the Mosquito T Mk III two-seat trainer. This version, powered by two Rolls-Royce Merlin 21s, was unarmed and had a modified cockpit fitted with dual control arrangements. A total of 348 of the T Mk III were built for the RAF and Fleet Air Arm. de Havilland Australia built 11 T Mk 43 trainers, similar to the Mk III.

To meet specification N.15/44 for a navalised Mosquito for Royal Navy use as a torpedo bomber, de Havilland produced a carrier-borne variant. A Mosquito FB.VI was modified as a prototype designated Sea Mosquito TR Mk 33 with folding wings, arrester hook, thimble nose radome, Merlin 25 engines with four-bladed propellers and a new oleo-pneumatic landing gear rather than the standard rubber-in-compression gear. Initial carrier tests of the Sea Mosquito were carried out by Eric "Winkle" Brown aboard HMS "Indefatigable", the first landing-on taking place on 25 March 1944. An order for 100 TR.33s was placed although only 50 were built at Leavesden. Armament was four 20 mm cannon, two 500 lb bombs in the bomb bay (another two could be fitted under the wings), eight 60 lb rockets (four under each wing) and a standard torpedo under the fuselage. The first production TR.33 flew on 10 November 1945. This series was followed by six Sea Mosquito TR Mk 37s, which differed in having ASV Mk XIII radar instead of the TR.33's AN/APS-6.

The RAF's target tug version was the Mosquito TT Mk 35, which were the last aircraft to remain in operational service with No 3 CAACU at Exeter, being finally retired in 1963. These aircraft were then featured in the film 633 Squadron.
A number of B Mk XVIs bombers were converted into TT Mk 39 target tug aircraft. The Royal Navy also operated the Mosquito TT Mk 39 for target towing.
Two ex-RAF FB.6s were converted to TT.6 standard at Manchester (Ringway) Airport by Fairey Aviation in 1953–1954, and delivered to the Belgian Air Force for use as towing aircraft from the Sylt firing ranges.

A total of 1,032 (wartime 
+ 2 afterwards) Mosquitos were built by De Havilland Canada at Downsview Airfield in Downsview Ontario (now Downsview Park in Toronto Ontario).


A number of Mosquito IVs were modified by Vickers-Armstrongs to carry Highball "bouncing bombs" and were allocated Vickers Type numbers:

About 5,000 of the total of 7,781 Mosquitos built had major structural components fabricated from wood in High Wycombe, Buckinghamshire, England. Fuselages, wings and tailplanes were made at furniture companies such as Ronson, E. Gomme, Parker Knoll, Austinsuite and Styles & Mealing. Wing spars were made by J. B. Heath and Dancer & Hearne. Many of the other parts, including flaps, flap shrouds, fins, leading edge assemblies and bomb doors were also produced in the Buckinghamshire town. Dancer & Hearne processed much of the wood from start to finish, receiving timber and transforming it into finished wing spars at their factory on the outskirts of High Wycombe.

Initially much of the specialised yellow birch wood veneer and finished plywood used for the prototypes and early production aircraft was brought in Liberty Ships from firms in Wisconsin, US. Prominent in this role were Roddis Plywood and Veneer Manufacturing in Marshfield. In conjunction with the USDA Forest Products Laboratory, Hamilton Roddis had developed new plywood adhesives and hot pressing technology. 

In July 1941, it was decided that DH Canada would build Mosquitos at Downsview, Ontario. This was to continue even if Germany invaded Great Britain. Packard Merlin engines produced under licence were bench-tested by August and the first two aircraft were built in September. Production was to increase to fifty per month by early 1942. Initially, the Canadian production was for bomber variants; later, fighters, fighter-bombers and training aircraft were also made. DH Chief Production Engineer, Harry Povey, was sent first, then W. D. Hunter followed on an extended stay, to liaise with materials and parts suppliers. As was the case with initial UK production, Tego-bonded plywood and birch veneer was obtained from firms in Wisconsin, principally Roddis Plywood and Veneer Manufacturing, Marshfield. Enemy action delayed the shipping of jigs and moulds and it was decided to build these locally. During 1942, production improved to over 80 machines per month, as sub-contractors and suppliers became established. A mechanised production line based in part on car building methods started in 1944. When flight testing could no longer keep up, this was moved to the Central Aircraft Company airfield, London, Ontario, where the approved Mosquitos left for commissioning and subsequent ferry transfer to Europe.

Ferrying Mosquitos and many other types of WWII aircraft from Canada to Europe was dangerous, resulting in losses of lives and machines, but in the exigencies of war it was regarded as the best option for twin-engine and multi-engine aircraft. In the parlance of the day, among RAF personnel, ″it was no piece of cake.″ 
. Considerable efforts were made by de Havilland Canada to resolve problems with engine and oil systems and an additional five hours of flight testing were introduced before the ferry flight. Although this reduced the number of aircraft missing, the actual cause of some of the losses was unknown. Nevertheless, by the end of the war, nearly 500 Mosquito bombers and fighter-bombers had been ferried successfully by the Canadian operation.

After DH Canada had been established for the Mosquito, further manufacturing was set up at DH Australia, in Sydney. One of the DH staff who travelled there was the distinguished test pilot, Pat Fillingham. These production lines added totals of 1,133 aircraft of varying types from Canada plus 212 aircraft from Australia.

In total, both during the war and after, de Havilland exported 46 FB.VIs and 29 PR. XVIs to Australia; two FB.VI and 18 NF.30s to Belgium; approximately 250 FB.26, T.29 and T.27s from Canada to Nationalist China. A significant number never went into service due to deterioration on the voyage and to crashes during Chinese pilot training, however five were captured by the People's Liberation Army during the Chinese Civil War; 19 FB.VIs to Czechoslovakia in 1948; 6 FB.VIs to Dominica; a few B.IVs, 57 FB.VIs, 29 PR.XVIs and 23 NF.XXXs to France. Some T.IIIs were exported to Israel along with 60 FB.VIs, and at least five PR.XVIs and 14 naval versions. Four T.IIIs, 76 FB.VIs, one FB.40 and four T.43s were exported to New Zealand. Three T.IIIs were exported to Norway, and 18 FB.VIs, which were later converted to night fighter standard. South Africa received two F.II and 14 PR.XVI/XIs and Sweden received 60 NF.XIXs. Turkey received 96 FB.VIs and several T.IIIs, and Yugoslavia had 60 NF.38s, 80 FB.VIs and three T.IIIs delivered.

Total Mosquito production was 7,781, of which 6,710 were built during the war.

A number of Mosquitos were lost in civilian airline service, mostly with British Overseas Airways Corporation during World War II.

There are approximately 30 non-flying Mosquitos around the world with three airworthy examples, two in the United States and one in Canada. The largest collection of Mosquitos is at the de Havilland Aircraft Heritage Centre in the United Kingdom, which owns three aircraft, including the first prototype, "W4050", the only initial prototype of a Second World War British aircraft design still in existence in the 21st century.

Fighter version.
The definitive bomber version.





</doc>
<doc id="9099" url="https://en.wikipedia.org/wiki?curid=9099" title="Dave Thomas (businessman)">
Dave Thomas (businessman)

Rex David "Dave" Thomas (July 2, 1932January 8, 2002) was an American businessman and philanthropist. Thomas was the founder and chief executive officer of Wendy's, a fast-food restaurant chain specializing in hamburgers. He is also known for appearing in more than 800 commercial advertisements for the chain from 1989 to 2002, more than any other company founder in television history.

Thomas was born on July 2, 1932 in Atlantic City, New Jersey to a young unmarried woman he never knew. He was adopted at six weeks by Rex and Auleva Thomas, and as an adult became a well-known advocate for adoption, founding the Dave Thomas Foundation for Adoption. After his adoptive mother's death when he was 5, his father moved around the country seeking work. Thomas spent some of his early childhood near Kalamazoo, Michigan with his grandmother, Minnie Sinclair, whom he credited with teaching him the importance of service and treating others well and with respect, lessons that helped him in his future business life.

At 12, Thomas had his first job at Regas Restaurant, a fine dining restaurant in downtown Knoxville, Tennessee, then lost it in a dispute with his boss; decades later, Regas Restaurant installed a large autographed poster-photo of Thomas just inside their entrance until the business closed down December 31, 2010. He vowed never to lose another job. Moving with his father, by 15 he was working in Fort Wayne, Indiana at the Hobby House Restaurant owned by the Clauss family. When his father prepared to move again, Dave decided to stay in Fort Wayne, dropping out of high school to work full-time at the restaurant. Thomas, who considered ending his schooling the greatest mistake of his life, did not graduate from high school until 1993, when he obtained a GED.

He subsequently became an education advocate and founded the Dave Thomas Education Center in Coconut Creek, Florida, which offers GED classes to young adults.

At the outbreak of the Korean War in 1950, rather than waiting for the draft, he volunteered for the U.S. Army to have some choice in assignments. Having food production and service experience, Thomas requested the Cook's and Baker's School at Fort Benning, Georgia. He was sent to Germany as a mess sergeant and was responsible for the daily meals of 2000 soldiers, rising to the rank of staff sergeant. After his discharge in 1953, Thomas returned to Fort Wayne and the Hobby House.

In the mid-1950s, Kentucky Fried Chicken founder Col. Harland Sanders came to Fort Wayne to find restaurateurs with established businesses in order to try to sell KFC franchises to them. At first, Thomas, who was the head cook at a restaurant, and the Clausses declined Sanders' offer, but Sanders persisted and the Clauss family franchised their restaurant with KFC and later also owned many other KFC franchises in the Midwest. During this time, Thomas worked with Sanders on many projects to make KFC more profitable and to give it brand recognition. Among other things Thomas suggested to Sanders, that were implemented, was that KFC reduce the number of items on the menu and focus on a signature dish. Thomas also suggested Sanders make commercials that he appear in himself. Thomas was sent by the Clauss family in the mid-1960s to help turn around four failing KFC stores they owned in Columbus, Ohio.

By 1968 Thomas had increased sales in the four fried chicken restaurants so much that he sold his share in them back to Sanders for more than $1.5 million. This experience would prove invaluable to Thomas when he began Wendy's about a year later.

After serving as a regional director for Kentucky Fried Chicken, Thomas became part of the investor group which founded Arthur Treacher's. His involvement with the new restaurant lasted less than a year before he went on to found Wendy's.

Thomas opened his first Wendy's in Columbus, Ohio, November 15, 1969. (This original restaurant remained operational until March 2, 2007, when it was closed due to lagging sales.) Thomas named the restaurant after his eight-year-old daughter Melinda Lou, whose nickname was "Wendy", stemming from the child's inability to say her own name at a young age. According to "Bio TV", Dave claims himself that people nicknamed his daughter "Wenda. Not Wendy, but Wenda. 'I'm going to call it Wendy's Old Fashioned Hamburgers'."

In 1982, Thomas resigned from his day-to-day operations at Wendy's. However, by 1985, several company business decisions, including an awkward new breakfast menu and loss in brand awareness due to fizzled marketing efforts, caused the company's new president to urge Thomas back into a more active role with Wendy's. Thomas began to visit franchises and espouse his hardworking, so-called "mop-bucket attitude." In 1989, he took on a significant role as the TV spokesperson in a series of commercials for the brand. Thomas was not a natural actor, and initially, his performances were criticized as stiff and ineffective by advertising critics.

By 1990, after efforts by Wendy's agency, Backer Spielvolgel Bates, to get humor into the campaign, a decision was made to portray Thomas in a more self-deprecating and folksy manner, which proved much more popular with test audiences. Consumer brand awareness of Wendy's eventually regained levels it had not achieved since octogenarian Clara Peller's wildly popular "Where's the beef?" campaign of 1984.

With his natural self-effacing style and his relaxed manner, Thomas quickly became a household name. A company survey during the 1990s, a decade during which Thomas starred in every Wendy's commercial that aired, found that 90% of Americans knew who Thomas was. After more than 800 commercials, it was clear that Thomas played a major role in Wendy's status as the country's third most popular burger restaurant.

In 1994, Thomas made a cameo appearance as himself in "Bionic Ever After?", a reunion TV movie based upon "The Six Million Dollar Man" and "The Bionic Woman".

Dave Thomas was married for 47 years to Lorraine. She is the mother of their five children. In addition to Melinda they had three more daughters, Pam, Lori and Molly, and a son, Kenny. Though Kenny died in 2013, Dave's daughters still continue to own and run multiple Wendy's locations. Thomas founded the chain Sisters Chicken and Biscuits in 1978, named in reference to his other 3 daughters.

Thomas, realizing that his success as a high school dropout might convince other teenagers to quit school (something he later claimed was a mistake), became a student at Coconut Creek High School. He earned a GED in 1993. Thomas was inducted into the Junior Achievement U.S. Business Hall of Fame in 1999.

Thomas was an honorary Kentucky colonel, as was former boss Colonel Sanders.

Thomas was posthumously awarded the Presidential Medal of Freedom in 2003.

Thomas was raised a Master Mason in Sol. D. Bayless Lodge No. 359 of Fort Wayne, Indiana, and became a 32° Mason, N.M.J., on November 16, 1961, in the Scottish Rite Bodies of Fort Wayne. He affiliated with the Miami, Florida, Scottish Rite Bodies on December 18, 1991; was invested with the Rank and Decoration of Knight Commander Court of Honour on November 13, 1993, in Jacksonville, Florida; and was coroneted an Inspector General Honorary, S.J., on November 25, 1995, in Atlanta, Georgia, and unanimously elected to the Scottish Rite's highest honor, the Grand Cross, by The Supreme Council, 33°, in Executive Session on October 3, 1997, in Washington, D.C.

Thomas had been battling a carcinoid neuroendocrine tumor for ten years, before it metastasized to his liver. He died on January 8, 2002 at his home in Fort Lauderdale, Florida at the age of 69. Thomas was buried in Union Cemetery in Columbus, Ohio. At the time of his death, there were more than 6,000 Wendy's restaurants operating in North America.



</doc>
<doc id="9101" url="https://en.wikipedia.org/wiki?curid=9101" title="Device driver">
Device driver

In computing, a device driver is a computer program that operates or controls a particular type of device that is attached to a computer. A driver provides a software interface to hardware devices, enabling operating systems and other computer programs to access hardware functions without needing to know precise details about the hardware being used.

A driver communicates with the device through the computer bus or communications subsystem to which the hardware connects. When a calling program invokes a routine in the driver, the driver issues commands to the device. Once the device sends data back to the driver, the driver may invoke routines in the original calling program. Drivers are hardware dependent and operating-system-specific. They usually provide the interrupt handling required for any necessary asynchronous time-dependent hardware interface.

The main purpose of device drivers is to provide abstraction by acting as a translator between a hardware device and the applications or operating systems that use it. Programmers can write the higher-level application code independently of whatever specific hardware the end-user is using.

For example, a high-level application for interacting with a serial port may simply have two functions for "send data" and "receive data". At a lower level, a device driver implementing these functions would communicate to the particular serial port controller installed on a user's computer. The commands needed to control a 16550 UART are much different from the commands needed to control an FTDI serial port converter, but each hardware-specific device driver abstracts these details into the same (or similar) software interface.

Writing a device driver requires an in-depth understanding of how the hardware and the software works for a given platform function. Because drivers require low-level access to hardware functions in order to operate, drivers typically operate in a highly privileged environment and can cause system operational issues if something goes wrong. In contrast, most user-level software on modern operating systems can be stopped without greatly affecting the rest of the system. Even drivers executing in user mode can crash a system if the device is erroneously programmed. These factors make it more difficult and dangerous to diagnose problems.

The task of writing drivers thus usually falls to software engineers or computer engineers who work for hardware-development companies. This is because they have better information than most outsiders about the design of their hardware. Moreover, it was traditionally considered in the hardware manufacturer's interest to guarantee that their clients can use their hardware in an optimum way. Typically, the Logical Device Driver (LDD) is written by the operating system vendor, while the Physical Device Driver (PDD) is implemented by the device vendor. But in recent years non-vendors have written numerous device drivers, mainly for use with free and open source operating systems. In such cases, it is important that the hardware manufacturer provides information on how the device communicates. Although this information can instead be learned by reverse engineering, this is much more difficult with hardware than it is with software.

Microsoft has attempted to reduce system instability due to poorly written device drivers by creating a new framework for driver development, called Windows Driver Foundation (WDF). This includes User-Mode Driver Framework (UMDF) that encourages development of certain types of drivers—primarily those that implement a message-based protocol for communicating with their devices—as user-mode drivers. If such drivers malfunction, they do not cause system instability. The Kernel-Mode Driver Framework (KMDF) model continues to allow development of kernel-mode device drivers, but attempts to provide standard implementations of functions that are known to cause problems, including cancellation of I/O operations, power management, and plug and play device support.

Apple has an open-source framework for developing drivers on macOS called the I/O Kit.

In Linux environments, programmers can build device drivers as parts of the kernel, separately as loadable modules, or as user-mode drivers (for certain types of devices where kernel interfaces exist, such as for USB devices). Makedev includes a list of the devices in Linux: ttyS (terminal), lp (parallel port), hd (disk), loop, sound (these include mixer, sequencer, dsp, and audio)...

The Microsoft Windows .sys files and Linux .ko modules contain loadable device drivers. The advantage of loadable device drivers is that they can be loaded only when necessary and then unloaded, thus saving kernel memory.

Device drivers, particularly on Microsoft Windows platforms, can run in kernel-mode (Ring 0 on x86 CPUs) or in user-mode (Ring 3 on x86 CPUs). The primary benefit of running a driver in user mode is improved stability, since a poorly written user mode device driver cannot crash the system by overwriting kernel memory. On the other hand, user/kernel-mode transitions usually impose a considerable performance overhead, thereby prohibiting user-mode drivers for low latency and high throughput requirements.

Kernel space can be accessed by user module only through the use of system calls. End user programs like the UNIX shell or other GUI-based applications are part of the user space. These applications interact with hardware through kernel supported functions.

Because of the diversity of hardware and operating systems, drivers operate in many different environments. Drivers may interface with:


Common levels of abstraction for device drivers include:

So choosing and installing the correct device drivers for given hardware is often a key component of computer system configuration.

Virtual device drivers represent a particular variant of device drivers. They are used to emulate a hardware device, particularly in virtualization environments, for example when a DOS program is run on a Microsoft Windows computer or when a guest operating system is run on, for example, a Xen host. Instead of enabling the guest operating system to dialog with hardware, virtual device drivers take the opposite role and emulates a piece of hardware, so that the guest operating system and its drivers running inside a virtual machine can have the illusion of accessing real hardware. Attempts by the guest operating system to access the hardware are routed to the virtual device driver in the host operating system as e.g., function calls. The virtual device driver can also send simulated processor-level events like interrupts into the virtual machine.

Virtual devices may also operate in a non-virtualized environment. For example, a virtual network adapter is used with a virtual private network, while a virtual disk device is used with iSCSI. A good example for virtual device drivers can be Daemon Tools.

There are several variants of virtual device drivers, such as VxDs, VLMs, and VDDs.


Solaris descriptions of commonly used device drivers:


A device on the PCI bus or USB is identified by two IDs which consist of 4 hexadecimal numbers each. The vendor ID identifies the vendor of the device. The device ID identifies a specific device from that manufacturer/vendor.

A PCI device has often an ID pair for the main chip of the device, and also a subsystem ID pair which identifies the vendor, which may be different from the chip manufacturer.
</div>



</doc>
<doc id="9103" url="https://en.wikipedia.org/wiki?curid=9103" title="Dimona">
Dimona

Dimona () is an Israeli city in the Negev desert, to the south of Beersheba and west of the Dead Sea above the Arava valley in the Southern District of Israel. In its population was .

The city's name is derived from a biblical town, mentioned in Joshua 15:21-22.

Dimona was one of the development towns created in the 1950s under the leadership of Israel's first Prime Minister, David Ben-Gurion. Dimona itself was conceived in 1953. The location chosen was close to the Dead Sea Works. It was established in 1955. The first residents were Jewish immigrants from North Africa, with an initial 36 families being the first to settle there. Its population in 1955 was about 300. The North African immigrants also constructed the city's houses. In the late 1950s and early 1960s, immigrants from Eastern Europe arrived. A textile factory was opened in 1958. That same year, Dimona became a local council. In 1961, it had a population of 5,000. The emblem of Dimona (as a local council), adopted 2 March 1961, appeared on a stamp issued on 24 March 1965. Dimona was declared a city in 1969. In 1971, it had a population of 23,700.

When the Israeli nuclear program started in 1958, a location not far from the city was chosen for the Negev Nuclear Research Center due to its relative isolation in the desert and availability of housing. This has resulted in Dimona widely being identified with the Israeli nuclear program.

In spite of a gradual decrease during the 1980s, the city's population began to grow once again with the beginning of the Russian immigration in the 1990s. Currently, Dimona is the third largest city in the Negev, with the population of almost 34,000. Due to projected rapid population growth in the Negev, the city is expected to triple in size by 2025.

Dimona is described as "mini-India" by many for its 7,500-strong Indian Jewish community. It is also home to Israel's Black Hebrew community, formerly governed by its founder and spiritual leader, Ben Ammi Ben-Israel, now deceased. The Black Hebrews number about 3,000 in Dimona, with additional families in Arad, Mitzpe Ramon and the Tiberias area. Their official status in Israel was an ongoing issue for many years, but in May 1990, the issue was resolved with the issuing of first B/1 visas, and a year later, issuing of temporary residency. Status was extended to August 2003, when the Israeli Ministry of Interior granted permanent residency.

In the early 1980s, textile plants, such as Dimona Textiles Ltd., dominated the industrial landscape. Many plants have since closed. Dimona Silica Industries Ltd. manufactures precipitated silica and calcium carbonate fillers. About a third of the city's population works in industrial workplaces (chemical plants near the Dead Sea like the Dead Sea Works, high-tech companies and textile shops), and another third in the area of services. Due to the introduction of new technologies, many workers have been made redundant in the recent years, creating a total unemployment rate of about 10%. Dimona has taken part of Israel's solar transformation. The Rotem Industrial Complex outside of the city has dozens of solar mirrors that focus the sun's rays on a tower that in turn heats a water boiler to create steam, turning a turbine to create electricity. Luz II, Ltd. plans to use the solar array to test new technology for the three new solar plants to be built in California for Pacific Gas and Electric Company.

Dimona is at an average height of about above sea level. It is in the Negev Desert, therefore it has a desert climate with low humidity for most of the year and little precipitation. Summers are hot with an average max temperature of about in August, the hottest month of the year.

Average annual precipitation is about , mostly during the winter.

In the early 1950s, an extension to Dimona and south was constructed from the Railway to Beersheba, designed for freight traffic. A passenger service began in 2005, after pressure from Dimona's municipality. Dimona Railway Station is located in the southwestern part of the city. The main bus terminal is the Dimona Central Bus Station, with lines to Beersheba, Tel Aviv, Eilat, and nearby towns.


Dimona is twinned with:



</doc>
<doc id="9105" url="https://en.wikipedia.org/wiki?curid=9105" title="DC Comics">
DC Comics

DC Comics, Inc. is an American comic book publisher. It is the publishing unit of DC Entertainment, a subsidiary of Warner Bros. DC Comics is one of the largest and oldest American comic book companies, and produces material featuring numerous culturally iconic heroic characters including: Batman, Superman, and Wonder Woman.

Most of their material takes place in the fictional DC Universe, which also features teams such as the Justice League, the Justice Society of America, the Suicide Squad, and the Teen Titans, and well-known villains such as The Joker, Lex Luthor, Catwoman, and Darkseid. The company has also published non-DC Universe-related material, including "Watchmen", "V for Vendetta", and many titles under their alternative imprint Vertigo.

The initials "DC" came from the company's popular series "Detective Comics", which featured Batman's debut and subsequently became part of the company's name. Originally in Manhattan at 432 Fourth Avenue, the DC Comics offices have been located at 480 and later 575 Lexington Avenue; 909 Third Avenue; 75 Rockefeller Plaza; 666 Fifth Avenue; and 1325 Avenue of the Americas. DC had its headquarters at 1700 Broadway, Midtown Manhattan, New York City, but it was announced in October 2013 that DC Entertainment would relocate its headquarters from New York to Burbank, California in April 2015.

Random House distributes DC Comics' books to the bookstore market, while Diamond Comic Distributors supplies the comics shop specialty market. DC Comics and its longtime major competitor Marvel Comics (acquired in 2009 by The Walt Disney Company, WarnerMedia's main competitor) together shared approximately 70% of the American comic book market in 2017.

Entrepreneur Major Malcolm Wheeler-Nicholson founded National Allied Publications in autumn 1934. The company debuted with the tabloid-sized "" #1 with a cover date of February 1935. The company's second title, "New Comics" #1 (Dec. 1935), appeared in a size close to what would become comic books' standard during the period fans and historians call the Golden Age of Comic Books, with slightly larger dimensions than today's. That title evolved into "Adventure Comics", which continued through issue #503 in 1983, becoming one of the longest-running comic-book series. In 2009 DC revived "Adventure Comics" with its original numbering. In 1935, Jerry Siegel and Joe Shuster, the future creators of Superman, created Doctor Occult, who is the earliest DC Comics character to still be in the DC Universe.

Wheeler-Nicholson's third and final title, "Detective Comics", advertised with a cover illustration dated December 1936, eventually premiered three months late with a March 1937 cover date. The themed anthology series would become a sensation with the introduction of Batman in issue #27 (May 1939). By then, however, Wheeler-Nicholson had gone. In 1937, in debt to printing-plant owner and magazine distributor Harry Donenfeld—who also published pulp magazines and operated as a principal in the magazine distributorship Independent News—Wheeler-Nicholson had to take Donenfeld on as a partner in order to publish "Detective Comics" #1. Detective Comics, Inc. was formed, with Wheeler-Nicholson and Jack S. Liebowitz, Donenfeld's accountant, listed as owners. Major Wheeler-Nicholson remained for a year, but cash-flow problems continued, and he was forced out. Shortly afterwards, Detective Comics, Inc. purchased the remains of National Allied, also known as Nicholson Publishing, at a bankruptcy auction.

Detective Comics, Inc. soon launched a fourth title, "Action Comics", the premiere of which introduced Superman. "Action Comics" #1 (June 1938), the first comic book to feature the new character archetype—soon known as "superheroes"—proved a sales hit. The company quickly introduced such other popular characters as the Sandman and Batman.

On February 22, 2010, a copy of "Action Comics" #1 (June 1938) sold at an auction from an anonymous seller to an anonymous buyer for $1 million, besting the $317,000 record for a comic book set by a different copy, in lesser condition, the previous year.

National Allied Publications soon merged with Detective Comics, Inc. to form National Comics Publications on September 30, 1946, which absorbed an affiliated concern, Max Gaines' and Liebowitz' All-American Publications. That year, Gaines let Liebowitz buy him out, and kept only "Picture Stories from the Bible" as the foundation of his own new company, EC Comics. At that point, "Liebowitz promptly orchestrated the merger of All-American and Detective Comics into National Comics... Next he took charge of organizing National Comics, [the self-distributorship] Independent News, and their affiliated firms into a single corporate entity, National Periodical Publications". National Periodical Publications became publicly traded on the stock market in 1961.

Despite the official names "National Comics" and "National Periodical Publications", the company began branding itself as "Superman-DC" as early as 1940, and the company became known colloquially as DC Comics for years before the official adoption of that name in 1977.

The company began to move aggressively against what it saw as copyright-violating imitations from other companies, such as Fox Comics' Wonder Man, which (according to court testimony) Fox started as a copy of Superman. This extended to DC suing Fawcett Comics over Captain Marvel, at the time comics' top-selling character (see "National Comics Publications, Inc. v. Fawcett Publications, Inc."). Faced with declining sales and the prospect of bankruptcy if it lost, Fawcett capitulated in 1953 and ceased comics publication. Years later, Fawcett sold the rights for Captain Marvel to DC—which in 1972 revived Captain Marvel in the new title "Shazam!" featuring artwork by his creator, C. C. Beck. In the meantime, the abandoned trademark had been seized by Marvel Comics in 1967, with the creation of their Captain Marvel, forbidding the DC comic itself to be called that. While Captain Marvel did not recapture his old popularity, he later appeared in a Saturday morning live action TV adaptation and gained a prominent place in the mainstream continuity DC calls the DC Universe.

When the popularity of superheroes faded in the late 1940s, the company focused on such genres as science fiction, Westerns, humor, and romance. DC also published crime and horror titles, but relatively tame ones, and thus avoided the mid-1950s backlash against such comics. A handful of the most popular superhero-titles, including "Action Comics" and "Detective Comics", the medium's two longest-running titles, continued publication.

In the mid-1950s, editorial director Irwin Donenfeld and publisher Liebowitz directed editor Julius Schwartz (whose roots lay in the science-fiction book market) to produce a one-shot Flash story in the try-out title "Showcase". Instead of reviving the old character, Schwartz had writers Robert Kanigher and John Broome, penciler Carmine Infantino, and inker Joe Kubert create an entirely new super-speedster, updating and modernizing the Flash's civilian identity, costume, and origin with a science-fiction bent. The Flash's reimagining in "Showcase" #4 (October 1956) proved sufficiently popular that it soon led to a similar revamping of the Green Lantern character, the introduction of the modern all-star team Justice League of America (JLA), and many more superheroes, heralding what historians and fans call the Silver Age of comic books.

National did not reimagine its continuing characters (primarily Superman, Batman, and Wonder Woman), but radically overhauled them. The Superman family of titles, under editor Mort Weisinger, introduced such enduring characters as Supergirl, Bizarro, and Brainiac. The Batman titles, under editor Jack Schiff, introduced the successful Batwoman, Bat-Girl, Ace the Bat-Hound, and Bat-Mite in an attempt to modernize the strip with non-science-fiction elements. Schwartz, together with artist Infantino, then revitalized Batman in what the company promoted as the "New Look", re-emphasizing Batman as a detective. Meanwhile, editor Kanigher successfully introduced a whole family of Wonder Woman characters having fantastic adventures in a mythological context.

Since the 1940s, when Superman, Batman, and many of the company's other heroes began appearing in stories together, DC's characters inhabited a shared continuity that, decades later, was dubbed the "DC Universe" by fans. With the story "Flash of Two Worlds", in "Flash" #123 (September 1961), editor Schwartz (with writer Gardner Fox and artists Infantino and Joe Giella) introduced a concept that allowed slotting the 1930s and 1940s Golden Age heroes into this continuity via the explanation that they lived on an other-dimensional "Earth 2", as opposed to the modern heroes' "Earth 1"—in the process creating the foundation for what would later be called the DC Multiverse.

DC's introduction of the reimagined superheroes did not go unnoticed by other comics companies. In 1961, with DC's JLA as the specific spur, Marvel Comics writer-editor Stan Lee and a robust creator Jack Kirby ushered in the sub-Silver Age "Marvel Age" of comics with the debut issue of "The Fantastic Four". Reportedly, DC ignored the initial success of Marvel with this editorial change until its consistently strengthening sales made that impossible. However, the senior DC staff were reportedly at a loss at this time to understand how this small publishing house was achieving this increasingly threatening commercial strength. For instance, when Marvel's product was examined in a meeting, Marvel's emphasis on more sophisticated character-based narrative and artist-driven visual storytelling was apparently ignored for self-deluding guesses at the brand's popularity which included superficial reasons like the presence of the color red or word balloons on the cover, or that the perceived crudeness of the interior art was somehow more appealing to readers. When Lee learned about DC's subsequent experimental attempts to imitate these perceived details, he amused himself by arranging direct defiance of those assumptions in Marvel's publications as sales strengthened further to frustrate the competition.

However, this ignorance of Marvel's true appeal did not extend to some of the writing talent during this period, from which there were some attempts to emulate Marvel's narrative approach. For instance, there was the "Doom Patrol" series by Arnold Drake, a superhero team of outsiders who resented their freakish powers, which Drake later speculated was plagiarized by Stan Lee to create "The X-Men". There was also the young Jim Shooter who purposely emulated Marvel's writing when he wrote for DC after much study of both companies' styles, such as for the "Legion of Super-Heroes" feature.

A 1966 Batman TV show on the ABC network sparked a temporary spike in comic book sales, and a brief fad for superheroes in Saturday morning animation (Filmation created most of DC's initial cartoons) and other media. DC significantly lightened the tone of many DC comics—particularly "Batman" and "Detective Comics"—to better complement the "camp" tone of the TV series. This tone coincided with the famous "Go-Go Checks" checkerboard cover-dress which featured a black-and-white checkerboard strip (all DC books cover dated February 1966 until August 1967) at the top of each comic, a misguided attempt by then-managing editor Irwin Donenfeld to make DC's output "stand out on the newsracks".

In 1967, Batman artist Infantino (who had designed popular Silver Age characters Batgirl and the Phantom Stranger) rose from art director to become DC's editorial director. With the growing popularity of upstart rival Marvel Comics threatening to topple DC from its longtime number-one position in the comics industry, he attempted to infuse the company with more focus towards marketing new and existing titles and characters with more adult sensibilities towards an emerging older age group of superhero comic book fans that grew out of Marvel's efforts to market their superhero line to college-aged adults. He also recruited major talents such as ex-Marvel artist and Spider-Man co-creator Steve Ditko and promising newcomers Neal Adams and Denny O'Neil and replaced some existing DC editors with artist-editors, including Joe Kubert and Dick Giordano, to give DC's output a more artistic critical eye.

In 1967, National Periodical Publications was purchased by Kinney National Company, which later purchased Warner Bros.-Seven Arts and later became Warner Communications in 1972.

In 1970, Jack Kirby moved from Marvel Comics to DC, at the end of the Silver Age of Comics, in which Kirby's contributions to Marvel played a large, integral role. Given carte blanche to write and illustrate his own stories, he created a handful of thematically linked series he called collectively The Fourth World. In the existing series "Superman's Pal Jimmy Olsen" and in his own, newly launched series "New Gods", "Mister Miracle", and "The Forever People", Kirby introduced such enduring characters and concepts as archvillain Darkseid and the other-dimensional realm Apokolips. Furthermore, Kirby intended their stories to be later reprinted in collected editions in a publishing format that would later be called the trade paperback, which would become a standard industry practice decades later. While sales were respectable, they did not meet DC management's initially high expectations, and also suffered from a lack of comprehension and internal support from Infantino. By 1973 the "Fourth World" was all cancelled, although Kirby's conceptions would soon become integral to the broadening of the DC Universe. Obligated by his contract, Kirby created other unrelated series for DC, including "Kamandi", "The Demon", and "OMAC", before ultimately returning to Marvel Comics.

Following the science-fiction innovations of the Silver Age, the comics of the 1970s and 1980s would become known as the Bronze Age, as fantasy gave way to more naturalistic and sometimes darker themes. Illegal drug use, banned by the Comics Code Authority, explicitly appeared in comics for the first time in Marvel Comics' story "Green Goblin Reborn!" in "The Amazing Spider-Man" #96 (May 1971), and after the Code's updating in response, DC offered a drug-fueled storyline in writer Dennis O'Neil and artist Neal Adams' "Green Lantern", beginning with the story "Snowbirds Don't Fly" in the retitled "Green Lantern / Green Arrow" #85 (Sept. 1971), which depicted Speedy, the teen sidekick of superhero archer Green Arrow, as having become a heroin addict.

Jenette Kahn, a former children's magazine publisher, replaced Infantino as editorial director in January 1976. DC had attempted to compete with the now-surging Marvel by dramatically increasing its output and attempting to win the market by flooding it. This included launching series featuring such new characters as "Firestorm" and "Shade, the Changing Man", as well as an increasing array of non-superhero titles, in an attempt to recapture the pre-Wertham days of post-War comicdom. In June 1978, five months before the release of the first Superman movie, Kahn expanded the line further, increasing the number of titles and story pages, and raising the price from 35 cents to 50 cents. Most series received eight-page back-up features while some had full-length twenty-five-page stories. This was a move the company called the "DC Explosion". The move was not successful, however, and corporate parent Warner dramatically cut back on these largely unsuccessful titles, firing many staffers in what industry watchers dubbed "the DC Implosion". In September 1978, the line was dramatically reduced and standard-size books returned to 17 story pages but for a still increased 40 cents. By 1980, the books returned to 50 cents with a 25-page story count but the story pages replaced house ads in the books.

Seeking new ways to boost market share, the new team of publisher Kahn, vice president Paul Levitz, and managing editor Giordano addressed the issue of talent instability. To that end—and following the example of Atlas/Seaboard Comics and such independent companies as Eclipse Comics—DC began to offer royalties in place of the industry-standard work-for-hire agreement in which creators worked for a flat fee and signed away all rights, giving talent a financial incentive tied to the success of their work. In addition, emulating the era's new television form, the miniseries while addressing the matter of an excessive number of ongoing titles fizzling out within a few issues of their start, DC created the industry concept of the comic book limited series. This publishing format allowed for the deliberate creation of finite storylines within a more flexible publishing format that could showcase creations without forcing the talent into unsustainable open-ended commitments.

These changes in policy shaped the future of the medium as a whole, and in the short term allowed DC to entice creators away from rival Marvel, and encourage stability on individual titles. In November 1980 DC launched the ongoing series "The New Teen Titans", by writer Marv Wolfman and artist George Pérez, two popular talents with a history of success. Their superhero-team comic, superficially similar to Marvel's ensemble series "X-Men", but rooted in DC history, earned significant sales in part due to the stability of the creative team, who both continued with the title for six full years. In addition, Wolfman and Pérez took advantage of the limited-series option to create a spin-off title, "Tales of the New Teen Titans", to present origin stories of their original characters without having to break the narrative flow of the main series or oblige them to double their work load with another ongoing title.

This successful revitalization of the Silver Age Teen Titans led DC's editors to seek the same for the wider DC Universe. The result, the Wolfman/Pérez 12-issue limited series "Crisis on Infinite Earths", gave the company an opportunity to realign and jettison some of the characters' complicated backstory and continuity discrepancies. A companion publication, two volumes entitled "The History of the DC Universe", set out the revised history of the major DC characters. "Crisis" featured many key deaths that would shape the DC Universe for the following decades, and separate the timeline of DC publications into pre- and post-"Crisis".

Meanwhile, a parallel update had started in the non-superhero and horror titles. Since early 1984, the work of British writer Alan Moore had revitalized the horror series "The Saga of the Swamp Thing", and soon numerous British writers, including Neil Gaiman and Grant Morrison, began freelancing for the company. The resulting influx of sophisticated horror-fantasy material led to DC in 1993 establishing the Vertigo mature-readers imprint, which did not subscribe to the Comics Code Authority.

Two DC limited series, "" by Frank Miller and "Watchmen" by Moore and artist Dave Gibbons, drew attention in the mainstream press for their dark psychological complexity and promotion of the antihero. These titles helped pave the way for comics to be more widely accepted in literary-criticism circles and to make inroads into the book industry, with collected editions of these series as commercially successful trade paperbacks.

The mid-1980s also saw the end of many long-running DC war comics, including series that had been in print since the 1960s. These titles, all with over 100 issues, included "Sgt. Rock", "G.I. Combat", "The Unknown Soldier", and "Weird War Tales".

In March 1989, Warner Communications merged with Time Inc., making DC Comics a subsidiary of Time Warner. In June, the first Tim Burton directed Batman movie was released, and DC began publishing its hardcover series of DC Archive Editions, collections of many of their early, key comics series, featuring rare and expensive stories unseen by many modern fans. Restoration for many of the Archive Editions was handled by Rick Keene with colour restoration by DC's long-time resident colourist, Bob LeRose. These collections attempted to retroactively credit many of the writers and artists who had worked without much recognition for DC during the early period of comics when individual credits were few and far between.

The comics industry experienced a brief boom in the early 1990s, thanks to a combination of speculative purchasing (mass purchase of the books as collectible items, with intent to resell at a higher value as the rising value of older issues, was thought to imply that "all" comics would rise dramatically in price) and several storylines which gained attention from the mainstream media. DC's extended storylines in which Superman was killed, and superhero "Green Lantern" turned into the supervillain Parallax resulted in dramatically increased sales, but the increases were as temporary as the hero's replacements. Sales dropped off as the industry went into a major slump, while manufactured "collectables" numbering in the millions replaced quality with quantity until fans and speculators alike deserted the medium in droves.

DC's Piranha Press and other imprints (including the mature readers line Vertigo, and Helix, a short-lived science fiction imprint) were introduced to facilitate compartmentalized diversification and allow for specialized marketing of individual product lines. They increased the use of non-traditional contractual arrangements, including the dramatic rise of creator-owned projects, leading to a significant increase in critically lauded work (much of it for Vertigo) and the licensing of material from other companies. DC also increased publication of book-store friendly formats, including trade paperback collections of individual serial comics, as well as original graphic novels.

One of the other imprints was Impact Comics from 1991 to 1992 in which the Archie Comics superheroes were licensed and revamped. The stories in the line were part of its own shared universe.

DC entered into a publishing agreement with Milestone Media that gave DC a line of comics featuring a culturally and racially diverse range of superhero characters. Although the Milestone line ceased publication after a few years, it yielded the popular animated series "Static Shock". DC established Paradox Press to publish material such as the large-format "Big Book of..." series of multi-artist interpretations on individual themes, and such crime fiction as the graphic novel "Road to Perdition". In 1998, DC purchased WildStorm Comics, Jim Lee's imprint under the Image Comics banner, continuing it for many years as a wholly separate imprint – and fictional universe – with its own style and audience. As part of this purchase, DC also began to publish titles under the fledgling WildStorm sub-imprint America's Best Comics (ABC), a series of titles created by Alan Moore, including "The League of Extraordinary Gentlemen", "Tom Strong", and "Promethea". Moore strongly contested this situation, and DC eventually stopped publishing ABC.

In March 2003 DC acquired publishing and merchandising rights to the long-running fantasy series "Elfquest", previously self-published by creators Wendy and Richard Pini under their WaRP Graphics publication banner. This series then followed another non-DC title, Tower Comics' series T.H.U.N.D.E.R. Agents, in collection into DC Archive Editions. In 2004 DC temporarily acquired the North American publishing rights to graphic novels from European publishers 2000 AD and Humanoids. It also rebranded its younger-audience titles with the mascot Johnny DC and established the CMX imprint to reprint translated manga. In 2006, CMX took over from Dark Horse Comics publication of the webcomic "Megatokyo" in print form. DC also took advantage of the demise of Kitchen Sink Press and acquired the rights to much of the work of Will Eisner, such as his "The Spirit" series and his graphic novels.

In 2004, DC began laying the groundwork for a full continuity-reshuffling sequel to "Crisis on Infinite Earths", promising substantial changes to the DC Universe (and side-stepping the 1994 "Zero Hour" event which similarly tried to ret-con the history of the DCU). In 2005, the critically lauded "Batman Begins" film was released; also, the company published several limited series establishing increasingly escalated conflicts among DC's heroes, with events climaxing in the "Infinite Crisis" limited series. Immediately after this event, DC's ongoing series jumped forward a full year in their in-story continuity, as DC launched a weekly series, "52", to gradually fill in the missing time. Concurrently, DC lost the copyright to "Superboy" (while retaining the trademark) when the heirs of Jerry Siegel used a provision of the 1976 revision to the copyright law to regain ownership.

In 2005, DC launched its "All-Star" line (evoking the title of the 1940s publication), designed to feature some of the company's best-known characters in stories that eschewed the long and convoluted continuity of the DC Universe. The line began with "All-Star Batman & Robin the Boy Wonder" and "All-Star Superman", with "All-Star Wonder Woman" and "All-Star Batgirl" announced in 2006 but neither being released nor scheduled as of the end of 2009.

DC licensed characters from the Archie Comics imprint Red Circle Comics by 2007. They appeared in the Red Circle line, based in the DC Universe, with a series of one-shots followed by a miniseries that lead into two ongoing titles, each lasting 10 issues.

In 2011, DC rebooted all of its running titles following the Flashpoint storyline. The reboot called The New 52 gave new origin stories and costume designs to many of DC's characters.

DC licensed pulp characters including Doc Savage and the Spirit which it then used, along with some DC heroes, as part of the First Wave comics line launched in 2010 and lasting through fall 2011.

In May 2011, DC announced it would begin releasing digital versions of their comics on the same day as paper versions.

On June 1, 2011, DC announced that it would end all ongoing series set in the DC Universe in August and relaunch its comic line with 52 issue #1s, starting with "Justice League" on August 31 (written by Geoff Johns and drawn by Jim Lee), with the rest to follow later on in September.

On June 4, 2013, DC unveiled two new digital comic innovations to enhance interactivity: "DC" and "DC Multiverse". "DC" layers dynamic artwork onto digital comic panels, adding a new level of dimension to digital storytelling, while "DC Multiverse" allows readers to determine a specific story outcome by selecting individual characters, storylines and plot developments while reading the comic, meaning one digital comic has multiple outcomes. "DC" will first appear in the upcoming digital-first title, "Batman '66", based on the 1960s television series and "DC Multiverse" will first appear in "", a digital-first title based on the .

In 2014, DC announced an eight-issue miniseries titled ""Convergence"" which began in April 2015.

In 2016, DC announced a line-wide relaunch titled DC Rebirth. The new line would launch with an 80-page one-shot titled DC Universe: Rebirth, written by Geoff Johns, with art from Gary Frank, Ethan Van Sciver, and more. After that, many new series would launch with a twice-monthly release schedule and new creative teams for nearly every title. The relaunch was meant to bring back the legacy and heart many felt had been missing from DC characters since the launch of the New 52. Rebirth brought huge success, both financially and critically. 

DC Entertainment, Inc. is Warner Bros. subsidiary that manages it comic book units and its intellectual property (characters) in other units as they work with other Warner Bros units.

In September 2009, Warner Bros. announced that DC Comics would become a subsidiary of DC Entertainment, Inc., with Diane Nelson, President of Warner Premiere, becoming president of the newly formed holding company and DC Comics President and Publisher Paul Levitz moving to the position of Contributing Editor and Overall Consultant there.

On February 18, 2010, DC Entertainment named Jim Lee and Dan DiDio as Co-Publishers of DC Comics, Geoff Johns as Chief Creative Officer, John Rood as EVP (Executive Vice President) of Sales, Marketing and Business Development, and Patrick Caldon as EVP of Finance and Administration.

In October 2013, DC Entertainment announced that the DC Comics offices would be moved from New York City to Warner Bros. Burbank, California, headquarters in 2015. The other units, animation, movie, TV and portfolio planning, had preceded DC Comics by moving there in 2010.

DC Entertainment announced its first franchise, the DC Super Hero Girls universe, in April 2015 with multi-platform content, toys and apparel to start appearing in 2016.

Warner Bros. Pictures reorganized in May 2016 to have genre responsible film executives, thus DC Entertainment franchise films under Warner Bros. were placed under a newly created division, DC Films, created under Warner Bros. executive vice president Jon Berg and DC chief content officer Geoff Johns. This was done in the same vein as Marvel Studios in unifying DC-related filmmaking under a single vision and clarifying the greenlighting process. Johns also kept his existing role at DC Comics. Johns was promoted to DC president & CCO with the addition of his DC Films while still reporting to DCE President Nelson. In August 2016, Amit Desai was promoted from senior vice president, marketing & global franchise management to exec vice president, business and marketing strategy, direct-to-consumer and global franchise management.

DC Entertainment and Warner Bros. Digital Networks announced in April 2017 DC Universe digital service to be launched in 2018 with two original series.

With frustration over DC Films not matching Marvel Studios' results and Berg wanting to step back to being a producer in January 2018, it was announced that Warner Bros. executive Walter Hamada was appointed president of DC film prodcution. After a leave of absence starting in March 2018, Diane Nelson resigned as president of DC Entertainment. The company's executive management were to report to WB Chief Digital Officer Thomas Gewecke until a new president is selected. In June 2018, Johns was also moved out of his position as chief creative officer and DC Entertainment president for a writing and producing deal with the DC and WB companies. Jim Lee added DC Entertainment chief creative officer title to his DC co-publisher post.

DC's first logo appeared on the April 1940 issues of its titles. The letters "DC" stood for "Detective Comics", the name of Batman's flagship title. The small logo, with no background, read simply, "A DC Publication".

The November 1941 DC titles introduced an updated logo. This version was almost twice the size of the previous one and was the first version with a white background. The name "Superman" was added to "A DC Publication", effectively acknowledging both Superman and Batman. This logo was the first to occupy the top-left corner of the cover, where the logo has usually resided since. The company now referred to itself in its advertising as "Superman-DC".

In November 1949, the logo was modified to incorporate the company's formal name, National Comics Publications. This logo would also serve as the round body of Johnny DC, DC's mascot in the 1960s.

In October 1970, DC briefly retired the circular logo in favour of a simple "DC" in a rectangle with the name of the title, or the star of the book; the logo on many issues of "Action Comics", for example, read "DC Superman". An image of the lead character either appeared above or below the rectangle. For books that did not have a single star, such as anthologies like "House of Mystery" or team series such as "Justice League of America", the title and "DC" appeared in a stylized logo, such as a bat for "House of Mystery". This use of characters as logos helped to establish the likenesses as trademarks, and was similar to Marvel's contemporaneous use of characters as part of its cover branding.

DC's "100 Page Super-Spectacular" titles and later 100-page and "Giant" issues published from 1972 to 1974 featured a logo exclusive to these editions: the letters "DC" in a simple sans-serif typeface within a circle. A variant had the letters in a square.

The July 1972 DC titles featured a new circular logo. The letters "DC" were rendered in a block-like typeface that would remain through later logo revisions until 2005. The title of the book usually appeared inside the circle, either above or below the letters.

In December 1973, this logo was modified with the addition of the words "The Line of DC Super-Stars" and the star motif that would continue in later logos. This logo was placed in the top center of the cover from August 1975 to October 1976.

When Jenette Kahn became DC's publisher in late 1976, she commissioned graphic designer Milton Glaser to design a new logo. Popularly referred to as the "DC bullet", this logo premiered on the February 1977 titles. Although it varied in size and colour and was at times cropped by the edges of the cover, or briefly rotated 4 degrees, it remained essentially unchanged for nearly three decades. Despite logo changes since 2005, the old "DC bullet" continues to be used only on the DC Archive Editions series.

In July 1987, DC released variant editions of "Justice League" #3 and "The Fury of Firestorm" #61 with a new DC logo. It featured a picture of Superman in a circle surrounded by the words "SUPERMAN COMICS". The company released these variants to newsstands in certain markets as a marketing test.

On May 8, 2005, a new logo (dubbed the "DC spin") was unveiled, debuting on DC titles in June 2005 with "DC Special: The Return of Donna Troy" #1 and the rest of the titles the following week. In addition to comics, it was designed for DC properties in other media, which was used for movies since "Batman Begins", with "Superman Returns" showing the logo's normal variant, and the TV series "Smallville", the animated series "Justice League Unlimited" and others, as well as for collectibles and other merchandise. The logo was designed by Josh Beatman of Brainchild Studios and DC executive Richard Bruning.

In March 2012, DC unveiled a new logo consisting of the letter “D” flipping back to reveal the letter “C” and "DC ENTERTAINMENT". "The Dark Knight Rises" was the first film to use the new logo, while the TV series "Arrow" was the first series to feature the new logo.

DC Entertainment announced a new identity and logo for another iconic DC Comics universe brand on May 17, 2016. The new logo was first used on May 25, 2016, in conjunction with the release of "DC Universe: Rebirth Special #1" by Geoff Johns.



DC Universe is an upcoming video on demand service operated by DC Entertainment. It was announced in April 2017, with the title and service formally announced in May 2018. DC Universe is expected to offer more than video content through the inclusion of an immersive experience with fan interaction that encompasses comics in addition to television.

! Year
! Film
! Directed by
! Written by
! Based on
! Production by
! Budget
! Gross


</doc>
<doc id="9109" url="https://en.wikipedia.org/wiki?curid=9109" title="Diophantine equation">
Diophantine equation

In mathematics, a Diophantine equation is a polynomial equation, usually in two or more unknowns, such that only the integer solutions are sought or studied (an integer solution is a solution such that all the unknowns take integer values). A linear Diophantine equation equates the sum of two or more monomials, each of degree 1 in one of the variables, to a constant. An exponential Diophantine equation is one in which exponents on terms can be unknowns.

Diophantine problems have fewer equations than unknown variables and involve finding integers that work correctly for all equations. In more technical language, they define an algebraic curve, algebraic surface, or more general object, and ask about the lattice points on it.

The word "Diophantine" refers to the Hellenistic mathematician of the 3rd century, Diophantus of Alexandria, who made a study of such equations and was one of the first mathematicians to introduce symbolism into algebra. The mathematical study of Diophantine problems that Diophantus initiated is now called Diophantine analysis.

While individual equations present a kind of puzzle and have been considered throughout history, the formulation of general theories of Diophantine equations (beyond the theory of quadratic forms) was an achievement of the twentieth century.

In the following Diophantine equations, , , , and are the unknowns and the other letters are given constants:

The simplest linear Diophantine equation takes the form , where , and are given integers. The solutions are described by the following theorem:

Proof: If is this greatest common divisor, Bézout's identity asserts the existence of integers and such that . If is a multiple of , then for some integer , and is a solution. On the other hand, for every pair of integers and , the greatest common divisor of and divides . Thus, if the equation has a solution, then must be a multiple of . If and , then for every solution , we have
showing that is another solution. Finally, given two solutions such that , one deduces that . As and are coprime, Euclid's lemma shows that divides , and thus that
there exists an integer such that and . Therefore, and , which completes the proof.

The Chinese remainder theorem describes an important class of linear Diophantine systems of equations: let be pairwise coprime integers greater than one, be arbitrary integers, and be the product . The Chinese remainder theorem asserts that the following linear Diophantine system has exactly one solution such that , and that the other solutions are obtained by adding to a multiple of :

More generally, every system of linear Diophantine equations may be solved by computing the Smith normal form of its matrix, in a way that is similar to the use of the reduced row echelon form to solve a system of linear equations over a field. Using matrix notation every system of linear Diophantine equations may be written
where is an matrix of integers, is an column matrix of unknowns and is an column matrix of integers.

The computation of the Smith normal form of provides two unimodular matrices (that is matrices that are invertible over the integers and have ±1 as determinant) and of respective dimensions and , such that the matrix
is such that is not zero for not greater than some integer , and all the other entries are zero. The system to be solved may thus be rewritten as
Calling the entries of and those of , this leads to the system
This system is equivalent to the given one in the following sense: A column matrix of integers is a solution of the given system if and only if for some column matrix of integers such that .

It follows that the system has a solution if and only if divides for and for . If this condition is fulfilled, the solutions of the given system are
where are arbitrary integers.

Hermite normal form may also be used for solving systems of linear Diophantine equations. However, Hermite normal form does not directly provide the solutions; to get the solutions from the Hermite normal form, one has to successively solve several linear equations. Nevertheless, Richard Zippel wrote that the Smith normal form "is somewhat more than is actually needed to solve linear diophantine equations. Instead of reducing the equation to diagonal form, we only need to make it triangular, which is called the Hermite normal form. The Hermite normal form is substantially easier to compute than the Smith normal form."

Integer linear programming amounts to finding some integer solutions (optimal in some sense) of linear systems that include also inequations. Thus systems of linear Diophantine equations are basic in this context, and textbooks on integer programming usually have a treatment of systems of linear Diophantine equations.

A homogeneous Diophantine equation is a Diophantine equation that is defined by a homogeneous polynomial. A typical such equation is the equation of Fermat's Last Theorem

As a homogeneous polynomial in indeterminates defines a hypersurface in the projective space of dimension , solving a homogeneous Diophantine equation is the same as finding the rational points of a projective hypersurface.

Solving a homogeneous Diophantine equation is generally a very difficult problem, even in the simplest non-trivial case of three indeterminates (in the case of two indeterminates the problem is equivalent with testing if a rational number is the th power of another rational number). A witness of the difficulty of the problem is Fermat's Last Theorem (for , there is no integer solution of the above equation), which needed more than three centuries of mathematicians efforts for being solved.

For degrees higher than three, most known results are theorems asserting that there are no solutions (for example Fermat's Last Theorem) or that the number of solutions is finite (for example Falting's theorem). 

For the degree three, there are general solving methods, which work on almost all equations that are encountered in practice, but no algorithm is known that works for every cubic equation.

Homogeneous Diophantine equations of degree two are easier to solve. The standard solving method proceed in two steps. One has first to find one solution, or to prove that there is no solution. When a solution has been found, all solutions are then deduced.

For proving that there is no solution, one may reduce the equation modulo. For example, the Diophantine equation
does not have any other solution than the trivial solution . In fact, by dividing and by their greatest common divisor, one may suppose that they are coprime. The squares modulo 4 are congruent to 0 and 1. Thus the left-hand side of the equation is congruent to 0, 1, or 2, and the right-hand side is congruent to 0 or 3. Thus the equality may be obtained only if and are all even, and are thus not coprime. Thus the only solution is the trivial solution . This shows that there is no rational point on a circle of radius formula_5 centered at the origin.

More generally, Hasse principle allows deciding whether a homogeneous Diophantine equation of degree two has an integer solution, and computing a solution if there exist. 

If a non-trivial integer solution is known, one may produce all other solutions in the following way.

Let 
be a homogeneous Diophantine equation, where formula_7 is a quadratic form (that is, a homogeneous polynomial of degree 2), with integer coefficients. The "trivial solution" is the solution where all formula_8 are zero. If formula_9 is a non-trivial integer solution of this equation, then formula_10 are the homogeneous coordinates of a rational point of the hypersurface defined by . Conversely, if formula_11 are homogeneous coordinates of a rational point of this hypersurface, where formula_12 are integers, then formula_13 is an integer solution of the Diophantine equation. Moreover, the integer solutions that define a given rational point are all sequences of the form 
where is any integer, and is the greatest common divisor of the formula_15

It follows that solving the Diophantine equation formula_6 is completely reduced to finding the rational points of the corresponding projective hypersurface.

Let now formula_17 be an integer solution of the equation formula_18 As is a polynomial of degree two, a line passing through crosses the hypersurface at a single other point, which is rational if and only if the line is rational (that is, if the line is defined by rational parameters). This allows parameterizing the hypersurface by the lines passing through , and the rational points are the those that are obtained from rational lines, that is, those that correspond to rational values of the parameters.

More precisely, one may proceed as follows. 

By permuting the indices, one may suppose, without loss of generality that formula_19 Then one may pass to the affine case by considering the affine hypersurface defined by 
which has the rational point

If this rational point is a singular point, that is if all partial derivatives are zero at , all line passing through are contained in the hypersurface, and one has a cone. The change of variables 
does not change the rational points, and transforms into a homogeneous polynomial in variables. In this case, the problem may thus be solved by applying the method to an equation with fewer variables.

If the polynomial is a product of linear polynomials (possibly with non-rational coefficients), then it defines two hyperplanes. The intersection of these hyperplanes is a rational flat, and contains rational singular points. This case is thus a special instance of the preceding case.

In the general case, let consider the parametric equation of a line passing through :
Substituting this in , one gets a polynomial of degree two in formula_24 that is zero for formula_25 It is thus divisible by formula_26. The quotient is linear in formula_24 and may be solved for expressing formula_28 as a quotient of two polynomials of degree at most two in formula_29 with integer coefficients:
Substituting this in the expressions for formula_31 one gets, for ,
where formula_33 are polynomials of degree at most two with integer coefficients.

Then, one can return to the homogeneous case. Let, for , 
be the homogenization of formula_35 These quadratic polynomials with integer coefficients form a parameterization of the projective hypersurface defined by :

A point of the projective hypersurface defined by is rational if and only if it may be obtained from rational values of formula_37 As formula_38 are homogeneous polynomials, the point is not changed if all formula_39 are multiplied by the same rational number. Thus, one may suppose that formula_40 are coprime integers. It follows that the integer solutions of the Diophantine equation are exactly the sequences formula_41 where, for ,
where is an integer, formula_40 are coprime integers, and is the greatest common divisor of the integers formula_44

One could hope that the coprimality of the formula_39 could imply that . Unfortunately this is not the case, as shown in the next section.

The equation 
is probably the first homogeneous Diophantine equation of degree two that has been studied. Its solutions are the Pythagorean triples. This is also the homogeneous equation of the unit circle. In this section, we show how the above method allows retrieving Euclid's formula for generating Pythagorean triples.

For retrieving exactly Euclid's formula, we start from the solution , corresponding to the point of the unit circle. A line passing through this point may be parameterized by its slope:
Putting this in the circle equation 
one gets 
Dividing by , results in
which is easy to solve in :
It follows
Homogenizing as described above one gets all solutions as 
where is any integer, and are coprime integers, and is the greatest common divisor of the three numerators. In fact, if and are both odd, and if one is odd and the other is even.

The "primitive triples" are the solutions where and .

This description of the solutions differs slightly from Euclid's formula because Euclid's formula considers only the solutions such that and are all positive, and does not distinguish between two triples that differ by the exchange of and ,

The questions asked in Diophantine analysis include:


These traditional problems often lay unsolved for centuries, and mathematicians gradually came to understand their depth (in some cases), rather than treat them as puzzles.

The given information is that a father's age is 1 less than twice that of his son, and that the digits making up the father's age are reversed in the son's age (i.e. ). This leads to the equation , thus . Inspection gives the result , , and thus equals 73 years and equals 37 years. One may easily show that there is not any other solution with and positive integers less than 10.

In 1637, Pierre de Fermat scribbled on the margin of his copy of "Arithmetica": "It is impossible to separate a cube into two cubes, or a fourth power into two fourth powers, or in general, any power higher than the second into two like powers." Stated in more modern language, "The equation has no solutions for any higher than 2." And then he wrote, intriguingly: "I have discovered a truly marvelous proof of this proposition, which this margin is too narrow to contain." Such a proof eluded mathematicians for centuries, however, and as such his statement became famous as Fermat's Last Theorem. It wasn't until 1995 that it was proven by the British mathematician Andrew Wiles.

In 1657, Fermat attempted to solve the Diophantine equation (solved by Brahmagupta over 1000 years earlier). The equation was eventually solved by Euler in the early 18th century, who also solved a number of other Diophantine equations. The smallest solution of this equation in positive integers is , (see Chakravala method).

In 1900, David Hilbert proposed the solvability of all Diophantine equations as the tenth of his fundamental problems. In 1970, Yuri Matiyasevich solved it negatively, by proving that a general algorithm for solving all Diophantine equations cannot exist.

Diophantine geometry, which is the application of techniques from algebraic geometry in this field, has continued to grow as a result; since treating arbitrary equations is a dead end, attention turns to equations that also have a geometric meaning. The central idea of Diophantine geometry is that of a rational point, namely a solution to a polynomial equation or a system of polynomial equations, which is a vector in a prescribed field , when is "not" algebraically closed.

One of the few general approaches is through the Hasse principle. Infinite descent is the traditional method, and has been pushed a long way.

The depth of the study of general Diophantine equations is shown by the characterisation of Diophantine sets as equivalently described as recursively enumerable. In other words, the general problem of Diophantine analysis is blessed or cursed with universality, and in any case is not something that will be solved except by re-expressing it in other terms.

The field of Diophantine approximation deals with the cases of "Diophantine inequalities". Here variables are still supposed to be integral, but some coefficients may be irrational numbers, and the equality sign is replaced by upper and lower bounds.

The most celebrated single question in the field, the conjecture known as Fermat's Last Theorem, was solved by Andrew Wiles but using tools from algebraic geometry developed during the last century rather than within number theory where the conjecture was originally formulated. Other major results, such as Faltings's theorem, have disposed of old conjectures.

An example of an infinite diophantine equation is:
which can be expressed as "How many ways can a given integer be written as the sum of a square plus twice a square plus thrice a square and so on?" The number of ways this can be done for each forms an integer sequence. Infinite Diophantine equations are related to theta functions and infinite dimensional lattices. This equation always has a solution for any positive . Compare this to:
which does not always have a solution for positive .

If a Diophantine equation has as an additional variable or variables occurring as exponents, it is an exponential Diophantine equation. Examples include the Ramanujan–Nagell equation, , and the equation of the Fermat-Catalan conjecture and Beal's conjecture, with inequality restrictions on the exponents. A general theory for such equations is not available; particular cases such as Catalan's conjecture have been tackled. However, the majority are solved via ad hoc methods such as Størmer's theorem or even trial and error.





</doc>
<doc id="9110" url="https://en.wikipedia.org/wiki?curid=9110" title="Diophantus">
Diophantus

Little is known about the life of Diophantus. He lived in Alexandria, Egypt, during the Roman era, probably from between AD 200 and 214 to 284 or 298. Diophantus has variously been described by historians as either Greek, non-Greek, Hellenized Egyptian, Hellenized Babylonian, Jewish, or Chaldean. Much of our knowledge of the life of Diophantus is derived from a 5th-century Greek anthology of number games and puzzles created by Metrodorus. One of the problems (sometimes called his epitaph) states:

This puzzle implies that Diophantus' age can be expressed as

which gives a value of 84 years. However, the accuracy of the information cannot be independently confirmed.

In popular culture, this puzzle was the Puzzle No.142 in "Professor Layton and Pandora's Box" as one of the hardest solving puzzles in the game, which needed to be unlocked by solving other puzzles first.

"Arithmetica" is the major work of Diophantus and the most prominent work on algebra in Greek mathematics. It is a collection of problems giving numerical solutions of both determinate and indeterminate equations. Of the original thirteen books of which "Arithmetica" consisted only six have survived, though there are some who believe that four Arab books discovered in 1968 are also by Diophantus. Some Diophantine problems from "Arithmetica" have been found in Arabic sources.

It should be mentioned here that Diophantus never used general methods in his solutions. Hermann Hankel, renowned German mathematician made the following remark regarding Diophantus.

“Our author (Diophantos) not the slightest trace of a general, comprehensive method is discernible; each problem calls for some special method which refuses to work even for the most closely related problems. For this reason it is difficult for the modern scholar to solve the 101st problem even after having studied 100 of Diophantos’s solutions” 

Like many other Greek mathematical treatises, Diophantus was forgotten in Western Europe during the so-called Dark Ages, since the study of ancient Greek, and literacy in general, had greatly declined. The portion of the Greek "Arithmetica" that survived, however, was, like all ancient Greek texts transmitted to the early modern world, copied by, and thus known to, medieval Byzantine scholars. Scholia on Diophantus by the Byzantine Greek scholar John Chortasmenos (1370–1437) are preserved together with a comprehensive commentary written by the earlier Greek scholar Maximos Planudes (1260 – 1305), who produced an edition of Diophantus within the library of the Chora Monastery in Byzantine Constantinople. In addition, some portion of the "Arithmetica" probably survived in the Arab tradition (see above). In 1463 German mathematician Regiomontanus wrote:

"Arithmetica" was first translated from Greek into Latin by Bombelli in 1570, but the translation was never published. However, Bombelli borrowed many of the problems for his own book "Algebra". The "editio princeps" of "Arithmetica" was published in 1575 by Xylander. The best known Latin translation of "Arithmetica" was made by Bachet in 1621 and became the first Latin edition that was widely available. Pierre de Fermat owned a copy, studied it, and made notes in the margins.

The 1621 edition of "Arithmetica" by Bachet gained fame after Pierre de Fermat wrote his famous "Last Theorem" in the margins of his copy:

Fermat's proof was never found, and the problem of finding a proof for the theorem went unsolved for centuries. A proof was finally found in 1994 by Andrew Wiles after working on it for seven years. It is believed that Fermat did not actually have the proof he claimed to have. Although the original copy in which Fermat wrote this is lost today, Fermat's son edited the next edition of Diophantus, published in 1670. Even though the text is otherwise inferior to the 1621 edition, Fermat's annotations—including the "Last Theorem"—were printed in this version.

Fermat was not the first mathematician so moved to write in his own marginal notes to Diophantus; the Byzantine scholar John Chortasmenos (1370–1437) had written "Thy soul, Diophantus, be with Satan because of the difficulty of your other theorems and particularly of the present theorem" next to the same problem.

Diophantus wrote several other books besides "Arithmetica", but very few of them have survived.

Diophantus himself refers to a work which consists of a collection of lemmas called "The Porisms" (or "Porismata"), but this book is entirely lost.

Although "The Porisms" is lost, we know three lemmas contained there, since Diophantus refers to them in the "Arithmetica". One lemma states that the difference of the cubes of two rational numbers is equal to the sum of the cubes of two other rational numbers, i.e. given any and , with , there exist , all positive and rational, such that

Diophantus is also known to have written on polygonal numbers, a topic of great interest to Pythagoras and Pythagoreans. Fragments of a book dealing with polygonal numbers are extant.

A book called "Preliminaries to the Geometric Elements" has been traditionally attributed to Hero of Alexandria. It has been studied recently by Wilbur Knorr, who suggested that the attribution to Hero is incorrect, and that the true author is Diophantus.

Diophantus' work has had a large influence in history. Editions of Arithmetica exerted a profound influence on the development of algebra in Europe in the late sixteenth and through the 17th and 18th centuries. Diophantus and his works have also influenced Arab mathematics and were of great fame among Arab mathematicians. Diophantus' work created a foundation for work on algebra and in fact much of advanced mathematics is based on algebra. As far as we know Diophantus did not affect the lands of the Orient much and how much he affected India is a matter of debate.

Diophantus is often called “the father of algebra" because he contributed greatly to number theory, mathematical notation, and because Arithmetica contains the earliest known use of syncopated notation.

Today, Diophantine analysis is the area of study where integer (whole-number) solutions are sought for equations, and Diophantine equations are polynomial equations with integer coefficients to which only integer solutions are sought. It is usually rather difficult to tell whether a given Diophantine equation is solvable. Most of the problems in Arithmetica lead to quadratic equations. Diophantus looked at 3 different types of quadratic equations: , , and . The reason why there were three cases to Diophantus, while today we have only one case, is that he did not have any notion for zero and he avoided negative coefficients by considering the given numbers , , to all be positive in each of the three cases above. Diophantus was always satisfied with a rational solution and did not require a whole number which means he accepted fractions as solutions to his problems. Diophantus considered negative or irrational square root solutions "useless", "meaningless", and even "absurd". To give one specific example, he calls the equation 'absurd' because it would lead to a negative value for . One solution was all he looked for in a quadratic equation. There is no evidence that suggests Diophantus even realized that there could be two solutions to a quadratic equation. He also considered simultaneous quadratic equations.

Diophantus made important advances in mathematical notation, becoming the first person known to use algebraic notation and symbolism. Before him everyone wrote out equations completely. Diophantus introduced an algebraic symbolism that used an abridged notation for frequently occurring operations, and an abbreviation for the unknown and for the powers of the unknown. Mathematical historian Kurt Vogel states:

“The symbolism that Diophantus introduced for the first time, and undoubtedly devised himself, provided a short and readily comprehensible means of expressing an equation... Since an abbreviation is also employed for the word ‘equals’, Diophantus took a fundamental step from verbal algebra towards symbolic algebra.”

Although Diophantus made important advances in symbolism, he still lacked the necessary notation to express more general methods. This caused his work to be more concerned with particular problems rather than general situations. Some of the limitations of Diophantus' notation are that he only had notation for one unknown and, when problems involved more than a single unknown, Diophantus was reduced to expressing "first unknown", "second unknown", etc. in words. He also lacked a symbol for a general number . Where we would write , Diophantus has to resort to constructions like: "... a sixfold number increased by twelve, which is divided by the difference by which the square of the number exceeds three".

Algebra still had a long way to go before very general problems could be written down and solved succinctly.






</doc>
<doc id="9111" url="https://en.wikipedia.org/wiki?curid=9111" title="Dong">
Dong

Dong or DONG may refer to:






</doc>
<doc id="9118" url="https://en.wikipedia.org/wiki?curid=9118" title="Duke Kahanamoku">
Duke Kahanamoku

Duke Paoa Kahinu Mokoe Hulikohola Kahanamoku (August 24, 1890 – January 22, 1968) was a Native Hawaiian competition swimmer who popularized the ancient Hawaiian sport of surfing. He was born towards the end of the Kingdom of Hawaii, just before the overthrow, living into statehood as a United States citizen. He was a five-time Olympic medalist in swimming. Duke was also a law enforcement officer, an actor, a beach volleyball player and businessman.

According to Kahanamoku, he was born in Honolulu at Haleʻākala, the home of Bernice Pauahi Bishop which was later converted into the Arlington Hotel. He had five brothers and three sisters, including Samuel Kahanamoku and Sargent Kahanamoku. In 1893, the family moved to Kālia, Waikiki (near the present site of the Hilton Hawaiian Village), to be closer to his mother's parents and family. Duke grew up with his siblings and 31 Paoa cousins. Duke attended the Waikiki Grammar School, Kaahumanu School, and the Kamehameha Schools, although he never graduated because he had to quit to help support the family.

"Duke" was not a title or a nickname, but a given name. He was named after his father, Duke Halapu Kahanamoku, who was christened by Bernice Pauahi Bishop in honor of Prince Alfred, Duke of Edinburgh, who was visiting Hawaii at the time. His father was a policeman. His mother Julia Paakonia Lonokahikina Paoa was a deeply religious woman with a strong sense of family ancestry.

Even though not of the formal Hawaiian Royal Family, his parents were from prominent Hawaiian ohana (family); the Kahanamoku and the Paoa ohana were considered to be lower-ranking nobles, who were in service to the "aliʻi nui" or royalty. His paternal grandfather was Kahanamoku and his grandmother, Kapiolani Kaoeha (sometimes spelled "Kahoea"), a descendant of Alapainui. They were "kahu", retainers and trusted advisors of the Kamehamehas, to whom they were related. His maternal grandparents Paoa, son of Paoa Hoolae and Hiikaalani, and Mele Uliama were also of aliʻi descent.

Growing up on the outskirts of Waikiki, Kahanamoku spent his youth as a bronzed beach boy. At Waikiki Beach he developed his surfing and swimming skills. In his youth, Kahanamoku preferred a traditional surf board, which he called his "papa nui", constructed after the fashion of ancient Hawaiian "olo" boards. Made from the wood of a koa tree, it was long and weighed . The board was without a skeg, which had yet to be invented. In his later career, he would often use smaller boards but always preferred those made of wood.

On August 11, 1911, Kahanamoku was timed at 55.4 seconds in the freestyle, beating the existing world record by 4.6 seconds, in the salt water of Honolulu Harbor. He also broke the record in the and equaled it in the . But the Amateur Athletic Union (AAU), in disbelief, would not recognize these feats until many years later. The AAU initially claimed that the judges must have been using alarm clocks rather than stopwatches and later claimed that ocean currents aided Kahanamoku.

Kahanamoku easily qualified for the U.S. Olympic swimming team in 1912. At the 1912 Summer Olympics in Stockholm, he won a gold medal in the 100-meter freestyle, and a silver medal with the second-place U.S. team in the men's 4×200-meter freestyle relay. During the 1920 Olympics in Antwerp, he won gold medals both in the 100 meters (bettering fellow Hawaiian Pua Kealoha) and in the relay. He finished the 100 meters with a silver medal during the 1924 Olympics in Paris, with the gold going to Johnny Weissmuller and the bronze to Duke's brother, Samuel Kahanamoku. At age 34, this was Kahanamoku's last Olympic medal. He also was an alternate for the U.S. water polo team at the 1932 Summer Olympics.

Between Olympic competitions, and after retiring from the Olympics, Kahanamoku traveled internationally to give swimming exhibitions. It was during this period that he popularized the sport of surfing, previously known only in Hawaii, by incorporating surfing exhibitions into these visits as well. His surfing exhibition at Sydney's Freshwater Beach on December 24, 1914 is widely regarded as a seminal event in the development of surfing in Australia. The board that Kahanamoku built from a piece of pine from a local hardware store is retained by the Freshwater Surf Club. There is a statue of Kahanamoku on the Northern headland of Freshwater Beach, New South Wales. He made surfing popular in mainland America first in 1912 while in Southern California.

During his time living in Southern California, Kahanamoku performed in Hollywood as a background actor and a character actor in several films. In this way, he made connections with people who could further publicize the sport of surfing. Kahanamoku was involved with the Los Angeles Athletic Club, acting as lifeguard and competing on both swimming and water polo teams.

While living in Newport Beach, California on June 14, 1925, Kahanamoku rescued eight men from a fishing vessel that capsized in heavy surf while attempting to enter the city's harbor. 29 fishermen went into the water and 17 perished. Using his surfboard, he was able to make quick trips back and forth to shore to increase the number of sailors rescued. Two other surfers saved four more fishermen. Newport's police chief at the time called Duke's efforts "the most superhuman surfboard rescue act the world has ever seen". It also caused U.S. lifeguards to begin using surfboards in their water rescues.

In 1940, he married Nadine Alexander, who accompanied him when he traveled. Kahanamoku was the first person to be inducted into both the Swimming Hall of Fame and the Surfing Hall of Fame. The Duke Kahanamoku Invitational Surfing Championships are named in his honor. He is a member of the U.S. Olympic Hall of Fame. He served as sheriff of Honolulu, Hawaii from 1932 to 1961, serving 13 consecutive terms. During this period, he also appeared in a number of television programs and films, such as "Mister Roberts" (1955).

Kahanamoku was a friend and surfing companion of heiress Doris Duke, who built a home (now a museum) on Oahu named Shangri-la.

In 1946, Kahanamoku was the "pro forma" defendant in the landmark Supreme Court case "Duncan v. Kahanamoku". While Kahanamoku was a military police officer during World War II, he arrested Duncan, a civilian shipfitter, for public intoxication. At the time, Hawaii, not yet a state, was being administered under the Hawaiian Organic Act which effectively instituted martial law on the island. Duncan was therefore tried by a military tribunal and appealed to the Supreme Court. In a "post hoc" ruling, the court ruled that trial by military tribunal was, in this case, unconstitutional.

Kahanamoku died of a heart attack on January 22, 1968, at the age of 77. For his burial at sea a long motorcade of mourners, accompanied by a 30-man police escort, moved across town to Waikiki Beach. Reverend Abraham Akaka, the pastor of Kawaiahao Church, performed the service, a group of beach boys sang Hawaiian songs, including "Aloha Oe", and his ashes were scattered into the ocean. The City of Honolulu commemorated this Waikiki Beach burial site in 1990 with a 9-foot cast bronze statue of Kahanamoku by Jan Gordon Fisher that shows Kahanamoku with outstretched arms in front of his surfboard and honorary Hawaiian spears, and includes a dedication plaque and a historic information marker. The statue's orientation that placed Kahanamoku's back to the sea was initially criticized as being contrary to Hawaiian custom, yet its orientation toward the pedestrian activity and public realm on Kalakaua Avenue created an immensely popular local shrine and tourist locale; the outstretched arms are always adorned with fresh leis. The Kahanamoku statue receives annual tribute ceremonies and thousands of yearly tourist visits. In 1997, the historic public artwork "The Stones of Life", containing four ancient basaltic stones or pōhaku (circa 1500), was installed nearby. A 24-hour live web camera is also installed at the statue so that visitors from around the world can be viewed.

Hawaii music promoter Kimo Wilder McVay capitalized on Kahanamoku's popularity by naming his Waikiki showroom "Duke Kahanamoku's," and giving Kahanamoku a financial interest in the showroom in exchange for the use of his name. It was a major Waikiki showroom in the 1960s and is remembered as the home of Don Ho & The Aliis from 1964 through 1969. The showroom continued to be known as Duke Kahanamoku's until Hawaii showman Jack Cione bought it in the mid-1970s and renamed it Le Boom Boom.

Kahanamoku's name is also used by Duke's Canoe Club & Barefoot Bar, known as Duke's Waikiki, a beachfront bar and restaurant in the Outrigger Waikiki on the Beach Hotel. There is a chain of restaurants named after him in California, Florida and Hawaii called Duke's.

On August 24, 2002, the 112th anniversary of Kahanamoku's birth, the U.S. Postal Service issued a first-class commemorative stamp with Duke's picture on it. The First Day Ceremony was held at the Hilton Hawaiian Village in Waikiki and was attended by thousands. At this ceremony, attendees could attach the Duke stamp to an envelope and get it canceled with a First Day of Issue postmark. These first day covers are very collectible.

On February 28, 2015, a monument featuring a replica of Kahanamoku's surfboard was unveiled at New Brighton beach, Christchurch, New Zealand in honour of the 100th anniversary of Kahanamoku's visit to New Brighton.

On August 24, 2015, a Google Doodle honored the 125th anniversary of Duke Kahanamoku's birthday.





</doc>
<doc id="9119" url="https://en.wikipedia.org/wiki?curid=9119" title="Distinguished Service Medal (U.S. Army)">
Distinguished Service Medal (U.S. Army)

The Distinguished Service Medal (DSM) is a military award of the United States Army that is presented to any person who, while serving in any capacity with the United States military, has distinguished himself or herself by exceptionally meritorious service to the Government in a duty of great responsibility. The performance must be such as to merit recognition for service that is clearly exceptional. Exceptional performance of normal duty will not alone justify an award of this decoration.

Separate Distinguished Service Medals exist for the different branches of the military as well as a fifth version of the medal which is a senior award of the United States Department of Defense. The Army version of the Distinguished Service Medal is typically referred to simply as the "Distinguished Service Medal" while the other branches of service use the service name as a prefix.

For service not related to actual war, the term "duty of a great responsibility" applies to a narrower range of positions than in time of war, and requires evidence of conspicuously significant achievement. However, justification of the award may accrue by virtue of exceptionally meritorious service in a succession of high positions of great importance.

Awards may be made to persons other than members of the Armed Forces of the United States for wartime services only, and then only under exceptional circumstances, with the express approval of the President in each case.




The Distinguished Service Medal is awarded to any person who, while serving in any capacity with the United States Army, has distinguished himself or herself by exceptionally meritorious service to the Government in a duty of great responsibility.
The performance must be such as to merit recognition for service which is clearly exceptional. Exceptional performance of normal duty will not alone justify an award of this decoration. For service not related to actual war, the term "duty of a great responsibility" applies to a narrower range of positions than in time of war and requires evidence of a conspicuously significant achievement. However, justification of the award may accrue by virtue of exceptionally meritorious service in a succession of high positions of great importance. Awards may be made to persons other than members of the Armed Forces of the United States for wartime services only, and only then under exceptional circumstances with the express approval of the President in each case.


The Distinguished Service Medal was authorized by Presidential Order dated 01-02-1918, and confirmed by Congress on 07-09-1918. It was announced by War Department General Order No. 6, 1918-01-12, with the following information concerning the medal: "A bronze medal of appropriate design and a ribbon to be worn in lieu thereof, to be awarded by the President to any person who, while serving in any capacity with the Army shall hereafter distinguish himself or herself, or who, since 04-06-1917, has distinguished himself or herself by exceptionally meritorious service to the Government in a duty of great responsibility in time of war or in connection with military operations against an armed enemy of the United States." The Act of Congress on 07-09-1918, recognized the need for different types and degrees of heroism and meritorious service and included such provisions for award criteria. The current statutory authorization for the Distinguished Service Medal is Title 10, United States Code, Section 3743.


More than 2,000 awards were made during World War I, and by the time the United States entered World War II, approximately 2,800 awards had been made. From July 1, 1941 to June 6, 1969, when the Army stopped publishing awards of the DSM in Department of the Army General Orders, over 2,800 further awards were made.

Prior to World War II the DSM was the only decoration for non-combat service in the U.S. Army. As a result, before World War II the DSM was awarded to a wider range of recipients than during and after World War II. During World War I awards of the DSM to officers below the rank of brigadier general were fairly common but became rare once the Legion of Merit was established in 1942. 

Until the first award of the Air Force Distinguished Service Medal in 1965, United States Air Force personnel received this award as well, as was the case with several other Army decorations until the Air Force fully established its own system of decorations.

Because the Army Distinguished Service Medal is principally awarded to general officers, a list of notable recipients would include nearly every general, and some admirals, since 1918, many of whom received multiple awards, as well as a few civilians and sergeants major prominent for their contributions to national defense.

General Martin Dempsey, former Chairman of the Joint Chiefs of Staff, has the record for the greatest number of awards received of the Army Distinguished Service Medal at six. He also received three awards of the Defense Distinguished Service Medal as well as one award each of the Navy Distinguished Service Medal, the Air Force Distinguished Service Medal, and the Coast Guard Distinguished Service Medal, for a total of twelve Distinguished Service Medals. 

Generals of the Army Douglas MacArthur and Dwight Eisenhower are tied with five awards each received of the Army Distinguished Service Medal. They also each received one award of the Navy Distinguished Service Medal, for a total of six DSMs each.

General Norman Schwarzkopf received two awards of the Army DSM and one award each of the Defense DSM, Navy DSM, the Air Force DSM and the Coast Guard DSM, for a total of six DSMs.

General Lloyd Austin received four awards of the Army DSM and five awards of the Defense DSM for a total of nine DSMs.

Among notable recipients below flag rank are: X-1 test pilot Chuck Yeager and X-15 test pilot Robert M. White, who both received the DSM as U.S. Air Force majors; Air Force Major Rudolf Anderson, the U-2 pilot shot down during the Cuban Missile Crisis; director Frank Capra, decorated in 1945 as an Army colonel; actor James Stewart, decorated in 1945 as an Army Air Forces colonel (later Air Force Brigadier General); Colonel Wendell Fertig, who led Filipino guerrillas behind Japanese lines; Colonel (later Major General) John K. Singlaub, who led partisan forces in the Korean War; and Major Maude C. Davison, who led the "Angels of Bataan and Corregidor" during their imprisonment by the Japanese, and Colonel William S. Taylor, Program Manager Multiple Launch Rocket System. Among notable civilian recipients are Harry L. Hopkins, Robert S. McNamara and Henry L. Stimson.

Notable American and foreign recipients include:



Note - includes Army Air Service, Army Air Corps and Army Air Forces







</doc>
